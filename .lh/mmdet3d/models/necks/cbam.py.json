{
    "sourceFile": "mmdet3d/models/necks/cbam.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1716015887370,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1716015887370,
            "name": "Commit-0",
            "content": "import torch\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .. import builder\n\n\nclass BasicConv(nn.Module):\n    def __init__(\n        self,\n        in_planes,\n        out_planes,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups=1,\n        relu=True,\n        bn=True,\n        bias=False,\n    ):\n        super(BasicConv, self).__init__()\n        self.out_channels = out_planes\n        self.conv = nn.Conv2d(\n            in_planes,\n            out_planes,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias,\n        )\n        self.bn = (\n            nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True)\n            # nn.LazyBatchNorm2d(eps=1e-5, momentum=0.01, affine=True)\n            if bn\n            else None\n        )\n        self.relu = nn.ReLU() if relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n\nclass ChannelGate(nn.Module):\n    def __init__(self, gate_channels, reduction_ratio=16, pool_types=[\"avg\", \"max\"]):\n        super(ChannelGate, self).__init__()\n        self.gate_channels = gate_channels\n        self.mlp = nn.Sequential(\n            Flatten(),\n            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n            nn.ReLU(),\n            nn.Linear(gate_channels // reduction_ratio, gate_channels),\n        )\n        self.pool_types = pool_types\n\n    def forward(self, x):\n        channel_att_sum = None\n        for pool_type in self.pool_types:\n            if pool_type == \"avg\":\n                avg_pool = F.avg_pool2d(\n                    x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3))\n                )\n                channel_att_raw = self.mlp(avg_pool)\n            elif pool_type == \"max\":\n                max_pool = F.max_pool2d(\n                    x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3))\n                )\n                channel_att_raw = self.mlp(max_pool)\n            elif pool_type == \"lp\":\n                lp_pool = F.lp_pool2d(\n                    x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3))\n                )\n                channel_att_raw = self.mlp(lp_pool)\n            elif pool_type == \"lse\":\n                # LSE pool only\n                lse_pool = logsumexp_2d(x)\n                channel_att_raw = self.mlp(lse_pool)\n\n            if channel_att_sum is None:\n                channel_att_sum = channel_att_raw\n            else:\n                channel_att_sum = channel_att_sum + channel_att_raw\n\n        scale = F.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n        return x * scale\n\n\ndef logsumexp_2d(tensor):\n    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n    return outputs\n\n\nclass ChannelPool(nn.Module):\n    def forward(self, x):\n        return torch.cat(\n            (torch.max(x, 1)[0].unsqueeze(1), torch.mean(x, 1).unsqueeze(1)), dim=1\n        )\n\n\nclass SpatialGate(nn.Module):\n    def __init__(self):\n        super(SpatialGate, self).__init__()\n        kernel_size = 7\n        self.compress = ChannelPool()\n        self.spatial = BasicConv(\n            2, 1, kernel_size, stride=1, padding=(kernel_size - 1) // 2, relu=False\n        )\n\n    def forward(self, x):\n        x_compress = self.compress(x)\n        x_out = self.spatial(x_compress)\n        scale = F.sigmoid(x_out)  # broadcasting\n        return x * scale\n\n\n@builder.NECKS.register_module()\nclass CBAM(nn.Module):\n    def __init__(\n        self,\n        gate_channels,\n        reduction_ratio=16,\n        pool_types=[\"avg\", \"max\"],\n        no_spatial=False,\n        norm=False,\n    ):\n        super(CBAM, self).__init__()\n        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n        self.no_spatial = no_spatial\n        if not no_spatial:\n            self.SpatialGate = SpatialGate()\n        if norm:\n            self.norm = nn.BatchNorm2d(gate_channels, eps=1e-3, momentum=1e-2)\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        residual = x\n        x_out = self.ChannelGate(x)\n        if not self.no_spatial:\n            x_out = self.SpatialGate(x_out)\n\n        x_out += residual\n        x_out = self.relu(x_out)\n        return x_out\n"
        }
    ]
}