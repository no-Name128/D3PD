{
    "sourceFile": "mmdet3d/models/necks/fast_voxel_gen.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 7,
            "patches": [
                {
                    "date": 1720088187858,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1720088229216,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,8 +29,10 @@\n         \"Warning: kornia is not installed correctly, please ignore this warning if you do not use CaDDN. Otherwise, it is recommended to use torch version greater than 1.2 to use kornia properly.\"\n     )\n \n \n+class FastVoxelGen\n+\n def gen_dx_bx(xbound, ybound, zbound):\n     dx = torch.Tensor([row[2] for row in [xbound, ybound, zbound]])\n     bx = torch.Tensor([row[0] + row[2] / 2.0 for row in [xbound, ybound, zbound]])\n     nx = torch.Tensor([(row[1] - row[0]) / row[2] for row in [xbound, ybound, zbound]])\n"
                },
                {
                    "date": 1720088258074,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,10 +29,11 @@\n         \"Warning: kornia is not installed correctly, please ignore this warning if you do not use CaDDN. Otherwise, it is recommended to use torch version greater than 1.2 to use kornia properly.\"\n     )\n \n \n-class FastVoxelGen\n+# class FastVoxelGen(nn.Module):\n \n+\n def gen_dx_bx(xbound, ybound, zbound):\n     dx = torch.Tensor([row[2] for row in [xbound, ybound, zbound]])\n     bx = torch.Tensor([row[0] + row[2] / 2.0 for row in [xbound, ybound, zbound]])\n     nx = torch.Tensor([(row[1] - row[0]) / row[2] for row in [xbound, ybound, zbound]])\n"
                },
                {
                    "date": 1720088288981,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,9 +29,13 @@\n         \"Warning: kornia is not installed correctly, please ignore this warning if you do not use CaDDN. Otherwise, it is recommended to use torch version greater than 1.2 to use kornia properly.\"\n     )\n \n \n-# class FastVoxelGen(nn.Module):\n+class FastVoxelGen(nn.Module):\n+    def __init__(self):\n+        pass\n+    def forward(self,img_feats):\n+        pass\n \n \n def gen_dx_bx(xbound, ybound, zbound):\n     dx = torch.Tensor([row[2] for row in [xbound, ybound, zbound]])\n"
                },
                {
                    "date": 1720232974343,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -32,9 +32,10 @@\n \n class FastVoxelGen(nn.Module):\n     def __init__(self):\n         pass\n-    def forward(self,img_feats):\n+\n+    def forward(self, img_feats):\n         pass\n \n \n def gen_dx_bx(xbound, ybound, zbound):\n@@ -1374,8 +1375,9 @@\n                     se_like.in_channel, se_like.in_channel, kernel_size=1, stride=1\n                 ),\n                 nn.Sigmoid(),\n             )\n+            \n         if self.selfcalib_conv_config:\n             self.selfcalib_encoder = BACKBONES.build(self.selfcalib_conv_config)\n         self.xy_transpose = xy_transpose\n \n"
                },
                {
                    "date": 1720267579010,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1375,9 +1375,9 @@\n                     se_like.in_channel, se_like.in_channel, kernel_size=1, stride=1\n                 ),\n                 nn.Sigmoid(),\n             )\n-            \n+\n         if self.selfcalib_conv_config:\n             self.selfcalib_encoder = BACKBONES.build(self.selfcalib_conv_config)\n         self.xy_transpose = xy_transpose\n \n"
                },
                {
                    "date": 1720662066087,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -32,8 +32,11 @@\n \n class FastVoxelGen(nn.Module):\n     def __init__(self):\n         pass\n+    \n+    def get_cams_sampling_point(self):\n+        pass\n \n     def forward(self, img_feats):\n         pass\n \n"
                },
                {
                    "date": 1720662269853,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,19 +29,10 @@\n         \"Warning: kornia is not installed correctly, please ignore this warning if you do not use CaDDN. Otherwise, it is recommended to use torch version greater than 1.2 to use kornia properly.\"\n     )\n \n \n-class FastVoxelGen(nn.Module):\n-    def __init__(self):\n-        pass\n-    \n-    def get_cams_sampling_point(self):\n-        pass\n \n-    def forward(self, img_feats):\n-        pass\n \n-\n def gen_dx_bx(xbound, ybound, zbound):\n     dx = torch.Tensor([row[2] for row in [xbound, ybound, zbound]])\n     bx = torch.Tensor([row[0] + row[2] / 2.0 for row in [xbound, ybound, zbound]])\n     nx = torch.Tensor([(row[1] - row[0]) / row[2] for row in [xbound, ybound, zbound]])\n"
                }
            ],
            "date": 1720088187858,
            "name": "Commit-0",
            "content": "import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom mmcv.runner import BaseModule, force_fp32\nfrom mmcv.cnn import build_conv_layer, ConvModule\nfrom ..builder import NECKS, BACKBONES\nfrom .. import builder\nimport numpy as np\nfrom termcolor import colored\nfrom einops import rearrange\nimport math\n\nfrom mmseg.ops import resize\n\nimport os\nfrom torch import nn\nimport torch.utils.checkpoint as cp\n\nfrom mmdet.models import NECKS\nfrom mmcv.runner import auto_fp16\nfrom mmdet3d.models.utils.self_print import print2file\n\ntry:\n    from kornia.utils.grid import create_meshgrid3d\n    from kornia.geometry.linalg import transform_points\nexcept Exception as e:\n    # Note: Kornia team will fix this import issue to try to allow the usage of lower torch versions.\n    print(\n        \"Warning: kornia is not installed correctly, please ignore this warning if you do not use CaDDN. Otherwise, it is recommended to use torch version greater than 1.2 to use kornia properly.\"\n    )\n\n\ndef gen_dx_bx(xbound, ybound, zbound):\n    dx = torch.Tensor([row[2] for row in [xbound, ybound, zbound]])\n    bx = torch.Tensor([row[0] + row[2] / 2.0 for row in [xbound, ybound, zbound]])\n    nx = torch.Tensor([(row[1] - row[0]) / row[2] for row in [xbound, ybound, zbound]])\n    return dx, bx, nx\n\n\n@torch.no_grad()\ndef get_lidar_to_cams_project(rots, trans, cam_inner):\n    \"\"\"\n    NOTE:\n        cams_load_order:  ['CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT',\n         'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT'],\n    \"\"\"\n    _device = rots.device\n    _dtype = rots.dtype\n    rots = rots.cpu().numpy()\n    trans = trans.cpu().numpy()\n    cam_inner = cam_inner.cpu().numpy()\n\n    lidar2img_rts = []\n    for INDICES in range(len(rots)):\n        # inverse sensor2lidar_rotation\n        lidar2cam_r = np.linalg.inv(rots[INDICES])\n        lidar2cam_t = trans[INDICES] @ lidar2cam_r.T\n        lidar2cam_rt = np.eye(4)\n        lidar2cam_rt[:3, :3] = lidar2cam_r.T\n        lidar2cam_rt[3, :3] = -lidar2cam_t\n        intrinsic = cam_inner[INDICES]\n        viewpad = np.eye(4)\n        viewpad[: intrinsic.shape[0], : intrinsic.shape[1]] = intrinsic\n        lidar2img_rt = viewpad @ lidar2cam_rt.T\n        lidar2img_rt = torch.tensor(lidar2img_rt, dtype=_dtype)\n        lidar2img_rts.append(lidar2img_rt)\n\n    assert len(lidar2img_rts) == 6\n\n    lidar2img_rts = torch.stack(lidar2img_rts).to(_device)\n\n    return lidar2img_rts\n\n\nclass ResModule2D(nn.Module):\n    def __init__(self, n_channels, norm_cfg=dict(type=\"BN2d\"), groups=1):\n        super().__init__()\n        self.conv0 = ConvModule(\n            in_channels=n_channels,\n            out_channels=n_channels,\n            kernel_size=3,\n            padding=1,\n            groups=groups,\n            conv_cfg=dict(type=\"Conv2d\"),\n            norm_cfg=norm_cfg,\n            # act_cfg=dict(type=\"ReLU\", inplace=True),\n            act_cfg=dict(type=\"LeakyReLU\", inplace=True),\n        )\n        self.conv1 = ConvModule(\n            in_channels=n_channels,\n            out_channels=n_channels,\n            kernel_size=3,\n            padding=1,\n            groups=groups,\n            conv_cfg=dict(type=\"Conv2d\"),\n            norm_cfg=norm_cfg,\n            act_cfg=None,\n        )\n        # self.activation = nn.ReLU(inplace=True)\n        self.activation = nn.LeakyReLU(inplace=True)\n\n    @auto_fp16()\n    def forward(self, x):\n        \"\"\"Forward function.\n\n        Args:\n            x (torch.Tensor): of shape (N, C, N_x, N_y, N_z).\n\n        Returns:\n            torch.Tensor: 5d feature map.\n        \"\"\"\n        identity = x\n        x = self.conv0(x)\n        x = self.conv1(x)\n        x = identity + x\n        x = self.activation(x)\n        return x\n\n\nclass ResModule2D_LReLU(ResModule2D):\n    def __init__(self, n_channels, norm_cfg=dict(type=\"BN2d\"), groups=1):\n        super().__init__()\n        self.conv0 = ConvModule(\n            in_channels=n_channels,\n            out_channels=n_channels,\n            kernel_size=3,\n            padding=1,\n            groups=groups,\n            conv_cfg=dict(type=\"Conv2d\"),\n            norm_cfg=norm_cfg,\n            act_cfg=dict(type=\"LeakyReLU\", inplace=True),\n        )\n        self.conv1 = ConvModule(\n            in_channels=n_channels,\n            out_channels=n_channels,\n            kernel_size=3,\n            padding=1,\n            groups=groups,\n            conv_cfg=dict(type=\"Conv2d\"),\n            norm_cfg=norm_cfg,\n            act_cfg=None,\n        )\n        self.activation = nn.LeakyReLU(inplace=True)\n\n\n@NECKS.register_module()\nclass M2BevNeck(nn.Module):\n    \"\"\"Neck for M2BEV.\"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        num_layers=2,\n        norm_cfg=dict(type=\"BN2d\"),\n        stride=2,\n        fuse=None,\n        with_cp=False,\n    ):\n        super().__init__()\n\n        self.with_cp = with_cp\n\n        if fuse is not None:\n            self.fuse = nn.Conv2d(\n                fuse[\"in_channels\"], fuse[\"out_channels\"], kernel_size=1\n            )\n        else:\n            self.fuse = None\n\n        model = nn.ModuleList()\n        model.append(ResModule2D(in_channels, norm_cfg))\n        model.append(\n            ConvModule(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                kernel_size=3,\n                stride=stride,\n                padding=1,\n                conv_cfg=dict(type=\"Conv2d\"),\n                norm_cfg=norm_cfg,\n                act_cfg=dict(type=\"ReLU\", inplace=True),\n            )\n        )\n        for i in range(num_layers):\n            model.append(ResModule2D(out_channels, norm_cfg))\n            model.append(\n                ConvModule(\n                    in_channels=out_channels,\n                    out_channels=out_channels,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=dict(type=\"Conv2d\"),\n                    norm_cfg=norm_cfg,\n                    act_cfg=dict(type=\"ReLU\", inplace=True),\n                )\n            )\n        self.model = nn.Sequential(*model)\n\n    @auto_fp16()\n    def forward(self, x):\n        \"\"\"Forward function.\n\n        Args:\n            x (torch.Tensor): of shape (N, C_in, N_x, N_y, N_z).\n\n        Returns:\n            list[torch.Tensor]: of shape (N, C_out, N_y, N_x).\n        \"\"\"\n\n        def _inner_forward(x):\n            out = self.model.forward(x)\n            return out\n\n        if self.fuse is not None:\n            x = self.fuse(x)\n\n        if self.with_cp and x.requires_grad:\n            x = cp.checkpoint(_inner_forward, x)\n        else:\n            x = _inner_forward(x)\n\n        return x\n\n\n@NECKS.register_module()\nclass M2BevNeck_LReLU(M2BevNeck):\n    \"\"\"Neck for M2BEV.\"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        num_layers=2,\n        norm_cfg=dict(type=\"BN2d\"),\n        stride=2,\n    ):\n        super().__init__()\n\n        model = nn.ModuleList()\n        model.append(ResModule2D_LReLU(in_channels, norm_cfg))\n        model.append(\n            ConvModule(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                kernel_size=3,\n                stride=stride,\n                padding=1,\n                conv_cfg=dict(type=\"Conv2d\"),\n                norm_cfg=norm_cfg,\n                # act_cfg=dict(type=\"ReLU\", inplace=True),\n                act_cfg=dict(type=\"LeakyReLU\", inplace=True),\n            )\n        )\n        for i in range(num_layers):\n            model.append(ResModule2D_LReLU(out_channels, norm_cfg))\n            model.append(\n                ConvModule(\n                    in_channels=out_channels,\n                    out_channels=out_channels,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=dict(type=\"Conv2d\"),\n                    norm_cfg=norm_cfg,\n                    # act_cfg=dict(type=\"ReLU\", inplace=True),\n                    act_cfg=dict(type=\"LeakyReLU\", inplace=True),\n                )\n            )\n        self.model = nn.Sequential(*model)\n\n\nclass SELikeModule(nn.Module):\n    def __init__(self, in_channel=512, feat_channel=256, intrinsic_channel=33):\n        super(SELikeModule, self).__init__()\n        self.input_conv = nn.Conv2d(in_channel, feat_channel, kernel_size=1, padding=0)\n        self.fc = nn.Sequential(\n            nn.BatchNorm1d(intrinsic_channel),\n            nn.Linear(intrinsic_channel, feat_channel),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x, cam_params):\n        x = self.input_conv(x)\n        b, c, _, _ = x.shape\n        y = self.fc(cam_params).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n\n\n@NECKS.register_module()\nclass InterDistill_FastBEV(BaseModule):\n    def __init__(\n        self,\n        grid_config=None,\n        data_config=None,\n        numC_input=512,\n        pct_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],\n        voxel_size=[0.1, 0.1, 0.2],\n        numC_Trans=64,\n        unproject=True,\n        unproject_postion=\"after\",\n        z_axis_times=4,\n        downsample=16,\n        accelerate=False,\n        max_drop_point_rate=0.0,\n        use_bev_pool=True,\n        is_ms_bev=False,\n        loss_depth_weight=None,\n        extra_depth_net=None,\n        se_config=dict(),\n        dcn_config=dict(bias=True),\n        selfcalib_conv_config=None,\n        bev_channel_reduce=None,\n        **kwargs,\n    ):\n        super(InterDistill_FastBEV, self).__init__()\n        if grid_config is None:\n            grid_config = {\n                \"xbound\": [-51.2, 51.2, 0.8],\n                \"ybound\": [-51.2, 51.2, 0.8],\n                \"zbound\": [-10.0, 10.0, 20.0],\n                \"dbound\": [1.0, 60.0, 1.0],\n            }\n        self.grid_config = grid_config\n        dx, bx, nx = gen_dx_bx(\n            self.grid_config[\"xbound\"],\n            self.grid_config[\"ybound\"],\n            self.grid_config[\"zbound\"],\n        )\n        self.dx = nn.Parameter(dx, requires_grad=False)\n        self.bx = nn.Parameter(bx, requires_grad=False)\n        self.nx = nn.Parameter(nx, requires_grad=False)\n\n        if data_config is None:\n            data_config = {\"input_size\": (256, 704)}\n        self.data_config = data_config\n        self.downsample = downsample\n\n        self.pct_range = pct_range\n        self.voxel_size = voxel_size\n        self.D = 60\n        self.unproject = unproject\n        self.unproject_postion = unproject_postion\n        self.z_axis_times = z_axis_times\n        self.is_ms_bev = is_ms_bev\n        self.bev_channel_reduce = bev_channel_reduce\n\n        # NOTE the following is the implemented of the original BEVPooling with LSS-BEVDet\n        self.numC_input = numC_input\n        self.numC_Trans = numC_Trans\n        self.depthnet = nn.Conv2d(\n            self.numC_input, self.D + self.numC_Trans, kernel_size=1, padding=0\n        )\n        self.geom_feats = None\n        self.accelerate = accelerate\n        self.max_drop_point_rate = max_drop_point_rate\n        self.use_bev_pool = use_bev_pool\n        self.is_ms_bev = is_ms_bev\n\n        self.n_voxels = kwargs[\"n_voxels\"] if \"n_voxels\" in kwargs.keys() else None\n\n        self.featnet = nn.Conv2d(\n            self.numC_input, self.numC_Trans, kernel_size=1, padding=0\n        )\n\n        # config the cams depth estimationm, not suitable for fastbev\n        self.loss_depth_weight = loss_depth_weight\n        if loss_depth_weight > 0:\n            self.loss_depth_weight = loss_depth_weight\n            self.extra_depthnet = builder.build_backbone(extra_depth_net)\n            self.dcn = nn.Sequential(\n                *[\n                    build_conv_layer(\n                        dict(type=\"DCNv2\", deform_groups=1),\n                        extra_depth_net[\"num_channels\"][0],\n                        extra_depth_net[\"num_channels\"][0],\n                        kernel_size=3,\n                        stride=1,\n                        padding=1,\n                        dilation=1,\n                        **dcn_config,\n                    ),\n                    nn.BatchNorm2d(extra_depth_net[\"num_channels\"][0]),\n                ]\n            )\n\n            self.depthnet = nn.Conv2d(\n                extra_depth_net[\"num_channels\"][0], self.D, kernel_size=1, padding=0\n            )\n            self.se = SELikeModule(\n                self.numC_input,\n                feat_channel=extra_depth_net[\"num_channels\"][0],\n                **se_config,\n            )\n\n        self.selfcalib_conv_config = selfcalib_conv_config\n        if selfcalib_conv_config is None:\n            # 是否需要 normal？？？？？？？？？\n            self.bev_reduce = nn.Sequential(\n                ConvModule(\n                    self.z_axis_times * numC_Trans,\n                    64,\n                    kernel_size=1,\n                    stride=1,\n                    norm_cfg=dict(type=\"BN\", eps=1e-3, momentum=0.01),\n                    act_cfg=dict(type=\"ReLU\"),\n                ),\n                ConvModule(\n                    64,\n                    64,\n                    kernel_size=1,\n                    stride=1,\n                    norm_cfg=dict(type=\"BN\", eps=1e-3, momentum=0.01),\n                    act_cfg=dict(type=\"ReLU\"),\n                ),\n            )\n            self.ch_reduce = ConvModule(\n                self.z_axis_times * numC_Trans,\n                64,\n                kernel_size=1,\n                stride=1,\n                norm_cfg=dict(type=\"BN\", eps=1e-3, momentum=0.01),\n                act_cfg=dict(type=\"ReLU\"),\n            )\n\n        else:\n            self.bev_reduce = BACKBONES.build(selfcalib_conv_config)\n            self.ch_reduce = ConvModule(\n                self.z_axis_times * numC_Trans,\n                64,\n                kernel_size=1,\n                stride=1,\n                norm_cfg=dict(type=\"BN\", eps=1e-3, momentum=0.01),\n                act_cfg=dict(type=\"ReLU\"),\n            )\n\n        self.conv3d_modify = nn.Sequential(\n            nn.Conv3d(64, 64, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm3d(64),\n            nn.ReLU(inplace=True),\n        )\n\n        self.other_args = kwargs\n\n    def wrap_lidar2img_project(\n        self,\n        rots,\n        trans,\n        cam_inner,\n        img_metas=None,\n        device=None,\n        dtype=None,\n        lidar2img=False,\n    ):\n        \"\"\"\n        Args:\n            rots: b\n        Return:\n\n        \"\"\"\n        lidar2img_project = None\n\n        if img_metas is None:\n            lidar2img_project = get_lidar_to_cams_project(rots, trans, cam_inner)\n        elif lidar2img:\n            lidar2img_project = [\n                torch.tensor(lidar2imgs[\"lidar2img\"]) for lidar2imgs in img_metas\n            ]\n            lidar2img_project = torch.stack(lidar2img_project).to(device).to(dtype)\n        else:\n            pass\n\n        return lidar2img_project\n\n    @torch.no_grad()\n    def scenes_voxel_point(self, z_times=4):\n        if self.n_voxels is None:\n            n_voxel_x = self.nx[0]\n            n_voxel_y = self.nx[1]\n            n_voxel_z = self.nx[2] * z_times\n\n        else:\n            n_voxel_x = self.n_voxels[0]\n            n_voxel_y = self.n_voxels[1]\n            n_voxel_z = self.n_voxels[2]\n\n        voxel_points = torch.stack(\n            torch.meshgrid(\n                [\n                    torch.arange(n_voxel_x),\n                    torch.arange(n_voxel_y),\n                    torch.arange(n_voxel_z),\n                ]\n            )\n        )\n\n        # print(voxel_points, voxel_points.shape)\n\n        return voxel_points  # (3, x, y, z)\n\n    def voxel_grid_to_lidar_points(\n        self,\n        bs,\n        pc_range,\n        voxel_size,\n        voxel_points,\n        trans_version=\"v3\",\n        data_type=None,\n        device=None,\n    ):\n        \"\"\"\n        Calculate grid to LiDAR unprojection for each plane\n        Args:\n            pc_min: [x_min, y_min, z_min], Minimum of point cloud range (m)\n            voxel_size: [x, y, z], Size of each voxel (m)\n            trans_version [ default : v1]: Controls the method used to complete predefined voxel point transformations, Configured as v2, which means using the official code of fastbev. It is more recommended to use v1 or v3.\n        Params:\n            unproject: (4, 4), Voxel grid to LiDAR unprojection matrix\n        Return:\n            lidar_points (torch.tensor): [B, N, 3]\n        \"\"\"\n        trans_version = self.other_args[\"trans_version\"]\n        B = bs\n        if trans_version == \"v1\":\n            assert len(voxel_points.shape) == 3  # B N 3\n            x_size, y_size, z_size = voxel_size\n            x_min, y_min, z_min = pc_range[:3]\n            unproject = torch.tensor(\n                [\n                    [x_size, 0, 0, x_min],\n                    [0, y_size, 0, y_min],\n                    [0, 0, z_size, z_min],\n                    [0, 0, 0, 1],\n                ],\n                dtype=data_type,\n                device=device,\n            )  # (4, 4)\n\n            lidar_points = transform_points(\n                trans_01=unproject.unsqueeze(0), points_1=voxel_points\n            )\n            # lidar_points = lidar_points.squeeze()\n            return lidar_points  # B  N  3\n\n        elif trans_version == \"v2\":\n            voxel_size = voxel_points.new_tensor(voxel_size)\n            pc_range = np.array(pc_range, dtype=np.float32)\n            origin = torch.tensor(\n                (pc_range[:3] + pc_range[3:]) / 2.0,\n                device=voxel_points.device,\n                dtype=data_type,\n            )\n\n            n_voxels = self.nx.cpu().numpy()\n            n_voxels = torch.tensor(\n                np.array([n_voxels[0], n_voxels[1], n_voxels[2] * self.z_axis_times]),\n                device=voxel_points.device,\n                dtype=data_type,\n            )\n\n            new_origin = origin - n_voxels / 2.0 * voxel_size\n            lidar_points = voxel_points * voxel_size.view(1, 1, 3) + new_origin.view(\n                1, 1, 3\n            )\n\n            return lidar_points\n\n        elif trans_version == \"v3\":\n            # self.z_axis_times = 1 if self.n_voxels is not None else self.z_axis_times\n            # # Redefine the generated voxel points\n            # voxel_shape = (\n            #     self.nx.cpu().numpy().astype(np.int)\n            #     if self.n_voxels is None\n            #     else self.n_voxels\n            # )\n            if self.n_voxels is None:\n                voxel_shape = self.nx.cpu().numpy().astype(np.int)\n            else:\n                self.z_axis_times = 1\n                voxel_shape = self.n_voxels\n\n            # _width = torch.linspace(\n            #     0.5, voxel_shape[0] - 0.5, voxel_shape[0], device=device\n            # )\n            # _hight = torch.linspace(\n            #     0.5, voxel_shape[1] - 0.5, voxel_shape[1], device=device\n            # )\n\n            # _depth_voxel_shape = voxel_shape[2] * self.z_axis_times\n            # _depth = torch.linspace(\n            #     0.5, _depth_voxel_shape - 0.5, _depth_voxel_shape, device=device\n            # )\n\n            _width = torch.linspace(0, 1, voxel_shape[0], device=device)\n            _hight = torch.linspace(0, 1, voxel_shape[1], device=device)\n\n            _depth_voxel_shape = voxel_shape[2] * self.z_axis_times\n            _depth = torch.linspace(0, 1, _depth_voxel_shape, device=device)\n\n            reference_voxel = torch.stack(\n                torch.meshgrid([_width, _hight, _depth]), dim=-1\n            )  # x y z 3\n\n            reference_voxel = reference_voxel.unsqueeze(0).repeat(B, 1, 1, 1, 1)\n\n            pc_range = np.array(pc_range)\n\n            reference_voxel[..., 0:1] = (\n                reference_voxel[..., 0:1] * (pc_range[3] - pc_range[0]) + pc_range[0]\n            )  # X\n            reference_voxel[..., 1:2] = (\n                reference_voxel[..., 1:2] * (pc_range[4] - pc_range[1]) + pc_range[1]\n            )  # Y\n            reference_voxel[..., 2:3] = (\n                reference_voxel[..., 2:3] * (pc_range[5] - pc_range[2]) + pc_range[2]\n            )  # Z\n\n            # import pdb\n            # pdb.set_trace()\n\n            # B n_points 3\n            # to lidar point from the predefined voxel point.\n            return reference_voxel  # b x y z 3\n\n        else:\n            raise NotImplementedError\n\n    @force_fp32(apply_to=())\n    def lidar_points_to_cams_points(\n        self,\n        lidar_points,\n        rots,\n        trans,\n        cam_inner,\n        feats_shape,\n        post_rots,\n        post_trans,\n        unproject=True,\n        img_metas=None,\n        unproject_order=1,\n        normal_lidar2cams_point=False,\n    ):\n        \"\"\"lidar_points_to_cams_points\n\n        - execute fastbev transformation\n        - N for points, n for num of multi-view\n        - lidar_points  : b N 3\n        - rots          : b n 3 3\n        - trans         : b n 1 3\n        - post_rots     : b n 3 3\n        - post_trans    : b n 1 3\n        - cams_inner    : b n 3 3\n\n        Args:\n            lidar_points (torch.tensor): [b, N, 3] or [b x y z 3]\n            unproject_order: 0 for first unproject, 1 for after unprojcet\n            normal_lidar2cams_point: normalization the porjected lidar point( points in cams space )\n        Return:\n            cams_points (torch.tensor): [B, n, N, 2]\n            mask (torch.tensor): [B, n, N, 1]\n        \"\"\"\n\n        B = lidar_points.size(0)\n        NV = rots.size(1)\n\n        # post trans and rots to lidar to cams space, before trans the lidar_point to cams 2d points\n        if self.unproject_postion == \"before\" and self.unproject:\n            if len(lidar_points.shape) == 5:\n                lidar_points = rearrange(lidar_points, \"b x y z 3 -> b (x y z) 3\")\n            lidar_points = lidar_points[:, None, ...].repeat(1, NV, 1, 1)\n\n            # b n N 3 -> b n N 3 1\n            # b n 3 3 -> b n 1 3 3\n            # b n 1 3 3 @ b n N 3 1 -> b n N 3 1 -> b n N 3\n            lidar_points = torch.matmul(\n                post_rots.unsqueeze(2), lidar_points.unsqueeze(-1)\n            ).squeeze(-1)\n            # b n N 3 + b n 1 3\n            lidar_points = lidar_points + post_trans.unsqueeze(2)\n\n        lidar_points = torch.cat(\n            [lidar_points, torch.ones_like(lidar_points[..., :1])], -1\n        )  # b N 3 -> b N 4  or b x y z 3 -> b x y z 3\n\n        if len(lidar_points.shape) == 5:\n            lidar_points = lidar_points.flatten(1, 3)\n\n        n_points = lidar_points.size(1)\n        lidar_points = (\n            lidar_points.view(B, 1, n_points, 4).repeat(1, NV, 1, 1)\n            if len(lidar_points.shape) == 3\n            else lidar_points\n        )  # b n_cams n_points 4\n\n        lidar2img_project = self.wrap_lidar2img_project(\n            rots,\n            trans,\n            cam_inner,\n            img_metas=img_metas,\n            device=lidar_points.device,\n            dtype=lidar_points.dtype,\n            lidar2img=False,\n        )  # b n_cams 4 4\n\n        if lidar2img_project is None:\n            cams_points, ref_depth = self.l2c_w_rots_trans(\n                lidar_points[..., :3],\n                rots=rots,\n                trans=trans,\n                intrins=cam_inner,\n                post_rots=post_rots,\n                post_trans=post_trans,\n            )\n        else:\n            lidar_points = lidar_points.unsqueeze(-1)  # b n_cams n_points 4 1\n            lidar2img_project = lidar2img_project.view(B, NV, 1, 4, 4)  # b n 1 4 4\n\n            # if fp16_enabled:\n            #     lidar2img_project = lidar2img_project.half()\n            #     lidar_points = lidar_points.half()  # b n_cams n_points 4\n\n            cams_points = torch.matmul(lidar2img_project, lidar_points).squeeze(\n                -1\n            )  # b n_cams n_points 4\n\n        eps = 1e-5\n        # cams_points_ref_depth = cams_points[..., 2:3].clone()\n        # mask = cams_points_ref_depth > eps  # b n_cams n_points 1\n        mask = cams_points[..., 2:3] > eps\n\n        cams_points = cams_points[..., 0:2] / torch.maximum(\n            cams_points[..., 2:3], torch.ones_like(cams_points[..., 2:3]) * eps\n        )\n\n        # image enhancement inverse mapping\n        if (\n            self.unproject_postion == \"after\" and self.unproject\n        ):  # Inverse mapping transformation of image enhancement\n            cams_points[..., :2] = (\n                cams_points[..., :2] + post_trans.unsqueeze(-2)[..., :2]\n            )\n            cams_points[..., :2] = torch.matmul(\n                post_rots[..., :2, :2].unsqueeze(-3), cams_points[..., :2].unsqueeze(-1)\n            ).squeeze(-1)\n            # b n_cams n_points 2 @ b n_cams 3[:2] 3[:2]\n            # cams_points[..., :2] = torch.matmul(\n            #     cams_points.unsqueeze(-2), post_rots[..., :2, :2].unsqueeze(-3)\n            # ).squeeze(-2)\n\n        cams_points[..., 0] /= self.data_config[\"input_size\"][1]\n        cams_points[..., 1] /= self.data_config[\"input_size\"][0]\n\n        mask = (\n            mask\n            & (cams_points[..., 0:1] > 0.0)\n            & (cams_points[..., 1:2] > 0.0)\n            & (cams_points[..., 0:1] < 1.0)\n            & (cams_points[..., 1:2] < 1.0)\n        )\n\n        H, W = feats_shape\n        # cams_points[..., 0] *= W / self.data_config[\"input_size\"][1]\n        # cams_points[..., 1] *= H / self.data_config[\"input_size\"][0]\n\n        # mask = (\n        #     mask\n        #     & (cams_points[..., 0:1] > 0)\n        #     & (cams_points[..., 1:2] > 0)\n        #     & (cams_points[..., 0:1] < W)\n        #     & (cams_points[..., 1:2] < H)\n        # )\n\n        masks = mask  # b n_cams n_points 1\n\n        # cams_points = []\n        # masks = []\n\n        # for bs_indices in range(B):\n        #     H, W = feats_shape  # feat size shape\n\n        #     cams_point_bs = []\n        #     mask_bs = []\n        #     # for view_indices in range(NV):\n        #     #     pass\n\n        #     lidar2img_project = _inner_wrap_lidar2img_project(\n        #         rots[bs_indices], trans[bs_indices], cam_inner[bs_indices], img_metas=img_metas[bs_indices])\n\n        #     eps = 1e-5\n        #     cams_points_single_view = torch.matmul(lidar2img_project.view(NV, 1, 4, 4), lidar_points[bs_indices].view(\n        #         NV, n_points, 4, 1)).squeeze(-1)  # n 1 4 4 @ n N 4 1 -> n N 4 1 -> n  N  4\n        #     cams_points_single_view_depth = cams_points_single_view[..., 2:3].clone(\n        #     )\n\n        #     mask = (cams_points_single_view_depth > eps)\n\n        #     cams_points_single_view = cams_points_single_view[..., 0:2] / torch.maximum(\n        #         cams_points_single_view[..., 2:3], torch.ones_like(cams_points_single_view[..., 2:3]) * eps)\n\n        #     if unproject_order == 1:  # Inverse mapping transformation of image enhancement\n\n        #         if unproject:\n        #             cams_points_single_view[..., :2] = torch.matmul(post_rots[bs_indices][:, :2, :2].unsqueeze(\n        #                 1), cams_points_single_view[..., :2].unsqueeze(-1)).squeeze(-1)\n        #             cams_points_single_view[..., :2] = cams_points_single_view[...,\n        #                                                                        :2] + post_trans[bs_indices][..., None, :2]\n        #         else:\n        #             pass\n\n        #     # Camera mapping point scaling based on downsampling of image features\n        #     cams_points_single_view[...,\n        #                             0] *= (W/self.data_config['input_size'][1])\n        #     cams_points_single_view[...,\n        #                             1] *= (H/self.data_config['input_size'][0])\n\n        #     if normal_lidar2cams_point:\n        #         cams_points_single_view[...,\n        #                                 0] /= self.data_config['input_size'][1]\n        #         cams_points_single_view[...,\n        #                                 1] /= self.data_config['input_size'][0]\n        #         cams_points_single_view = (cams_points_single_view - 0.5) * 2\n        #         # use normalized intra-interval parameter thresholds\n        #         mask = (mask & (cams_points_single_view[..., 0:1] > -1)\n        #                 & (cams_points_single_view[..., 1:2] > -1)\n        #                 & (cams_points_single_view[..., 0:1] < 1)\n        #                 & (cams_points_single_view[..., 1:2] < 1))\n        #     else:\n        #         # threshold limits directly against image feature size\n        #         # maybe this is error, wrong\n        #         if True:\n        #             mask = (mask & (cams_points_single_view[..., 0:1] > 0)\n        #                     & (cams_points_single_view[..., 1:2] > 0)\n        #                     & (cams_points_single_view[..., 0:1] < W)\n        #                     & (cams_points_single_view[..., 1:2] < H))\n        #         else:\n        #             W = self.data_config['input_size'][1]\n        #             H = self.data_config['input_size'][0]\n        #             mask = (mask & (cams_points_single_view[..., 0:1] > 0)\n        #                     & (cams_points_single_view[..., 1:2] > 0)\n        #                     & (cams_points_single_view[..., 0:1] < W)\n        #                     & (cams_points_single_view[..., 1:2] < H))\n\n        #     cams_points.append(cams_points_single_view)  # n N 2\n        #     masks.append(mask)\n\n        # cams_points = torch.stack(cams_points)  # B n N 2\n        # masks = torch.stack(masks)  # B n N 1\n\n        return cams_points, masks\n\n    def fastbev_trans_single(self, img_feats, points, valid):\n        \"\"\"\n        function: 2d feature + predefined point cloud -> 3d volume\n        input:\n            img features: [n_cams, C, H, W]\n            points: [ n_cams, n_point, 2]\n            valid: [n_cams, n_points, 1]\n        output:\n            volume: [C, *n_voxels]\n        \"\"\"\n        n_images, n_channels, _, _ = img_feats.shape\n\n        if self.n_voxels is None:\n            n_x_voxels, n_y_voxels, n_z_voxels = (\n                int(self.nx[0]),\n                int(self.nx[1]),\n                int(self.nx[2] * self.z_axis_times),\n            )\n        else:\n            n_x_voxels, n_y_voxels, n_z_voxels = self.n_voxels\n\n        x = points[..., 0].round().long()  # n_cams n_points 1\n        y = points[..., 1].round().long()  # n_cams n_points 1\n\n        # x = points[..., 0]\n        # y = points[..., 1]\n\n        valid = valid.squeeze(-1) if len(valid.shape) == 3 else valid\n\n        # method2：特征填充，只填充有效特征，重复特征直接覆盖\n        volume = torch.zeros(\n            (n_channels, points.shape[-2]), device=img_feats.device\n        ).type_as(img_feats)\n\n        for i in range(n_images):\n            volume[:, valid[i]] = img_feats[i, :, y[i, valid[i]], x[i, valid[i]]]\n\n        volume = volume.view(n_channels, n_x_voxels, n_y_voxels, n_z_voxels)\n\n        return volume\n\n    def get_volumes(\n        self,\n        rots,\n        trans,\n        post_rots,\n        post_trans,\n        cams_inner,\n        img_feats,\n        unproject=True,\n        img_metas=None,\n    ):\n        \"\"\"lidar points project to cams pixel points\n        Args:\n            rots (torch.tensr): [B,N,3,3] the rotation from lidar points to cams points\n            trans (torch.tensor): [B,N,3,1] the translate from lidar points to cams\n            post_rots (torch.tensor): [B,N,2,2] the rotation for augmentation imgs\n            post_trans (torch.tensor): [B,N,2,1] ...\n            cams_inner (torch.tensor): [B,N,3,3] ...\n            img_feats (torch.tensor): [B, n, C, H, W] ...\n        Return:\n            volumes (torch.tensor): [B, C_cams, X, Y, Z]\n        \"\"\"\n        B, _, _, H, W = img_feats.shape\n\n        if self.other_args[\"trans_version\"] == \"v3\":\n            predefined_voxel_points = None\n        else:\n            # 3,x,y,z -> 3,N -> B,3,N -> B,N,3\n            predefined_voxel_points = (\n                self.scenes_voxel_point(z_times=self.z_axis_times)\n                .view(1, 3, -1)\n                .repeat(B, 1, 1)\n                .permute(0, 2, 1)\n                .to(img_feats.device)\n            )\n\n        lidar_points = self.voxel_grid_to_lidar_points(\n            B,\n            self.pct_range,\n            self.voxel_size,\n            predefined_voxel_points,\n            data_type=img_feats.dtype,\n            device=img_feats.device,\n        )  # b N 3\n\n        feats_shape = (H, W)\n        cams_points, masks = self.lidar_points_to_cams_points(\n            lidar_points,\n            rots,\n            trans,\n            cams_inner,\n            feats_shape,\n            post_rots,\n            post_trans,\n            unproject,\n            img_metas=img_metas,\n        )  # b n_cams N_points 2, b n_cams N_points 1\n\n        volumes = []\n        for bs_indice in range(B):\n            volume = self.fastbev_trans_single(\n                img_feats[bs_indice], cams_points[bs_indice], masks[bs_indice]\n            )\n            volumes.append(volume)  # b...  |  c x y z\n\n        # volumes = torch.stack(volumes).permute(0, 1, 4, 3, 2) # b c x y z -> b c z y x\n        volumes = torch.stack(volumes)\n\n        return volumes\n\n    def forward(self, input, img_metas):\n        x, rots, trans, intrins, post_rots, post_trans, _ = input\n        B, N, C, H, W = x.shape\n        x = x.view(B * N, C, H, W)\n\n        if C != self.numC_Trans:\n            img_feat = self.featnet(x)\n        else:\n            img_feat = x\n\n        depth_digit = 0\n        if self.loss_depth_weight > 0:\n            depth_feat = x\n            cam_params = torch.cat(\n                [\n                    intrins.reshape(B * N, -1),\n                    post_rots.reshape(B * N, -1),\n                    post_trans.reshape(B * N, -1),\n                    rots.reshape(B * N, -1),\n                    trans.reshape(B * N, -1),\n                ],\n                dim=1,\n            )\n\n            depth_feat = self.se(depth_feat, cam_params)\n            depth_feat = self.extra_depthnet(depth_feat)[0]\n            depth_feat = self.dcn(depth_feat)\n            depth_digit = self.depthnet(depth_feat)\n            # depth_prob = self.get_depth_dist(depth_digit)\n\n        volumes = self.get_volumes(\n            rots,\n            trans,\n            post_rots,\n            post_trans,\n            intrins,\n            img_feat.view(B, N, -1, H, W),\n            self.unproject,\n            img_metas=img_metas,\n        )\n\n        # volumes = self.conv3d_modify(volumes.contiguous())\n\n        ll_voxel_feats = volumes.clone()  # b, 64, 12, 128, 128\n        # ll_voxel_feats = volumes  # b, 64, 12, 128, 128\n\n        # print(ll_voxel_feats)\n\n        def _inner_bev_process(volumes):\n            # b c d h w -> [(b c h w)-1, (b c h w)2，...,(b c h w)-d]\n            if False:\n                volumes_list = torch.unbind(volumes, dim=2)\n                _bev_feats = torch.cat(volumes_list, dim=1)\n            else:\n                # B, C, X, Y, Z = volumes.shape\n\n                # volumes_list = torch.unbind(volumes, dim=2)\n\n                # _bev_feats = volumes.reshape(B, X, Y, Z*C).permute(0, 3, 1, 2)\n                _bev_feats = rearrange(volumes, \"b c x y z -> b x y (z c)\")\n\n            if self.selfcalib_conv_config:\n                _bev_feats = self.bev_reduce(_bev_feats)\n\n            if self.bev_channel_reduce:\n                _bev_feats = self.ch_reduce(_bev_feats)  # 256 -> 64 warining\n            else:\n                pass\n\n            return _bev_feats  # b, 64 128, 128\n\n        bev_feat = _inner_bev_process(volumes=volumes)\n\n        # print(colored(bev_feat, 'yellow'), bev_feat.shape)\n        # raise NotImplemented\n\n        return bev_feat, depth_digit, ll_voxel_feats\n\n    def original_fastbev_imple(self, features, points, projection):\n        \"\"\"\n        function: 2d feature + predefined point cloud -> 3d volume\n        input:\n            img features: [6, 64, 225, 400]\n            points: [3, 200, 200, 12]\n            projection (Optinal): [6, 3, 4]\n        output:\n            volume: [64, 200, 200, 12]\n        \"\"\"\n        n_images, n_channels, height, width = features.shape\n        n_x_voxels, n_y_voxels, n_z_voxels = points.shape[-3:]\n        # [3, 200, 200, 12] -> [1, 3, 480000] -> [6, 3, 480000]\n        points = points.view(1, 3, -1).expand(n_images, 3, -1)\n        # [6, 3, 480000] -> [6, 4, 480000]\n        points = torch.cat((points, torch.ones_like(points[:, :1])), dim=1)\n        # ego_to_cam\n        # [6, 3, 4] * [6, 4, 480000] -> [6, 3, 480000]\n\n        points_2d_3 = torch.bmm(projection, points)  # lidar2img\n        x = (points_2d_3[:, 0] / points_2d_3[:, 2]).round().long()  # [6, 480000]\n        y = (points_2d_3[:, 1] / points_2d_3[:, 2]).round().long()  # [6, 480000]\n        z = points_2d_3[:, 2]  # [6, 480000]\n        valid = (\n            (x >= 0) & (y >= 0) & (x < width) & (y < height) & (z > 0)\n        )  # [6, 480000]\n\n        # method2：特征填充，只填充有效特征，重复特征直接覆盖\n        volume = torch.zeros(\n            (n_channels, points.shape[-1]), device=features.device\n        ).type_as(features)\n        for i in range(n_images):\n            volume[:, valid[i]] = features[i, :, y[i, valid[i]], x[i, valid[i]]]\n\n        volume = volume.view(n_channels, n_x_voxels, n_y_voxels, n_z_voxels)\n        return volume\n\n\n@NECKS.register_module()\nclass InterDistill_FastBEV_L2C_V2(InterDistill_FastBEV):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    @force_fp32(apply_to=(\"rots\", \"trans\", \"intrins\", \"post_rots\", \"post_trans\"))\n    def l2c_w_rots_trans(self, points, rots, trans, intrins, post_rots, post_trans):\n        \"\"\"\n        Args:\n            points:  B n_cams n_points 3\n            rots :   B n_cams 3 3\n            trans:   B n_cams 3 1\n            intrins: B n_cams 3 3\n        Return:\n            points_img: B n_cams n_points 2\n        \"\"\"\n        assert points.size(-1) <= 3  # b n_cams n_points 3\n        combine = rots.matmul(torch.inverse(intrins))\n        combine_inv = torch.inverse(combine)  # b n_cams 3 3\n\n        # points_img = (points - trans.squeeze(-1).unsqueeze(-2)).matmul(\n        #     combine_inv.permute(0, 1, 3, 2)\n        # )  # b n_cams n_points 3\n\n        points_img = (\n            (points - trans.squeeze(-1).unsqueeze(-2))\n            .matmul(torch.inverse(rots).permute(0, 1, 3, 2))\n            .matmul(intrins.permute(0, 1, 3, 2))\n        )\n        reference_depth = points_img[..., 2:3]\n\n        points_img = torch.cat(\n            [points_img[..., :2] / points_img[..., 2:3], points_img[..., 2:3]], -1\n        )\n\n        points_img = points_img.matmul(\n            post_rots.permute(0, 1, 3, 2)\n        ) + post_trans.squeeze(-1).unsqueeze(-2)\n\n        return points_img[..., :2], reference_depth\n\n    @force_fp32(\n        apply_to=(\n            \"lidar_points\",\n            \"rots\",\n            \"trans\",\n            \"cam_inner\",\n            \"post_rots\",\n            \"post_trans\",\n        )\n    )\n    def lidar_points_to_cams_points(\n        self,\n        lidar_points,\n        rots,\n        trans,\n        cam_inner,\n        feats_shape,\n        post_rots,\n        post_trans,\n        unproject=True,\n        img_metas=None,\n        unproject_order=1,\n        normal_lidar2cams_point=False,\n    ):\n        \"\"\"lidar_points_to_cams_points\n\n        - execute fastbev transformation\n        - N for points, n for num of multi-view\n        - lidar_points  : b N 3\n        - rots          : b n 3 3\n        - trans         : b n 1 3\n        - post_rots     : b n 3 3\n        - post_trans    : b n 1 3\n        - cams_inner    : b n 3 3\n\n        Args:\n            lidar_points (torch.tensor): [b, N, 3] or [b x y z 3]\n            unproject_order: 0 for first unproject, 1 for after unprojcet\n            normal_lidar2cams_point: normalization the porjected lidar point( points in cams space ).\n        Return:\n            cams_points (torch.tensor): [B, n, N, 2]\n            mask (torch.tensor): [B, n, N, 1]\n        \"\"\"\n        print(colored(\"v2 version excuting!!\", \"red\"))\n\n        B = lidar_points.size(0)\n        NV = rots.size(1)\n\n        if len(lidar_points.shape) == 5:\n            lidar_points = lidar_points.flatten(1, 3)\n        n_points = lidar_points.size(1)\n        lidar_points = lidar_points.view(B, 1, n_points, 3).repeat(1, NV, 1, 1)\n        # b n_cams n_points 4\n\n        cams_points, ref_depth = self.l2c_w_rots_trans(\n            lidar_points,\n            rots=rots,\n            trans=trans,\n            intrins=cam_inner,\n            post_rots=post_rots,\n            post_trans=post_trans,\n        )\n\n        cams_points = cams_points / torch.maximum(\n            ref_depth, torch.ones_like(ref_depth) * 1e-5\n        )\n\n        # cams_points[..., 0:1] /= ref_depth\n        # cams_points[..., 1:2] /= ref_depth\n\n        mask = ref_depth > 0\n\n        # cams_points[..., 0] /= self.data_config[\"input_size\"][1]\n        # cams_points[..., 1] /= self.data_config[\"input_size\"][0]\n\n        # print(colored(cams_points, \"yellow\"))\n        # print(colored(ref_depth, \"yellow\"))\n        # mask = (\n        #     mask\n        #     & (cams_points[..., 0:1] > 0.0)\n        #     & (cams_points[..., 1:2] > 0.0)\n        #     & (cams_points[..., 0:1] < 1.0)\n        #     & (cams_points[..., 1:2] < 1.0)\n        # )\n\n        H, W = feats_shape\n        # cams_points[..., 0] *= W / self.data_config[\"input_size\"][1]\n        # cams_points[..., 1] *= H / self.data_config[\"input_size\"][0]\n\n        cams_points[..., 0] /= W\n        cams_points[..., 1] /= H\n\n        # print(colored(cams_points, \"green\"))\n\n        mask = (\n            mask\n            & (cams_points[..., 0:1] > 0.0)\n            & (cams_points[..., 1:2] > 0.0)\n            & (cams_points[..., 0:1] < 1.0)\n            & (cams_points[..., 1:2] < 1.0)\n        )\n\n        masks = mask  # b n_cams n_points 1\n\n        return cams_points, masks\n\n\n@NECKS.register_module()\nclass InterDistill_FastBEV_L2C_V3(InterDistill_FastBEV):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    @force_fp32(apply_to=(\"lidar_points\", \"img_metas\"))\n    def lidar_points_to_cams_points(\n        self,\n        lidar_points,\n        rots,\n        trans,\n        cam_inner,\n        feats_shape,\n        post_rots,\n        post_trans,\n        unproject=True,\n        img_metas=None,\n        unproject_order=1,\n        normal_lidar2cams_point=False,\n    ):\n        print(colored(\"v3 version excuting!!\", \"red\"))\n\n        assert \"lidar2imgs\" in img_metas[0].keys()\n\n        lidar2img = []\n        for data in img_metas:\n            lidar2img.append(data[\"lidar2imgs\"])\n\n        lidar2imgs = torch.stack(lidar2img).to(lidar_points.device)  # b num_cams 4 4\n\n        B = lidar_points.size(0)\n        lidar_points = F.pad(lidar_points, (0, 1), value=1)\n        num_cams = img_metas[0][\"lidar2imgs\"].size(0)\n        lidar_points = (\n            lidar_points.unsqueeze(1).repeat(1, num_cams, 1, 1, 1, 1).unsqueeze(-1)\n        )  # b num_cams x y z 4 1\n\n        # b ncams 4 4 @ b ncams x y z 4 1 -> b ncams x y z 4\n        img_points = torch.matmul(\n            lidar2imgs.to(torch.float32).view(B, num_cams, 1, 1, 1, 4, 4),\n            lidar_points.to(torch.float32),\n        ).squeeze(-1)\n\n        eps = 1e-5\n        masks = img_points[..., 2:3] > eps\n        img_points = img_points[..., 0:2] / torch.maximum(\n            img_points[..., 2:3],\n            torch.ones_like(img_points[..., 2:3]) * eps,\n        )  # b ncams x y z 2\n\n        img_points = img_points - post_trans[..., :2].view(B, num_cams, 1, 1, 1, 2)\n        img_points = (\n            torch.inverse(post_rots[..., :2, :2])\n            .view(B, num_cams, 1, 1, 1, 2, 2)\n            .matmul(img_points.unsqueeze(-1))\n        ).squeeze(-1)\n\n        resize_img_shape = self.data_config[\"input_size\"]\n\n        img_points[..., 0] /= resize_img_shape[1]\n        img_points[..., 1] /= resize_img_shape[0]\n\n        # img_points[..., 0] /= feats_shape[1]\n        # img_points[..., 1] /= feats_shape[0]\n\n        masks = (\n            masks\n            & (img_points[..., 1:2] > 0.0)\n            & (img_points[..., 1:2] < 1.0)\n            & (img_points[..., 0:1] < 1.0)\n            & (img_points[..., 0:1] > 0.0)\n        )\n\n        img_points = img_points.flatten(2, -2)\n        masks = masks.flatten(2, -2)\n\n        return img_points, masks\n\n\n@NECKS.register_module()\nclass InterDistill_FastBEV_MS_BEV(InterDistill_FastBEV_L2C_V3):\n    \"\"\"Still testing! ! !\"\"\"\n\n    def __init__(\n        self,\n        is_ms_bev=False,\n        num_ms_fusion=4,\n        ms_bev_fusion=None,\n        neck_bev=None,\n        se_like=None,\n        xy_transpose=False,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        if not is_ms_bev:\n            assert \"Multi-scale bev adopts this class!\"\n\n        self.is_ms_bev = is_ms_bev\n        self.is_neck_bev = neck_bev\n        self.num_ms_fusion = num_ms_fusion\n        self.se_like = se_like\n\n        self.img_fusion_feats_fuse_1 = nn.Conv2d(\n            self.numC_Trans * 4, self.numC_Trans, 3, 1, 1\n        )\n        self.img_fusion_feats_fuse_2 = nn.Conv2d(\n            self.numC_Trans * 3, self.numC_Trans, 3, 1, 1\n        )\n        self.img_fusion_feats_fuse_3 = nn.Conv2d(\n            self.numC_Trans * 2, self.numC_Trans, 3, 1, 1\n        )\n\n        self.conv3d_modify = nn.Conv3d(\n            self.numC_Trans * num_ms_fusion, self.numC_Trans, kernel_size=1\n        )\n\n        self.ms_bev_fusion = ConvModule(\n            self.numC_Trans * num_ms_fusion * self.n_voxels[2],\n            ms_bev_fusion.out_channel,\n            kernel_size=1,\n            act_cfg=ms_bev_fusion.act_cfg if ms_bev_fusion.act_cfg else None,\n            norm_cfg=ms_bev_fusion.norm_cfg if ms_bev_fusion.norm_cfg else None,\n        )\n\n        if neck_bev:\n            self.neck_bev = NECKS.build(neck_bev)\n\n        if self.bev_channel_reduce:\n            self.ms_bev_ch_reduce = ConvModule(\n                self.bev_channel_reduce.in_channel,\n                self.bev_channel_reduce.out_channel,\n                1,\n                act_cfg=self.bev_channel_reduce.act_cfg,\n            )\n\n        if se_like:\n            self.att = nn.Sequential(\n                nn.AdaptiveAvgPool2d(1),\n                nn.Conv2d(\n                    se_like.in_channel, se_like.in_channel, kernel_size=1, stride=1\n                ),\n                nn.Sigmoid(),\n            )\n        if self.selfcalib_conv_config:\n            self.selfcalib_encoder = BACKBONES.build(self.selfcalib_conv_config)\n        self.xy_transpose = xy_transpose\n\n    def img_unify_resize_and_fusion(self, ms_img_feats, index=1):\n        assert isinstance(ms_img_feats, tuple) or isinstance(ms_img_feats, list)\n\n        ms_level_start = 0\n        img_feats_0_size = ms_img_feats[0].shape[-2:]\n        # print(img_feats_0_size, '============')\n        fuse_feats = [ms_img_feats[0]]\n        for i in range(ms_level_start + 1, len(ms_img_feats)):\n            fuse_feats.append(\n                resize(\n                    ms_img_feats[i],\n                    img_feats_0_size,\n                    mode=\"bilinear\",\n                    align_corners=False,\n                )\n            )\n        if len(fuse_feats) > 1:\n            fuse_feats = torch.cat(fuse_feats, dim=1)  # bn c*4 h w\n        else:\n            fuse_feats = fuse_feats[0]\n\n        fuse_feats = self.__getattr__(f\"img_fusion_feats_fuse_{index}\")(fuse_feats)\n\n        return fuse_feats\n\n    def forward(self, input, img_metas):\n        x, rots, trans, intrins, post_rots, post_trans, _ = input\n        B = rots.size(0)\n        depth_logits = 0\n\n        # assert isinstance(x, tuple)  # Multi-scale image features\n\n        low_voxel_feats = None\n\n        ms_img_feats = []\n        ms_img_feats.append(x[0])\n        # ms_img_feats.append(self.img_unify_resize_and_fusion(x, index=1))\n        # ms_img_feats.append(self.img_unify_resize_and_fusion(x[1:], index=2))\n        # ms_img_feats.append(self.img_unify_resize_and_fusion(x[2:], index=3))\n\n        assert ms_img_feats.__len__() == self.num_ms_fusion\n\n        ms_volumes = []\n        for i, img_feat in enumerate(ms_img_feats):\n            _, C, H, W = img_feat.shape\n            img_feat = img_feat.view(B, -1, C, H, W)\n\n            # n_ms b c x y z\n            ms_volumes.append(\n                self.get_volumes(\n                    rots,\n                    trans,\n                    post_rots,\n                    post_trans,\n                    intrins,\n                    img_feats=img_feat,\n                    unproject=True,\n                    img_metas=img_metas,\n                )\n            )\n\n        # ms_volumes = torch.stack(ms_volumes)\n        # b c*num_ms_fusion x y z\n\n        if self.num_ms_fusion > 1:\n            ms_volumes = torch.cat(ms_volumes, dim=1)\n        else:\n            ms_volumes = ms_volumes[0]\n\n        # print(\n        #     colored(\n        #         f\"{ms_volumes},{ torch.sum(torch.where(ms_volumes > 0, 1, 0))}\", \"red\"\n        #     )\n        # )\n\n        # if ms_volumes.size(1) == self.numC_Trans * self.num_ms_fusion:\n        #     low_voxel_feats = self.conv3d_modify(ms_volumes.permute(0, 1, 4, 2, 3))\n        # else:\n        #     low_voxel_feats = ms_volumes\n\n        _, C, X, Y, Z = ms_volumes.shape\n\n        if self.xy_transpose:\n            ms_volumes = (\n                ms_volumes.permute(0, 2, 3, 4, 1)\n                .reshape(B, X, Y, Z * C)\n                .permute(0, 3, 1, 2)\n            )\n        else:\n            ms_volumes = (\n                ms_volumes.permute(0, 2, 3, 4, 1)\n                .reshape(B, X, Y, Z * C)\n                .permute(0, 3, 2, 1)\n            )\n\n        bev_feats = self.ms_bev_fusion(ms_volumes)\n\n        if self.is_neck_bev:\n            bev_feats = self.neck_bev(bev_feats)\n\n        if self.se_like:\n            bev_feats = bev_feats * self.att(bev_feats)\n\n        if self.bev_channel_reduce:\n            bev_feats = self.ms_bev_ch_reduce(bev_feats)\n\n        if self.selfcalib_conv_config:\n            bev_feats = self.selfcalib_encoder(bev_feats)\n\n        if self.xy_transpose:\n            bev_feats = bev_feats.transpose(-1, -2)\n\n        # print(bev_feats.shape, '===========',bev_feats)\n\n        return bev_feats, depth_logits, low_voxel_feats\n"
        }
    ]
}