{
    "sourceFile": "mmdet3d/models/necks/fpn.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1716024827476,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1734679782608,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,8 +6,208 @@\n \n from ..builder import NECKS\n \n \n+# @NECKS.register_module()\n+# class CustomFPN(BaseModule):\n+#     r\"\"\"Feature Pyramid Network.\n+\n+#     This is an implementation of paper `Feature Pyramid Networks for Object\n+#     Detection <https://arxiv.org/abs/1612.03144>`_.\n+\n+#     Args:\n+#         in_channels (List[int]): Number of input channels per scale.\n+#         out_channels (int): Number of output channels (used at each scale)\n+#         num_outs (int): Number of output scales.\n+#         start_level (int): Index of the start input backbone level used to\n+#             build the feature pyramid. Default: 0.\n+#         end_level (int): Index of the end input backbone level (exclusive) to\n+#             build the feature pyramid. Default: -1, which means the last level.\n+#         add_extra_convs (bool | str): If bool, it decides whether to add conv\n+#             layers on top of the original feature maps. Default to False.\n+#             If True, it is equivalent to `add_extra_convs='on_input'`.\n+#             If str, it specifies the source feature map of the extra convs.\n+#             Only the following options are allowed\n+\n+#             - 'on_input': Last feat map of neck inputs (i.e. backbone feature).\n+#             - 'on_lateral':  Last feature map after lateral convs.\n+#             - 'on_output': The last output feature map after fpn convs.\n+#         relu_before_extra_convs (bool): Whether to apply relu before the extra\n+#             conv. Default: False.\n+#         no_norm_on_lateral (bool): Whether to apply norm on lateral.\n+#             Default: False.\n+#         conv_cfg (dict): Config dict for convolution layer. Default: None.\n+#         norm_cfg (dict): Config dict for normalization layer. Default: None.\n+#         act_cfg (str): Config dict for activation layer in ConvModule.\n+#             Default: None.\n+#         upsample_cfg (dict): Config dict for interpolate layer.\n+#             Default: `dict(mode='nearest')`\n+#         init_cfg (dict or list[dict], optional): Initialization config dict.\n+\n+#     Example:\n+#         >>> import torch\n+#         >>> in_channels = [2, 3, 5, 7]\n+#         >>> scales = [340, 170, 84, 43]\n+#         >>> inputs = [torch.rand(1, c, s, s)\n+#         ...           for c, s in zip(in_channels, scales)]\n+#         >>> self = FPN(in_channels, 11, len(in_channels)).eval()\n+#         >>> outputs = self.forward(inputs)\n+#         >>> for i in range(len(outputs)):\n+#         ...     print(f'outputs[{i}].shape = {outputs[i].shape}')\n+#         outputs[0].shape = torch.Size([1, 11, 340, 340])\n+#         outputs[1].shape = torch.Size([1, 11, 170, 170])\n+#         outputs[2].shape = torch.Size([1, 11, 84, 84])\n+#         outputs[3].shape = torch.Size([1, 11, 43, 43])\n+#     \"\"\"\n+\n+#     def __init__(\n+#         self,\n+#         in_channels,\n+#         out_channels,\n+#         num_outs,\n+#         start_level=0,\n+#         end_level=-1,\n+#         out_ids=[],\n+#         add_extra_convs=False,\n+#         relu_before_extra_convs=False,\n+#         no_norm_on_lateral=False,\n+#         conv_cfg=None,\n+#         norm_cfg=None,\n+#         act_cfg=None,\n+#         upsample_cfg=dict(mode=\"nearest\"),\n+#         init_cfg=dict(type=\"Xavier\", layer=\"Conv2d\", distribution=\"uniform\"),\n+#     ):\n+#         super(CustomFPN, self).__init__(init_cfg)\n+#         assert isinstance(in_channels, list)\n+#         self.in_channels = in_channels\n+#         self.out_channels = out_channels\n+#         self.num_ins = len(in_channels)\n+#         self.num_outs = num_outs\n+#         self.relu_before_extra_convs = relu_before_extra_convs\n+#         self.no_norm_on_lateral = no_norm_on_lateral\n+#         self.fp16_enabled = False\n+#         self.upsample_cfg = upsample_cfg.copy()\n+#         self.out_ids = out_ids\n+#         if end_level == -1:\n+#             self.backbone_end_level = self.num_ins\n+#             # assert num_outs >= self.num_ins - start_level\n+#         else:\n+#             # if end_level < inputs, no extra level is allowed\n+#             self.backbone_end_level = end_level\n+#             assert end_level <= len(in_channels)\n+#             assert num_outs == end_level - start_level\n+#         self.start_level = start_level\n+#         self.end_level = end_level\n+#         self.add_extra_convs = add_extra_convs\n+#         assert isinstance(add_extra_convs, (str, bool))\n+#         if isinstance(add_extra_convs, str):\n+#             # Extra_convs_source choices: 'on_input', 'on_lateral', 'on_output'\n+#             assert add_extra_convs in (\"on_input\", \"on_lateral\", \"on_output\")\n+#         elif add_extra_convs:  # True\n+#             self.add_extra_convs = \"on_input\"\n+\n+#         self.lateral_convs = nn.ModuleList()\n+#         self.fpn_convs = nn.ModuleList()\n+\n+#         for i in range(self.start_level, self.backbone_end_level):\n+#             l_conv = ConvModule(\n+#                 in_channels[i],\n+#                 out_channels,\n+#                 1,\n+#                 conv_cfg=conv_cfg,\n+#                 norm_cfg=norm_cfg if not self.no_norm_on_lateral else None,\n+#                 act_cfg=act_cfg,\n+#                 inplace=False,\n+#             )\n+\n+#             self.lateral_convs.append(l_conv)\n+#             if i in self.out_ids:\n+#                 fpn_conv = ConvModule(\n+#                     out_channels,\n+#                     out_channels,\n+#                     3,\n+#                     padding=1,\n+#                     conv_cfg=conv_cfg,\n+#                     norm_cfg=norm_cfg,\n+#                     act_cfg=act_cfg,\n+#                     inplace=False,\n+#                 )\n+#                 self.fpn_convs.append(fpn_conv)\n+\n+#         # add extra conv layers (e.g., RetinaNet)\n+#         extra_levels = num_outs - self.backbone_end_level + self.start_level\n+#         if self.add_extra_convs and extra_levels >= 1:\n+#             for i in range(extra_levels):\n+#                 if i == 0 and self.add_extra_convs == \"on_input\":\n+#                     in_channels = self.in_channels[self.backbone_end_level - 1]\n+#                 else:\n+#                     in_channels = out_channels\n+#                 extra_fpn_conv = ConvModule(\n+#                     in_channels,\n+#                     out_channels,\n+#                     3,\n+#                     stride=2,\n+#                     padding=1,\n+#                     conv_cfg=conv_cfg,\n+#                     norm_cfg=norm_cfg,\n+#                     act_cfg=act_cfg,\n+#                     inplace=False,\n+#                 )\n+#                 self.fpn_convs.append(extra_fpn_conv)\n+\n+#     @auto_fp16()\n+#     def forward(self, inputs):\n+#         \"\"\"Forward function.\"\"\"\n+#         assert len(inputs) == len(self.in_channels)\n+\n+#         # build laterals\n+#         laterals = [\n+#             lateral_conv(inputs[i + self.start_level])\n+#             for i, lateral_conv in enumerate(self.lateral_convs)\n+#         ]\n+\n+#         # build top-down path\n+#         used_backbone_levels = len(laterals)\n+#         for i in range(used_backbone_levels - 1, 0, -1):\n+#             # In some cases, fixing `scale factor` (e.g. 2) is preferred, but\n+#             #  it cannot co-exist with `size` in `F.interpolate`.\n+#             if \"scale_factor\" in self.upsample_cfg:\n+#                 laterals[i - 1] += F.interpolate(laterals[i], **self.upsample_cfg)\n+#             else:\n+#                 prev_shape = laterals[i - 1].shape[2:]\n+#                 laterals[i - 1] += F.interpolate(\n+#                     laterals[i], size=prev_shape, **self.upsample_cfg\n+#                 )\n+\n+#         # build outputs\n+#         # part 1: from original levels\n+#         outs = [self.fpn_convs[i](laterals[i]) for i in self.out_ids]\n+#         # part 2: add extra levels\n+#         if self.num_outs > len(outs):\n+#             # use max pool to get more levels on top of outputs\n+#             # (e.g., Faster R-CNN, Mask R-CNN)\n+#             if not self.add_extra_convs:\n+#                 for i in range(self.num_outs - used_backbone_levels):\n+#                     outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n+#             # add conv layers on top of original feature maps (RetinaNet)\n+#             else:\n+#                 if self.add_extra_convs == \"on_input\":\n+#                     extra_source = inputs[self.backbone_end_level - 1]\n+#                 elif self.add_extra_convs == \"on_lateral\":\n+#                     extra_source = laterals[-1]\n+#                 elif self.add_extra_convs == \"on_output\":\n+#                     extra_source = outs[-1]\n+#                 else:\n+#                     raise NotImplementedError\n+#                 outs.append(self.fpn_convs[used_backbone_levels](extra_source))\n+#                 for i in range(used_backbone_levels + 1, self.num_outs):\n+#                     if self.relu_before_extra_convs:\n+#                         outs.append(self.fpn_convs[i](F.relu(outs[-1])))\n+#                     else:\n+#                         outs.append(self.fpn_convs[i](outs[-1]))\n+#         return outs[0]\n+\n+\n @NECKS.register_module()\n class CustomFPN(BaseModule):\n     r\"\"\"Feature Pyramid Network.\n \n"
                }
            ],
            "date": 1716024827476,
            "name": "Commit-0",
            "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import ConvModule\nfrom mmcv.runner import BaseModule, auto_fp16\n\nfrom ..builder import NECKS\n\n\n@NECKS.register_module()\nclass CustomFPN(BaseModule):\n    r\"\"\"Feature Pyramid Network.\n\n    This is an implementation of paper `Feature Pyramid Networks for Object\n    Detection <https://arxiv.org/abs/1612.03144>`_.\n\n    Args:\n        in_channels (List[int]): Number of input channels per scale.\n        out_channels (int): Number of output channels (used at each scale)\n        num_outs (int): Number of output scales.\n        start_level (int): Index of the start input backbone level used to\n            build the feature pyramid. Default: 0.\n        end_level (int): Index of the end input backbone level (exclusive) to\n            build the feature pyramid. Default: -1, which means the last level.\n        add_extra_convs (bool | str): If bool, it decides whether to add conv\n            layers on top of the original feature maps. Default to False.\n            If True, it is equivalent to `add_extra_convs='on_input'`.\n            If str, it specifies the source feature map of the extra convs.\n            Only the following options are allowed\n\n            - 'on_input': Last feat map of neck inputs (i.e. backbone feature).\n            - 'on_lateral':  Last feature map after lateral convs.\n            - 'on_output': The last output feature map after fpn convs.\n        relu_before_extra_convs (bool): Whether to apply relu before the extra\n            conv. Default: False.\n        no_norm_on_lateral (bool): Whether to apply norm on lateral.\n            Default: False.\n        conv_cfg (dict): Config dict for convolution layer. Default: None.\n        norm_cfg (dict): Config dict for normalization layer. Default: None.\n        act_cfg (str): Config dict for activation layer in ConvModule.\n            Default: None.\n        upsample_cfg (dict): Config dict for interpolate layer.\n            Default: `dict(mode='nearest')`\n        init_cfg (dict or list[dict], optional): Initialization config dict.\n\n    Example:\n        >>> import torch\n        >>> in_channels = [2, 3, 5, 7]\n        >>> scales = [340, 170, 84, 43]\n        >>> inputs = [torch.rand(1, c, s, s)\n        ...           for c, s in zip(in_channels, scales)]\n        >>> self = FPN(in_channels, 11, len(in_channels)).eval()\n        >>> outputs = self.forward(inputs)\n        >>> for i in range(len(outputs)):\n        ...     print(f'outputs[{i}].shape = {outputs[i].shape}')\n        outputs[0].shape = torch.Size([1, 11, 340, 340])\n        outputs[1].shape = torch.Size([1, 11, 170, 170])\n        outputs[2].shape = torch.Size([1, 11, 84, 84])\n        outputs[3].shape = torch.Size([1, 11, 43, 43])\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        num_outs,\n        start_level=0,\n        end_level=-1,\n        out_ids=[],\n        add_extra_convs=False,\n        relu_before_extra_convs=False,\n        no_norm_on_lateral=False,\n        conv_cfg=None,\n        norm_cfg=None,\n        act_cfg=None,\n        upsample_cfg=dict(mode=\"nearest\"),\n        init_cfg=dict(type=\"Xavier\", layer=\"Conv2d\", distribution=\"uniform\"),\n    ):\n        super(CustomFPN, self).__init__(init_cfg)\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)\n        self.num_outs = num_outs\n        self.relu_before_extra_convs = relu_before_extra_convs\n        self.no_norm_on_lateral = no_norm_on_lateral\n        self.fp16_enabled = False\n        self.upsample_cfg = upsample_cfg.copy()\n        self.out_ids = out_ids\n        if end_level == -1:\n            self.backbone_end_level = self.num_ins\n            # assert num_outs >= self.num_ins - start_level\n        else:\n            # if end_level < inputs, no extra level is allowed\n            self.backbone_end_level = end_level\n            assert end_level <= len(in_channels)\n            assert num_outs == end_level - start_level\n        self.start_level = start_level\n        self.end_level = end_level\n        self.add_extra_convs = add_extra_convs\n        assert isinstance(add_extra_convs, (str, bool))\n        if isinstance(add_extra_convs, str):\n            # Extra_convs_source choices: 'on_input', 'on_lateral', 'on_output'\n            assert add_extra_convs in (\"on_input\", \"on_lateral\", \"on_output\")\n        elif add_extra_convs:  # True\n            self.add_extra_convs = \"on_input\"\n\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n\n        for i in range(self.start_level, self.backbone_end_level):\n            l_conv = ConvModule(\n                in_channels[i],\n                out_channels,\n                1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg if not self.no_norm_on_lateral else None,\n                act_cfg=act_cfg,\n                inplace=False,\n            )\n\n            self.lateral_convs.append(l_conv)\n            if i in self.out_ids:\n                fpn_conv = ConvModule(\n                    out_channels,\n                    out_channels,\n                    3,\n                    padding=1,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    act_cfg=act_cfg,\n                    inplace=False,\n                )\n                self.fpn_convs.append(fpn_conv)\n\n        # add extra conv layers (e.g., RetinaNet)\n        extra_levels = num_outs - self.backbone_end_level + self.start_level\n        if self.add_extra_convs and extra_levels >= 1:\n            for i in range(extra_levels):\n                if i == 0 and self.add_extra_convs == \"on_input\":\n                    in_channels = self.in_channels[self.backbone_end_level - 1]\n                else:\n                    in_channels = out_channels\n                extra_fpn_conv = ConvModule(\n                    in_channels,\n                    out_channels,\n                    3,\n                    stride=2,\n                    padding=1,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    act_cfg=act_cfg,\n                    inplace=False,\n                )\n                self.fpn_convs.append(extra_fpn_conv)\n\n    @auto_fp16()\n    def forward(self, inputs):\n        \"\"\"Forward function.\"\"\"\n        assert len(inputs) == len(self.in_channels)\n\n        # build laterals\n        laterals = [\n            lateral_conv(inputs[i + self.start_level])\n            for i, lateral_conv in enumerate(self.lateral_convs)\n        ]\n\n        # build top-down path\n        used_backbone_levels = len(laterals)\n        for i in range(used_backbone_levels - 1, 0, -1):\n            # In some cases, fixing `scale factor` (e.g. 2) is preferred, but\n            #  it cannot co-exist with `size` in `F.interpolate`.\n            if \"scale_factor\" in self.upsample_cfg:\n                laterals[i - 1] += F.interpolate(laterals[i], **self.upsample_cfg)\n            else:\n                prev_shape = laterals[i - 1].shape[2:]\n                laterals[i - 1] += F.interpolate(\n                    laterals[i], size=prev_shape, **self.upsample_cfg\n                )\n\n        # build outputs\n        # part 1: from original levels\n        outs = [self.fpn_convs[i](laterals[i]) for i in self.out_ids]\n        # part 2: add extra levels\n        if self.num_outs > len(outs):\n            # use max pool to get more levels on top of outputs\n            # (e.g., Faster R-CNN, Mask R-CNN)\n            if not self.add_extra_convs:\n                for i in range(self.num_outs - used_backbone_levels):\n                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n            # add conv layers on top of original feature maps (RetinaNet)\n            else:\n                if self.add_extra_convs == \"on_input\":\n                    extra_source = inputs[self.backbone_end_level - 1]\n                elif self.add_extra_convs == \"on_lateral\":\n                    extra_source = laterals[-1]\n                elif self.add_extra_convs == \"on_output\":\n                    extra_source = outs[-1]\n                else:\n                    raise NotImplementedError\n                outs.append(self.fpn_convs[used_backbone_levels](extra_source))\n                for i in range(used_backbone_levels + 1, self.num_outs):\n                    if self.relu_before_extra_convs:\n                        outs.append(self.fpn_convs[i](F.relu(outs[-1])))\n                    else:\n                        outs.append(self.fpn_convs[i](outs[-1]))\n        return outs[0]\n\n\n@NECKS.register_module()\nclass FPNForBEVDet(BaseModule):\n    r\"\"\"Feature Pyramid Network.\n\n    This is an implementation of paper `Feature Pyramid Networks for Object\n    Detection <https://arxiv.org/abs/1612.03144>`_.\n\n    Args:\n        in_channels (List[int]): Number of input channels per scale.\n        out_channels (int): Number of output channels (used at each scale)\n        num_outs (int): Number of output scales.\n        start_level (int): Index of the start input backbone level used to\n            build the feature pyramid. Default: 0.\n        end_level (int): Index of the end input backbone level (exclusive) to\n            build the feature pyramid. Default: -1, which means the last level.\n        add_extra_convs (bool | str): If bool, it decides whether to add conv\n            layers on top of the original feature maps. Default to False.\n            If True, it is equivalent to `add_extra_convs='on_input'`.\n            If str, it specifies the source feature map of the extra convs.\n            Only the following options are allowed\n\n            - 'on_input': Last feat map of neck inputs (i.e. backbone feature).\n            - 'on_lateral':  Last feature map after lateral convs.\n            - 'on_output': The last output feature map after fpn convs.\n        relu_before_extra_convs (bool): Whether to apply relu before the extra\n            conv. Default: False.\n        no_norm_on_lateral (bool): Whether to apply norm on lateral.\n            Default: False.\n        conv_cfg (dict): Config dict for convolution layer. Default: None.\n        norm_cfg (dict): Config dict for normalization layer. Default: None.\n        act_cfg (str): Config dict for activation layer in ConvModule.\n            Default: None.\n        upsample_cfg (dict): Config dict for interpolate layer.\n            Default: `dict(mode='nearest')`\n        init_cfg (dict or list[dict], optional): Initialization config dict.\n\n    Example:\n        >>> import torch\n        >>> in_channels = [2, 3, 5, 7]\n        >>> scales = [340, 170, 84, 43]\n        >>> inputs = [torch.rand(1, c, s, s)\n        ...           for c, s in zip(in_channels, scales)]\n        >>> self = FPN(in_channels, 11, len(in_channels)).eval()\n        >>> outputs = self.forward(inputs)\n        >>> for i in range(len(outputs)):\n        ...     print(f'outputs[{i}].shape = {outputs[i].shape}')\n        outputs[0].shape = torch.Size([1, 11, 340, 340])\n        outputs[1].shape = torch.Size([1, 11, 170, 170])\n        outputs[2].shape = torch.Size([1, 11, 84, 84])\n        outputs[3].shape = torch.Size([1, 11, 43, 43])\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        num_outs,\n        start_level=0,\n        end_level=-1,\n        out_ids=[],\n        add_extra_convs=False,\n        relu_before_extra_convs=False,\n        no_norm_on_lateral=False,\n        conv_cfg=None,\n        norm_cfg=None,\n        act_cfg=None,\n        upsample_cfg=dict(mode=\"nearest\"),\n        init_cfg=dict(type=\"Xavier\", layer=\"Conv2d\", distribution=\"uniform\"),\n    ):\n        super(FPNForBEVDet, self).__init__(init_cfg)\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)\n        self.num_outs = num_outs\n        self.relu_before_extra_convs = relu_before_extra_convs\n        self.no_norm_on_lateral = no_norm_on_lateral\n        self.fp16_enabled = False\n        self.upsample_cfg = upsample_cfg.copy()\n        self.out_ids = out_ids\n        if end_level == -1:\n            self.backbone_end_level = self.num_ins\n            # assert num_outs >= self.num_ins - start_level\n        else:\n            # if end_level < inputs, no extra level is allowed\n            self.backbone_end_level = end_level\n            assert end_level <= len(in_channels)\n            assert num_outs == end_level - start_level\n        self.start_level = start_level\n        self.end_level = end_level\n        self.add_extra_convs = add_extra_convs\n        assert isinstance(add_extra_convs, (str, bool))\n        if isinstance(add_extra_convs, str):\n            # Extra_convs_source choices: 'on_input', 'on_lateral', 'on_output'\n            assert add_extra_convs in (\"on_input\", \"on_lateral\", \"on_output\")\n        elif add_extra_convs:  # True\n            self.add_extra_convs = \"on_input\"\n\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n\n        for i in range(self.start_level, self.backbone_end_level):\n            l_conv = ConvModule(\n                in_channels[i],\n                out_channels,\n                1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg if not self.no_norm_on_lateral else None,\n                act_cfg=act_cfg,\n                inplace=False,\n            )\n\n            self.lateral_convs.append(l_conv)\n            if i in self.out_ids:\n                fpn_conv = ConvModule(\n                    out_channels,\n                    out_channels,\n                    3,\n                    padding=1,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    act_cfg=act_cfg,\n                    inplace=False,\n                )\n                self.fpn_convs.append(fpn_conv)\n\n        # add extra conv layers (e.g., RetinaNet)\n        extra_levels = num_outs - self.backbone_end_level + self.start_level\n        if self.add_extra_convs and extra_levels >= 1:\n            for i in range(extra_levels):\n                if i == 0 and self.add_extra_convs == \"on_input\":\n                    in_channels = self.in_channels[self.backbone_end_level - 1]\n                else:\n                    in_channels = out_channels\n                extra_fpn_conv = ConvModule(\n                    in_channels,\n                    out_channels,\n                    3,\n                    stride=2,\n                    padding=1,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    act_cfg=act_cfg,\n                    inplace=False,\n                )\n                self.fpn_convs.append(extra_fpn_conv)\n\n    @auto_fp16()\n    def forward(self, inputs):\n        \"\"\"Forward function.\"\"\"\n        assert len(inputs) == len(self.in_channels)\n\n        # build laterals\n        laterals = [\n            lateral_conv(inputs[i + self.start_level])\n            for i, lateral_conv in enumerate(self.lateral_convs)\n        ]\n\n        # build top-down path\n        used_backbone_levels = len(laterals)\n        for i in range(used_backbone_levels - 1, 0, -1):\n            # In some cases, fixing `scale factor` (e.g. 2) is preferred, but\n            #  it cannot co-exist with `size` in `F.interpolate`.\n            if \"scale_factor\" in self.upsample_cfg:\n                laterals[i - 1] += F.interpolate(laterals[i], **self.upsample_cfg)\n            else:\n                prev_shape = laterals[i - 1].shape[2:]\n                laterals[i - 1] += F.interpolate(\n                    laterals[i], size=prev_shape, **self.upsample_cfg\n                )\n\n        # build outputs\n        # part 1: from original levels\n        outs = [self.fpn_convs[i](laterals[i]) for i in self.out_ids]\n        # part 2: add extra levels\n        if self.num_outs > len(outs):\n            # use max pool to get more levels on top of outputs\n            # (e.g., Faster R-CNN, Mask R-CNN)\n            if not self.add_extra_convs:\n                for i in range(self.num_outs - used_backbone_levels):\n                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n            # add conv layers on top of original feature maps (RetinaNet)\n            else:\n                if self.add_extra_convs == \"on_input\":\n                    extra_source = inputs[self.backbone_end_level - 1]\n                elif self.add_extra_convs == \"on_lateral\":\n                    extra_source = laterals[-1]\n                elif self.add_extra_convs == \"on_output\":\n                    extra_source = outs[-1]\n                else:\n                    raise NotImplementedError\n                outs.append(self.fpn_convs[used_backbone_levels](extra_source))\n                for i in range(used_backbone_levels + 1, self.num_outs):\n                    if self.relu_before_extra_convs:\n                        outs.append(self.fpn_convs[i](F.relu(outs[-1])))\n                    else:\n                        outs.append(self.fpn_convs[i](outs[-1]))\n        return outs[0]\n"
        }
    ]
}