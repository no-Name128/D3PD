{
    "sourceFile": "mmdet3d/models/necks/view_transformer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 5,
            "patches": [
                {
                    "date": 1716034861098,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1716035019506,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -57,14 +57,14 @@\n         self.grid_config = grid_config\n         self.downsample = downsample\n         self.create_grid_infos(**grid_config)\n         self.sid = sid\n-        self.frustum = self.create_frustum(grid_config['depth'],\n-                                           input_size, downsample)\n+        self.frustum = self.create_frustum(grid_config[\"depth\"], input_size, downsample)\n         self.out_channels = out_channels\n         self.in_channels = in_channels\n         self.depth_net = nn.Conv2d(\n-            in_channels, self.D + self.out_channels, kernel_size=1, padding=0)\n+            in_channels, self.D + self.out_channels, kernel_size=1, padding=0\n+        )\n         self.accelerate = accelerate\n         self.initial_flag = True\n         self.collapse_z = collapse_z\n         self.with_depth_from_lidar = with_depth_from_lidar\n@@ -75,21 +75,22 @@\n                 nn.ReLU(True),\n                 nn.Conv2d(8, 32, 5, stride=4, padding=2),\n                 nn.BatchNorm2d(32),\n                 nn.ReLU(True),\n-                nn.Conv2d(32, 64, 5, stride=int(2 * self.downsample / 8),\n-                          padding=2),\n+                nn.Conv2d(32, 64, 5, stride=int(2 * self.downsample / 8), padding=2),\n                 nn.BatchNorm2d(64),\n-                nn.ReLU(True))\n+                nn.ReLU(True),\n+            )\n             out_channels = self.D + self.out_channels\n             self.depth_net = nn.Sequential(\n                 nn.Conv2d(in_channels + 64, in_channels, 3, padding=1),\n                 nn.BatchNorm2d(in_channels),\n                 nn.ReLU(True),\n                 nn.Conv2d(in_channels, in_channels, 3, padding=1),\n                 nn.BatchNorm2d(in_channels),\n                 nn.ReLU(True),\n-                nn.Conv2d(in_channels, out_channels, 1))\n+                nn.Conv2d(in_channels, out_channels, 1),\n+            )\n \n     def create_grid_infos(self, x, y, z, **kwargs):\n         \"\"\"Generate the grid information including the lower bound, interval,\n         and size.\n@@ -104,10 +105,9 @@\n             **kwargs: Container for other potential parameters\n         \"\"\"\n         self.grid_lower_bound = torch.Tensor([cfg[0] for cfg in [x, y, z]])\n         self.grid_interval = torch.Tensor([cfg[2] for cfg in [x, y, z]])\n-        self.grid_size = torch.Tensor([(cfg[1] - cfg[0]) / cfg[2]\n-                                       for cfg in [x, y, z]])\n+        self.grid_size = torch.Tensor([(cfg[1] - cfg[0]) / cfg[2] for cfg in [x, y, z]])\n \n     def create_frustum(self, depth_cfg, input_size, downsample):\n         \"\"\"Generate the frustum template for each image.\n \n@@ -120,27 +120,41 @@\n                 the feature size.\n         \"\"\"\n         H_in, W_in = input_size\n         H_feat, W_feat = H_in // downsample, W_in // downsample\n-        d = torch.arange(*depth_cfg, dtype=torch.float)\\\n-            .view(-1, 1, 1).expand(-1, H_feat, W_feat)\n+        d = (\n+            torch.arange(*depth_cfg, dtype=torch.float)\n+            .view(-1, 1, 1)\n+            .expand(-1, H_feat, W_feat)\n+        )\n         self.D = d.shape[0]\n         if self.sid:\n             d_sid = torch.arange(self.D).float()\n             depth_cfg_t = torch.tensor(depth_cfg).float()\n-            d_sid = torch.exp(torch.log(depth_cfg_t[0]) + d_sid / (self.D-1) *\n-                              torch.log((depth_cfg_t[1]-1) / depth_cfg_t[0]))\n+            d_sid = torch.exp(\n+                torch.log(depth_cfg_t[0])\n+                + d_sid\n+                / (self.D - 1)\n+                * torch.log((depth_cfg_t[1] - 1) / depth_cfg_t[0])\n+            )\n             d = d_sid.view(-1, 1, 1).expand(-1, H_feat, W_feat)\n-        x = torch.linspace(0, W_in - 1, W_feat,  dtype=torch.float)\\\n-            .view(1, 1, W_feat).expand(self.D, H_feat, W_feat)\n-        y = torch.linspace(0, H_in - 1, H_feat,  dtype=torch.float)\\\n-            .view(1, H_feat, 1).expand(self.D, H_feat, W_feat)\n+        x = (\n+            torch.linspace(0, W_in - 1, W_feat, dtype=torch.float)\n+            .view(1, 1, W_feat)\n+            .expand(self.D, H_feat, W_feat)\n+        )\n+        y = (\n+            torch.linspace(0, H_in - 1, H_feat, dtype=torch.float)\n+            .view(1, H_feat, 1)\n+            .expand(self.D, H_feat, W_feat)\n+        )\n \n         # D x H x W x 3\n         return torch.stack((x, y, d), -1)\n \n-    def get_lidar_coor(self, sensor2ego, ego2global, cam2imgs, post_rots, post_trans,\n-                       bda):\n+    def get_lidar_coor(\n+        self, sensor2ego, ego2global, cam2imgs, post_rots, post_trans, bda\n+    ):\n         \"\"\"Calculate the locations of the frustum points in the lidar\n         coordinate system.\n \n         Args:\n@@ -164,19 +178,27 @@\n \n         # post-transformation\n         # B x N x D x H x W x 3\n         points = self.frustum.to(sensor2ego) - post_trans.view(B, N, 1, 1, 1, 3)\n-        points = torch.inverse(post_rots).view(B, N, 1, 1, 1, 3, 3)\\\n+        points = (\n+            torch.inverse(post_rots)\n+            .view(B, N, 1, 1, 1, 3, 3)\n             .matmul(points.unsqueeze(-1))\n+        )\n \n         # cam_to_ego\n         points = torch.cat(\n-            (points[..., :2, :] * points[..., 2:3, :], points[..., 2:3, :]), 5)\n-        combine = sensor2ego[:,:,:3,:3].matmul(torch.inverse(cam2imgs))\n+            (points[..., :2, :] * points[..., 2:3, :], points[..., 2:3, :]), 5\n+        )\n+        combine = sensor2ego[:, :, :3, :3].matmul(torch.inverse(cam2imgs))\n         points = combine.view(B, N, 1, 1, 1, 3, 3).matmul(points).squeeze(-1)\n-        points += sensor2ego[:,:,:3, 3].view(B, N, 1, 1, 1, 3)\n-        points = bda[:, :3, :3].view(B, 1, 1, 1, 1, 3, 3).matmul(\n-            points.unsqueeze(-1)).squeeze(-1)\n+        points += sensor2ego[:, :, :3, 3].view(B, N, 1, 1, 1, 3)\n+        points = (\n+            bda[:, :3, :3]\n+            .view(B, 1, 1, 1, 1, 3, 3)\n+            .matmul(points.unsqueeze(-1))\n+            .squeeze(-1)\n+        )\n         points += bda[:, :3, 3].view(B, 1, 1, 1, 1, 3)\n         return points\n \n     def init_acceleration_v2(self, coor):\n@@ -189,40 +211,53 @@\n             x (torch.tensor): Feature of points in shape\n                 (B, N_cams, D, H, W, C).\n         \"\"\"\n \n-        ranks_bev, ranks_depth, ranks_feat, \\\n-            interval_starts, interval_lengths = \\\n+        ranks_bev, ranks_depth, ranks_feat, interval_starts, interval_lengths = (\n             self.voxel_pooling_prepare_v2(coor)\n+        )\n \n         self.ranks_bev = ranks_bev.int().contiguous()\n         self.ranks_feat = ranks_feat.int().contiguous()\n         self.ranks_depth = ranks_depth.int().contiguous()\n         self.interval_starts = interval_starts.int().contiguous()\n         self.interval_lengths = interval_lengths.int().contiguous()\n \n     def voxel_pooling_v2(self, coor, depth, feat):\n-        ranks_bev, ranks_depth, ranks_feat, \\\n-            interval_starts, interval_lengths = \\\n+        ranks_bev, ranks_depth, ranks_feat, interval_starts, interval_lengths = (\n             self.voxel_pooling_prepare_v2(coor)\n+        )\n         if ranks_feat is None:\n-            print('warning ---> no points within the predefined '\n-                  'bev receptive field')\n-            dummy = torch.zeros(size=[\n-                feat.shape[0], feat.shape[2],\n-                int(self.grid_size[2]),\n-                int(self.grid_size[0]),\n-                int(self.grid_size[1])\n-            ]).to(feat)\n+            print(\"warning ---> no points within the predefined \" \"bev receptive field\")\n+            dummy = torch.zeros(\n+                size=[\n+                    feat.shape[0],\n+                    feat.shape[2],\n+                    int(self.grid_size[2]),\n+                    int(self.grid_size[0]),\n+                    int(self.grid_size[1]),\n+                ]\n+            ).to(feat)\n             dummy = torch.cat(dummy.unbind(dim=2), 1)\n             return dummy\n         feat = feat.permute(0, 1, 3, 4, 2)\n-        bev_feat_shape = (depth.shape[0], int(self.grid_size[2]),\n-                          int(self.grid_size[1]), int(self.grid_size[0]),\n-                          feat.shape[-1])  # (B, Z, Y, X, C)\n-        bev_feat = bev_pool_v2(depth, feat, ranks_depth, ranks_feat, ranks_bev,\n-                               bev_feat_shape, interval_starts,\n-                               interval_lengths)\n+        bev_feat_shape = (\n+            depth.shape[0],\n+            int(self.grid_size[2]),\n+            int(self.grid_size[1]),\n+            int(self.grid_size[0]),\n+            feat.shape[-1],\n+        )  # (B, Z, Y, X, C)\n+        bev_feat = bev_pool_v2(\n+            depth,\n+            feat,\n+            ranks_depth,\n+            ranks_feat,\n+            ranks_bev,\n+            bev_feat_shape,\n+            interval_starts,\n+            interval_lengths,\n+        )\n         # collapse Z\n         if self.collapse_z:\n             bev_feat = torch.cat(bev_feat.unbind(dim=2), 1)\n         return bev_feat\n@@ -243,50 +278,67 @@\n         B, N, D, H, W, _ = coor.shape\n         num_points = B * N * D * H * W\n         # record the index of selected points for acceleration purpose\n         ranks_depth = torch.range(\n-            0, num_points - 1, dtype=torch.int, device=coor.device)\n+            0, num_points - 1, dtype=torch.int, device=coor.device\n+        )\n         ranks_feat = torch.range(\n-            0, num_points // D - 1, dtype=torch.int, device=coor.device)\n+            0, num_points // D - 1, dtype=torch.int, device=coor.device\n+        )\n         ranks_feat = ranks_feat.reshape(B, N, 1, H, W)\n         ranks_feat = ranks_feat.expand(B, N, D, H, W).flatten()\n         # convert coordinate into the voxel space\n-        coor = ((coor - self.grid_lower_bound.to(coor)) /\n-                self.grid_interval.to(coor))\n+        coor = (coor - self.grid_lower_bound.to(coor)) / self.grid_interval.to(coor)\n         coor = coor.long().view(num_points, 3)\n-        batch_idx = torch.range(0, B - 1).reshape(B, 1). \\\n-            expand(B, num_points // B).reshape(num_points, 1).to(coor)\n+        batch_idx = (\n+            torch.range(0, B - 1)\n+            .reshape(B, 1)\n+            .expand(B, num_points // B)\n+            .reshape(num_points, 1)\n+            .to(coor)\n+        )\n         coor = torch.cat((coor, batch_idx), 1)\n \n         # filter out points that are outside box\n-        kept = (coor[:, 0] >= 0) & (coor[:, 0] < self.grid_size[0]) & \\\n-               (coor[:, 1] >= 0) & (coor[:, 1] < self.grid_size[1]) & \\\n-               (coor[:, 2] >= 0) & (coor[:, 2] < self.grid_size[2])\n+        kept = (\n+            (coor[:, 0] >= 0)\n+            & (coor[:, 0] < self.grid_size[0])\n+            & (coor[:, 1] >= 0)\n+            & (coor[:, 1] < self.grid_size[1])\n+            & (coor[:, 2] >= 0)\n+            & (coor[:, 2] < self.grid_size[2])\n+        )\n         if len(kept) == 0:\n             return None, None, None, None, None\n-        coor, ranks_depth, ranks_feat = \\\n-            coor[kept], ranks_depth[kept], ranks_feat[kept]\n+        coor, ranks_depth, ranks_feat = coor[kept], ranks_depth[kept], ranks_feat[kept]\n         # get tensors from the same voxel next to each other\n         ranks_bev = coor[:, 3] * (\n-            self.grid_size[2] * self.grid_size[1] * self.grid_size[0])\n+            self.grid_size[2] * self.grid_size[1] * self.grid_size[0]\n+        )\n         ranks_bev += coor[:, 2] * (self.grid_size[1] * self.grid_size[0])\n         ranks_bev += coor[:, 1] * self.grid_size[0] + coor[:, 0]\n         order = ranks_bev.argsort()\n-        ranks_bev, ranks_depth, ranks_feat = \\\n-            ranks_bev[order], ranks_depth[order], ranks_feat[order]\n+        ranks_bev, ranks_depth, ranks_feat = (\n+            ranks_bev[order],\n+            ranks_depth[order],\n+            ranks_feat[order],\n+        )\n \n-        kept = torch.ones(\n-            ranks_bev.shape[0], device=ranks_bev.device, dtype=torch.bool)\n+        kept = torch.ones(ranks_bev.shape[0], device=ranks_bev.device, dtype=torch.bool)\n         kept[1:] = ranks_bev[1:] != ranks_bev[:-1]\n         interval_starts = torch.where(kept)[0].int()\n         if len(interval_starts) == 0:\n             return None, None, None, None, None\n         interval_lengths = torch.zeros_like(interval_starts)\n         interval_lengths[:-1] = interval_starts[1:] - interval_starts[:-1]\n         interval_lengths[-1] = ranks_bev.shape[0] - interval_starts[-1]\n-        return ranks_bev.int().contiguous(), ranks_depth.int().contiguous(\n-        ), ranks_feat.int().contiguous(), interval_starts.int().contiguous(\n-        ), interval_lengths.int().contiguous()\n+        return (\n+            ranks_bev.int().contiguous(),\n+            ranks_depth.int().contiguous(),\n+            ranks_feat.int().contiguous(),\n+            interval_starts.int().contiguous(),\n+            interval_lengths.int().contiguous(),\n+        )\n \n     def pre_compute(self, input):\n         if self.initial_flag:\n             coor = self.get_lidar_coor(*input[1:7])\n@@ -300,27 +352,39 @@\n         if self.accelerate:\n             feat = tran_feat.view(B, N, self.out_channels, H, W)\n             feat = feat.permute(0, 1, 3, 4, 2)\n             depth = depth.view(B, N, self.D, H, W)\n-            bev_feat_shape = (depth.shape[0], int(self.grid_size[2]),\n-                              int(self.grid_size[1]), int(self.grid_size[0]),\n-                              feat.shape[-1])  # (B, Z, Y, X, C)\n-            bev_feat = bev_pool_v2(depth, feat, self.ranks_depth,\n-                                   self.ranks_feat, self.ranks_bev,\n-                                   bev_feat_shape, self.interval_starts,\n-                                   self.interval_lengths)\n+            bev_feat_shape = (\n+                depth.shape[0],\n+                int(self.grid_size[2]),\n+                int(self.grid_size[1]),\n+                int(self.grid_size[0]),\n+                feat.shape[-1],\n+            )  # (B, Z, Y, X, C)\n+            bev_feat = bev_pool_v2(\n+                depth,\n+                feat,\n+                self.ranks_depth,\n+                self.ranks_feat,\n+                self.ranks_bev,\n+                bev_feat_shape,\n+                self.interval_starts,\n+                self.interval_lengths,\n+            )\n \n             bev_feat = bev_feat.squeeze(2)\n         else:\n             coor = self.get_lidar_coor(*input[1:7])\n             bev_feat = self.voxel_pooling_v2(\n-                coor, depth.view(B, N, self.D, H, W),\n-                tran_feat.view(B, N, self.out_channels, H, W))\n+                coor,\n+                depth.view(B, N, self.D, H, W),\n+                tran_feat.view(B, N, self.out_channels, H, W),\n+            )\n         return bev_feat, depth\n \n     def view_transform(self, input, depth, tran_feat):\n         for shape_id in range(3):\n-            assert depth.shape[shape_id+1] == self.frustum.shape[shape_id]\n+            assert depth.shape[shape_id + 1] == self.frustum.shape[shape_id]\n         if self.accelerate:\n             self.pre_compute(input)\n         return self.view_transform_core(input, depth, tran_feat)\n \n@@ -346,14 +410,14 @@\n             depth_from_lidar = depth_from_lidar.view(B * N, 1, h_img, w_img)\n             depth_from_lidar = self.lidar_input_net(depth_from_lidar)\n             x = torch.cat([x, depth_from_lidar], dim=1)\n         if self.with_cp:\n-            x =checkpoint(self.depth_net, x)\n+            x = checkpoint(self.depth_net, x)\n         else:\n             x = self.depth_net(x)\n \n-        depth_digit = x[:, :self.D, ...]\n-        tran_feat = x[:, self.D:self.D + self.out_channels, ...]\n+        depth_digit = x[:, : self.D, ...]\n+        tran_feat = x[:, self.D : self.D + self.out_channels, ...]\n         depth = depth_digit.softmax(dim=1)\n         return self.view_transform(input, depth, tran_feat)\n \n     def get_mlp_input(self, rot, tran, intrin, post_rot, post_tran, bda):\n@@ -361,19 +425,19 @@\n \n \n class _ASPPModule(nn.Module):\n \n-    def __init__(self, inplanes, planes, kernel_size, padding, dilation,\n-                 BatchNorm):\n+    def __init__(self, inplanes, planes, kernel_size, padding, dilation, BatchNorm):\n         super(_ASPPModule, self).__init__()\n         self.atrous_conv = nn.Conv2d(\n             inplanes,\n             planes,\n             kernel_size=kernel_size,\n             stride=1,\n             padding=padding,\n             dilation=dilation,\n-            bias=False)\n+            bias=False,\n+        )\n         self.bn = BatchNorm(planes)\n         self.relu = nn.ReLU()\n \n         self._init_weight()\n@@ -405,39 +469,42 @@\n             mid_channels,\n             1,\n             padding=0,\n             dilation=dilations[0],\n-            BatchNorm=BatchNorm)\n+            BatchNorm=BatchNorm,\n+        )\n         self.aspp2 = _ASPPModule(\n             inplanes,\n             mid_channels,\n             3,\n             padding=dilations[1],\n             dilation=dilations[1],\n-            BatchNorm=BatchNorm)\n+            BatchNorm=BatchNorm,\n+        )\n         self.aspp3 = _ASPPModule(\n             inplanes,\n             mid_channels,\n             3,\n             padding=dilations[2],\n             dilation=dilations[2],\n-            BatchNorm=BatchNorm)\n+            BatchNorm=BatchNorm,\n+        )\n         self.aspp4 = _ASPPModule(\n             inplanes,\n             mid_channels,\n             3,\n             padding=dilations[3],\n             dilation=dilations[3],\n-            BatchNorm=BatchNorm)\n+            BatchNorm=BatchNorm,\n+        )\n \n         self.global_avg_pool = nn.Sequential(\n             nn.AdaptiveAvgPool2d((1, 1)),\n             nn.Conv2d(inplanes, mid_channels, 1, stride=1, bias=False),\n             BatchNorm(mid_channels),\n             nn.ReLU(),\n         )\n-        self.conv1 = nn.Conv2d(\n-            int(mid_channels * 5), inplanes, 1, bias=False)\n+        self.conv1 = nn.Conv2d(int(mid_channels * 5), inplanes, 1, bias=False)\n         self.bn1 = BatchNorm(inplanes)\n         self.relu = nn.ReLU()\n         self.dropout = nn.Dropout(0.5)\n         self._init_weight()\n@@ -447,10 +514,9 @@\n         x2 = self.aspp2(x)\n         x3 = self.aspp3(x)\n         x4 = self.aspp4(x)\n         x5 = self.global_avg_pool(x)\n-        x5 = F.interpolate(\n-            x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n+        x5 = F.interpolate(x5, size=x4.size()[2:], mode=\"bilinear\", align_corners=True)\n         x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n \n         x = self.conv1(x)\n         x = self.bn1(x)\n@@ -468,14 +534,16 @@\n \n \n class Mlp(nn.Module):\n \n-    def __init__(self,\n-                 in_features,\n-                 hidden_features=None,\n-                 out_features=None,\n-                 act_layer=nn.ReLU,\n-                 drop=0.0):\n+    def __init__(\n+        self,\n+        in_features,\n+        hidden_features=None,\n+        out_features=None,\n+        act_layer=nn.ReLU,\n+        drop=0.0,\n+    ):\n         super().__init__()\n         out_features = out_features or in_features\n         hidden_features = hidden_features or in_features\n         self.fc1 = nn.Linear(in_features, hidden_features)\n@@ -510,28 +578,30 @@\n \n \n class DepthNet(nn.Module):\n \n-    def __init__(self,\n-                 in_channels,\n-                 mid_channels,\n-                 context_channels,\n-                 depth_channels,\n-                 use_dcn=True,\n-                 use_aspp=True,\n-                 with_cp=False,\n-                 stereo=False,\n-                 bias=0.0,\n-                 aspp_mid_channels=-1):\n+    def __init__(\n+        self,\n+        in_channels,\n+        mid_channels,\n+        context_channels,\n+        depth_channels,\n+        use_dcn=True,\n+        use_aspp=True,\n+        with_cp=False,\n+        stereo=False,\n+        bias=0.0,\n+        aspp_mid_channels=-1,\n+    ):\n         super(DepthNet, self).__init__()\n         self.reduce_conv = nn.Sequential(\n-            nn.Conv2d(\n-                in_channels, mid_channels, kernel_size=3, stride=1, padding=1),\n+            nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1),\n             nn.BatchNorm2d(mid_channels),\n             nn.ReLU(inplace=True),\n         )\n         self.context_conv = nn.Conv2d(\n-            mid_channels, context_channels, kernel_size=1, stride=1, padding=0)\n+            mid_channels, context_channels, kernel_size=1, stride=1, padding=0\n+        )\n         self.bn = nn.BatchNorm1d(27)\n         self.depth_mlp = Mlp(27, mid_channels, mid_channels)\n         self.depth_se = SELayer(mid_channels)  # NOTE: add camera-aware\n         self.context_mlp = Mlp(27, mid_channels, mid_channels)\n@@ -540,70 +610,84 @@\n         downsample = None\n \n         if stereo:\n             depth_conv_input_channels += depth_channels\n-            downsample = nn.Conv2d(depth_conv_input_channels,\n-                                    mid_channels, 1, 1, 0)\n+            downsample = nn.Conv2d(depth_conv_input_channels, mid_channels, 1, 1, 0)\n             cost_volumn_net = []\n             for stage in range(int(2)):\n-                cost_volumn_net.extend([\n-                    nn.Conv2d(depth_channels, depth_channels, kernel_size=3,\n-                              stride=2, padding=1),\n-                    nn.BatchNorm2d(depth_channels)])\n+                cost_volumn_net.extend(\n+                    [\n+                        nn.Conv2d(\n+                            depth_channels,\n+                            depth_channels,\n+                            kernel_size=3,\n+                            stride=2,\n+                            padding=1,\n+                        ),\n+                        nn.BatchNorm2d(depth_channels),\n+                    ]\n+                )\n             self.cost_volumn_net = nn.Sequential(*cost_volumn_net)\n             self.bias = bias\n-        depth_conv_list = [BasicBlock(depth_conv_input_channels, mid_channels,\n-                                      downsample=downsample),\n-                           BasicBlock(mid_channels, mid_channels),\n-                           BasicBlock(mid_channels, mid_channels)]\n+        depth_conv_list = [\n+            BasicBlock(depth_conv_input_channels, mid_channels, downsample=downsample),\n+            BasicBlock(mid_channels, mid_channels),\n+            BasicBlock(mid_channels, mid_channels),\n+        ]\n         if use_aspp:\n-            if aspp_mid_channels<0:\n+            if aspp_mid_channels < 0:\n                 aspp_mid_channels = mid_channels\n             depth_conv_list.append(ASPP(mid_channels, aspp_mid_channels))\n         if use_dcn:\n             depth_conv_list.append(\n                 build_conv_layer(\n                     cfg=dict(\n-                        type='DCN',\n+                        type=\"DCN\",\n                         in_channels=mid_channels,\n                         out_channels=mid_channels,\n                         kernel_size=3,\n                         padding=1,\n                         groups=4,\n                         im2col_step=128,\n-                    )))\n+                    )\n+                )\n+            )\n         depth_conv_list.append(\n-            nn.Conv2d(\n-                mid_channels,\n-                depth_channels,\n-                kernel_size=1,\n-                stride=1,\n-                padding=0))\n+            nn.Conv2d(mid_channels, depth_channels, kernel_size=1, stride=1, padding=0)\n+        )\n         self.depth_conv = nn.Sequential(*depth_conv_list)\n         self.with_cp = with_cp\n         self.depth_channels = depth_channels\n \n     def gen_grid(self, metas, B, N, D, H, W, hi, wi):\n-        frustum = metas['frustum']\n-        points = frustum - metas['post_trans'].view(B, N, 1, 1, 1, 3)\n-        points = torch.inverse(metas['post_rots']).view(B, N, 1, 1, 1, 3, 3) \\\n+        frustum = metas[\"frustum\"]\n+        points = frustum - metas[\"post_trans\"].view(B, N, 1, 1, 1, 3)\n+        points = (\n+            torch.inverse(metas[\"post_rots\"])\n+            .view(B, N, 1, 1, 1, 3, 3)\n             .matmul(points.unsqueeze(-1))\n+        )\n         points = torch.cat(\n-            (points[..., :2, :] * points[..., 2:3, :], points[..., 2:3, :]), 5)\n+            (points[..., :2, :] * points[..., 2:3, :], points[..., 2:3, :]), 5\n+        )\n \n-        rots = metas['k2s_sensor'][:, :, :3, :3].contiguous()\n-        trans = metas['k2s_sensor'][:, :, :3, 3].contiguous()\n-        combine = rots.matmul(torch.inverse(metas['intrins']))\n+        rots = metas[\"k2s_sensor\"][:, :, :3, :3].contiguous()\n+        trans = metas[\"k2s_sensor\"][:, :, :3, 3].contiguous()\n+        combine = rots.matmul(torch.inverse(metas[\"intrins\"]))\n \n         points = combine.view(B, N, 1, 1, 1, 3, 3).matmul(points)\n         points += trans.view(B, N, 1, 1, 1, 3, 1)\n         neg_mask = points[..., 2, 0] < 1e-3\n-        points = metas['intrins'].view(B, N, 1, 1, 1, 3, 3).matmul(points)\n+        points = metas[\"intrins\"].view(B, N, 1, 1, 1, 3, 3).matmul(points)\n         points = points[..., :2, :] / points[..., 2:3, :]\n \n-        points = metas['post_rots'][...,:2,:2].view(B, N, 1, 1, 1, 2, 2).matmul(\n-            points).squeeze(-1)\n-        points += metas['post_trans'][...,:2].view(B, N, 1, 1, 1, 2)\n+        points = (\n+            metas[\"post_rots\"][..., :2, :2]\n+            .view(B, N, 1, 1, 1, 2, 2)\n+            .matmul(points)\n+            .squeeze(-1)\n+        )\n+        points += metas[\"post_trans\"][..., :2].view(B, N, 1, 1, 1, 2)\n \n         px = points[..., 0] / (wi - 1.0) * 2.0 - 1.0\n         py = points[..., 1] / (hi - 1.0) * 2.0 - 1.0\n         px[neg_mask] = -2\n@@ -612,34 +696,33 @@\n         grid = grid.view(B * N, D * H, W, 2)\n         return grid\n \n     def calculate_cost_volumn(self, metas):\n-        prev, curr = metas['cv_feat_list']\n+        prev, curr = metas[\"cv_feat_list\"]\n         group_size = 4\n         _, c, hf, wf = curr.shape\n         hi, wi = hf * 4, wf * 4\n-        B, N, _ = metas['post_trans'].shape\n-        D, H, W, _ = metas['frustum'].shape\n+        B, N, _ = metas[\"post_trans\"].shape\n+        D, H, W, _ = metas[\"frustum\"].shape\n         grid = self.gen_grid(metas, B, N, D, H, W, hi, wi).to(curr.dtype)\n \n         prev = prev.view(B * N, -1, H, W)\n         curr = curr.view(B * N, -1, H, W)\n         cost_volumn = 0\n         # process in group wise to save memory\n         for fid in range(curr.shape[1] // group_size):\n-            prev_curr = prev[:, fid * group_size:(fid + 1) * group_size, ...]\n-            wrap_prev = F.grid_sample(prev_curr, grid,\n-                                      align_corners=True,\n-                                      padding_mode='zeros')\n-            curr_tmp = curr[:, fid * group_size:(fid + 1) * group_size, ...]\n-            cost_volumn_tmp = curr_tmp.unsqueeze(2) - \\\n-                              wrap_prev.view(B * N, -1, D, H, W)\n+            prev_curr = prev[:, fid * group_size : (fid + 1) * group_size, ...]\n+            wrap_prev = F.grid_sample(\n+                prev_curr, grid, align_corners=True, padding_mode=\"zeros\"\n+            )\n+            curr_tmp = curr[:, fid * group_size : (fid + 1) * group_size, ...]\n+            cost_volumn_tmp = curr_tmp.unsqueeze(2) - wrap_prev.view(B * N, -1, D, H, W)\n             cost_volumn_tmp = cost_volumn_tmp.abs().sum(dim=1)\n             cost_volumn += cost_volumn_tmp\n         if not self.bias == 0:\n             invalid = wrap_prev[:, 0, ...].view(B * N, D, H, W) == 0\n             cost_volumn[invalid] = cost_volumn[invalid] + self.bias\n-        cost_volumn = - cost_volumn\n+        cost_volumn = -cost_volumn\n         cost_volumn = cost_volumn.softmax(dim=1)\n         return cost_volumn\n \n     def forward(self, x, mlp_input, stereo_metas=None):\n@@ -651,16 +734,21 @@\n         depth_se = self.depth_mlp(mlp_input)[..., None, None]\n         depth = self.depth_se(x, depth_se)\n \n         if not stereo_metas is None:\n-            if stereo_metas['cv_feat_list'][0] is None:\n+            if stereo_metas[\"cv_feat_list\"][0] is None:\n                 BN, _, H, W = x.shape\n-                scale_factor = float(stereo_metas['downsample'])/\\\n-                               stereo_metas['cv_downsample']\n-                cost_volumn = \\\n-                    torch.zeros((BN, self.depth_channels,\n-                                 int(H*scale_factor),\n-                                 int(W*scale_factor))).to(x)\n+                scale_factor = (\n+                    float(stereo_metas[\"downsample\"]) / stereo_metas[\"cv_downsample\"]\n+                )\n+                cost_volumn = torch.zeros(\n+                    (\n+                        BN,\n+                        self.depth_channels,\n+                        int(H * scale_factor),\n+                        int(W * scale_factor),\n+                    )\n+                ).to(x)\n             else:\n                 with torch.no_grad():\n                     cost_volumn = self.calculate_cost_volumn(stereo_metas)\n             cost_volumn = self.cost_volumn_net(cost_volumn)\n@@ -684,9 +772,10 @@\n                 mid_channels,\n                 kernel_size=3,\n                 stride=1,\n                 padding=1,\n-                bias=False),\n+                bias=False,\n+            ),\n             nn.BatchNorm2d(mid_channels),\n             nn.ReLU(inplace=True),\n         )\n \n@@ -696,18 +785,20 @@\n                 mid_channels,\n                 kernel_size=3,\n                 stride=1,\n                 padding=1,\n-                bias=False),\n+                bias=False,\n+            ),\n             nn.BatchNorm2d(mid_channels),\n             nn.ReLU(inplace=True),\n             nn.Conv2d(\n                 mid_channels,\n                 mid_channels,\n                 kernel_size=3,\n                 stride=1,\n                 padding=1,\n-                bias=False),\n+                bias=False,\n+            ),\n             nn.BatchNorm2d(mid_channels),\n             nn.ReLU(inplace=True),\n         )\n \n@@ -717,9 +808,10 @@\n                 out_channels,\n                 kernel_size=3,\n                 stride=1,\n                 padding=1,\n-                bias=True),\n+                bias=True,\n+            ),\n             # nn.BatchNorm3d(out_channels),\n             # nn.ReLU(inplace=True),\n         )\n \n@@ -738,31 +830,40 @@\n \n     def __init__(self, loss_depth_weight=3.0, depthnet_cfg=dict(), **kwargs):\n         super(LSSViewTransformerBEVDepth, self).__init__(**kwargs)\n         self.loss_depth_weight = loss_depth_weight\n-        self.depth_net = DepthNet(self.in_channels, self.in_channels,\n-                                  self.out_channels, self.D, **depthnet_cfg)\n+        self.depth_net = DepthNet(\n+            self.in_channels,\n+            self.in_channels,\n+            self.out_channels,\n+            self.D,\n+            **depthnet_cfg\n+        )\n \n     def get_mlp_input(self, sensor2ego, ego2global, intrin, post_rot, post_tran, bda):\n         B, N, _, _ = sensor2ego.shape\n         bda = bda.view(B, 1, 4, 4).repeat(1, N, 1, 1)\n-        mlp_input = torch.stack([\n-            intrin[:, :, 0, 0],\n-            intrin[:, :, 1, 1],\n-            intrin[:, :, 0, 2],\n-            intrin[:, :, 1, 2],\n-            post_rot[:, :, 0, 0],\n-            post_rot[:, :, 0, 1],\n-            post_tran[:, :, 0],\n-            post_rot[:, :, 1, 0],\n-            post_rot[:, :, 1, 1],\n-            post_tran[:, :, 1],\n-            bda[:, :, 0, 0],\n-            bda[:, :, 0, 1],\n-            bda[:, :, 1, 0],\n-            bda[:, :, 1, 1],\n-            bda[:, :, 2, 2],], dim=-1)\n-        sensor2ego = sensor2ego[:,:,:3,:].reshape(B, N, -1)\n+        mlp_input = torch.stack(\n+            [\n+                intrin[:, :, 0, 0],\n+                intrin[:, :, 1, 1],\n+                intrin[:, :, 0, 2],\n+                intrin[:, :, 1, 2],\n+                post_rot[:, :, 0, 0],\n+                post_rot[:, :, 0, 1],\n+                post_tran[:, :, 0],\n+                post_rot[:, :, 1, 0],\n+                post_rot[:, :, 1, 1],\n+                post_tran[:, :, 1],\n+                bda[:, :, 0, 0],\n+                bda[:, :, 0, 1],\n+                bda[:, :, 1, 0],\n+                bda[:, :, 1, 1],\n+                bda[:, :, 2, 2],\n+            ],\n+            dim=-1,\n+        )\n+        sensor2ego = sensor2ego[:, :, :3, :].reshape(B, N, -1)\n         mlp_input = torch.cat([mlp_input, sensor2ego], dim=-1)\n         return mlp_input\n \n     def get_downsampled_gt_depth(self, gt_depths):\n@@ -772,72 +873,84 @@\n         Output:\n             gt_depths: [B*N*h*w, d]\n         \"\"\"\n         B, N, H, W = gt_depths.shape\n-        gt_depths = gt_depths.view(B * N, H // self.downsample,\n-                                   self.downsample, W // self.downsample,\n-                                   self.downsample, 1)\n+        gt_depths = gt_depths.view(\n+            B * N,\n+            H // self.downsample,\n+            self.downsample,\n+            W // self.downsample,\n+            self.downsample,\n+            1,\n+        )\n         gt_depths = gt_depths.permute(0, 1, 3, 5, 2, 4).contiguous()\n         gt_depths = gt_depths.view(-1, self.downsample * self.downsample)\n-        gt_depths_tmp = torch.where(gt_depths == 0.0,\n-                                    1e5 * torch.ones_like(gt_depths),\n-                                    gt_depths)\n+        gt_depths_tmp = torch.where(\n+            gt_depths == 0.0, 1e5 * torch.ones_like(gt_depths), gt_depths\n+        )\n         gt_depths = torch.min(gt_depths_tmp, dim=-1).values\n-        gt_depths = gt_depths.view(B * N, H // self.downsample,\n-                                   W // self.downsample)\n+        gt_depths = gt_depths.view(B * N, H // self.downsample, W // self.downsample)\n \n         if not self.sid:\n-            gt_depths = (gt_depths - (self.grid_config['depth'][0] -\n-                                      self.grid_config['depth'][2])) / \\\n-                        self.grid_config['depth'][2]\n+            gt_depths = (\n+                gt_depths\n+                - (self.grid_config[\"depth\"][0] - self.grid_config[\"depth\"][2])\n+            ) / self.grid_config[\"depth\"][2]\n         else:\n             gt_depths = torch.log(gt_depths) - torch.log(\n-                torch.tensor(self.grid_config['depth'][0]).float())\n-            gt_depths = gt_depths * (self.D - 1) / torch.log(\n-                torch.tensor(self.grid_config['depth'][1] - 1.).float() /\n-                self.grid_config['depth'][0])\n-            gt_depths = gt_depths + 1.\n-        gt_depths = torch.where((gt_depths < self.D + 1) & (gt_depths >= 0.0),\n-                                gt_depths, torch.zeros_like(gt_depths))\n-        gt_depths = F.one_hot(\n-            gt_depths.long(), num_classes=self.D + 1).view(-1, self.D + 1)[:,\n-                                                                           1:]\n+                torch.tensor(self.grid_config[\"depth\"][0]).float()\n+            )\n+            gt_depths = (\n+                gt_depths\n+                * (self.D - 1)\n+                / torch.log(\n+                    torch.tensor(self.grid_config[\"depth\"][1] - 1.0).float()\n+                    / self.grid_config[\"depth\"][0]\n+                )\n+            )\n+            gt_depths = gt_depths + 1.0\n+        gt_depths = torch.where(\n+            (gt_depths < self.D + 1) & (gt_depths >= 0.0),\n+            gt_depths,\n+            torch.zeros_like(gt_depths),\n+        )\n+        gt_depths = F.one_hot(gt_depths.long(), num_classes=self.D + 1).view(\n+            -1, self.D + 1\n+        )[:, 1:]\n         return gt_depths.float()\n \n     @force_fp32()\n     def get_depth_loss(self, depth_labels, depth_preds):\n         depth_labels = self.get_downsampled_gt_depth(depth_labels)\n-        depth_preds = depth_preds.permute(0, 2, 3,\n-                                          1).contiguous().view(-1, self.D)\n+        depth_preds = depth_preds.permute(0, 2, 3, 1).contiguous().view(-1, self.D)\n         fg_mask = torch.max(depth_labels, dim=1).values > 0.0\n         depth_labels = depth_labels[fg_mask]\n         depth_preds = depth_preds[fg_mask]\n         with autocast(enabled=False):\n             depth_loss = F.binary_cross_entropy(\n                 depth_preds,\n                 depth_labels,\n-                reduction='none',\n+                reduction=\"none\",\n             ).sum() / max(1.0, fg_mask.sum())\n         return self.loss_depth_weight * depth_loss\n \n     def forward(self, input, stereo_metas=None):\n-        (x, rots, trans, intrins, post_rots, post_trans, bda,\n-         mlp_input) = input[:8]\n+        (x, rots, trans, intrins, post_rots, post_trans, bda, mlp_input) = input[:8]\n \n         B, N, C, H, W = x.shape\n         x = x.view(B * N, C, H, W)\n         x = self.depth_net(x, mlp_input, stereo_metas)\n-        depth_digit = x[:, :self.D, ...]\n-        tran_feat = x[:, self.D:self.D + self.out_channels, ...]\n+        depth_digit = x[:, : self.D, ...]\n+        tran_feat = x[:, self.D : self.D + self.out_channels, ...]\n         depth = depth_digit.softmax(dim=1)\n         bev_feat, depth = self.view_transform(input, depth, tran_feat)\n         return bev_feat, depth\n \n \n @NECKS.register_module()\n class LSSViewTransformerBEVStereo(LSSViewTransformerBEVDepth):\n \n-    def __init__(self,  **kwargs):\n+    def __init__(self, **kwargs):\n         super(LSSViewTransformerBEVStereo, self).__init__(**kwargs)\n-        self.cv_frustum = self.create_frustum(kwargs['grid_config']['depth'],\n-                                              kwargs['input_size'],\n-                                              downsample=4)\n+        self.cv_frustum = self.create_frustum(\n+            kwargs[\"grid_config\"][\"depth\"], kwargs[\"input_size\"], downsample=4\n+        )\n"
                },
                {
                    "date": 1716644697648,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -840,8 +840,10 @@\n         )\n \n     def get_mlp_input(self, sensor2ego, ego2global, intrin, post_rot, post_tran, bda):\n         B, N, _, _ = sensor2ego.shape\n+        import pdb\n+        pdb.set_trace()\n         bda = bda.view(B, 1, 4, 4).repeat(1, N, 1, 1)\n         mlp_input = torch.stack(\n             [\n                 intrin[:, :, 0, 0],\n"
                },
                {
                    "date": 1716644778125,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -840,10 +840,8 @@\n         )\n \n     def get_mlp_input(self, sensor2ego, ego2global, intrin, post_rot, post_tran, bda):\n         B, N, _, _ = sensor2ego.shape\n-        import pdb\n-        pdb.set_trace()\n         bda = bda.view(B, 1, 4, 4).repeat(1, N, 1, 1)\n         mlp_input = torch.stack(\n             [\n                 intrin[:, :, 0, 0],\n"
                },
                {
                    "date": 1734396104117,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -385,9 +385,9 @@\n         for shape_id in range(3):\n             assert depth.shape[shape_id + 1] == self.frustum.shape[shape_id]\n         if self.accelerate:\n             self.pre_compute(input)\n-        return self.view_transform_core(input, depth, tran_feat)\n+        return self.view_transform_core(input, depth, tran_\\feat)\n \n     def forward(self, input, depth_from_lidar=None):\n         \"\"\"Transform image-view feature into bird-eye-view feature.\n \n"
                },
                {
                    "date": 1734396130136,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -385,9 +385,9 @@\n         for shape_id in range(3):\n             assert depth.shape[shape_id + 1] == self.frustum.shape[shape_id]\n         if self.accelerate:\n             self.pre_compute(input)\n-        return self.view_transform_core(input, depth, tran_\\feat)\n+        return self.view_transform_core(input, depth, tran_feat)\n \n     def forward(self, input, depth_from_lidar=None):\n         \"\"\"Transform image-view feature into bird-eye-view feature.\n \n"
                }
            ],
            "date": 1716034861098,
            "name": "Commit-0",
            "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import build_conv_layer\nfrom mmcv.runner import BaseModule, force_fp32\nfrom torch.cuda.amp.autocast_mode import autocast\nfrom torch.utils.checkpoint import checkpoint\n\nfrom mmdet3d.ops.bev_pool_v2.bev_pool import bev_pool_v2\nfrom mmdet.models.backbones.resnet import BasicBlock\nfrom ..builder import NECKS\n\nfrom torch.utils.checkpoint import checkpoint\n\n\n@NECKS.register_module()\nclass LSSViewTransformer(BaseModule):\n    r\"\"\"Lift-Splat-Shoot view transformer with BEVPoolv2 implementation.\n\n    Please refer to the `paper <https://arxiv.org/abs/2008.05711>`_ and\n        `paper <https://arxiv.org/abs/2211.17111>`\n\n    Args:\n        grid_config (dict): Config of grid alone each axis in format of\n            (lower_bound, upper_bound, interval). axis in {x,y,z,depth}.\n        input_size (tuple(int)): Size of input images in format of (height,\n            width).\n        downsample (int): Down sample factor from the input size to the feature\n            size.\n        in_channels (int): Channels of input feature.\n        out_channels (int): Channels of transformed feature.\n        accelerate (bool): Whether the view transformation is conducted with\n            acceleration. Note: the intrinsic and extrinsic of cameras should\n            be constant when 'accelerate' is set true.\n        sid (bool): Whether to use Spacing Increasing Discretization (SID)\n            depth distribution as `STS: Surround-view Temporal Stereo for\n            Multi-view 3D Detection`.\n        collapse_z (bool): Whether to collapse in z direction.\n    \"\"\"\n\n    def __init__(\n        self,\n        grid_config,\n        input_size,\n        downsample=16,\n        in_channels=512,\n        out_channels=64,\n        accelerate=False,\n        sid=False,\n        collapse_z=True,\n        with_cp=False,\n        with_depth_from_lidar=False,\n    ):\n        super(LSSViewTransformer, self).__init__()\n        self.with_cp = with_cp\n        self.grid_config = grid_config\n        self.downsample = downsample\n        self.create_grid_infos(**grid_config)\n        self.sid = sid\n        self.frustum = self.create_frustum(grid_config['depth'],\n                                           input_size, downsample)\n        self.out_channels = out_channels\n        self.in_channels = in_channels\n        self.depth_net = nn.Conv2d(\n            in_channels, self.D + self.out_channels, kernel_size=1, padding=0)\n        self.accelerate = accelerate\n        self.initial_flag = True\n        self.collapse_z = collapse_z\n        self.with_depth_from_lidar = with_depth_from_lidar\n        if self.with_depth_from_lidar:\n            self.lidar_input_net = nn.Sequential(\n                nn.Conv2d(1, 8, 1),\n                nn.BatchNorm2d(8),\n                nn.ReLU(True),\n                nn.Conv2d(8, 32, 5, stride=4, padding=2),\n                nn.BatchNorm2d(32),\n                nn.ReLU(True),\n                nn.Conv2d(32, 64, 5, stride=int(2 * self.downsample / 8),\n                          padding=2),\n                nn.BatchNorm2d(64),\n                nn.ReLU(True))\n            out_channels = self.D + self.out_channels\n            self.depth_net = nn.Sequential(\n                nn.Conv2d(in_channels + 64, in_channels, 3, padding=1),\n                nn.BatchNorm2d(in_channels),\n                nn.ReLU(True),\n                nn.Conv2d(in_channels, in_channels, 3, padding=1),\n                nn.BatchNorm2d(in_channels),\n                nn.ReLU(True),\n                nn.Conv2d(in_channels, out_channels, 1))\n\n    def create_grid_infos(self, x, y, z, **kwargs):\n        \"\"\"Generate the grid information including the lower bound, interval,\n        and size.\n\n        Args:\n            x (tuple(float)): Config of grid alone x axis in format of\n                (lower_bound, upper_bound, interval).\n            y (tuple(float)): Config of grid alone y axis in format of\n                (lower_bound, upper_bound, interval).\n            z (tuple(float)): Config of grid alone z axis in format of\n                (lower_bound, upper_bound, interval).\n            **kwargs: Container for other potential parameters\n        \"\"\"\n        self.grid_lower_bound = torch.Tensor([cfg[0] for cfg in [x, y, z]])\n        self.grid_interval = torch.Tensor([cfg[2] for cfg in [x, y, z]])\n        self.grid_size = torch.Tensor([(cfg[1] - cfg[0]) / cfg[2]\n                                       for cfg in [x, y, z]])\n\n    def create_frustum(self, depth_cfg, input_size, downsample):\n        \"\"\"Generate the frustum template for each image.\n\n        Args:\n            depth_cfg (tuple(float)): Config of grid alone depth axis in format\n                of (lower_bound, upper_bound, interval).\n            input_size (tuple(int)): Size of input images in format of (height,\n                width).\n            downsample (int): Down sample scale factor from the input size to\n                the feature size.\n        \"\"\"\n        H_in, W_in = input_size\n        H_feat, W_feat = H_in // downsample, W_in // downsample\n        d = torch.arange(*depth_cfg, dtype=torch.float)\\\n            .view(-1, 1, 1).expand(-1, H_feat, W_feat)\n        self.D = d.shape[0]\n        if self.sid:\n            d_sid = torch.arange(self.D).float()\n            depth_cfg_t = torch.tensor(depth_cfg).float()\n            d_sid = torch.exp(torch.log(depth_cfg_t[0]) + d_sid / (self.D-1) *\n                              torch.log((depth_cfg_t[1]-1) / depth_cfg_t[0]))\n            d = d_sid.view(-1, 1, 1).expand(-1, H_feat, W_feat)\n        x = torch.linspace(0, W_in - 1, W_feat,  dtype=torch.float)\\\n            .view(1, 1, W_feat).expand(self.D, H_feat, W_feat)\n        y = torch.linspace(0, H_in - 1, H_feat,  dtype=torch.float)\\\n            .view(1, H_feat, 1).expand(self.D, H_feat, W_feat)\n\n        # D x H x W x 3\n        return torch.stack((x, y, d), -1)\n\n    def get_lidar_coor(self, sensor2ego, ego2global, cam2imgs, post_rots, post_trans,\n                       bda):\n        \"\"\"Calculate the locations of the frustum points in the lidar\n        coordinate system.\n\n        Args:\n            rots (torch.Tensor): Rotation from camera coordinate system to\n                lidar coordinate system in shape (B, N_cams, 3, 3).\n            trans (torch.Tensor): Translation from camera coordinate system to\n                lidar coordinate system in shape (B, N_cams, 3).\n            cam2imgs (torch.Tensor): Camera intrinsic matrixes in shape\n                (B, N_cams, 3, 3).\n            post_rots (torch.Tensor): Rotation in camera coordinate system in\n                shape (B, N_cams, 3, 3). It is derived from the image view\n                augmentation.\n            post_trans (torch.Tensor): Translation in camera coordinate system\n                derived from image view augmentation in shape (B, N_cams, 3).\n\n        Returns:\n            torch.tensor: Point coordinates in shape\n                (B, N_cams, D, ownsample, 3)\n        \"\"\"\n        B, N, _, _ = sensor2ego.shape\n\n        # post-transformation\n        # B x N x D x H x W x 3\n        points = self.frustum.to(sensor2ego) - post_trans.view(B, N, 1, 1, 1, 3)\n        points = torch.inverse(post_rots).view(B, N, 1, 1, 1, 3, 3)\\\n            .matmul(points.unsqueeze(-1))\n\n        # cam_to_ego\n        points = torch.cat(\n            (points[..., :2, :] * points[..., 2:3, :], points[..., 2:3, :]), 5)\n        combine = sensor2ego[:,:,:3,:3].matmul(torch.inverse(cam2imgs))\n        points = combine.view(B, N, 1, 1, 1, 3, 3).matmul(points).squeeze(-1)\n        points += sensor2ego[:,:,:3, 3].view(B, N, 1, 1, 1, 3)\n        points = bda[:, :3, :3].view(B, 1, 1, 1, 1, 3, 3).matmul(\n            points.unsqueeze(-1)).squeeze(-1)\n        points += bda[:, :3, 3].view(B, 1, 1, 1, 1, 3)\n        return points\n\n    def init_acceleration_v2(self, coor):\n        \"\"\"Pre-compute the necessary information in acceleration including the\n        index of points in the final feature.\n\n        Args:\n            coor (torch.tensor): Coordinate of points in lidar space in shape\n                (B, N_cams, D, H, W, 3).\n            x (torch.tensor): Feature of points in shape\n                (B, N_cams, D, H, W, C).\n        \"\"\"\n\n        ranks_bev, ranks_depth, ranks_feat, \\\n            interval_starts, interval_lengths = \\\n            self.voxel_pooling_prepare_v2(coor)\n\n        self.ranks_bev = ranks_bev.int().contiguous()\n        self.ranks_feat = ranks_feat.int().contiguous()\n        self.ranks_depth = ranks_depth.int().contiguous()\n        self.interval_starts = interval_starts.int().contiguous()\n        self.interval_lengths = interval_lengths.int().contiguous()\n\n    def voxel_pooling_v2(self, coor, depth, feat):\n        ranks_bev, ranks_depth, ranks_feat, \\\n            interval_starts, interval_lengths = \\\n            self.voxel_pooling_prepare_v2(coor)\n        if ranks_feat is None:\n            print('warning ---> no points within the predefined '\n                  'bev receptive field')\n            dummy = torch.zeros(size=[\n                feat.shape[0], feat.shape[2],\n                int(self.grid_size[2]),\n                int(self.grid_size[0]),\n                int(self.grid_size[1])\n            ]).to(feat)\n            dummy = torch.cat(dummy.unbind(dim=2), 1)\n            return dummy\n        feat = feat.permute(0, 1, 3, 4, 2)\n        bev_feat_shape = (depth.shape[0], int(self.grid_size[2]),\n                          int(self.grid_size[1]), int(self.grid_size[0]),\n                          feat.shape[-1])  # (B, Z, Y, X, C)\n        bev_feat = bev_pool_v2(depth, feat, ranks_depth, ranks_feat, ranks_bev,\n                               bev_feat_shape, interval_starts,\n                               interval_lengths)\n        # collapse Z\n        if self.collapse_z:\n            bev_feat = torch.cat(bev_feat.unbind(dim=2), 1)\n        return bev_feat\n\n    def voxel_pooling_prepare_v2(self, coor):\n        \"\"\"Data preparation for voxel pooling.\n\n        Args:\n            coor (torch.tensor): Coordinate of points in the lidar space in\n                shape (B, N, D, H, W, 3).\n\n        Returns:\n            tuple[torch.tensor]: Rank of the voxel that a point is belong to\n                in shape (N_Points); Reserved index of points in the depth\n                space in shape (N_Points). Reserved index of points in the\n                feature space in shape (N_Points).\n        \"\"\"\n        B, N, D, H, W, _ = coor.shape\n        num_points = B * N * D * H * W\n        # record the index of selected points for acceleration purpose\n        ranks_depth = torch.range(\n            0, num_points - 1, dtype=torch.int, device=coor.device)\n        ranks_feat = torch.range(\n            0, num_points // D - 1, dtype=torch.int, device=coor.device)\n        ranks_feat = ranks_feat.reshape(B, N, 1, H, W)\n        ranks_feat = ranks_feat.expand(B, N, D, H, W).flatten()\n        # convert coordinate into the voxel space\n        coor = ((coor - self.grid_lower_bound.to(coor)) /\n                self.grid_interval.to(coor))\n        coor = coor.long().view(num_points, 3)\n        batch_idx = torch.range(0, B - 1).reshape(B, 1). \\\n            expand(B, num_points // B).reshape(num_points, 1).to(coor)\n        coor = torch.cat((coor, batch_idx), 1)\n\n        # filter out points that are outside box\n        kept = (coor[:, 0] >= 0) & (coor[:, 0] < self.grid_size[0]) & \\\n               (coor[:, 1] >= 0) & (coor[:, 1] < self.grid_size[1]) & \\\n               (coor[:, 2] >= 0) & (coor[:, 2] < self.grid_size[2])\n        if len(kept) == 0:\n            return None, None, None, None, None\n        coor, ranks_depth, ranks_feat = \\\n            coor[kept], ranks_depth[kept], ranks_feat[kept]\n        # get tensors from the same voxel next to each other\n        ranks_bev = coor[:, 3] * (\n            self.grid_size[2] * self.grid_size[1] * self.grid_size[0])\n        ranks_bev += coor[:, 2] * (self.grid_size[1] * self.grid_size[0])\n        ranks_bev += coor[:, 1] * self.grid_size[0] + coor[:, 0]\n        order = ranks_bev.argsort()\n        ranks_bev, ranks_depth, ranks_feat = \\\n            ranks_bev[order], ranks_depth[order], ranks_feat[order]\n\n        kept = torch.ones(\n            ranks_bev.shape[0], device=ranks_bev.device, dtype=torch.bool)\n        kept[1:] = ranks_bev[1:] != ranks_bev[:-1]\n        interval_starts = torch.where(kept)[0].int()\n        if len(interval_starts) == 0:\n            return None, None, None, None, None\n        interval_lengths = torch.zeros_like(interval_starts)\n        interval_lengths[:-1] = interval_starts[1:] - interval_starts[:-1]\n        interval_lengths[-1] = ranks_bev.shape[0] - interval_starts[-1]\n        return ranks_bev.int().contiguous(), ranks_depth.int().contiguous(\n        ), ranks_feat.int().contiguous(), interval_starts.int().contiguous(\n        ), interval_lengths.int().contiguous()\n\n    def pre_compute(self, input):\n        if self.initial_flag:\n            coor = self.get_lidar_coor(*input[1:7])\n            self.init_acceleration_v2(coor)\n            self.initial_flag = False\n\n    def view_transform_core(self, input, depth, tran_feat):\n        B, N, C, H, W = input[0].shape\n\n        # Lift-Splat\n        if self.accelerate:\n            feat = tran_feat.view(B, N, self.out_channels, H, W)\n            feat = feat.permute(0, 1, 3, 4, 2)\n            depth = depth.view(B, N, self.D, H, W)\n            bev_feat_shape = (depth.shape[0], int(self.grid_size[2]),\n                              int(self.grid_size[1]), int(self.grid_size[0]),\n                              feat.shape[-1])  # (B, Z, Y, X, C)\n            bev_feat = bev_pool_v2(depth, feat, self.ranks_depth,\n                                   self.ranks_feat, self.ranks_bev,\n                                   bev_feat_shape, self.interval_starts,\n                                   self.interval_lengths)\n\n            bev_feat = bev_feat.squeeze(2)\n        else:\n            coor = self.get_lidar_coor(*input[1:7])\n            bev_feat = self.voxel_pooling_v2(\n                coor, depth.view(B, N, self.D, H, W),\n                tran_feat.view(B, N, self.out_channels, H, W))\n        return bev_feat, depth\n\n    def view_transform(self, input, depth, tran_feat):\n        for shape_id in range(3):\n            assert depth.shape[shape_id+1] == self.frustum.shape[shape_id]\n        if self.accelerate:\n            self.pre_compute(input)\n        return self.view_transform_core(input, depth, tran_feat)\n\n    def forward(self, input, depth_from_lidar=None):\n        \"\"\"Transform image-view feature into bird-eye-view feature.\n\n        Args:\n            input (list(torch.tensor)): of (image-view feature, rots, trans,\n                intrins, post_rots, post_trans)\n\n        Returns:\n            torch.tensor: Bird-eye-view feature in shape (B, C, H_BEV, W_BEV)\n        \"\"\"\n        x = input[0]\n        B, N, C, H, W = x.shape\n        x = x.view(B * N, C, H, W)\n        if self.with_depth_from_lidar:\n            assert depth_from_lidar is not None\n            if isinstance(depth_from_lidar, list):\n                assert len(depth_from_lidar) == 1\n                depth_from_lidar = depth_from_lidar[0]\n            h_img, w_img = depth_from_lidar.shape[2:]\n            depth_from_lidar = depth_from_lidar.view(B * N, 1, h_img, w_img)\n            depth_from_lidar = self.lidar_input_net(depth_from_lidar)\n            x = torch.cat([x, depth_from_lidar], dim=1)\n        if self.with_cp:\n            x =checkpoint(self.depth_net, x)\n        else:\n            x = self.depth_net(x)\n\n        depth_digit = x[:, :self.D, ...]\n        tran_feat = x[:, self.D:self.D + self.out_channels, ...]\n        depth = depth_digit.softmax(dim=1)\n        return self.view_transform(input, depth, tran_feat)\n\n    def get_mlp_input(self, rot, tran, intrin, post_rot, post_tran, bda):\n        return None\n\n\nclass _ASPPModule(nn.Module):\n\n    def __init__(self, inplanes, planes, kernel_size, padding, dilation,\n                 BatchNorm):\n        super(_ASPPModule, self).__init__()\n        self.atrous_conv = nn.Conv2d(\n            inplanes,\n            planes,\n            kernel_size=kernel_size,\n            stride=1,\n            padding=padding,\n            dilation=dilation,\n            bias=False)\n        self.bn = BatchNorm(planes)\n        self.relu = nn.ReLU()\n\n        self._init_weight()\n\n    def forward(self, x):\n        x = self.atrous_conv(x)\n        x = self.bn(x)\n\n        return self.relu(x)\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\nclass ASPP(nn.Module):\n\n    def __init__(self, inplanes, mid_channels=256, BatchNorm=nn.BatchNorm2d):\n        super(ASPP, self).__init__()\n\n        dilations = [1, 6, 12, 18]\n\n        self.aspp1 = _ASPPModule(\n            inplanes,\n            mid_channels,\n            1,\n            padding=0,\n            dilation=dilations[0],\n            BatchNorm=BatchNorm)\n        self.aspp2 = _ASPPModule(\n            inplanes,\n            mid_channels,\n            3,\n            padding=dilations[1],\n            dilation=dilations[1],\n            BatchNorm=BatchNorm)\n        self.aspp3 = _ASPPModule(\n            inplanes,\n            mid_channels,\n            3,\n            padding=dilations[2],\n            dilation=dilations[2],\n            BatchNorm=BatchNorm)\n        self.aspp4 = _ASPPModule(\n            inplanes,\n            mid_channels,\n            3,\n            padding=dilations[3],\n            dilation=dilations[3],\n            BatchNorm=BatchNorm)\n\n        self.global_avg_pool = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Conv2d(inplanes, mid_channels, 1, stride=1, bias=False),\n            BatchNorm(mid_channels),\n            nn.ReLU(),\n        )\n        self.conv1 = nn.Conv2d(\n            int(mid_channels * 5), inplanes, 1, bias=False)\n        self.bn1 = BatchNorm(inplanes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        self._init_weight()\n\n    def forward(self, x):\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x4 = self.aspp4(x)\n        x5 = self.global_avg_pool(x)\n        x5 = F.interpolate(\n            x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        return self.dropout(x)\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\nclass Mlp(nn.Module):\n\n    def __init__(self,\n                 in_features,\n                 hidden_features=None,\n                 out_features=None,\n                 act_layer=nn.ReLU,\n                 drop=0.0):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.drop1 = nn.Dropout(drop)\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop2 = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop1(x)\n        x = self.fc2(x)\n        x = self.drop2(x)\n        return x\n\n\nclass SELayer(nn.Module):\n\n    def __init__(self, channels, act_layer=nn.ReLU, gate_layer=nn.Sigmoid):\n        super().__init__()\n        self.conv_reduce = nn.Conv2d(channels, channels, 1, bias=True)\n        self.act1 = act_layer()\n        self.conv_expand = nn.Conv2d(channels, channels, 1, bias=True)\n        self.gate = gate_layer()\n\n    def forward(self, x, x_se):\n        x_se = self.conv_reduce(x_se)\n        x_se = self.act1(x_se)\n        x_se = self.conv_expand(x_se)\n        return x * self.gate(x_se)\n\n\nclass DepthNet(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 mid_channels,\n                 context_channels,\n                 depth_channels,\n                 use_dcn=True,\n                 use_aspp=True,\n                 with_cp=False,\n                 stereo=False,\n                 bias=0.0,\n                 aspp_mid_channels=-1):\n        super(DepthNet, self).__init__()\n        self.reduce_conv = nn.Sequential(\n            nn.Conv2d(\n                in_channels, mid_channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n        )\n        self.context_conv = nn.Conv2d(\n            mid_channels, context_channels, kernel_size=1, stride=1, padding=0)\n        self.bn = nn.BatchNorm1d(27)\n        self.depth_mlp = Mlp(27, mid_channels, mid_channels)\n        self.depth_se = SELayer(mid_channels)  # NOTE: add camera-aware\n        self.context_mlp = Mlp(27, mid_channels, mid_channels)\n        self.context_se = SELayer(mid_channels)  # NOTE: add camera-aware\n        depth_conv_input_channels = mid_channels\n        downsample = None\n\n        if stereo:\n            depth_conv_input_channels += depth_channels\n            downsample = nn.Conv2d(depth_conv_input_channels,\n                                    mid_channels, 1, 1, 0)\n            cost_volumn_net = []\n            for stage in range(int(2)):\n                cost_volumn_net.extend([\n                    nn.Conv2d(depth_channels, depth_channels, kernel_size=3,\n                              stride=2, padding=1),\n                    nn.BatchNorm2d(depth_channels)])\n            self.cost_volumn_net = nn.Sequential(*cost_volumn_net)\n            self.bias = bias\n        depth_conv_list = [BasicBlock(depth_conv_input_channels, mid_channels,\n                                      downsample=downsample),\n                           BasicBlock(mid_channels, mid_channels),\n                           BasicBlock(mid_channels, mid_channels)]\n        if use_aspp:\n            if aspp_mid_channels<0:\n                aspp_mid_channels = mid_channels\n            depth_conv_list.append(ASPP(mid_channels, aspp_mid_channels))\n        if use_dcn:\n            depth_conv_list.append(\n                build_conv_layer(\n                    cfg=dict(\n                        type='DCN',\n                        in_channels=mid_channels,\n                        out_channels=mid_channels,\n                        kernel_size=3,\n                        padding=1,\n                        groups=4,\n                        im2col_step=128,\n                    )))\n        depth_conv_list.append(\n            nn.Conv2d(\n                mid_channels,\n                depth_channels,\n                kernel_size=1,\n                stride=1,\n                padding=0))\n        self.depth_conv = nn.Sequential(*depth_conv_list)\n        self.with_cp = with_cp\n        self.depth_channels = depth_channels\n\n    def gen_grid(self, metas, B, N, D, H, W, hi, wi):\n        frustum = metas['frustum']\n        points = frustum - metas['post_trans'].view(B, N, 1, 1, 1, 3)\n        points = torch.inverse(metas['post_rots']).view(B, N, 1, 1, 1, 3, 3) \\\n            .matmul(points.unsqueeze(-1))\n        points = torch.cat(\n            (points[..., :2, :] * points[..., 2:3, :], points[..., 2:3, :]), 5)\n\n        rots = metas['k2s_sensor'][:, :, :3, :3].contiguous()\n        trans = metas['k2s_sensor'][:, :, :3, 3].contiguous()\n        combine = rots.matmul(torch.inverse(metas['intrins']))\n\n        points = combine.view(B, N, 1, 1, 1, 3, 3).matmul(points)\n        points += trans.view(B, N, 1, 1, 1, 3, 1)\n        neg_mask = points[..., 2, 0] < 1e-3\n        points = metas['intrins'].view(B, N, 1, 1, 1, 3, 3).matmul(points)\n        points = points[..., :2, :] / points[..., 2:3, :]\n\n        points = metas['post_rots'][...,:2,:2].view(B, N, 1, 1, 1, 2, 2).matmul(\n            points).squeeze(-1)\n        points += metas['post_trans'][...,:2].view(B, N, 1, 1, 1, 2)\n\n        px = points[..., 0] / (wi - 1.0) * 2.0 - 1.0\n        py = points[..., 1] / (hi - 1.0) * 2.0 - 1.0\n        px[neg_mask] = -2\n        py[neg_mask] = -2\n        grid = torch.stack([px, py], dim=-1)\n        grid = grid.view(B * N, D * H, W, 2)\n        return grid\n\n    def calculate_cost_volumn(self, metas):\n        prev, curr = metas['cv_feat_list']\n        group_size = 4\n        _, c, hf, wf = curr.shape\n        hi, wi = hf * 4, wf * 4\n        B, N, _ = metas['post_trans'].shape\n        D, H, W, _ = metas['frustum'].shape\n        grid = self.gen_grid(metas, B, N, D, H, W, hi, wi).to(curr.dtype)\n\n        prev = prev.view(B * N, -1, H, W)\n        curr = curr.view(B * N, -1, H, W)\n        cost_volumn = 0\n        # process in group wise to save memory\n        for fid in range(curr.shape[1] // group_size):\n            prev_curr = prev[:, fid * group_size:(fid + 1) * group_size, ...]\n            wrap_prev = F.grid_sample(prev_curr, grid,\n                                      align_corners=True,\n                                      padding_mode='zeros')\n            curr_tmp = curr[:, fid * group_size:(fid + 1) * group_size, ...]\n            cost_volumn_tmp = curr_tmp.unsqueeze(2) - \\\n                              wrap_prev.view(B * N, -1, D, H, W)\n            cost_volumn_tmp = cost_volumn_tmp.abs().sum(dim=1)\n            cost_volumn += cost_volumn_tmp\n        if not self.bias == 0:\n            invalid = wrap_prev[:, 0, ...].view(B * N, D, H, W) == 0\n            cost_volumn[invalid] = cost_volumn[invalid] + self.bias\n        cost_volumn = - cost_volumn\n        cost_volumn = cost_volumn.softmax(dim=1)\n        return cost_volumn\n\n    def forward(self, x, mlp_input, stereo_metas=None):\n        mlp_input = self.bn(mlp_input.reshape(-1, mlp_input.shape[-1]))\n        x = self.reduce_conv(x)\n        context_se = self.context_mlp(mlp_input)[..., None, None]\n        context = self.context_se(x, context_se)\n        context = self.context_conv(context)\n        depth_se = self.depth_mlp(mlp_input)[..., None, None]\n        depth = self.depth_se(x, depth_se)\n\n        if not stereo_metas is None:\n            if stereo_metas['cv_feat_list'][0] is None:\n                BN, _, H, W = x.shape\n                scale_factor = float(stereo_metas['downsample'])/\\\n                               stereo_metas['cv_downsample']\n                cost_volumn = \\\n                    torch.zeros((BN, self.depth_channels,\n                                 int(H*scale_factor),\n                                 int(W*scale_factor))).to(x)\n            else:\n                with torch.no_grad():\n                    cost_volumn = self.calculate_cost_volumn(stereo_metas)\n            cost_volumn = self.cost_volumn_net(cost_volumn)\n            depth = torch.cat([depth, cost_volumn], dim=1)\n        if self.with_cp:\n            depth = checkpoint(self.depth_conv, depth)\n        else:\n            depth = self.depth_conv(depth)\n        return torch.cat([depth, context], dim=1)\n\n\nclass DepthAggregation(nn.Module):\n    \"\"\"pixel cloud feature extraction.\"\"\"\n\n    def __init__(self, in_channels, mid_channels, out_channels):\n        super(DepthAggregation, self).__init__()\n\n        self.reduce_conv = nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                mid_channels,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n        )\n\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                mid_channels,\n                mid_channels,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                mid_channels,\n                mid_channels,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n        )\n\n        self.out_conv = nn.Sequential(\n            nn.Conv2d(\n                mid_channels,\n                out_channels,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=True),\n            # nn.BatchNorm3d(out_channels),\n            # nn.ReLU(inplace=True),\n        )\n\n    @autocast(False)\n    def forward(self, x):\n        x = checkpoint(self.reduce_conv, x)\n        short_cut = x\n        x = checkpoint(self.conv, x)\n        x = short_cut + x\n        x = self.out_conv(x)\n        return x\n\n\n@NECKS.register_module()\nclass LSSViewTransformerBEVDepth(LSSViewTransformer):\n\n    def __init__(self, loss_depth_weight=3.0, depthnet_cfg=dict(), **kwargs):\n        super(LSSViewTransformerBEVDepth, self).__init__(**kwargs)\n        self.loss_depth_weight = loss_depth_weight\n        self.depth_net = DepthNet(self.in_channels, self.in_channels,\n                                  self.out_channels, self.D, **depthnet_cfg)\n\n    def get_mlp_input(self, sensor2ego, ego2global, intrin, post_rot, post_tran, bda):\n        B, N, _, _ = sensor2ego.shape\n        bda = bda.view(B, 1, 4, 4).repeat(1, N, 1, 1)\n        mlp_input = torch.stack([\n            intrin[:, :, 0, 0],\n            intrin[:, :, 1, 1],\n            intrin[:, :, 0, 2],\n            intrin[:, :, 1, 2],\n            post_rot[:, :, 0, 0],\n            post_rot[:, :, 0, 1],\n            post_tran[:, :, 0],\n            post_rot[:, :, 1, 0],\n            post_rot[:, :, 1, 1],\n            post_tran[:, :, 1],\n            bda[:, :, 0, 0],\n            bda[:, :, 0, 1],\n            bda[:, :, 1, 0],\n            bda[:, :, 1, 1],\n            bda[:, :, 2, 2],], dim=-1)\n        sensor2ego = sensor2ego[:,:,:3,:].reshape(B, N, -1)\n        mlp_input = torch.cat([mlp_input, sensor2ego], dim=-1)\n        return mlp_input\n\n    def get_downsampled_gt_depth(self, gt_depths):\n        \"\"\"\n        Input:\n            gt_depths: [B, N, H, W]\n        Output:\n            gt_depths: [B*N*h*w, d]\n        \"\"\"\n        B, N, H, W = gt_depths.shape\n        gt_depths = gt_depths.view(B * N, H // self.downsample,\n                                   self.downsample, W // self.downsample,\n                                   self.downsample, 1)\n        gt_depths = gt_depths.permute(0, 1, 3, 5, 2, 4).contiguous()\n        gt_depths = gt_depths.view(-1, self.downsample * self.downsample)\n        gt_depths_tmp = torch.where(gt_depths == 0.0,\n                                    1e5 * torch.ones_like(gt_depths),\n                                    gt_depths)\n        gt_depths = torch.min(gt_depths_tmp, dim=-1).values\n        gt_depths = gt_depths.view(B * N, H // self.downsample,\n                                   W // self.downsample)\n\n        if not self.sid:\n            gt_depths = (gt_depths - (self.grid_config['depth'][0] -\n                                      self.grid_config['depth'][2])) / \\\n                        self.grid_config['depth'][2]\n        else:\n            gt_depths = torch.log(gt_depths) - torch.log(\n                torch.tensor(self.grid_config['depth'][0]).float())\n            gt_depths = gt_depths * (self.D - 1) / torch.log(\n                torch.tensor(self.grid_config['depth'][1] - 1.).float() /\n                self.grid_config['depth'][0])\n            gt_depths = gt_depths + 1.\n        gt_depths = torch.where((gt_depths < self.D + 1) & (gt_depths >= 0.0),\n                                gt_depths, torch.zeros_like(gt_depths))\n        gt_depths = F.one_hot(\n            gt_depths.long(), num_classes=self.D + 1).view(-1, self.D + 1)[:,\n                                                                           1:]\n        return gt_depths.float()\n\n    @force_fp32()\n    def get_depth_loss(self, depth_labels, depth_preds):\n        depth_labels = self.get_downsampled_gt_depth(depth_labels)\n        depth_preds = depth_preds.permute(0, 2, 3,\n                                          1).contiguous().view(-1, self.D)\n        fg_mask = torch.max(depth_labels, dim=1).values > 0.0\n        depth_labels = depth_labels[fg_mask]\n        depth_preds = depth_preds[fg_mask]\n        with autocast(enabled=False):\n            depth_loss = F.binary_cross_entropy(\n                depth_preds,\n                depth_labels,\n                reduction='none',\n            ).sum() / max(1.0, fg_mask.sum())\n        return self.loss_depth_weight * depth_loss\n\n    def forward(self, input, stereo_metas=None):\n        (x, rots, trans, intrins, post_rots, post_trans, bda,\n         mlp_input) = input[:8]\n\n        B, N, C, H, W = x.shape\n        x = x.view(B * N, C, H, W)\n        x = self.depth_net(x, mlp_input, stereo_metas)\n        depth_digit = x[:, :self.D, ...]\n        tran_feat = x[:, self.D:self.D + self.out_channels, ...]\n        depth = depth_digit.softmax(dim=1)\n        bev_feat, depth = self.view_transform(input, depth, tran_feat)\n        return bev_feat, depth\n\n\n@NECKS.register_module()\nclass LSSViewTransformerBEVStereo(LSSViewTransformerBEVDepth):\n\n    def __init__(self,  **kwargs):\n        super(LSSViewTransformerBEVStereo, self).__init__(**kwargs)\n        self.cv_frustum = self.create_frustum(kwargs['grid_config']['depth'],\n                                              kwargs['input_size'],\n                                              downsample=4)\n"
        }
    ]
}