{
    "sourceFile": "mmdet3d/models/necks/fusion.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 5,
            "patches": [
                {
                    "date": 1716003255644,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1716020695823,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -659,8 +659,10 @@\n             output, (h, w), delta1\n         )  # _,256,_,_\n \n         output = torch.cat([output, sampling_feats], dim=1)\n+        \n+        \n \n         ret_dict.update(dict(det_feats=[output]))\n         ret_dict.update(dict(student_sampling_feats=sampling_feats))\n \n"
                },
                {
                    "date": 1716030344348,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -609,9 +609,9 @@\n         output = self.SpatialProbAtten(img_bev, radar_bev)\n         output = self.DualWeight_Fusion(output)\n \n         output = self.output_conv(output)\n-        \n+\n         # feats_to_img(output, base_path=base_path,suffix='fusionbev')\n \n         return output\n \n@@ -659,10 +659,8 @@\n             output, (h, w), delta1\n         )  # _,256,_,_\n \n         output = torch.cat([output, sampling_feats], dim=1)\n-        \n-        \n \n         ret_dict.update(dict(det_feats=[output]))\n         ret_dict.update(dict(student_sampling_feats=sampling_feats))\n \n"
                },
                {
                    "date": 1716033705851,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -644,22 +644,26 @@\n         # )\n         self.output_conv = CBAM(self.low_feats_channels)\n \n     def forward(self, img_bev, radar_bev):\n+        \n+        h, w = output.size(2), output.size(3)\n+\n+        delta1 = self.sampling_pos_gen(output)\n+\n+        sampling_feats = self.bilinear_interpolate_torch_gridsample2(\n+            output, (h, w), delta1\n+        )  # _,256,_,_\n+        \n+        \n         ret_dict = {}\n         output = self.SpatialProbAtten(img_bev, radar_bev)\n         output = self.DualWeight_Fusion(output)\n \n         # output = self.output_conv(output)\n \n-        h, w = output.size(2), output.size(3)\n \n-        delta1 = self.sampling_pos_gen(output)\n \n-        sampling_feats = self.bilinear_interpolate_torch_gridsample2(\n-            output, (h, w), delta1\n-        )  # _,256,_,_\n-\n         output = torch.cat([output, sampling_feats], dim=1)\n \n         ret_dict.update(dict(det_feats=[output]))\n         ret_dict.update(dict(student_sampling_feats=sampling_feats))\n"
                },
                {
                    "date": 1716033730961,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -644,27 +644,24 @@\n         # )\n         self.output_conv = CBAM(self.low_feats_channels)\n \n     def forward(self, img_bev, radar_bev):\n-        \n-        new_cat=torch.cat([img_bev,radar_bev],dim=1)\n+\n+        new_cat = torch.cat([img_bev, radar_bev], dim=1)\n         h, w = output.size(2), output.size(3)\n \n         delta1 = self.sampling_pos_gen(output)\n \n         sampling_feats = self.bilinear_interpolate_torch_gridsample2(\n             output, (h, w), delta1\n         )  # _,256,_,_\n-        \n-        \n+\n         ret_dict = {}\n         output = self.SpatialProbAtten(img_bev, radar_bev)\n         output = self.DualWeight_Fusion(output)\n \n         # output = self.output_conv(output)\n \n-\n-\n         output = torch.cat([output, sampling_feats], dim=1)\n \n         ret_dict.update(dict(det_feats=[output]))\n         ret_dict.update(dict(student_sampling_feats=sampling_feats))\n"
                },
                {
                    "date": 1716033897939,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -626,15 +626,13 @@\n             img_bev_channels=self.low_feats_channels,\n             radar_bev_channle=self.hight_feats_channels,\n         )\n         self.DualWeight_Fusion = DualWeight_Fusion(in_channels=self.low_feats_channels)\n-        \n+\n         in_channels = self.low_feats_channels + self.hight_feats_channels\n         hidden_channels = self.low_feats_channels // 2\n         self.sampling_pos_gen = nn.Sequential(\n-            nn.Conv2d(\n-                in_channels, hidden_channels, kernel_size=1, bias=False\n-            ),\n+            nn.Conv2d(in_channels, hidden_channels, kernel_size=1, bias=False),\n             InPlaceABNSync(hidden_channels),\n             nn.Conv2d(hidden_channels, 2, kernel_size=3, padding=1, bias=False),\n         )\n \n"
                }
            ],
            "date": 1716003255644,
            "name": "Commit-0",
            "content": "import torch\nfrom torch import nn\nfrom inplace_abn import InPlaceABNSync\nimport torch.nn.functional as F\nfrom .. import builder\nfrom mmcv.cnn.bricks import ConvModule, build_conv_layer\nfrom mmdet3d.models.necks.cbam import CBAM\nfrom mmdet3d.models.utils.self_print import feats_to_img\n\n\n@builder.NECKS.register_module()\nclass SamplingWarpFusion(nn.Module):\n    def __init__(\n        self, features=256, reduce_ch=dict(in_channels=512, in_channels2=512), **kwargs\n    ):\n        super(SamplingWarpFusion, self).__init__()\n\n        self.delta_gen1 = nn.Sequential(\n            nn.Conv2d(reduce_ch.in_channels, features, kernel_size=1, bias=False),\n            InPlaceABNSync(features),\n            nn.Conv2d(features, 2, kernel_size=3, padding=1, bias=False),\n        )\n\n        self.tea_delta_gen = nn.Sequential(\n            nn.Conv2d(reduce_ch.in_channels, features, kernel_size=1, bias=False),\n            InPlaceABNSync(features),\n            nn.Conv2d(features, 2, kernel_size=3, padding=1, bias=False),\n        )\n\n        if \"out_conv_cfg\" in kwargs.keys():\n            self.output_conv = ConvModule(**kwargs[\"out_conv_cfg\"])\n            # self.output_conv = nn.Conv2d(\n            #     reduce_ch.in_channels2,\n            #     kwargs[\"out_channels\"],\n            #     kernel_size=1,\n            #     stride=1,\n            #     bias=False,\n            # )\n        else:\n            self.output_conv = None\n\n        self.delta_gen1[2].weight.data.zero_()\n        self.tea_delta_gen[2].weight.data.zero_()\n\n        self.ret_sampling_feats = kwargs[\"ret_sampling_feats\"]\n\n        if \"aug_radar_conv\" in kwargs.keys():\n            self.aug_radar_conv = True\n\n            hidden_feats = 320\n            kwargs[\"aug_radar_conv\"].update(dict(out_channels=hidden_feats))\n            self.aug_radar_conv1 = ConvModule(**kwargs[\"aug_radar_conv\"])\n            hidden_feats2 = 256\n            kwargs[\"aug_radar_conv\"].update(\n                dict(in_channels=hidden_feats, out_channels=hidden_feats2)\n            )\n            self.aug_radar_conv2 = ConvModule(**kwargs[\"aug_radar_conv\"])\n            kwargs[\"aug_radar_conv\"].update(dict(in_channels=hidden_feats2))\n            self.aug_radar_conv3 = ConvModule(**kwargs[\"aug_radar_conv\"])\n        else:\n            self.aug_radar_conv = None\n\n        if \"bi_dir\" in kwargs.keys():\n            self.bi_dir = kwargs[\"bi_dir\"]\n\n            # self.delta_gen2 = nn.Sequential(\n            #     nn.Conv2d(self.bi_dir.in_channels, features, kernel_size=1, bias=False),\n            #     InPlaceABNSync(features),\n            #     nn.Conv2d(features, 2, kernel_size=3, padding=1, bias=False),\n            # )\n\n            self.teacher_reduce_channels = nn.Conv2d(\n                self.bi_dir.in_channels,\n                reduce_ch.in_channels,\n                kernel_size=1,\n            )  # ch:: 512->320\n            self.teacher_output_reduce = nn.Conv2d(\n                reduce_ch.in_channels, features, kernel_size=1\n            )  # ch: 320->256\n\n            if \"bi_weight\" in self.bi_dir.keys():\n                self.bi_weight_fusion = builder.build_neck(self.bi_dir.bi_weight_fusion)\n            else:\n                self.bi_weight_fusion = None\n        else:\n            self.bi_weight_fusion = None\n\n    def bilinear_interpolate_torch_gridsample2(self, input, size, delta=0):\n        out_h, out_w = size\n        n, c, h, w = input.shape\n        s = 2.0\n        norm = (\n            torch.tensor([[[[(out_w - 1) / s, (out_h - 1) / s]]]])\n            .type_as(input)\n            .to(input.device)\n        )\n        w_list = torch.linspace(-1.0, 1.0, out_h).view(-1, 1).repeat(1, out_w)\n        h_list = torch.linspace(-1.0, 1.0, out_w).repeat(out_h, 1)\n        grid = torch.cat((h_list.unsqueeze(2), w_list.unsqueeze(2)), 2)\n        grid = grid.repeat(n, 1, 1, 1).type_as(input).to(input.device)\n        grid = grid + delta.permute(0, 2, 3, 1) / norm\n\n        output = F.grid_sample(input, grid, align_corners=True)\n        return output\n\n    def teacher_sampling_warp(self, teach_feats, delta=None):\n        \"\"\"teacher_sampling_warp\n\n        Args:\n            teach_feats (_type_): _description_\n\n        Returns:\n            list: teacher_sampling_pos,teacher_sampling_feats\n        \"\"\"\n\n        h, w = teach_feats.size(2), teach_feats.size(3)\n        tea_delta = self.tea_delta_gen(teach_feats)\n\n        if delta is not None:\n            sampling_feats = self.bilinear_interpolate_torch_gridsample2(\n                teach_feats, (h, w), delta\n            )\n\n            return None, sampling_feats\n\n        sampling_feats = self.bilinear_interpolate_torch_gridsample2(\n            teach_feats, (h, w), tea_delta\n        )\n\n        return tea_delta, sampling_feats\n\n    def aug_radar_first(self, feats1, feats2):\n        x = torch.cat((feats1, feats2), dim=1)\n        x = self.aug_radar_conv1(x)\n        x = self.aug_radar_conv2(x)\n        x = self.aug_radar_conv3(x)\n        return x\n\n    # def forward(self, low_stage, high_stage, teacher_bev=None, **kwargs) -> list:\n    #     \"\"\"SamplingWarpFusion: Computes a list return value for use in distillation calculations.\n\n    #     Args:\n    #         low_stage (_type_): img_bev feats.\n    #         high_stage (_type_): radar_bev feats.\n    #         teacher_bev (_type_, optional): point cloud sparse encoder feats. Defaults to None.\n\n    #     Returns:\n    #         list: (student_sampling_feats,fusion_concat_feats,student_sampling_pos,teacher_sampling_pos)\n    #     \"\"\"\n\n    #     ret_feats = dict()\n    #     h, w = low_stage.size(2), low_stage.size(3)\n\n    #     concat = torch.cat((low_stage, high_stage), 1)\n    #     delta1 = self.delta_gen1(concat)\n    #     sampling_feats = self.bilinear_interpolate_torch_gridsample2(\n    #         low_stage, (h, w), delta1\n    #     )  # _,256,_,_\n\n    #     if self.ret_sampling_feats:\n    #         ret_feats.update(dict(student_sampling_feats=sampling_feats))\n\n    #     if self.aug_radar_conv:\n    #         high_stage = self.aug_radar_first(sampling_feats, high_stage)\n\n    #     if self.bi_weight_fusion is not None:\n    #         low_stage, high_stage = self.bi_weight_fusion(low_stage, high_stage)\n\n    #     concat = torch.cat((low_stage, high_stage), 1)\n\n    #     if self.output_conv is not None:\n    #         concat = self.output_conv(concat)\n\n    #     ret_feats.update(dict(det_feats=concat))\n    #     # ret_feats.append(concat)\n\n    #     # Sample position distillation return\n    #     ret_feats.update(dict(student_sampling_pos=delta1))\n    #     # ret_feats.append(delta1)\n    #     if teacher_bev is not None:\n    #         _teacher_bev = teacher_bev.clone()\n    #         delta2, teacher_sampleing_feats = self.teacher_sampling_warp(_teacher_bev)\n    #         # ret_feats.append(delta2)\n    #         # ret_feats.append(teacher_sampleing_feats)\n    #         ret_feats.update(dict(teacher_sampling_pos=delta2))\n    #         ret_feats.update(dict(teacher_sampling_feats=teacher_sampleing_feats))\n\n    #     return ret_feats\n\n    def forward(self, low_stage, high_stage, teacher_bev=None, **kwargs) -> list:\n\n        if self.bi_weight_fusion is not None:\n            low_stage, high_stage = self.bi_weight_fusion(low_stage, high_stage)\n\n        ret_feats = dict()\n        h, w = high_stage.size(2), high_stage.size(3)\n\n        concat = torch.cat((low_stage, high_stage), 1)\n        delta1 = self.delta_gen1(concat)\n        ret_feats.update(dict(student_sampling_pos=delta1))\n        sampling_feats = self.bilinear_interpolate_torch_gridsample2(\n            low_stage, (h, w), delta1\n        )  # _,256,_,_\n\n        if self.ret_sampling_feats:\n            ret_feats.update(dict(student_sampling_feats=sampling_feats))\n\n        if self.aug_radar_conv:\n            high_stage = self.aug_radar_first(sampling_feats, high_stage)\n\n        concat = torch.cat((low_stage, sampling_feats), 1)\n\n        if self.output_conv is not None:\n            concat = self.output_conv(concat)\n\n        ret_feats.update(dict(det_feats=concat))\n        # ret_feats.append(concat)\n\n        # Sample position distillation return\n        if teacher_bev is not None:\n            _teacher_bev = teacher_bev.clone()\n            delta2, teacher_sampleing_feats = self.teacher_sampling_warp(_teacher_bev)\n            ret_feats.update(dict(teacher_sampling_pos=delta2))\n            ret_feats.update(dict(teacher_sampling_feats=teacher_sampleing_feats))\n\n        return ret_feats\n\n\n@builder.NECKS.register_module()\nclass SamplingWarpFusion_V2(SamplingWarpFusion):\n    def __init__(self, low_feats_channels, hight_feats_channels, **kwargs):\n        super(SamplingWarpFusion, self).__init__()\n\n        self.ret_sampling_pos = kwargs.get(\"ret_sampling_pos\")\n        self.ret_sampling_feats = kwargs.get(\"ret_sampling_feats\")\n\n        concat_channels = low_feats_channels + hight_feats_channels\n        hidden_channels = concat_channels // 2\n\n        self.delta_gen1 = nn.Sequential(\n            nn.Conv2d(concat_channels, hidden_channels, kernel_size=1, bias=False),\n            InPlaceABNSync(hidden_channels),\n            nn.Conv2d(hidden_channels, 2, kernel_size=3, padding=1, bias=False),\n        )\n        self.tea_delta_gen = nn.Sequential(\n            nn.Conv2d(concat_channels, hidden_channels, kernel_size=1, bias=False),\n            InPlaceABNSync(hidden_channels),\n            nn.Conv2d(hidden_channels, 2, kernel_size=3, padding=1, bias=False),\n        )\n\n        if \"out_conv_cfg\" in kwargs.keys():\n            self.output_conv = ConvModule(**kwargs[\"out_conv_cfg\"])\n        else:\n            self.output_conv = None\n\n        self.delta_gen1[2].weight.data.zero_()\n        self.tea_delta_gen[2].weight.data.zero_()\n\n    def forward(self, low_stage, high_stage, teacher_bev=None, **kwargs):\n\n        ret_feats = dict()\n        h, w = low_stage.size(2), low_stage.size(3)\n\n        concat = torch.cat((low_stage, high_stage), 1)\n        delta1 = self.delta_gen1(concat)\n        ret_feats.update(dict(student_sampling_pos=delta1))\n        sampling_feats = self.bilinear_interpolate_torch_gridsample2(\n            low_stage, (h, w), delta1\n        )  # _,256,_,_\n\n        if self.ret_sampling_feats:\n            ret_feats.update(dict(student_sampling_feats=sampling_feats))\n\n        concat = torch.cat((low_stage, sampling_feats), 1)\n\n        if self.output_conv is not None:\n            concat = self.output_conv(concat)\n\n        ret_feats.update(dict(det_feats=concat))\n\n        # Sample position distillation return\n        if teacher_bev is not None:\n            _teacher_bev = teacher_bev.clone()\n            delta2, teacher_sampleing_feats = self.teacher_sampling_warp(_teacher_bev)\n            ret_feats.update(dict(teacher_sampling_pos=delta2))\n            ret_feats.update(dict(teacher_sampling_feats=teacher_sampleing_feats))\n\n        return ret_feats\n\n\n@builder.NECKS.register_module()\nclass SamplingFusion(SamplingWarpFusion_V2):\n\n    def __init__(\n        self,\n        low_feats_channels,\n        hight_feats_channels,\n        out_conv_cfg=dict(type=None),\n        **kwargs,\n    ):\n        super().__init__(low_feats_channels, hight_feats_channels, **kwargs)\n        self.low_feats_channels = low_feats_channels\n        self.hight_feats_channels = hight_feats_channels\n\n        cfg_type = out_conv_cfg.get(\"type\")\n        if cfg_type == \"cbam\".upper():\n            self.out_conv = builder.build_neck(out_conv_cfg)\n        elif cfg_type == \"cbr\":\n            self.out_conv = nn.Sequential(\n                nn.Conv2d(\n                    low_feats_channels + hight_feats_channels,\n                    hight_feats_channels,\n                    kernel_size=1,\n                    stride=1,\n                    padding=0,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(hight_feats_channels),\n            )\n        else:\n            self.out_conv = nn.Sequential(\n                nn.Conv2d(\n                    self.low_feats_channels * 2 + self.hight_feats_channels,\n                    self.low_feats_channels,\n                    kernel_size=1,\n                    stride=1,\n                    padding=0,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(self.low_feats_channels),\n            )\n\n    def forward(self, low_stage, high_stage, **kwargs):\n\n        h, w = low_stage.size(2), low_stage.size(3)\n\n        concat = torch.cat((low_stage, high_stage), 1)\n        delta1 = self.delta_gen1(concat)\n\n        sampling_feats = self.bilinear_interpolate_torch_gridsample2(\n            low_stage, (h, w), delta1\n        )  # _,160,_,_\n\n        concat = torch.cat((sampling_feats, high_stage), 1)\n\n        if self.out_conv is not None:\n            concat = self.out_conv(concat)\n\n        return concat\n\n\n@builder.NECKS.register_module()\nclass DenseRadarAug(SamplingFusion):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        concat_channels = self.low_feats_channels + self.hight_feats_channels\n        hidden_channels = concat_channels // 2\n        self.delta_gen1 = nn.Sequential(\n            nn.Conv2d(concat_channels, hidden_channels, kernel_size=1, bias=False),\n            InPlaceABNSync(hidden_channels),\n            nn.Conv2d(hidden_channels, 2, kernel_size=3, padding=1, bias=False),\n        )\n\n        self.low_feats_sampling_aug = nn.Sequential(\n            nn.Conv2d(\n                self.low_feats_channels,\n                concat_channels,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n            ),\n            # nn.BatchNorm2d(concat_channels),\n            # nn.ReLU(),\n            nn.Conv2d(\n                concat_channels,\n                self.low_feats_channels,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n            ),\n            nn.BatchNorm2d(self.low_feats_channels),\n            # nn.ReLU(),\n        )\n        self.high_feats_sampling_aug = nn.Sequential(\n            nn.Conv2d(\n                self.hight_feats_channels,\n                concat_channels,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n            ),\n            # nn.BatchNorm2d(concat_channels),\n            # nn.ReLU(),\n            nn.Conv2d(\n                concat_channels,\n                self.hight_feats_channels,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n            ),\n            nn.BatchNorm2d(self.hight_feats_channels),\n            # nn.ReLU(),\n        )\n\n    def forward(self, low_stage, high_stage, **kwargs):\n\n        h, w = low_stage.size(2), low_stage.size(3)\n\n        concat = torch.cat((low_stage, high_stage), 1)\n        delta1 = self.delta_gen1(concat)\n\n        sampling_feats_img_bev = self.bilinear_interpolate_torch_gridsample2(\n            low_stage, (h, w), delta1\n        )  # _,256,_,_\n        sampling_feats_img_bev = self.low_feats_sampling_aug(sampling_feats_img_bev)\n\n        sampling_feats_radar_bev = self.bilinear_interpolate_torch_gridsample2(\n            high_stage, (h, w), delta1\n        )  # _,224,_,_\n        sampling_feats_radar_bev = self.high_feats_sampling_aug(\n            sampling_feats_radar_bev\n        )\n\n        concat = torch.cat(\n            (sampling_feats_img_bev, sampling_feats_radar_bev, low_stage), 1\n        )\n\n        if self.out_conv is not None:\n            concat = self.out_conv(concat)\n\n        return dict(det_feats=concat)\n\n\n@builder.NECKS.register_module()\nclass BiDirectionWeightFusion(nn.Module):\n    def __init__(self, img_channels, radar_channels):\n        super().__init__()\n\n        self.img_channels = img_channels\n        self.radar_channels = radar_channels\n\n        weight_channels = img_channels + radar_channels\n        self.img_bev_weight = nn.Conv2d(\n            weight_channels, self.img_channels, 3, 1, padding=1\n        )\n        self.radar_bev_weight = nn.Conv2d(\n            weight_channels, self.radar_channels, 3, 1, padding=1\n        )\n        # self.img_bev_weight = nn.Conv2d(weight_channels, 1, 3, 1, padding=1)\n        # self.radar_bev_weight = nn.Conv2d(weight_channels, 1, 3, 1, padding=1)\n        self.sig1 = nn.Sigmoid()\n        self.sig2 = nn.Sigmoid()\n\n    def forward(self, img_bev, radar_bev):\n        concat = torch.cat([img_bev, radar_bev], dim=1)\n\n        # img_bev_weight = self.sig1(self.img_bev_weight(concat))\n        # radar_bev_weight = self.sig2(self.radar_bev_weight(concat))\n\n        img_bev_weight = torch.sigmoid(self.img_bev_weight(concat))\n        radar_bev_weight = torch.sigmoid(self.radar_bev_weight(concat))\n\n        assert img_bev_weight.size(2) == radar_bev_weight.size(2)\n\n        img_bev = img_bev * img_bev_weight\n        radar_bev = radar_bev * radar_bev_weight\n\n        return img_bev, radar_bev\n\n\n@builder.NECKS.register_module()\nclass SimpleAttenFusion(nn.Module):\n    def __init__(self, imgbev_ch=256, radarbev_ch=64, **kwargs):\n        super(SimpleAttenFusion, self).__init__()\n\n        concat_ch = imgbev_ch + radarbev_ch\n\n        self.reduce_conv = ConvModule(\n            concat_ch,\n            imgbev_ch,\n            kernel_size=1,\n            padding=0,\n            norm_cfg=dict(type=\"BN\", eps=1e-3, momentum=0.01),\n            act_cfg=dict(type=\"ReLU\"),\n            inplace=False,\n        )\n\n        self.atten = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(imgbev_ch, imgbev_ch, kernel_size=1, stride=1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, img_bev_feats, radar_bev_feats, **kwargs):\n\n        output = self.reduce_conv(torch.cat([img_bev_feats, radar_bev_feats], dim=1))\n        output = output * self.atten(output)\n\n        return output\n\n\n@builder.NECKS.register_module()\nclass SpatialProbAtten(nn.Module):\n    def __init__(self, img_bev_channels=256, radar_bev_channle=256, r=4):\n        super().__init__()\n\n        concat_channels = img_bev_channels + radar_bev_channle\n        hidden_channles = concat_channels // r\n\n        self.weight_b1 = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(concat_channels, hidden_channles, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_channles, img_bev_channels, kernel_size=1),\n        )\n\n        self.weight_b2 = nn.Sequential(\n            nn.Conv2d(concat_channels, hidden_channles, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden_channles, radar_bev_channle, kernel_size=1),\n        )\n\n        self.sig = nn.Sigmoid()\n\n    def forward(self, img_bev, radar_bev):\n        concat = torch.cat([img_bev, radar_bev], dim=1)\n\n        w1 = self.weight_b1(concat)\n        w2 = self.weight_b2(concat)\n\n        weight = self.sig(w1 + w2)\n\n        # feats_to_img(\n        #     weight, \"/mnt/data/exps/DenseRadar/out/v1_feats_out\", suffix=\"spatial_atten\"\n        # )\n\n        ret_feats = weight * img_bev + (1 - weight) * radar_bev\n\n        return ret_feats\n\n\n@builder.NECKS.register_module()\nclass DualWeight_Fusion(nn.Module):\n    def __init__(self, in_channels=256):\n        super().__init__()\n        self.branch_1_out = nn.Conv2d(2, 1, kernel_size=3, padding=1)\n        self.branch_1_out_7 = nn.Conv2d(2, 1, kernel_size=7, dilation=3, padding=9)\n\n        self.branch_2 = nn.Conv2d(\n            in_channels, in_channels // 2, kernel_size=3, padding=1\n        )\n        self.branch_2_cbam = CBAM(in_channels // 2)\n        self.branch_2_out = nn.Conv2d(in_channels // 2, 1, kernel_size=1)\n\n    def forward(self, bev_feats):\n        branch_1_max, _ = torch.max(bev_feats, dim=1, keepdim=True)\n        branch_1_avg = torch.mean(bev_feats, dim=1, keepdim=True)\n        branch_1 = torch.cat([branch_1_max, branch_1_avg], dim=1)\n        _branch_1 = branch_1\n        branch_1 = self.branch_1_out(_branch_1)\n        branch_1 = torch.mean(\n            branch_1 + self.branch_1_out_7(_branch_1), dim=1\n        ).unsqueeze(1)\n\n        branch_2 = self.branch_2(bev_feats)\n        branch_2 = self.branch_2_cbam(branch_2)\n        branch_2 = self.branch_2_out(branch_2)\n\n        attenion = torch.sigmoid(branch_1 + branch_2)\n\n        # feats_to_img(\n        #     attenion,\n        #     \"/mnt/data/exps/DenseRadar/out/v1_feats_out\",\n        #     suffix=\"dualweight_atten\",\n        # )\n\n        bev_feats = bev_feats * attenion\n\n        return bev_feats\n\n\n@builder.NECKS.register_module()\nclass RC_BEV_Fusion(nn.Module):\n    def __init__(self, img_bev_channels=256, radar_bev_channle=256, process=[]):\n        super().__init__()\n\n        self.img_bev_channels = img_bev_channels\n        self.radar_bev_channle = radar_bev_channle\n\n        self.process_cfgs = process\n        self.SpatialProbAtten = SpatialProbAtten(\n            img_bev_channels=img_bev_channels, radar_bev_channle=radar_bev_channle\n        )\n        self.DualWeight_Fusion = DualWeight_Fusion(in_channels=img_bev_channels)\n\n        # self.output_conv = nn.Sequential(\n        #     nn.Conv2d(img_bev_channels, img_bev_channels, kernel_size=1),\n        #     # nn.BatchNorm2d(img_bev_channels),\n        #     # nn.ReLU(inplace=True),\n        # )\n        self.output_conv = CBAM(img_bev_channels)\n\n    def forward(self, img_bev, radar_bev):\n        # base_path = \"/mnt/data/exps/DenseRadar/out/v1-sparse-feats-distill\"\n        # feats_to_img(img_bev, base_path=base_path,suffix='imgbev')\n        # feats_to_img(radar_bev, base_path=base_path,suffix='radarbev')\n\n        output = self.SpatialProbAtten(img_bev, radar_bev)\n        output = self.DualWeight_Fusion(output)\n\n        output = self.output_conv(output)\n        \n        # feats_to_img(output, base_path=base_path,suffix='fusionbev')\n\n        return output\n\n\n@builder.NECKS.register_module()\nclass RC_BEV_Fusion_Sampling(SamplingFusion):\n    def __init__(self, process=[], **kwargs):\n        super().__init__(**kwargs)\n\n        self.process_cfgs = process\n        self.SpatialProbAtten = SpatialProbAtten(\n            img_bev_channels=self.low_feats_channels,\n            radar_bev_channle=self.hight_feats_channels,\n        )\n        self.DualWeight_Fusion = DualWeight_Fusion(in_channels=self.low_feats_channels)\n\n        hidden_channels = self.low_feats_channels // 2\n        self.sampling_pos_gen = nn.Sequential(\n            nn.Conv2d(\n                self.low_feats_channels, hidden_channels, kernel_size=1, bias=False\n            ),\n            InPlaceABNSync(hidden_channels),\n            nn.Conv2d(hidden_channels, 2, kernel_size=3, padding=1, bias=False),\n        )\n\n        # self.output_conv = nn.Sequential(\n        #     nn.Conv2d(img_bev_channels, img_bev_channels, kernel_size=1),\n        #     # nn.BatchNorm2d(img_bev_channels),\n        #     # nn.ReLU(inplace=True),\n        # )\n        self.output_conv = CBAM(self.low_feats_channels)\n\n    def forward(self, img_bev, radar_bev):\n        ret_dict = {}\n        output = self.SpatialProbAtten(img_bev, radar_bev)\n        output = self.DualWeight_Fusion(output)\n\n        # output = self.output_conv(output)\n\n        h, w = output.size(2), output.size(3)\n\n        delta1 = self.sampling_pos_gen(output)\n\n        sampling_feats = self.bilinear_interpolate_torch_gridsample2(\n            output, (h, w), delta1\n        )  # _,256,_,_\n\n        output = torch.cat([output, sampling_feats], dim=1)\n\n        ret_dict.update(dict(det_feats=[output]))\n        ret_dict.update(dict(student_sampling_feats=sampling_feats))\n\n        return ret_dict\n"
        }
    ]
}