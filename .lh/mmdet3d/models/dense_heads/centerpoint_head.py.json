{
    "sourceFile": "mmdet3d/models/dense_heads/centerpoint_head.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 4,
            "patches": [
                {
                    "date": 1716020523495,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1716021228516,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -607,8 +607,10 @@\n             num_pos = heatmaps[task_id].eq(1).float().sum().item()\n             cls_avg_factor = torch.clamp(\n                 reduce_mean(heatmaps[task_id].new_tensor(num_pos)),\n                 min=1).item()\n+            \n+            \n             loss_heatmap = self.loss_cls(\n                 preds_dict[0]['heatmap'],\n                 heatmaps[task_id],\n                 avg_factor=cls_avg_factor)\n"
                },
                {
                    "date": 1716021399352,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -608,9 +608,8 @@\n             cls_avg_factor = torch.clamp(\n                 reduce_mean(heatmaps[task_id].new_tensor(num_pos)),\n                 min=1).item()\n             \n-            \n             loss_heatmap = self.loss_cls(\n                 preds_dict[0]['heatmap'],\n                 heatmaps[task_id],\n                 avg_factor=cls_avg_factor)\n"
                },
                {
                    "date": 1717333040441,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,857 @@\n+# Copyright (c) OpenMMLab. All rights reserved.\n+import copy\n+\n+import torch\n+from mmcv.cnn import ConvModule, build_conv_layer\n+from mmcv.runner import BaseModule\n+from torch import nn\n+\n+from mmdet3d.core import (circle_nms, draw_heatmap_gaussian, gaussian_radius,\n+                          xywhr2xyxyr)\n+from mmdet3d.core.post_processing import nms_bev\n+from mmdet3d.models import builder\n+from mmdet3d.models.utils import clip_sigmoid\n+from mmdet.core import build_bbox_coder, multi_apply, reduce_mean\n+from ..builder import HEADS, build_loss\n+\n+\n+@HEADS.register_module()\n+class SeparateHead(BaseModule):\n+    \"\"\"SeparateHead for CenterHead.\n+\n+    Args:\n+        in_channels (int): Input channels for conv_layer.\n+        heads (dict): Conv information.\n+        head_conv (int, optional): Output channels.\n+            Default: 64.\n+        final_kernel (int, optional): Kernel size for the last conv layer.\n+            Default: 1.\n+        init_bias (float, optional): Initial bias. Default: -2.19.\n+        conv_cfg (dict, optional): Config of conv layer.\n+            Default: dict(type='Conv2d')\n+        norm_cfg (dict, optional): Config of norm layer.\n+            Default: dict(type='BN2d').\n+        bias (str, optional): Type of bias. Default: 'auto'.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 in_channels,\n+                 heads,\n+                 head_conv=64,\n+                 final_kernel=1,\n+                 init_bias=-2.19,\n+                 conv_cfg=dict(type='Conv2d'),\n+                 norm_cfg=dict(type='BN2d'),\n+                 bias='auto',\n+                 init_cfg=None,\n+                 **kwargs):\n+        assert init_cfg is None, 'To prevent abnormal initialization ' \\\n+            'behavior, init_cfg is not allowed to be set'\n+        super(SeparateHead, self).__init__(init_cfg=init_cfg)\n+        self.heads = heads\n+        self.init_bias = init_bias\n+        for head in self.heads:\n+            classes, num_conv = self.heads[head]\n+\n+            conv_layers = []\n+            c_in = in_channels\n+            for i in range(num_conv - 1):\n+                conv_layers.append(\n+                    ConvModule(\n+                        c_in,\n+                        head_conv,\n+                        kernel_size=final_kernel,\n+                        stride=1,\n+                        padding=final_kernel // 2,\n+                        bias=bias,\n+                        conv_cfg=conv_cfg,\n+                        norm_cfg=norm_cfg))\n+                c_in = head_conv\n+\n+            conv_layers.append(\n+                build_conv_layer(\n+                    conv_cfg,\n+                    head_conv,\n+                    classes,\n+                    kernel_size=final_kernel,\n+                    stride=1,\n+                    padding=final_kernel // 2,\n+                    bias=True))\n+            conv_layers = nn.Sequential(*conv_layers)\n+\n+            self.__setattr__(head, conv_layers)\n+\n+            if init_cfg is None:\n+                self.init_cfg = dict(type='Kaiming', layer='Conv2d')\n+\n+    def init_weights(self):\n+        \"\"\"Initialize weights.\"\"\"\n+        super().init_weights()\n+        for head in self.heads:\n+            if head == 'heatmap':\n+                self.__getattr__(head)[-1].bias.data.fill_(self.init_bias)\n+\n+    def forward(self, x):\n+        \"\"\"Forward function for SepHead.\n+\n+        Args:\n+            x (torch.Tensor): Input feature map with the shape of\n+                [B, 512, 128, 128].\n+\n+        Returns:\n+            dict[str: torch.Tensor]: contains the following keys:\n+\n+                -reg （torch.Tensor): 2D regression value with the\n+                    shape of [B, 2, H, W].\n+                -height (torch.Tensor): Height value with the\n+                    shape of [B, 1, H, W].\n+                -dim (torch.Tensor): Size value with the shape\n+                    of [B, 3, H, W].\n+                -rot (torch.Tensor): Rotation value with the\n+                    shape of [B, 2, H, W].\n+                -vel (torch.Tensor): Velocity value with the\n+                    shape of [B, 2, H, W].\n+                -heatmap (torch.Tensor): Heatmap with the shape of\n+                    [B, N, H, W].\n+        \"\"\"\n+        ret_dict = dict()\n+        for head in self.heads:\n+            ret_dict[head] = self.__getattr__(head)(x)\n+\n+        return ret_dict\n+\n+\n+@HEADS.register_module()\n+class DCNSeparateHead(BaseModule):\n+    r\"\"\"DCNSeparateHead for CenterHead.\n+\n+    .. code-block:: none\n+            /-----> DCN for heatmap task -----> heatmap task.\n+    feature\n+            \\-----> DCN for regression tasks -----> regression tasks\n+\n+    Args:\n+        in_channels (int): Input channels for conv_layer.\n+        num_cls (int): Number of classes.\n+        heads (dict): Conv information.\n+        dcn_config (dict): Config of dcn layer.\n+        head_conv (int, optional): Output channels.\n+            Default: 64.\n+        final_kernel (int, optional): Kernel size for the last conv\n+            layer. Default: 1.\n+        init_bias (float, optional): Initial bias. Default: -2.19.\n+        conv_cfg (dict, optional): Config of conv layer.\n+            Default: dict(type='Conv2d')\n+        norm_cfg (dict, optional): Config of norm layer.\n+            Default: dict(type='BN2d').\n+        bias (str, optional): Type of bias. Default: 'auto'.\n+    \"\"\"  # noqa: W605\n+\n+    def __init__(self,\n+                 in_channels,\n+                 num_cls,\n+                 heads,\n+                 dcn_config,\n+                 head_conv=64,\n+                 final_kernel=1,\n+                 init_bias=-2.19,\n+                 conv_cfg=dict(type='Conv2d'),\n+                 norm_cfg=dict(type='BN2d'),\n+                 bias='auto',\n+                 init_cfg=None,\n+                 **kwargs):\n+        assert init_cfg is None, 'To prevent abnormal initialization ' \\\n+            'behavior, init_cfg is not allowed to be set'\n+        super(DCNSeparateHead, self).__init__(init_cfg=init_cfg)\n+        if 'heatmap' in heads:\n+            heads.pop('heatmap')\n+        # feature adaptation with dcn\n+        # use separate features for classification / regression\n+        self.feature_adapt_cls = build_conv_layer(dcn_config)\n+\n+        self.feature_adapt_reg = build_conv_layer(dcn_config)\n+\n+        # heatmap prediction head\n+        cls_head = [\n+            ConvModule(\n+                in_channels,\n+                head_conv,\n+                kernel_size=3,\n+                padding=1,\n+                conv_cfg=conv_cfg,\n+                bias=bias,\n+                norm_cfg=norm_cfg),\n+            build_conv_layer(\n+                conv_cfg,\n+                head_conv,\n+                num_cls,\n+                kernel_size=3,\n+                stride=1,\n+                padding=1,\n+                bias=bias)\n+        ]\n+        self.cls_head = nn.Sequential(*cls_head)\n+        self.init_bias = init_bias\n+        # other regression target\n+        self.task_head = SeparateHead(\n+            in_channels,\n+            heads,\n+            head_conv=head_conv,\n+            final_kernel=final_kernel,\n+            bias=bias)\n+        if init_cfg is None:\n+            self.init_cfg = dict(type='Kaiming', layer='Conv2d')\n+\n+    def init_weights(self):\n+        \"\"\"Initialize weights.\"\"\"\n+        super().init_weights()\n+        self.cls_head[-1].bias.data.fill_(self.init_bias)\n+\n+    def forward(self, x):\n+        \"\"\"Forward function for DCNSepHead.\n+\n+        Args:\n+            x (torch.Tensor): Input feature map with the shape of\n+                [B, 512, 128, 128].\n+\n+        Returns:\n+            dict[str: torch.Tensor]: contains the following keys:\n+\n+                -reg （torch.Tensor): 2D regression value with the\n+                    shape of [B, 2, H, W].\n+                -height (torch.Tensor): Height value with the\n+                    shape of [B, 1, H, W].\n+                -dim (torch.Tensor): Size value with the shape\n+                    of [B, 3, H, W].\n+                -rot (torch.Tensor): Rotation value with the\n+                    shape of [B, 2, H, W].\n+                -vel (torch.Tensor): Velocity value with the\n+                    shape of [B, 2, H, W].\n+                -heatmap (torch.Tensor): Heatmap with the shape of\n+                    [B, N, H, W].\n+        \"\"\"\n+        center_feat = self.feature_adapt_cls(x)\n+        reg_feat = self.feature_adapt_reg(x)\n+\n+        cls_score = self.cls_head(center_feat)\n+        ret = self.task_head(reg_feat)\n+        ret['heatmap'] = cls_score\n+\n+        return ret\n+\n+\n+@HEADS.register_module()\n+class CenterHead(BaseModule):\n+    \"\"\"CenterHead for CenterPoint.\n+\n+    Args:\n+        in_channels (list[int] | int, optional): Channels of the input\n+            feature map. Default: [128].\n+        tasks (list[dict], optional): Task information including class number\n+            and class names. Default: None.\n+        train_cfg (dict, optional): Train-time configs. Default: None.\n+        test_cfg (dict, optional): Test-time configs. Default: None.\n+        bbox_coder (dict, optional): Bbox coder configs. Default: None.\n+        common_heads (dict, optional): Conv information for common heads.\n+            Default: dict().\n+        loss_cls (dict, optional): Config of classification loss function.\n+            Default: dict(type='GaussianFocalLoss', reduction='mean').\n+        loss_bbox (dict, optional): Config of regression loss function.\n+            Default: dict(type='L1Loss', reduction='none').\n+        separate_head (dict, optional): Config of separate head. Default: dict(\n+            type='SeparateHead', init_bias=-2.19, final_kernel=3)\n+        share_conv_channel (int, optional): Output channels for share_conv\n+            layer. Default: 64.\n+        num_heatmap_convs (int, optional): Number of conv layers for heatmap\n+            conv layer. Default: 2.\n+        conv_cfg (dict, optional): Config of conv layer.\n+            Default: dict(type='Conv2d')\n+        norm_cfg (dict, optional): Config of norm layer.\n+            Default: dict(type='BN2d').\n+        bias (str, optional): Type of bias. Default: 'auto'.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 in_channels=[128],\n+                 tasks=None,\n+                 train_cfg=None,\n+                 test_cfg=None,\n+                 bbox_coder=None,\n+                 common_heads=dict(),\n+                 loss_cls=dict(type='GaussianFocalLoss', reduction='mean'),\n+                 loss_bbox=dict(\n+                     type='L1Loss', reduction='none', loss_weight=0.25),\n+                 separate_head=dict(\n+                     type='SeparateHead', init_bias=-2.19, final_kernel=3),\n+                 share_conv_channel=64,\n+                 num_heatmap_convs=2,\n+                 conv_cfg=dict(type='Conv2d'),\n+                 norm_cfg=dict(type='BN2d'),\n+                 bias='auto',\n+                 norm_bbox=True,\n+                 init_cfg=None,\n+                 task_specific=True):\n+        assert init_cfg is None, 'To prevent abnormal initialization ' \\\n+            'behavior, init_cfg is not allowed to be set'\n+        super(CenterHead, self).__init__(init_cfg=init_cfg)\n+\n+        num_classes = [len(t['class_names']) for t in tasks]\n+        self.class_names = [t['class_names'] for t in tasks]\n+        self.train_cfg = train_cfg\n+        self.test_cfg = test_cfg\n+        self.in_channels = in_channels\n+        self.num_classes = num_classes\n+        self.norm_bbox = norm_bbox\n+\n+        self.loss_cls = build_loss(loss_cls)\n+        self.loss_bbox = build_loss(loss_bbox)\n+        self.bbox_coder = build_bbox_coder(bbox_coder)\n+        self.num_anchor_per_locs = [n for n in num_classes]\n+        self.fp16_enabled = False\n+\n+        # a shared convolution\n+        self.shared_conv = ConvModule(\n+            in_channels,\n+            share_conv_channel,\n+            kernel_size=3,\n+            padding=1,\n+            conv_cfg=conv_cfg,\n+            norm_cfg=norm_cfg,\n+            bias=bias)\n+\n+        self.task_heads = nn.ModuleList()\n+\n+        for num_cls in num_classes:\n+            heads = copy.deepcopy(common_heads)\n+            heads.update(dict(heatmap=(num_cls, num_heatmap_convs)))\n+            separate_head.update(\n+                in_channels=share_conv_channel, heads=heads, num_cls=num_cls)\n+            self.task_heads.append(builder.build_head(separate_head))\n+\n+        self.with_velocity = 'vel' in common_heads.keys()\n+        self.task_specific = task_specific\n+\n+    def forward_single(self, x):\n+        \"\"\"Forward function for CenterPoint.\n+\n+        Args:\n+            x (torch.Tensor): Input feature map with the shape of\n+                [B, 512, 128, 128].\n+\n+        Returns:\n+            list[dict]: Output results for tasks.\n+        \"\"\"\n+        ret_dicts = []\n+\n+        x = self.shared_conv(x)\n+\n+        for task in self.task_heads:\n+            ret_dicts.append(task(x))\n+\n+        return ret_dicts\n+\n+    def forward(self, feats):\n+        \"\"\"Forward pass.\n+\n+        Args:\n+            feats (list[torch.Tensor]): Multi-level features, e.g.,\n+                features produced by FPN.\n+\n+        Returns:\n+            tuple(list[dict]): Output results for tasks.\n+        \"\"\"\n+        return multi_apply(self.forward_single, feats)\n+\n+    def _gather_feat(self, feat, ind, mask=None):\n+        \"\"\"Gather feature map.\n+\n+        Given feature map and index, return indexed feature map.\n+\n+        Args:\n+            feat (torch.tensor): Feature map with the shape of [B, H*W, 10].\n+            ind (torch.Tensor): Index of the ground truth boxes with the\n+                shape of [B, max_obj].\n+            mask (torch.Tensor, optional): Mask of the feature map with the\n+                shape of [B, max_obj]. Default: None.\n+\n+        Returns:\n+            torch.Tensor: Feature map after gathering with the shape\n+                of [B, max_obj, 10].\n+        \"\"\"\n+        dim = feat.size(2)\n+        ind = ind.unsqueeze(2).expand(ind.size(0), ind.size(1), dim)\n+        feat = feat.gather(1, ind)\n+        if mask is not None:\n+            mask = mask.unsqueeze(2).expand_as(feat)\n+            feat = feat[mask]\n+            feat = feat.view(-1, dim)\n+        return feat\n+\n+    def get_targets(self, gt_bboxes_3d, gt_labels_3d):\n+        \"\"\"Generate targets.\n+\n+        How each output is transformed:\n+\n+            Each nested list is transposed so that all same-index elements in\n+            each sub-list (1, ..., N) become the new sub-lists.\n+                [ [a0, a1, a2, ... ], [b0, b1, b2, ... ], ... ]\n+                ==> [ [a0, b0, ... ], [a1, b1, ... ], [a2, b2, ... ] ]\n+\n+            The new transposed nested list is converted into a list of N\n+            tensors generated by concatenating tensors in the new sub-lists.\n+                [ tensor0, tensor1, tensor2, ... ]\n+\n+        Args:\n+            gt_bboxes_3d (list[:obj:`LiDARInstance3DBoxes`]): Ground\n+                truth gt boxes.\n+            gt_labels_3d (list[torch.Tensor]): Labels of boxes.\n+\n+        Returns:\n+            Returns:\n+                tuple[list[torch.Tensor]]: Tuple of target including\n+                    the following results in order.\n+\n+                    - list[torch.Tensor]: Heatmap scores.\n+                    - list[torch.Tensor]: Ground truth boxes.\n+                    - list[torch.Tensor]: Indexes indicating the\n+                        position of the valid boxes.\n+                    - list[torch.Tensor]: Masks indicating which\n+                        boxes are valid.\n+        \"\"\"\n+        heatmaps, anno_boxes, inds, masks = multi_apply(\n+            self.get_targets_single, gt_bboxes_3d, gt_labels_3d)\n+        # Transpose heatmaps\n+        heatmaps = list(map(list, zip(*heatmaps)))\n+        heatmaps = [torch.stack(hms_) for hms_ in heatmaps]\n+        # Transpose anno_boxes\n+        anno_boxes = list(map(list, zip(*anno_boxes)))\n+        anno_boxes = [torch.stack(anno_boxes_) for anno_boxes_ in anno_boxes]\n+        # Transpose inds\n+        inds = list(map(list, zip(*inds)))\n+        inds = [torch.stack(inds_) for inds_ in inds]\n+        # Transpose inds\n+        masks = list(map(list, zip(*masks)))\n+        masks = [torch.stack(masks_) for masks_ in masks]\n+        return heatmaps, anno_boxes, inds, masks\n+\n+    def get_targets_single(self, gt_bboxes_3d, gt_labels_3d):\n+        \"\"\"Generate training targets for a single sample.\n+\n+        Args:\n+            gt_bboxes_3d (:obj:`LiDARInstance3DBoxes`): Ground truth gt boxes.\n+            gt_labels_3d (torch.Tensor): Labels of boxes.\n+\n+        Returns:\n+            tuple[list[torch.Tensor]]: Tuple of target including\n+                the following results in order.\n+\n+                - list[torch.Tensor]: Heatmap scores.\n+                - list[torch.Tensor]: Ground truth boxes.\n+                - list[torch.Tensor]: Indexes indicating the position\n+                    of the valid boxes.\n+                - list[torch.Tensor]: Masks indicating which boxes\n+                    are valid.\n+        \"\"\"\n+        device = gt_labels_3d.device\n+        gt_bboxes_3d = torch.cat(\n+            (gt_bboxes_3d.gravity_center, gt_bboxes_3d.tensor[:, 3:]),\n+            dim=1).to(device)\n+        max_objs = self.train_cfg['max_objs'] * self.train_cfg['dense_reg']\n+        grid_size = torch.tensor(self.train_cfg['grid_size'])\n+        pc_range = torch.tensor(self.train_cfg['point_cloud_range'])\n+        voxel_size = torch.tensor(self.train_cfg['voxel_size'])\n+\n+        feature_map_size = grid_size[:2] // self.train_cfg['out_size_factor']\n+\n+        # reorganize the gt_dict by tasks\n+        task_masks = []\n+        flag = 0\n+        for class_name in self.class_names:\n+            task_masks.append([\n+                torch.where(gt_labels_3d == class_name.index(i) + flag)\n+                for i in class_name\n+            ])\n+            flag += len(class_name)\n+\n+        task_boxes = []\n+        task_classes = []\n+        flag2 = 0\n+        for idx, mask in enumerate(task_masks):\n+            task_box = []\n+            task_class = []\n+            for m in mask:\n+                task_box.append(gt_bboxes_3d[m])\n+                # 0 is background for each task, so we need to add 1 here.\n+                task_class.append(gt_labels_3d[m] + 1 - flag2)\n+            task_boxes.append(torch.cat(task_box, axis=0).to(device))\n+            task_classes.append(torch.cat(task_class).long().to(device))\n+            flag2 += len(mask)\n+        draw_gaussian = draw_heatmap_gaussian\n+        heatmaps, anno_boxes, inds, masks = [], [], [], []\n+\n+        for idx, task_head in enumerate(self.task_heads):\n+            heatmap = gt_bboxes_3d.new_zeros(\n+                (len(self.class_names[idx]), feature_map_size[1],\n+                 feature_map_size[0]))\n+\n+            if self.with_velocity:\n+                anno_box = gt_bboxes_3d.new_zeros((max_objs, 10),\n+                                                  dtype=torch.float32)\n+            else:\n+                anno_box = gt_bboxes_3d.new_zeros((max_objs, 8),\n+                                                  dtype=torch.float32)\n+\n+            ind = gt_labels_3d.new_zeros((max_objs), dtype=torch.int64)\n+            mask = gt_bboxes_3d.new_zeros((max_objs), dtype=torch.uint8)\n+\n+            num_objs = min(task_boxes[idx].shape[0], max_objs)\n+\n+            for k in range(num_objs):\n+                cls_id = task_classes[idx][k] - 1\n+\n+                width = task_boxes[idx][k][3]\n+                length = task_boxes[idx][k][4]\n+                width = width / voxel_size[0] / self.train_cfg[\n+                    'out_size_factor']\n+                length = length / voxel_size[1] / self.train_cfg[\n+                    'out_size_factor']\n+\n+                if width > 0 and length > 0:\n+                    radius = gaussian_radius(\n+                        (length, width),\n+                        min_overlap=self.train_cfg['gaussian_overlap'])\n+                    radius = max(self.train_cfg['min_radius'], int(radius))\n+\n+                    # be really careful for the coordinate system of\n+                    # your box annotation.\n+                    x, y, z = task_boxes[idx][k][0], task_boxes[idx][k][\n+                        1], task_boxes[idx][k][2]\n+\n+                    coor_x = (\n+                        x - pc_range[0]\n+                    ) / voxel_size[0] / self.train_cfg['out_size_factor']\n+                    coor_y = (\n+                        y - pc_range[1]\n+                    ) / voxel_size[1] / self.train_cfg['out_size_factor']\n+\n+                    center = torch.tensor([coor_x, coor_y],\n+                                          dtype=torch.float32,\n+                                          device=device)\n+                    center_int = center.to(torch.int32)\n+\n+                    # throw out not in range objects to avoid out of array\n+                    # area when creating the heatmap\n+                    if not (0 <= center_int[0] < feature_map_size[0]\n+                            and 0 <= center_int[1] < feature_map_size[1]):\n+                        continue\n+\n+                    draw_gaussian(heatmap[cls_id], center_int, radius)\n+\n+                    new_idx = k\n+                    x, y = center_int[0], center_int[1]\n+\n+                    assert (y * feature_map_size[0] + x <\n+                            feature_map_size[0] * feature_map_size[1])\n+\n+                    ind[new_idx] = y * feature_map_size[0] + x\n+                    mask[new_idx] = 1\n+                    # TODO: support other outdoor dataset\n+                    rot = task_boxes[idx][k][6]\n+                    box_dim = task_boxes[idx][k][3:6]\n+                    if self.norm_bbox:\n+                        box_dim = box_dim.log()\n+                    if self.with_velocity:\n+                        vx, vy = task_boxes[idx][k][7:]\n+                        anno_box[new_idx] = torch.cat([\n+                            center - torch.tensor([x, y], device=device),\n+                            z.unsqueeze(0), box_dim,\n+                            torch.sin(rot).unsqueeze(0),\n+                            torch.cos(rot).unsqueeze(0),\n+                            vx.unsqueeze(0),\n+                            vy.unsqueeze(0)\n+                        ])\n+                    else:\n+                        anno_box[new_idx] = torch.cat([\n+                            center - torch.tensor([x, y], device=device),\n+                            z.unsqueeze(0), box_dim,\n+                            torch.sin(rot).unsqueeze(0),\n+                            torch.cos(rot).unsqueeze(0)\n+                        ])\n+\n+            heatmaps.append(heatmap)\n+            anno_boxes.append(anno_box)\n+            masks.append(mask)\n+            inds.append(ind)\n+        return heatmaps, anno_boxes, inds, masks\n+\n+    def loss(self, gt_bboxes_3d, gt_labels_3d, preds_dicts, **kwargs):\n+        \"\"\"Loss function for CenterHead.\n+\n+        Args:\n+            gt_bboxes_3d (list[:obj:`LiDARInstance3DBoxes`]): Ground\n+                truth gt boxes.\n+            gt_labels_3d (list[torch.Tensor]): Labels of boxes.\n+            preds_dicts (dict): Output of forward function.\n+\n+        Returns:\n+            dict[str:torch.Tensor]: Loss of heatmap and bbox of each task.\n+        \"\"\"\n+        heatmaps, anno_boxes, inds, masks = self.get_targets(\n+            gt_bboxes_3d, gt_labels_3d)\n+        loss_dict = dict()\n+        if not self.task_specific:\n+            loss_dict['loss'] = 0\n+        for task_id, preds_dict in enumerate(preds_dicts):\n+            # heatmap focal loss\n+            preds_dict[0]['heatmap'] = clip_sigmoid(preds_dict[0]['heatmap'])\n+            num_pos = heatmaps[task_id].eq(1).float().sum().item()\n+            cls_avg_factor = torch.clamp(\n+                reduce_mean(heatmaps[task_id].new_tensor(num_pos)),\n+                min=1).item()\n+            \n+            loss_heatmap = self.loss_cls(\n+                preds_dict[0]['heatmap'],\n+                heatmaps[task_id],\n+                avg_factor=cls_avg_factor)\n+            target_box = anno_boxes[task_id]\n+            # reconstruct the anno_box from multiple reg heads\n+            preds_dict[0]['anno_box'] = torch.cat(\n+                (\n+                    preds_dict[0]['reg'],\n+                    preds_dict[0]['height'],\n+                    preds_dict[0]['dim'],\n+                    preds_dict[0]['rot'],\n+                    preds_dict[0]['vel'],\n+                ),\n+                dim=1,\n+            )\n+\n+            # Regression loss for dimension, offset, height, rotation\n+            num = masks[task_id].float().sum()\n+            ind = inds[task_id]\n+            pred = preds_dict[0]['anno_box'].permute(0, 2, 3, 1).contiguous()\n+            pred = pred.view(pred.size(0), -1, pred.size(3))\n+            pred = self._gather_feat(pred, ind)\n+            mask = masks[task_id].unsqueeze(2).expand_as(target_box).float()\n+            num = torch.clamp(\n+                reduce_mean(target_box.new_tensor(num)), min=1e-4).item()\n+            isnotnan = (~torch.isnan(target_box)).float()\n+            mask *= isnotnan\n+            code_weights = self.train_cfg['code_weights']\n+            bbox_weights = mask * mask.new_tensor(code_weights)\n+            if self.task_specific:\n+                name_list = ['xy', 'z', 'whl', 'yaw', 'vel']\n+                clip_index = [0, 2, 3, 6, 8, 10]\n+                for reg_task_id in range(len(name_list)):\n+                    pred_tmp = pred[\n+                        ...,\n+                        clip_index[reg_task_id]:clip_index[reg_task_id + 1]]\n+                    target_box_tmp = target_box[\n+                        ...,\n+                        clip_index[reg_task_id]:clip_index[reg_task_id + 1]]\n+                    bbox_weights_tmp = bbox_weights[\n+                        ...,\n+                        clip_index[reg_task_id]:clip_index[reg_task_id + 1]]\n+                    loss_bbox_tmp = self.loss_bbox(\n+                        pred_tmp,\n+                        target_box_tmp,\n+                        bbox_weights_tmp,\n+                        avg_factor=(num + 1e-4))\n+                    loss_dict[f'task{task_id}.loss_%s' %\n+                              (name_list[reg_task_id])] = loss_bbox_tmp\n+                loss_dict[f'task{task_id}.loss_heatmap'] = loss_heatmap\n+            else:\n+                loss_bbox = self.loss_bbox(\n+                    pred, target_box, bbox_weights, avg_factor=num)\n+                loss_dict['loss'] += loss_bbox\n+                loss_dict['loss'] += loss_heatmap\n+\n+        return loss_dict\n+\n+    def get_bboxes(self, preds_dicts, img_metas, img=None, rescale=False):\n+        \"\"\"Generate bboxes from bbox head predictions.\n+\n+        Args:\n+            preds_dicts (tuple[list[dict]]): Prediction results.\n+            img_metas (list[dict]): Point cloud and image's meta info.\n+\n+        Returns:\n+            list[dict]: Decoded bbox, scores and labels after nms.\n+        \"\"\"\n+        rets = []\n+        for task_id, preds_dict in enumerate(preds_dicts):\n+            batch_size = preds_dict[0]['heatmap'].shape[0]\n+            batch_heatmap = preds_dict[0]['heatmap'].sigmoid()\n+\n+            batch_reg = preds_dict[0]['reg']\n+            batch_hei = preds_dict[0]['height']\n+\n+            if self.norm_bbox:\n+                batch_dim = torch.exp(preds_dict[0]['dim'])\n+            else:\n+                batch_dim = preds_dict[0]['dim']\n+\n+            batch_rots = preds_dict[0]['rot'][:, 0].unsqueeze(1)\n+            batch_rotc = preds_dict[0]['rot'][:, 1].unsqueeze(1)\n+\n+            if 'vel' in preds_dict[0]:\n+                batch_vel = preds_dict[0]['vel']\n+            else:\n+                batch_vel = None\n+            temp = self.bbox_coder.decode(\n+                batch_heatmap,\n+                batch_rots,\n+                batch_rotc,\n+                batch_hei,\n+                batch_dim,\n+                batch_vel,\n+                reg=batch_reg,\n+                task_id=task_id)\n+            batch_reg_preds = [box['bboxes'] for box in temp]\n+            batch_cls_preds = [box['scores'] for box in temp]\n+            batch_cls_labels = [box['labels'] for box in temp]\n+            nms_type = self.test_cfg.get('nms_type')\n+            if isinstance(nms_type, list):\n+                nms_type = nms_type[task_id]\n+            if nms_type == 'circle':\n+                ret_task = []\n+                for i in range(batch_size):\n+                    boxes3d = temp[i]['bboxes']\n+                    scores = temp[i]['scores']\n+                    labels = temp[i]['labels']\n+                    centers = boxes3d[:, [0, 1]]\n+                    boxes = torch.cat([centers, scores.view(-1, 1)], dim=1)\n+                    keep = torch.tensor(\n+                        circle_nms(\n+                            boxes.detach().cpu().numpy(),\n+                            self.test_cfg['min_radius'][task_id],\n+                            post_max_size=self.test_cfg['post_max_size']),\n+                        dtype=torch.long,\n+                        device=boxes.device)\n+\n+                    boxes3d = boxes3d[keep]\n+                    scores = scores[keep]\n+                    labels = labels[keep]\n+                    ret = dict(bboxes=boxes3d, scores=scores, labels=labels)\n+                    ret_task.append(ret)\n+                rets.append(ret_task)\n+            else:\n+                rets.append(\n+                    self.get_task_detections(batch_cls_preds, batch_reg_preds,\n+                                             batch_cls_labels, img_metas,\n+                                             task_id))\n+\n+        # Merge branches results\n+        num_samples = len(rets[0])\n+\n+        ret_list = []\n+        for i in range(num_samples):\n+            for k in rets[0][i].keys():\n+                if k == 'bboxes':\n+                    bboxes = torch.cat([ret[i][k] for ret in rets])\n+                    bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 5] * 0.5\n+                    bboxes = img_metas[i]['box_type_3d'](\n+                        bboxes, self.bbox_coder.code_size)\n+                elif k == 'scores':\n+                    scores = torch.cat([ret[i][k] for ret in rets])\n+                elif k == 'labels':\n+                    flag = 0\n+                    for j, num_class in enumerate(self.num_classes):\n+                        rets[j][i][k] += flag\n+                        flag += num_class\n+                    labels = torch.cat([ret[i][k].int() for ret in rets])\n+            ret_list.append([bboxes, scores, labels])\n+        return ret_list\n+\n+    def get_task_detections(self, batch_cls_preds,\n+                            batch_reg_preds, batch_cls_labels, img_metas,\n+                            task_id):\n+        \"\"\"Rotate nms for each task.\n+\n+        Args:\n+            batch_cls_preds (list[torch.Tensor]): Prediction score with the\n+                shape of [N].\n+            batch_reg_preds (list[torch.Tensor]): Prediction bbox with the\n+                shape of [N, 9].\n+            batch_cls_labels (list[torch.Tensor]): Prediction label with the\n+                shape of [N].\n+            img_metas (list[dict]): Meta information of each sample.\n+\n+        Returns:\n+            list[dict[str: torch.Tensor]]: contains the following keys:\n+\n+                -bboxes (torch.Tensor): Prediction bboxes after nms with the\n+                    shape of [N, 9].\n+                -scores (torch.Tensor): Prediction scores after nms with the\n+                    shape of [N].\n+                -labels (torch.Tensor): Prediction labels after nms with the\n+                    shape of [N].\n+        \"\"\"\n+        predictions_dicts = []\n+        for i, (box_preds, cls_preds, cls_labels) in enumerate(\n+                zip(batch_reg_preds, batch_cls_preds, batch_cls_labels)):\n+            default_val = [1.0 for _ in range(len(self.task_heads))]\n+            factor = self.test_cfg.get('nms_rescale_factor',\n+                                       default_val)[task_id]\n+            if isinstance(factor, list):\n+                for cid in range(len(factor)):\n+                    box_preds[cls_labels == cid, 3:6] = \\\n+                        box_preds[cls_labels == cid, 3:6] * factor[cid]\n+            else:\n+                box_preds[:, 3:6] = box_preds[:, 3:6] * factor\n+\n+            # Apply NMS in birdeye view\n+            top_labels = cls_labels.long()\n+            top_scores = cls_preds.squeeze(-1) if cls_preds.shape[0]>1 \\\n+                else cls_preds\n+\n+            if top_scores.shape[0] != 0:\n+                boxes_for_nms = img_metas[i]['box_type_3d'](\n+                    box_preds[:, :], self.bbox_coder.code_size).bev\n+                # the nms in 3d detection just remove overlap boxes.\n+                if isinstance(self.test_cfg['nms_thr'], list):\n+                    nms_thresh = self.test_cfg['nms_thr'][task_id]\n+                else:\n+                    nms_thresh = self.test_cfg['nms_thr']\n+                selected = nms_bev(\n+                    boxes_for_nms,\n+                    top_scores,\n+                    thresh=nms_thresh,\n+                    pre_max_size=self.test_cfg['pre_max_size'],\n+                    post_max_size=self.test_cfg['post_max_size'],\n+                    xyxyr2xywhr=False)\n+            else:\n+                selected = []\n+\n+            if isinstance(factor, list):\n+                for cid in range(len(factor)):\n+                    box_preds[top_labels == cid, 3:6] = \\\n+                        box_preds[top_labels == cid, 3:6] / factor[cid]\n+            else:\n+                box_preds[:, 3:6] = box_preds[:, 3:6] / factor\n+\n+            # if selected is not None:\n+            selected_boxes = box_preds[selected]\n+            selected_labels = top_labels[selected]\n+            selected_scores = top_scores[selected]\n+\n+            # finally generate predictions.\n+            if selected_boxes.shape[0] != 0:\n+                predictions_dict = dict(\n+                    bboxes=selected_boxes,\n+                    scores=selected_scores,\n+                    labels=selected_labels)\n+            else:\n+                dtype = batch_reg_preds[0].dtype\n+                device = batch_reg_preds[0].device\n+                predictions_dict = dict(\n+                    bboxes=torch.zeros([0, self.bbox_coder.code_size],\n+                                       dtype=dtype,\n+                                       device=device),\n+                    scores=torch.zeros([0], dtype=dtype, device=device),\n+                    labels=torch.zeros([0],\n+                                       dtype=top_labels.dtype,\n+                                       device=device))\n+\n+            predictions_dicts.append(predictions_dict)\n+        return predictions_dicts\n"
                },
                {
                    "date": 1720769093731,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -854,861 +854,4 @@\n                                        device=device))\n \n             predictions_dicts.append(predictions_dict)\n         return predictions_dicts\n-# Copyright (c) OpenMMLab. All rights reserved.\n-import copy\n-\n-import torch\n-from mmcv.cnn import ConvModule, build_conv_layer\n-from mmcv.runner import BaseModule\n-from torch import nn\n-\n-from mmdet3d.core import (circle_nms, draw_heatmap_gaussian, gaussian_radius,\n-                          xywhr2xyxyr)\n-from mmdet3d.core.post_processing import nms_bev\n-from mmdet3d.models import builder\n-from mmdet3d.models.utils import clip_sigmoid\n-from mmdet.core import build_bbox_coder, multi_apply, reduce_mean\n-from ..builder import HEADS, build_loss\n-\n-\n-@HEADS.register_module()\n-class SeparateHead(BaseModule):\n-    \"\"\"SeparateHead for CenterHead.\n-\n-    Args:\n-        in_channels (int): Input channels for conv_layer.\n-        heads (dict): Conv information.\n-        head_conv (int, optional): Output channels.\n-            Default: 64.\n-        final_kernel (int, optional): Kernel size for the last conv layer.\n-            Default: 1.\n-        init_bias (float, optional): Initial bias. Default: -2.19.\n-        conv_cfg (dict, optional): Config of conv layer.\n-            Default: dict(type='Conv2d')\n-        norm_cfg (dict, optional): Config of norm layer.\n-            Default: dict(type='BN2d').\n-        bias (str, optional): Type of bias. Default: 'auto'.\n-    \"\"\"\n-\n-    def __init__(self,\n-                 in_channels,\n-                 heads,\n-                 head_conv=64,\n-                 final_kernel=1,\n-                 init_bias=-2.19,\n-                 conv_cfg=dict(type='Conv2d'),\n-                 norm_cfg=dict(type='BN2d'),\n-                 bias='auto',\n-                 init_cfg=None,\n-                 **kwargs):\n-        assert init_cfg is None, 'To prevent abnormal initialization ' \\\n-            'behavior, init_cfg is not allowed to be set'\n-        super(SeparateHead, self).__init__(init_cfg=init_cfg)\n-        self.heads = heads\n-        self.init_bias = init_bias\n-        for head in self.heads:\n-            classes, num_conv = self.heads[head]\n-\n-            conv_layers = []\n-            c_in = in_channels\n-            for i in range(num_conv - 1):\n-                conv_layers.append(\n-                    ConvModule(\n-                        c_in,\n-                        head_conv,\n-                        kernel_size=final_kernel,\n-                        stride=1,\n-                        padding=final_kernel // 2,\n-                        bias=bias,\n-                        conv_cfg=conv_cfg,\n-                        norm_cfg=norm_cfg))\n-                c_in = head_conv\n-\n-            conv_layers.append(\n-                build_conv_layer(\n-                    conv_cfg,\n-                    head_conv,\n-                    classes,\n-                    kernel_size=final_kernel,\n-                    stride=1,\n-                    padding=final_kernel // 2,\n-                    bias=True))\n-            conv_layers = nn.Sequential(*conv_layers)\n-\n-            self.__setattr__(head, conv_layers)\n-\n-            if init_cfg is None:\n-                self.init_cfg = dict(type='Kaiming', layer='Conv2d')\n-\n-    def init_weights(self):\n-        \"\"\"Initialize weights.\"\"\"\n-        super().init_weights()\n-        for head in self.heads:\n-            if head == 'heatmap':\n-                self.__getattr__(head)[-1].bias.data.fill_(self.init_bias)\n-\n-    def forward(self, x):\n-        \"\"\"Forward function for SepHead.\n-\n-        Args:\n-            x (torch.Tensor): Input feature map with the shape of\n-                [B, 512, 128, 128].\n-\n-        Returns:\n-            dict[str: torch.Tensor]: contains the following keys:\n-\n-                -reg （torch.Tensor): 2D regression value with the\n-                    shape of [B, 2, H, W].\n-                -height (torch.Tensor): Height value with the\n-                    shape of [B, 1, H, W].\n-                -dim (torch.Tensor): Size value with the shape\n-                    of [B, 3, H, W].\n-                -rot (torch.Tensor): Rotation value with the\n-                    shape of [B, 2, H, W].\n-                -vel (torch.Tensor): Velocity value with the\n-                    shape of [B, 2, H, W].\n-                -heatmap (torch.Tensor): Heatmap with the shape of\n-                    [B, N, H, W].\n-        \"\"\"\n-        ret_dict = dict()\n-        for head in self.heads:\n-            ret_dict[head] = self.__getattr__(head)(x)\n-\n-        return ret_dict\n-\n-\n-@HEADS.register_module()\n-class DCNSeparateHead(BaseModule):\n-    r\"\"\"DCNSeparateHead for CenterHead.\n-\n-    .. code-block:: none\n-            /-----> DCN for heatmap task -----> heatmap task.\n-    feature\n-            \\-----> DCN for regression tasks -----> regression tasks\n-\n-    Args:\n-        in_channels (int): Input channels for conv_layer.\n-        num_cls (int): Number of classes.\n-        heads (dict): Conv information.\n-        dcn_config (dict): Config of dcn layer.\n-        head_conv (int, optional): Output channels.\n-            Default: 64.\n-        final_kernel (int, optional): Kernel size for the last conv\n-            layer. Default: 1.\n-        init_bias (float, optional): Initial bias. Default: -2.19.\n-        conv_cfg (dict, optional): Config of conv layer.\n-            Default: dict(type='Conv2d')\n-        norm_cfg (dict, optional): Config of norm layer.\n-            Default: dict(type='BN2d').\n-        bias (str, optional): Type of bias. Default: 'auto'.\n-    \"\"\"  # noqa: W605\n-\n-    def __init__(self,\n-                 in_channels,\n-                 num_cls,\n-                 heads,\n-                 dcn_config,\n-                 head_conv=64,\n-                 final_kernel=1,\n-                 init_bias=-2.19,\n-                 conv_cfg=dict(type='Conv2d'),\n-                 norm_cfg=dict(type='BN2d'),\n-                 bias='auto',\n-                 init_cfg=None,\n-                 **kwargs):\n-        assert init_cfg is None, 'To prevent abnormal initialization ' \\\n-            'behavior, init_cfg is not allowed to be set'\n-        super(DCNSeparateHead, self).__init__(init_cfg=init_cfg)\n-        if 'heatmap' in heads:\n-            heads.pop('heatmap')\n-        # feature adaptation with dcn\n-        # use separate features for classification / regression\n-        self.feature_adapt_cls = build_conv_layer(dcn_config)\n-\n-        self.feature_adapt_reg = build_conv_layer(dcn_config)\n-\n-        # heatmap prediction head\n-        cls_head = [\n-            ConvModule(\n-                in_channels,\n-                head_conv,\n-                kernel_size=3,\n-                padding=1,\n-                conv_cfg=conv_cfg,\n-                bias=bias,\n-                norm_cfg=norm_cfg),\n-            build_conv_layer(\n-                conv_cfg,\n-                head_conv,\n-                num_cls,\n-                kernel_size=3,\n-                stride=1,\n-                padding=1,\n-                bias=bias)\n-        ]\n-        self.cls_head = nn.Sequential(*cls_head)\n-        self.init_bias = init_bias\n-        # other regression target\n-        self.task_head = SeparateHead(\n-            in_channels,\n-            heads,\n-            head_conv=head_conv,\n-            final_kernel=final_kernel,\n-            bias=bias)\n-        if init_cfg is None:\n-            self.init_cfg = dict(type='Kaiming', layer='Conv2d')\n-\n-    def init_weights(self):\n-        \"\"\"Initialize weights.\"\"\"\n-        super().init_weights()\n-        self.cls_head[-1].bias.data.fill_(self.init_bias)\n-\n-    def forward(self, x):\n-        \"\"\"Forward function for DCNSepHead.\n-\n-        Args:\n-            x (torch.Tensor): Input feature map with the shape of\n-                [B, 512, 128, 128].\n-\n-        Returns:\n-            dict[str: torch.Tensor]: contains the following keys:\n-\n-                -reg （torch.Tensor): 2D regression value with the\n-                    shape of [B, 2, H, W].\n-                -height (torch.Tensor): Height value with the\n-                    shape of [B, 1, H, W].\n-                -dim (torch.Tensor): Size value with the shape\n-                    of [B, 3, H, W].\n-                -rot (torch.Tensor): Rotation value with the\n-                    shape of [B, 2, H, W].\n-                -vel (torch.Tensor): Velocity value with the\n-                    shape of [B, 2, H, W].\n-                -heatmap (torch.Tensor): Heatmap with the shape of\n-                    [B, N, H, W].\n-        \"\"\"\n-        center_feat = self.feature_adapt_cls(x)\n-        reg_feat = self.feature_adapt_reg(x)\n-\n-        cls_score = self.cls_head(center_feat)\n-        ret = self.task_head(reg_feat)\n-        ret['heatmap'] = cls_score\n-\n-        return ret\n-\n-\n-@HEADS.register_module()\n-class CenterHead(BaseModule):\n-    \"\"\"CenterHead for CenterPoint.\n-\n-    Args:\n-        in_channels (list[int] | int, optional): Channels of the input\n-            feature map. Default: [128].\n-        tasks (list[dict], optional): Task information including class number\n-            and class names. Default: None.\n-        train_cfg (dict, optional): Train-time configs. Default: None.\n-        test_cfg (dict, optional): Test-time configs. Default: None.\n-        bbox_coder (dict, optional): Bbox coder configs. Default: None.\n-        common_heads (dict, optional): Conv information for common heads.\n-            Default: dict().\n-        loss_cls (dict, optional): Config of classification loss function.\n-            Default: dict(type='GaussianFocalLoss', reduction='mean').\n-        loss_bbox (dict, optional): Config of regression loss function.\n-            Default: dict(type='L1Loss', reduction='none').\n-        separate_head (dict, optional): Config of separate head. Default: dict(\n-            type='SeparateHead', init_bias=-2.19, final_kernel=3)\n-        share_conv_channel (int, optional): Output channels for share_conv\n-            layer. Default: 64.\n-        num_heatmap_convs (int, optional): Number of conv layers for heatmap\n-            conv layer. Default: 2.\n-        conv_cfg (dict, optional): Config of conv layer.\n-            Default: dict(type='Conv2d')\n-        norm_cfg (dict, optional): Config of norm layer.\n-            Default: dict(type='BN2d').\n-        bias (str, optional): Type of bias. Default: 'auto'.\n-    \"\"\"\n-\n-    def __init__(self,\n-                 in_channels=[128],\n-                 tasks=None,\n-                 train_cfg=None,\n-                 test_cfg=None,\n-                 bbox_coder=None,\n-                 common_heads=dict(),\n-                 loss_cls=dict(type='GaussianFocalLoss', reduction='mean'),\n-                 loss_bbox=dict(\n-                     type='L1Loss', reduction='none', loss_weight=0.25),\n-                 separate_head=dict(\n-                     type='SeparateHead', init_bias=-2.19, final_kernel=3),\n-                 share_conv_channel=64,\n-                 num_heatmap_convs=2,\n-                 conv_cfg=dict(type='Conv2d'),\n-                 norm_cfg=dict(type='BN2d'),\n-                 bias='auto',\n-                 norm_bbox=True,\n-                 init_cfg=None,\n-                 task_specific=True):\n-        assert init_cfg is None, 'To prevent abnormal initialization ' \\\n-            'behavior, init_cfg is not allowed to be set'\n-        super(CenterHead, self).__init__(init_cfg=init_cfg)\n-\n-        num_classes = [len(t['class_names']) for t in tasks]\n-        self.class_names = [t['class_names'] for t in tasks]\n-        self.train_cfg = train_cfg\n-        self.test_cfg = test_cfg\n-        self.in_channels = in_channels\n-        self.num_classes = num_classes\n-        self.norm_bbox = norm_bbox\n-\n-        self.loss_cls = build_loss(loss_cls)\n-        self.loss_bbox = build_loss(loss_bbox)\n-        self.bbox_coder = build_bbox_coder(bbox_coder)\n-        self.num_anchor_per_locs = [n for n in num_classes]\n-        self.fp16_enabled = False\n-\n-        # a shared convolution\n-        self.shared_conv = ConvModule(\n-            in_channels,\n-            share_conv_channel,\n-            kernel_size=3,\n-            padding=1,\n-            conv_cfg=conv_cfg,\n-            norm_cfg=norm_cfg,\n-            bias=bias)\n-\n-        self.task_heads = nn.ModuleList()\n-\n-        for num_cls in num_classes:\n-            heads = copy.deepcopy(common_heads)\n-            heads.update(dict(heatmap=(num_cls, num_heatmap_convs)))\n-            separate_head.update(\n-                in_channels=share_conv_channel, heads=heads, num_cls=num_cls)\n-            self.task_heads.append(builder.build_head(separate_head))\n-\n-        self.with_velocity = 'vel' in common_heads.keys()\n-        self.task_specific = task_specific\n-\n-    def forward_single(self, x):\n-        \"\"\"Forward function for CenterPoint.\n-\n-        Args:\n-            x (torch.Tensor): Input feature map with the shape of\n-                [B, 512, 128, 128].\n-\n-        Returns:\n-            list[dict]: Output results for tasks.\n-        \"\"\"\n-        ret_dicts = []\n-\n-        x = self.shared_conv(x)\n-\n-        for task in self.task_heads:\n-            ret_dicts.append(task(x))\n-\n-        return ret_dicts\n-\n-    def forward(self, feats):\n-        \"\"\"Forward pass.\n-\n-        Args:\n-            feats (list[torch.Tensor]): Multi-level features, e.g.,\n-                features produced by FPN.\n-\n-        Returns:\n-            tuple(list[dict]): Output results for tasks.\n-        \"\"\"\n-        return multi_apply(self.forward_single, feats)\n-\n-    def _gather_feat(self, feat, ind, mask=None):\n-        \"\"\"Gather feature map.\n-\n-        Given feature map and index, return indexed feature map.\n-\n-        Args:\n-            feat (torch.tensor): Feature map with the shape of [B, H*W, 10].\n-            ind (torch.Tensor): Index of the ground truth boxes with the\n-                shape of [B, max_obj].\n-            mask (torch.Tensor, optional): Mask of the feature map with the\n-                shape of [B, max_obj]. Default: None.\n-\n-        Returns:\n-            torch.Tensor: Feature map after gathering with the shape\n-                of [B, max_obj, 10].\n-        \"\"\"\n-        dim = feat.size(2)\n-        ind = ind.unsqueeze(2).expand(ind.size(0), ind.size(1), dim)\n-        feat = feat.gather(1, ind)\n-        if mask is not None:\n-            mask = mask.unsqueeze(2).expand_as(feat)\n-            feat = feat[mask]\n-            feat = feat.view(-1, dim)\n-        return feat\n-\n-    def get_targets(self, gt_bboxes_3d, gt_labels_3d):\n-        \"\"\"Generate targets.\n-\n-        How each output is transformed:\n-\n-            Each nested list is transposed so that all same-index elements in\n-            each sub-list (1, ..., N) become the new sub-lists.\n-                [ [a0, a1, a2, ... ], [b0, b1, b2, ... ], ... ]\n-                ==> [ [a0, b0, ... ], [a1, b1, ... ], [a2, b2, ... ] ]\n-\n-            The new transposed nested list is converted into a list of N\n-            tensors generated by concatenating tensors in the new sub-lists.\n-                [ tensor0, tensor1, tensor2, ... ]\n-\n-        Args:\n-            gt_bboxes_3d (list[:obj:`LiDARInstance3DBoxes`]): Ground\n-                truth gt boxes.\n-            gt_labels_3d (list[torch.Tensor]): Labels of boxes.\n-\n-        Returns:\n-            Returns:\n-                tuple[list[torch.Tensor]]: Tuple of target including\n-                    the following results in order.\n-\n-                    - list[torch.Tensor]: Heatmap scores.\n-                    - list[torch.Tensor]: Ground truth boxes.\n-                    - list[torch.Tensor]: Indexes indicating the\n-                        position of the valid boxes.\n-                    - list[torch.Tensor]: Masks indicating which\n-                        boxes are valid.\n-        \"\"\"\n-        heatmaps, anno_boxes, inds, masks = multi_apply(\n-            self.get_targets_single, gt_bboxes_3d, gt_labels_3d)\n-        # Transpose heatmaps\n-        heatmaps = list(map(list, zip(*heatmaps)))\n-        heatmaps = [torch.stack(hms_) for hms_ in heatmaps]\n-        # Transpose anno_boxes\n-        anno_boxes = list(map(list, zip(*anno_boxes)))\n-        anno_boxes = [torch.stack(anno_boxes_) for anno_boxes_ in anno_boxes]\n-        # Transpose inds\n-        inds = list(map(list, zip(*inds)))\n-        inds = [torch.stack(inds_) for inds_ in inds]\n-        # Transpose inds\n-        masks = list(map(list, zip(*masks)))\n-        masks = [torch.stack(masks_) for masks_ in masks]\n-        return heatmaps, anno_boxes, inds, masks\n-\n-    def get_targets_single(self, gt_bboxes_3d, gt_labels_3d):\n-        \"\"\"Generate training targets for a single sample.\n-\n-        Args:\n-            gt_bboxes_3d (:obj:`LiDARInstance3DBoxes`): Ground truth gt boxes.\n-            gt_labels_3d (torch.Tensor): Labels of boxes.\n-\n-        Returns:\n-            tuple[list[torch.Tensor]]: Tuple of target including\n-                the following results in order.\n-\n-                - list[torch.Tensor]: Heatmap scores.\n-                - list[torch.Tensor]: Ground truth boxes.\n-                - list[torch.Tensor]: Indexes indicating the position\n-                    of the valid boxes.\n-                - list[torch.Tensor]: Masks indicating which boxes\n-                    are valid.\n-        \"\"\"\n-        device = gt_labels_3d.device\n-        gt_bboxes_3d = torch.cat(\n-            (gt_bboxes_3d.gravity_center, gt_bboxes_3d.tensor[:, 3:]),\n-            dim=1).to(device)\n-        max_objs = self.train_cfg['max_objs'] * self.train_cfg['dense_reg']\n-        grid_size = torch.tensor(self.train_cfg['grid_size'])\n-        pc_range = torch.tensor(self.train_cfg['point_cloud_range'])\n-        voxel_size = torch.tensor(self.train_cfg['voxel_size'])\n-\n-        feature_map_size = grid_size[:2] // self.train_cfg['out_size_factor']\n-\n-        # reorganize the gt_dict by tasks\n-        task_masks = []\n-        flag = 0\n-        for class_name in self.class_names:\n-            task_masks.append([\n-                torch.where(gt_labels_3d == class_name.index(i) + flag)\n-                for i in class_name\n-            ])\n-            flag += len(class_name)\n-\n-        task_boxes = []\n-        task_classes = []\n-        flag2 = 0\n-        for idx, mask in enumerate(task_masks):\n-            task_box = []\n-            task_class = []\n-            for m in mask:\n-                task_box.append(gt_bboxes_3d[m])\n-                # 0 is background for each task, so we need to add 1 here.\n-                task_class.append(gt_labels_3d[m] + 1 - flag2)\n-            task_boxes.append(torch.cat(task_box, axis=0).to(device))\n-            task_classes.append(torch.cat(task_class).long().to(device))\n-            flag2 += len(mask)\n-        draw_gaussian = draw_heatmap_gaussian\n-        heatmaps, anno_boxes, inds, masks = [], [], [], []\n-\n-        for idx, task_head in enumerate(self.task_heads):\n-            heatmap = gt_bboxes_3d.new_zeros(\n-                (len(self.class_names[idx]), feature_map_size[1],\n-                 feature_map_size[0]))\n-\n-            if self.with_velocity:\n-                anno_box = gt_bboxes_3d.new_zeros((max_objs, 10),\n-                                                  dtype=torch.float32)\n-            else:\n-                anno_box = gt_bboxes_3d.new_zeros((max_objs, 8),\n-                                                  dtype=torch.float32)\n-\n-            ind = gt_labels_3d.new_zeros((max_objs), dtype=torch.int64)\n-            mask = gt_bboxes_3d.new_zeros((max_objs), dtype=torch.uint8)\n-\n-            num_objs = min(task_boxes[idx].shape[0], max_objs)\n-\n-            for k in range(num_objs):\n-                cls_id = task_classes[idx][k] - 1\n-\n-                width = task_boxes[idx][k][3]\n-                length = task_boxes[idx][k][4]\n-                width = width / voxel_size[0] / self.train_cfg[\n-                    'out_size_factor']\n-                length = length / voxel_size[1] / self.train_cfg[\n-                    'out_size_factor']\n-\n-                if width > 0 and length > 0:\n-                    radius = gaussian_radius(\n-                        (length, width),\n-                        min_overlap=self.train_cfg['gaussian_overlap'])\n-                    radius = max(self.train_cfg['min_radius'], int(radius))\n-\n-                    # be really careful for the coordinate system of\n-                    # your box annotation.\n-                    x, y, z = task_boxes[idx][k][0], task_boxes[idx][k][\n-                        1], task_boxes[idx][k][2]\n-\n-                    coor_x = (\n-                        x - pc_range[0]\n-                    ) / voxel_size[0] / self.train_cfg['out_size_factor']\n-                    coor_y = (\n-                        y - pc_range[1]\n-                    ) / voxel_size[1] / self.train_cfg['out_size_factor']\n-\n-                    center = torch.tensor([coor_x, coor_y],\n-                                          dtype=torch.float32,\n-                                          device=device)\n-                    center_int = center.to(torch.int32)\n-\n-                    # throw out not in range objects to avoid out of array\n-                    # area when creating the heatmap\n-                    if not (0 <= center_int[0] < feature_map_size[0]\n-                            and 0 <= center_int[1] < feature_map_size[1]):\n-                        continue\n-\n-                    draw_gaussian(heatmap[cls_id], center_int, radius)\n-\n-                    new_idx = k\n-                    x, y = center_int[0], center_int[1]\n-\n-                    assert (y * feature_map_size[0] + x <\n-                            feature_map_size[0] * feature_map_size[1])\n-\n-                    ind[new_idx] = y * feature_map_size[0] + x\n-                    mask[new_idx] = 1\n-                    # TODO: support other outdoor dataset\n-                    rot = task_boxes[idx][k][6]\n-                    box_dim = task_boxes[idx][k][3:6]\n-                    if self.norm_bbox:\n-                        box_dim = box_dim.log()\n-                    if self.with_velocity:\n-                        vx, vy = task_boxes[idx][k][7:]\n-                        anno_box[new_idx] = torch.cat([\n-                            center - torch.tensor([x, y], device=device),\n-                            z.unsqueeze(0), box_dim,\n-                            torch.sin(rot).unsqueeze(0),\n-                            torch.cos(rot).unsqueeze(0),\n-                            vx.unsqueeze(0),\n-                            vy.unsqueeze(0)\n-                        ])\n-                    else:\n-                        anno_box[new_idx] = torch.cat([\n-                            center - torch.tensor([x, y], device=device),\n-                            z.unsqueeze(0), box_dim,\n-                            torch.sin(rot).unsqueeze(0),\n-                            torch.cos(rot).unsqueeze(0)\n-                        ])\n-\n-            heatmaps.append(heatmap)\n-            anno_boxes.append(anno_box)\n-            masks.append(mask)\n-            inds.append(ind)\n-        return heatmaps, anno_boxes, inds, masks\n-\n-    def loss(self, gt_bboxes_3d, gt_labels_3d, preds_dicts, **kwargs):\n-        \"\"\"Loss function for CenterHead.\n-\n-        Args:\n-            gt_bboxes_3d (list[:obj:`LiDARInstance3DBoxes`]): Ground\n-                truth gt boxes.\n-            gt_labels_3d (list[torch.Tensor]): Labels of boxes.\n-            preds_dicts (dict): Output of forward function.\n-\n-        Returns:\n-            dict[str:torch.Tensor]: Loss of heatmap and bbox of each task.\n-        \"\"\"\n-        heatmaps, anno_boxes, inds, masks = self.get_targets(\n-            gt_bboxes_3d, gt_labels_3d)\n-        loss_dict = dict()\n-        if not self.task_specific:\n-            loss_dict['loss'] = 0\n-        for task_id, preds_dict in enumerate(preds_dicts):\n-            # heatmap focal loss\n-            preds_dict[0]['heatmap'] = clip_sigmoid(preds_dict[0]['heatmap'])\n-            num_pos = heatmaps[task_id].eq(1).float().sum().item()\n-            cls_avg_factor = torch.clamp(\n-                reduce_mean(heatmaps[task_id].new_tensor(num_pos)),\n-                min=1).item()\n-            \n-            loss_heatmap = self.loss_cls(\n-                preds_dict[0]['heatmap'],\n-                heatmaps[task_id],\n-                avg_factor=cls_avg_factor)\n-            target_box = anno_boxes[task_id]\n-            # reconstruct the anno_box from multiple reg heads\n-            preds_dict[0]['anno_box'] = torch.cat(\n-                (\n-                    preds_dict[0]['reg'],\n-                    preds_dict[0]['height'],\n-                    preds_dict[0]['dim'],\n-                    preds_dict[0]['rot'],\n-                    preds_dict[0]['vel'],\n-                ),\n-                dim=1,\n-            )\n-\n-            # Regression loss for dimension, offset, height, rotation\n-            num = masks[task_id].float().sum()\n-            ind = inds[task_id]\n-            pred = preds_dict[0]['anno_box'].permute(0, 2, 3, 1).contiguous()\n-            pred = pred.view(pred.size(0), -1, pred.size(3))\n-            pred = self._gather_feat(pred, ind)\n-            mask = masks[task_id].unsqueeze(2).expand_as(target_box).float()\n-            num = torch.clamp(\n-                reduce_mean(target_box.new_tensor(num)), min=1e-4).item()\n-            isnotnan = (~torch.isnan(target_box)).float()\n-            mask *= isnotnan\n-            code_weights = self.train_cfg['code_weights']\n-            bbox_weights = mask * mask.new_tensor(code_weights)\n-            if self.task_specific:\n-                name_list = ['xy', 'z', 'whl', 'yaw', 'vel']\n-                clip_index = [0, 2, 3, 6, 8, 10]\n-                for reg_task_id in range(len(name_list)):\n-                    pred_tmp = pred[\n-                        ...,\n-                        clip_index[reg_task_id]:clip_index[reg_task_id + 1]]\n-                    target_box_tmp = target_box[\n-                        ...,\n-                        clip_index[reg_task_id]:clip_index[reg_task_id + 1]]\n-                    bbox_weights_tmp = bbox_weights[\n-                        ...,\n-                        clip_index[reg_task_id]:clip_index[reg_task_id + 1]]\n-                    loss_bbox_tmp = self.loss_bbox(\n-                        pred_tmp,\n-                        target_box_tmp,\n-                        bbox_weights_tmp,\n-                        avg_factor=(num + 1e-4))\n-                    loss_dict[f'task{task_id}.loss_%s' %\n-                              (name_list[reg_task_id])] = loss_bbox_tmp\n-                loss_dict[f'task{task_id}.loss_heatmap'] = loss_heatmap\n-            else:\n-                loss_bbox = self.loss_bbox(\n-                    pred, target_box, bbox_weights, avg_factor=num)\n-                loss_dict['loss'] += loss_bbox\n-                loss_dict['loss'] += loss_heatmap\n-\n-        return loss_dict\n-\n-    def get_bboxes(self, preds_dicts, img_metas, img=None, rescale=False):\n-        \"\"\"Generate bboxes from bbox head predictions.\n-\n-        Args:\n-            preds_dicts (tuple[list[dict]]): Prediction results.\n-            img_metas (list[dict]): Point cloud and image's meta info.\n-\n-        Returns:\n-            list[dict]: Decoded bbox, scores and labels after nms.\n-        \"\"\"\n-        rets = []\n-        for task_id, preds_dict in enumerate(preds_dicts):\n-            batch_size = preds_dict[0]['heatmap'].shape[0]\n-            batch_heatmap = preds_dict[0]['heatmap'].sigmoid()\n-\n-            batch_reg = preds_dict[0]['reg']\n-            batch_hei = preds_dict[0]['height']\n-\n-            if self.norm_bbox:\n-                batch_dim = torch.exp(preds_dict[0]['dim'])\n-            else:\n-                batch_dim = preds_dict[0]['dim']\n-\n-            batch_rots = preds_dict[0]['rot'][:, 0].unsqueeze(1)\n-            batch_rotc = preds_dict[0]['rot'][:, 1].unsqueeze(1)\n-\n-            if 'vel' in preds_dict[0]:\n-                batch_vel = preds_dict[0]['vel']\n-            else:\n-                batch_vel = None\n-            temp = self.bbox_coder.decode(\n-                batch_heatmap,\n-                batch_rots,\n-                batch_rotc,\n-                batch_hei,\n-                batch_dim,\n-                batch_vel,\n-                reg=batch_reg,\n-                task_id=task_id)\n-            batch_reg_preds = [box['bboxes'] for box in temp]\n-            batch_cls_preds = [box['scores'] for box in temp]\n-            batch_cls_labels = [box['labels'] for box in temp]\n-            nms_type = self.test_cfg.get('nms_type')\n-            if isinstance(nms_type, list):\n-                nms_type = nms_type[task_id]\n-            if nms_type == 'circle':\n-                ret_task = []\n-                for i in range(batch_size):\n-                    boxes3d = temp[i]['bboxes']\n-                    scores = temp[i]['scores']\n-                    labels = temp[i]['labels']\n-                    centers = boxes3d[:, [0, 1]]\n-                    boxes = torch.cat([centers, scores.view(-1, 1)], dim=1)\n-                    keep = torch.tensor(\n-                        circle_nms(\n-                            boxes.detach().cpu().numpy(),\n-                            self.test_cfg['min_radius'][task_id],\n-                            post_max_size=self.test_cfg['post_max_size']),\n-                        dtype=torch.long,\n-                        device=boxes.device)\n-\n-                    boxes3d = boxes3d[keep]\n-                    scores = scores[keep]\n-                    labels = labels[keep]\n-                    ret = dict(bboxes=boxes3d, scores=scores, labels=labels)\n-                    ret_task.append(ret)\n-                rets.append(ret_task)\n-            else:\n-                rets.append(\n-                    self.get_task_detections(batch_cls_preds, batch_reg_preds,\n-                                             batch_cls_labels, img_metas,\n-                                             task_id))\n-\n-        # Merge branches results\n-        num_samples = len(rets[0])\n-\n-        ret_list = []\n-        for i in range(num_samples):\n-            for k in rets[0][i].keys():\n-                if k == 'bboxes':\n-                    bboxes = torch.cat([ret[i][k] for ret in rets])\n-                    bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 5] * 0.5\n-                    bboxes = img_metas[i]['box_type_3d'](\n-                        bboxes, self.bbox_coder.code_size)\n-                elif k == 'scores':\n-                    scores = torch.cat([ret[i][k] for ret in rets])\n-                elif k == 'labels':\n-                    flag = 0\n-                    for j, num_class in enumerate(self.num_classes):\n-                        rets[j][i][k] += flag\n-                        flag += num_class\n-                    labels = torch.cat([ret[i][k].int() for ret in rets])\n-            ret_list.append([bboxes, scores, labels])\n-        return ret_list\n-\n-    def get_task_detections(self, batch_cls_preds,\n-                            batch_reg_preds, batch_cls_labels, img_metas,\n-                            task_id):\n-        \"\"\"Rotate nms for each task.\n-\n-        Args:\n-            batch_cls_preds (list[torch.Tensor]): Prediction score with the\n-                shape of [N].\n-            batch_reg_preds (list[torch.Tensor]): Prediction bbox with the\n-                shape of [N, 9].\n-            batch_cls_labels (list[torch.Tensor]): Prediction label with the\n-                shape of [N].\n-            img_metas (list[dict]): Meta information of each sample.\n-\n-        Returns:\n-            list[dict[str: torch.Tensor]]: contains the following keys:\n-\n-                -bboxes (torch.Tensor): Prediction bboxes after nms with the\n-                    shape of [N, 9].\n-                -scores (torch.Tensor): Prediction scores after nms with the\n-                    shape of [N].\n-                -labels (torch.Tensor): Prediction labels after nms with the\n-                    shape of [N].\n-        \"\"\"\n-        predictions_dicts = []\n-        for i, (box_preds, cls_preds, cls_labels) in enumerate(\n-                zip(batch_reg_preds, batch_cls_preds, batch_cls_labels)):\n-            default_val = [1.0 for _ in range(len(self.task_heads))]\n-            factor = self.test_cfg.get('nms_rescale_factor',\n-                                       default_val)[task_id]\n-            if isinstance(factor, list):\n-                for cid in range(len(factor)):\n-                    box_preds[cls_labels == cid, 3:6] = \\\n-                        box_preds[cls_labels == cid, 3:6] * factor[cid]\n-            else:\n-                box_preds[:, 3:6] = box_preds[:, 3:6] * factor\n-\n-            # Apply NMS in birdeye view\n-            top_labels = cls_labels.long()\n-            top_scores = cls_preds.squeeze(-1) if cls_preds.shape[0]>1 \\\n-                else cls_preds\n-\n-            if top_scores.shape[0] != 0:\n-                boxes_for_nms = img_metas[i]['box_type_3d'](\n-                    box_preds[:, :], self.bbox_coder.code_size).bev\n-                # the nms in 3d detection just remove overlap boxes.\n-                if isinstance(self.test_cfg['nms_thr'], list):\n-                    nms_thresh = self.test_cfg['nms_thr'][task_id]\n-                else:\n-                    nms_thresh = self.test_cfg['nms_thr']\n-                selected = nms_bev(\n-                    boxes_for_nms,\n-                    top_scores,\n-                    thresh=nms_thresh,\n-                    pre_max_size=self.test_cfg['pre_max_size'],\n-                    post_max_size=self.test_cfg['post_max_size'],\n-                    xyxyr2xywhr=False)\n-            else:\n-                selected = []\n-\n-            if isinstance(factor, list):\n-                for cid in range(len(factor)):\n-                    box_preds[top_labels == cid, 3:6] = \\\n-                        box_preds[top_labels == cid, 3:6] / factor[cid]\n-            else:\n-                box_preds[:, 3:6] = box_preds[:, 3:6] / factor\n-\n-            # if selected is not None:\n-            selected_boxes = box_preds[selected]\n-            selected_labels = top_labels[selected]\n-            selected_scores = top_scores[selected]\n-\n-            # finally generate predictions.\n-            if selected_boxes.shape[0] != 0:\n-                predictions_dict = dict(\n-                    bboxes=selected_boxes,\n-                    scores=selected_scores,\n-                    labels=selected_labels)\n-            else:\n-                dtype = batch_reg_preds[0].dtype\n-                device = batch_reg_preds[0].device\n-                predictions_dict = dict(\n-                    bboxes=torch.zeros([0, self.bbox_coder.code_size],\n-                                       dtype=dtype,\n-                                       device=device),\n-                    scores=torch.zeros([0], dtype=dtype, device=device),\n-                    labels=torch.zeros([0],\n-                                       dtype=top_labels.dtype,\n-                                       device=device))\n-\n-            predictions_dicts.append(predictions_dict)\n-        return predictions_dicts\n"
                }
            ],
            "date": 1716020523495,
            "name": "Commit-0",
            "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport copy\n\nimport torch\nfrom mmcv.cnn import ConvModule, build_conv_layer\nfrom mmcv.runner import BaseModule\nfrom torch import nn\n\nfrom mmdet3d.core import (circle_nms, draw_heatmap_gaussian, gaussian_radius,\n                          xywhr2xyxyr)\nfrom mmdet3d.core.post_processing import nms_bev\nfrom mmdet3d.models import builder\nfrom mmdet3d.models.utils import clip_sigmoid\nfrom mmdet.core import build_bbox_coder, multi_apply, reduce_mean\nfrom ..builder import HEADS, build_loss\n\n\n@HEADS.register_module()\nclass SeparateHead(BaseModule):\n    \"\"\"SeparateHead for CenterHead.\n\n    Args:\n        in_channels (int): Input channels for conv_layer.\n        heads (dict): Conv information.\n        head_conv (int, optional): Output channels.\n            Default: 64.\n        final_kernel (int, optional): Kernel size for the last conv layer.\n            Default: 1.\n        init_bias (float, optional): Initial bias. Default: -2.19.\n        conv_cfg (dict, optional): Config of conv layer.\n            Default: dict(type='Conv2d')\n        norm_cfg (dict, optional): Config of norm layer.\n            Default: dict(type='BN2d').\n        bias (str, optional): Type of bias. Default: 'auto'.\n    \"\"\"\n\n    def __init__(self,\n                 in_channels,\n                 heads,\n                 head_conv=64,\n                 final_kernel=1,\n                 init_bias=-2.19,\n                 conv_cfg=dict(type='Conv2d'),\n                 norm_cfg=dict(type='BN2d'),\n                 bias='auto',\n                 init_cfg=None,\n                 **kwargs):\n        assert init_cfg is None, 'To prevent abnormal initialization ' \\\n            'behavior, init_cfg is not allowed to be set'\n        super(SeparateHead, self).__init__(init_cfg=init_cfg)\n        self.heads = heads\n        self.init_bias = init_bias\n        for head in self.heads:\n            classes, num_conv = self.heads[head]\n\n            conv_layers = []\n            c_in = in_channels\n            for i in range(num_conv - 1):\n                conv_layers.append(\n                    ConvModule(\n                        c_in,\n                        head_conv,\n                        kernel_size=final_kernel,\n                        stride=1,\n                        padding=final_kernel // 2,\n                        bias=bias,\n                        conv_cfg=conv_cfg,\n                        norm_cfg=norm_cfg))\n                c_in = head_conv\n\n            conv_layers.append(\n                build_conv_layer(\n                    conv_cfg,\n                    head_conv,\n                    classes,\n                    kernel_size=final_kernel,\n                    stride=1,\n                    padding=final_kernel // 2,\n                    bias=True))\n            conv_layers = nn.Sequential(*conv_layers)\n\n            self.__setattr__(head, conv_layers)\n\n            if init_cfg is None:\n                self.init_cfg = dict(type='Kaiming', layer='Conv2d')\n\n    def init_weights(self):\n        \"\"\"Initialize weights.\"\"\"\n        super().init_weights()\n        for head in self.heads:\n            if head == 'heatmap':\n                self.__getattr__(head)[-1].bias.data.fill_(self.init_bias)\n\n    def forward(self, x):\n        \"\"\"Forward function for SepHead.\n\n        Args:\n            x (torch.Tensor): Input feature map with the shape of\n                [B, 512, 128, 128].\n\n        Returns:\n            dict[str: torch.Tensor]: contains the following keys:\n\n                -reg （torch.Tensor): 2D regression value with the\n                    shape of [B, 2, H, W].\n                -height (torch.Tensor): Height value with the\n                    shape of [B, 1, H, W].\n                -dim (torch.Tensor): Size value with the shape\n                    of [B, 3, H, W].\n                -rot (torch.Tensor): Rotation value with the\n                    shape of [B, 2, H, W].\n                -vel (torch.Tensor): Velocity value with the\n                    shape of [B, 2, H, W].\n                -heatmap (torch.Tensor): Heatmap with the shape of\n                    [B, N, H, W].\n        \"\"\"\n        ret_dict = dict()\n        for head in self.heads:\n            ret_dict[head] = self.__getattr__(head)(x)\n\n        return ret_dict\n\n\n@HEADS.register_module()\nclass DCNSeparateHead(BaseModule):\n    r\"\"\"DCNSeparateHead for CenterHead.\n\n    .. code-block:: none\n            /-----> DCN for heatmap task -----> heatmap task.\n    feature\n            \\-----> DCN for regression tasks -----> regression tasks\n\n    Args:\n        in_channels (int): Input channels for conv_layer.\n        num_cls (int): Number of classes.\n        heads (dict): Conv information.\n        dcn_config (dict): Config of dcn layer.\n        head_conv (int, optional): Output channels.\n            Default: 64.\n        final_kernel (int, optional): Kernel size for the last conv\n            layer. Default: 1.\n        init_bias (float, optional): Initial bias. Default: -2.19.\n        conv_cfg (dict, optional): Config of conv layer.\n            Default: dict(type='Conv2d')\n        norm_cfg (dict, optional): Config of norm layer.\n            Default: dict(type='BN2d').\n        bias (str, optional): Type of bias. Default: 'auto'.\n    \"\"\"  # noqa: W605\n\n    def __init__(self,\n                 in_channels,\n                 num_cls,\n                 heads,\n                 dcn_config,\n                 head_conv=64,\n                 final_kernel=1,\n                 init_bias=-2.19,\n                 conv_cfg=dict(type='Conv2d'),\n                 norm_cfg=dict(type='BN2d'),\n                 bias='auto',\n                 init_cfg=None,\n                 **kwargs):\n        assert init_cfg is None, 'To prevent abnormal initialization ' \\\n            'behavior, init_cfg is not allowed to be set'\n        super(DCNSeparateHead, self).__init__(init_cfg=init_cfg)\n        if 'heatmap' in heads:\n            heads.pop('heatmap')\n        # feature adaptation with dcn\n        # use separate features for classification / regression\n        self.feature_adapt_cls = build_conv_layer(dcn_config)\n\n        self.feature_adapt_reg = build_conv_layer(dcn_config)\n\n        # heatmap prediction head\n        cls_head = [\n            ConvModule(\n                in_channels,\n                head_conv,\n                kernel_size=3,\n                padding=1,\n                conv_cfg=conv_cfg,\n                bias=bias,\n                norm_cfg=norm_cfg),\n            build_conv_layer(\n                conv_cfg,\n                head_conv,\n                num_cls,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=bias)\n        ]\n        self.cls_head = nn.Sequential(*cls_head)\n        self.init_bias = init_bias\n        # other regression target\n        self.task_head = SeparateHead(\n            in_channels,\n            heads,\n            head_conv=head_conv,\n            final_kernel=final_kernel,\n            bias=bias)\n        if init_cfg is None:\n            self.init_cfg = dict(type='Kaiming', layer='Conv2d')\n\n    def init_weights(self):\n        \"\"\"Initialize weights.\"\"\"\n        super().init_weights()\n        self.cls_head[-1].bias.data.fill_(self.init_bias)\n\n    def forward(self, x):\n        \"\"\"Forward function for DCNSepHead.\n\n        Args:\n            x (torch.Tensor): Input feature map with the shape of\n                [B, 512, 128, 128].\n\n        Returns:\n            dict[str: torch.Tensor]: contains the following keys:\n\n                -reg （torch.Tensor): 2D regression value with the\n                    shape of [B, 2, H, W].\n                -height (torch.Tensor): Height value with the\n                    shape of [B, 1, H, W].\n                -dim (torch.Tensor): Size value with the shape\n                    of [B, 3, H, W].\n                -rot (torch.Tensor): Rotation value with the\n                    shape of [B, 2, H, W].\n                -vel (torch.Tensor): Velocity value with the\n                    shape of [B, 2, H, W].\n                -heatmap (torch.Tensor): Heatmap with the shape of\n                    [B, N, H, W].\n        \"\"\"\n        center_feat = self.feature_adapt_cls(x)\n        reg_feat = self.feature_adapt_reg(x)\n\n        cls_score = self.cls_head(center_feat)\n        ret = self.task_head(reg_feat)\n        ret['heatmap'] = cls_score\n\n        return ret\n\n\n@HEADS.register_module()\nclass CenterHead(BaseModule):\n    \"\"\"CenterHead for CenterPoint.\n\n    Args:\n        in_channels (list[int] | int, optional): Channels of the input\n            feature map. Default: [128].\n        tasks (list[dict], optional): Task information including class number\n            and class names. Default: None.\n        train_cfg (dict, optional): Train-time configs. Default: None.\n        test_cfg (dict, optional): Test-time configs. Default: None.\n        bbox_coder (dict, optional): Bbox coder configs. Default: None.\n        common_heads (dict, optional): Conv information for common heads.\n            Default: dict().\n        loss_cls (dict, optional): Config of classification loss function.\n            Default: dict(type='GaussianFocalLoss', reduction='mean').\n        loss_bbox (dict, optional): Config of regression loss function.\n            Default: dict(type='L1Loss', reduction='none').\n        separate_head (dict, optional): Config of separate head. Default: dict(\n            type='SeparateHead', init_bias=-2.19, final_kernel=3)\n        share_conv_channel (int, optional): Output channels for share_conv\n            layer. Default: 64.\n        num_heatmap_convs (int, optional): Number of conv layers for heatmap\n            conv layer. Default: 2.\n        conv_cfg (dict, optional): Config of conv layer.\n            Default: dict(type='Conv2d')\n        norm_cfg (dict, optional): Config of norm layer.\n            Default: dict(type='BN2d').\n        bias (str, optional): Type of bias. Default: 'auto'.\n    \"\"\"\n\n    def __init__(self,\n                 in_channels=[128],\n                 tasks=None,\n                 train_cfg=None,\n                 test_cfg=None,\n                 bbox_coder=None,\n                 common_heads=dict(),\n                 loss_cls=dict(type='GaussianFocalLoss', reduction='mean'),\n                 loss_bbox=dict(\n                     type='L1Loss', reduction='none', loss_weight=0.25),\n                 separate_head=dict(\n                     type='SeparateHead', init_bias=-2.19, final_kernel=3),\n                 share_conv_channel=64,\n                 num_heatmap_convs=2,\n                 conv_cfg=dict(type='Conv2d'),\n                 norm_cfg=dict(type='BN2d'),\n                 bias='auto',\n                 norm_bbox=True,\n                 init_cfg=None,\n                 task_specific=True):\n        assert init_cfg is None, 'To prevent abnormal initialization ' \\\n            'behavior, init_cfg is not allowed to be set'\n        super(CenterHead, self).__init__(init_cfg=init_cfg)\n\n        num_classes = [len(t['class_names']) for t in tasks]\n        self.class_names = [t['class_names'] for t in tasks]\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.norm_bbox = norm_bbox\n\n        self.loss_cls = build_loss(loss_cls)\n        self.loss_bbox = build_loss(loss_bbox)\n        self.bbox_coder = build_bbox_coder(bbox_coder)\n        self.num_anchor_per_locs = [n for n in num_classes]\n        self.fp16_enabled = False\n\n        # a shared convolution\n        self.shared_conv = ConvModule(\n            in_channels,\n            share_conv_channel,\n            kernel_size=3,\n            padding=1,\n            conv_cfg=conv_cfg,\n            norm_cfg=norm_cfg,\n            bias=bias)\n\n        self.task_heads = nn.ModuleList()\n\n        for num_cls in num_classes:\n            heads = copy.deepcopy(common_heads)\n            heads.update(dict(heatmap=(num_cls, num_heatmap_convs)))\n            separate_head.update(\n                in_channels=share_conv_channel, heads=heads, num_cls=num_cls)\n            self.task_heads.append(builder.build_head(separate_head))\n\n        self.with_velocity = 'vel' in common_heads.keys()\n        self.task_specific = task_specific\n\n    def forward_single(self, x):\n        \"\"\"Forward function for CenterPoint.\n\n        Args:\n            x (torch.Tensor): Input feature map with the shape of\n                [B, 512, 128, 128].\n\n        Returns:\n            list[dict]: Output results for tasks.\n        \"\"\"\n        ret_dicts = []\n\n        x = self.shared_conv(x)\n\n        for task in self.task_heads:\n            ret_dicts.append(task(x))\n\n        return ret_dicts\n\n    def forward(self, feats):\n        \"\"\"Forward pass.\n\n        Args:\n            feats (list[torch.Tensor]): Multi-level features, e.g.,\n                features produced by FPN.\n\n        Returns:\n            tuple(list[dict]): Output results for tasks.\n        \"\"\"\n        return multi_apply(self.forward_single, feats)\n\n    def _gather_feat(self, feat, ind, mask=None):\n        \"\"\"Gather feature map.\n\n        Given feature map and index, return indexed feature map.\n\n        Args:\n            feat (torch.tensor): Feature map with the shape of [B, H*W, 10].\n            ind (torch.Tensor): Index of the ground truth boxes with the\n                shape of [B, max_obj].\n            mask (torch.Tensor, optional): Mask of the feature map with the\n                shape of [B, max_obj]. Default: None.\n\n        Returns:\n            torch.Tensor: Feature map after gathering with the shape\n                of [B, max_obj, 10].\n        \"\"\"\n        dim = feat.size(2)\n        ind = ind.unsqueeze(2).expand(ind.size(0), ind.size(1), dim)\n        feat = feat.gather(1, ind)\n        if mask is not None:\n            mask = mask.unsqueeze(2).expand_as(feat)\n            feat = feat[mask]\n            feat = feat.view(-1, dim)\n        return feat\n\n    def get_targets(self, gt_bboxes_3d, gt_labels_3d):\n        \"\"\"Generate targets.\n\n        How each output is transformed:\n\n            Each nested list is transposed so that all same-index elements in\n            each sub-list (1, ..., N) become the new sub-lists.\n                [ [a0, a1, a2, ... ], [b0, b1, b2, ... ], ... ]\n                ==> [ [a0, b0, ... ], [a1, b1, ... ], [a2, b2, ... ] ]\n\n            The new transposed nested list is converted into a list of N\n            tensors generated by concatenating tensors in the new sub-lists.\n                [ tensor0, tensor1, tensor2, ... ]\n\n        Args:\n            gt_bboxes_3d (list[:obj:`LiDARInstance3DBoxes`]): Ground\n                truth gt boxes.\n            gt_labels_3d (list[torch.Tensor]): Labels of boxes.\n\n        Returns:\n            Returns:\n                tuple[list[torch.Tensor]]: Tuple of target including\n                    the following results in order.\n\n                    - list[torch.Tensor]: Heatmap scores.\n                    - list[torch.Tensor]: Ground truth boxes.\n                    - list[torch.Tensor]: Indexes indicating the\n                        position of the valid boxes.\n                    - list[torch.Tensor]: Masks indicating which\n                        boxes are valid.\n        \"\"\"\n        heatmaps, anno_boxes, inds, masks = multi_apply(\n            self.get_targets_single, gt_bboxes_3d, gt_labels_3d)\n        # Transpose heatmaps\n        heatmaps = list(map(list, zip(*heatmaps)))\n        heatmaps = [torch.stack(hms_) for hms_ in heatmaps]\n        # Transpose anno_boxes\n        anno_boxes = list(map(list, zip(*anno_boxes)))\n        anno_boxes = [torch.stack(anno_boxes_) for anno_boxes_ in anno_boxes]\n        # Transpose inds\n        inds = list(map(list, zip(*inds)))\n        inds = [torch.stack(inds_) for inds_ in inds]\n        # Transpose inds\n        masks = list(map(list, zip(*masks)))\n        masks = [torch.stack(masks_) for masks_ in masks]\n        return heatmaps, anno_boxes, inds, masks\n\n    def get_targets_single(self, gt_bboxes_3d, gt_labels_3d):\n        \"\"\"Generate training targets for a single sample.\n\n        Args:\n            gt_bboxes_3d (:obj:`LiDARInstance3DBoxes`): Ground truth gt boxes.\n            gt_labels_3d (torch.Tensor): Labels of boxes.\n\n        Returns:\n            tuple[list[torch.Tensor]]: Tuple of target including\n                the following results in order.\n\n                - list[torch.Tensor]: Heatmap scores.\n                - list[torch.Tensor]: Ground truth boxes.\n                - list[torch.Tensor]: Indexes indicating the position\n                    of the valid boxes.\n                - list[torch.Tensor]: Masks indicating which boxes\n                    are valid.\n        \"\"\"\n        device = gt_labels_3d.device\n        gt_bboxes_3d = torch.cat(\n            (gt_bboxes_3d.gravity_center, gt_bboxes_3d.tensor[:, 3:]),\n            dim=1).to(device)\n        max_objs = self.train_cfg['max_objs'] * self.train_cfg['dense_reg']\n        grid_size = torch.tensor(self.train_cfg['grid_size'])\n        pc_range = torch.tensor(self.train_cfg['point_cloud_range'])\n        voxel_size = torch.tensor(self.train_cfg['voxel_size'])\n\n        feature_map_size = grid_size[:2] // self.train_cfg['out_size_factor']\n\n        # reorganize the gt_dict by tasks\n        task_masks = []\n        flag = 0\n        for class_name in self.class_names:\n            task_masks.append([\n                torch.where(gt_labels_3d == class_name.index(i) + flag)\n                for i in class_name\n            ])\n            flag += len(class_name)\n\n        task_boxes = []\n        task_classes = []\n        flag2 = 0\n        for idx, mask in enumerate(task_masks):\n            task_box = []\n            task_class = []\n            for m in mask:\n                task_box.append(gt_bboxes_3d[m])\n                # 0 is background for each task, so we need to add 1 here.\n                task_class.append(gt_labels_3d[m] + 1 - flag2)\n            task_boxes.append(torch.cat(task_box, axis=0).to(device))\n            task_classes.append(torch.cat(task_class).long().to(device))\n            flag2 += len(mask)\n        draw_gaussian = draw_heatmap_gaussian\n        heatmaps, anno_boxes, inds, masks = [], [], [], []\n\n        for idx, task_head in enumerate(self.task_heads):\n            heatmap = gt_bboxes_3d.new_zeros(\n                (len(self.class_names[idx]), feature_map_size[1],\n                 feature_map_size[0]))\n\n            if self.with_velocity:\n                anno_box = gt_bboxes_3d.new_zeros((max_objs, 10),\n                                                  dtype=torch.float32)\n            else:\n                anno_box = gt_bboxes_3d.new_zeros((max_objs, 8),\n                                                  dtype=torch.float32)\n\n            ind = gt_labels_3d.new_zeros((max_objs), dtype=torch.int64)\n            mask = gt_bboxes_3d.new_zeros((max_objs), dtype=torch.uint8)\n\n            num_objs = min(task_boxes[idx].shape[0], max_objs)\n\n            for k in range(num_objs):\n                cls_id = task_classes[idx][k] - 1\n\n                width = task_boxes[idx][k][3]\n                length = task_boxes[idx][k][4]\n                width = width / voxel_size[0] / self.train_cfg[\n                    'out_size_factor']\n                length = length / voxel_size[1] / self.train_cfg[\n                    'out_size_factor']\n\n                if width > 0 and length > 0:\n                    radius = gaussian_radius(\n                        (length, width),\n                        min_overlap=self.train_cfg['gaussian_overlap'])\n                    radius = max(self.train_cfg['min_radius'], int(radius))\n\n                    # be really careful for the coordinate system of\n                    # your box annotation.\n                    x, y, z = task_boxes[idx][k][0], task_boxes[idx][k][\n                        1], task_boxes[idx][k][2]\n\n                    coor_x = (\n                        x - pc_range[0]\n                    ) / voxel_size[0] / self.train_cfg['out_size_factor']\n                    coor_y = (\n                        y - pc_range[1]\n                    ) / voxel_size[1] / self.train_cfg['out_size_factor']\n\n                    center = torch.tensor([coor_x, coor_y],\n                                          dtype=torch.float32,\n                                          device=device)\n                    center_int = center.to(torch.int32)\n\n                    # throw out not in range objects to avoid out of array\n                    # area when creating the heatmap\n                    if not (0 <= center_int[0] < feature_map_size[0]\n                            and 0 <= center_int[1] < feature_map_size[1]):\n                        continue\n\n                    draw_gaussian(heatmap[cls_id], center_int, radius)\n\n                    new_idx = k\n                    x, y = center_int[0], center_int[1]\n\n                    assert (y * feature_map_size[0] + x <\n                            feature_map_size[0] * feature_map_size[1])\n\n                    ind[new_idx] = y * feature_map_size[0] + x\n                    mask[new_idx] = 1\n                    # TODO: support other outdoor dataset\n                    rot = task_boxes[idx][k][6]\n                    box_dim = task_boxes[idx][k][3:6]\n                    if self.norm_bbox:\n                        box_dim = box_dim.log()\n                    if self.with_velocity:\n                        vx, vy = task_boxes[idx][k][7:]\n                        anno_box[new_idx] = torch.cat([\n                            center - torch.tensor([x, y], device=device),\n                            z.unsqueeze(0), box_dim,\n                            torch.sin(rot).unsqueeze(0),\n                            torch.cos(rot).unsqueeze(0),\n                            vx.unsqueeze(0),\n                            vy.unsqueeze(0)\n                        ])\n                    else:\n                        anno_box[new_idx] = torch.cat([\n                            center - torch.tensor([x, y], device=device),\n                            z.unsqueeze(0), box_dim,\n                            torch.sin(rot).unsqueeze(0),\n                            torch.cos(rot).unsqueeze(0)\n                        ])\n\n            heatmaps.append(heatmap)\n            anno_boxes.append(anno_box)\n            masks.append(mask)\n            inds.append(ind)\n        return heatmaps, anno_boxes, inds, masks\n\n    def loss(self, gt_bboxes_3d, gt_labels_3d, preds_dicts, **kwargs):\n        \"\"\"Loss function for CenterHead.\n\n        Args:\n            gt_bboxes_3d (list[:obj:`LiDARInstance3DBoxes`]): Ground\n                truth gt boxes.\n            gt_labels_3d (list[torch.Tensor]): Labels of boxes.\n            preds_dicts (dict): Output of forward function.\n\n        Returns:\n            dict[str:torch.Tensor]: Loss of heatmap and bbox of each task.\n        \"\"\"\n        heatmaps, anno_boxes, inds, masks = self.get_targets(\n            gt_bboxes_3d, gt_labels_3d)\n        loss_dict = dict()\n        if not self.task_specific:\n            loss_dict['loss'] = 0\n        for task_id, preds_dict in enumerate(preds_dicts):\n            # heatmap focal loss\n            preds_dict[0]['heatmap'] = clip_sigmoid(preds_dict[0]['heatmap'])\n            num_pos = heatmaps[task_id].eq(1).float().sum().item()\n            cls_avg_factor = torch.clamp(\n                reduce_mean(heatmaps[task_id].new_tensor(num_pos)),\n                min=1).item()\n            loss_heatmap = self.loss_cls(\n                preds_dict[0]['heatmap'],\n                heatmaps[task_id],\n                avg_factor=cls_avg_factor)\n            target_box = anno_boxes[task_id]\n            # reconstruct the anno_box from multiple reg heads\n            preds_dict[0]['anno_box'] = torch.cat(\n                (\n                    preds_dict[0]['reg'],\n                    preds_dict[0]['height'],\n                    preds_dict[0]['dim'],\n                    preds_dict[0]['rot'],\n                    preds_dict[0]['vel'],\n                ),\n                dim=1,\n            )\n\n            # Regression loss for dimension, offset, height, rotation\n            num = masks[task_id].float().sum()\n            ind = inds[task_id]\n            pred = preds_dict[0]['anno_box'].permute(0, 2, 3, 1).contiguous()\n            pred = pred.view(pred.size(0), -1, pred.size(3))\n            pred = self._gather_feat(pred, ind)\n            mask = masks[task_id].unsqueeze(2).expand_as(target_box).float()\n            num = torch.clamp(\n                reduce_mean(target_box.new_tensor(num)), min=1e-4).item()\n            isnotnan = (~torch.isnan(target_box)).float()\n            mask *= isnotnan\n            code_weights = self.train_cfg['code_weights']\n            bbox_weights = mask * mask.new_tensor(code_weights)\n            if self.task_specific:\n                name_list = ['xy', 'z', 'whl', 'yaw', 'vel']\n                clip_index = [0, 2, 3, 6, 8, 10]\n                for reg_task_id in range(len(name_list)):\n                    pred_tmp = pred[\n                        ...,\n                        clip_index[reg_task_id]:clip_index[reg_task_id + 1]]\n                    target_box_tmp = target_box[\n                        ...,\n                        clip_index[reg_task_id]:clip_index[reg_task_id + 1]]\n                    bbox_weights_tmp = bbox_weights[\n                        ...,\n                        clip_index[reg_task_id]:clip_index[reg_task_id + 1]]\n                    loss_bbox_tmp = self.loss_bbox(\n                        pred_tmp,\n                        target_box_tmp,\n                        bbox_weights_tmp,\n                        avg_factor=(num + 1e-4))\n                    loss_dict[f'task{task_id}.loss_%s' %\n                              (name_list[reg_task_id])] = loss_bbox_tmp\n                loss_dict[f'task{task_id}.loss_heatmap'] = loss_heatmap\n            else:\n                loss_bbox = self.loss_bbox(\n                    pred, target_box, bbox_weights, avg_factor=num)\n                loss_dict['loss'] += loss_bbox\n                loss_dict['loss'] += loss_heatmap\n\n        return loss_dict\n\n    def get_bboxes(self, preds_dicts, img_metas, img=None, rescale=False):\n        \"\"\"Generate bboxes from bbox head predictions.\n\n        Args:\n            preds_dicts (tuple[list[dict]]): Prediction results.\n            img_metas (list[dict]): Point cloud and image's meta info.\n\n        Returns:\n            list[dict]: Decoded bbox, scores and labels after nms.\n        \"\"\"\n        rets = []\n        for task_id, preds_dict in enumerate(preds_dicts):\n            batch_size = preds_dict[0]['heatmap'].shape[0]\n            batch_heatmap = preds_dict[0]['heatmap'].sigmoid()\n\n            batch_reg = preds_dict[0]['reg']\n            batch_hei = preds_dict[0]['height']\n\n            if self.norm_bbox:\n                batch_dim = torch.exp(preds_dict[0]['dim'])\n            else:\n                batch_dim = preds_dict[0]['dim']\n\n            batch_rots = preds_dict[0]['rot'][:, 0].unsqueeze(1)\n            batch_rotc = preds_dict[0]['rot'][:, 1].unsqueeze(1)\n\n            if 'vel' in preds_dict[0]:\n                batch_vel = preds_dict[0]['vel']\n            else:\n                batch_vel = None\n            temp = self.bbox_coder.decode(\n                batch_heatmap,\n                batch_rots,\n                batch_rotc,\n                batch_hei,\n                batch_dim,\n                batch_vel,\n                reg=batch_reg,\n                task_id=task_id)\n            batch_reg_preds = [box['bboxes'] for box in temp]\n            batch_cls_preds = [box['scores'] for box in temp]\n            batch_cls_labels = [box['labels'] for box in temp]\n            nms_type = self.test_cfg.get('nms_type')\n            if isinstance(nms_type, list):\n                nms_type = nms_type[task_id]\n            if nms_type == 'circle':\n                ret_task = []\n                for i in range(batch_size):\n                    boxes3d = temp[i]['bboxes']\n                    scores = temp[i]['scores']\n                    labels = temp[i]['labels']\n                    centers = boxes3d[:, [0, 1]]\n                    boxes = torch.cat([centers, scores.view(-1, 1)], dim=1)\n                    keep = torch.tensor(\n                        circle_nms(\n                            boxes.detach().cpu().numpy(),\n                            self.test_cfg['min_radius'][task_id],\n                            post_max_size=self.test_cfg['post_max_size']),\n                        dtype=torch.long,\n                        device=boxes.device)\n\n                    boxes3d = boxes3d[keep]\n                    scores = scores[keep]\n                    labels = labels[keep]\n                    ret = dict(bboxes=boxes3d, scores=scores, labels=labels)\n                    ret_task.append(ret)\n                rets.append(ret_task)\n            else:\n                rets.append(\n                    self.get_task_detections(batch_cls_preds, batch_reg_preds,\n                                             batch_cls_labels, img_metas,\n                                             task_id))\n\n        # Merge branches results\n        num_samples = len(rets[0])\n\n        ret_list = []\n        for i in range(num_samples):\n            for k in rets[0][i].keys():\n                if k == 'bboxes':\n                    bboxes = torch.cat([ret[i][k] for ret in rets])\n                    bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 5] * 0.5\n                    bboxes = img_metas[i]['box_type_3d'](\n                        bboxes, self.bbox_coder.code_size)\n                elif k == 'scores':\n                    scores = torch.cat([ret[i][k] for ret in rets])\n                elif k == 'labels':\n                    flag = 0\n                    for j, num_class in enumerate(self.num_classes):\n                        rets[j][i][k] += flag\n                        flag += num_class\n                    labels = torch.cat([ret[i][k].int() for ret in rets])\n            ret_list.append([bboxes, scores, labels])\n        return ret_list\n\n    def get_task_detections(self, batch_cls_preds,\n                            batch_reg_preds, batch_cls_labels, img_metas,\n                            task_id):\n        \"\"\"Rotate nms for each task.\n\n        Args:\n            batch_cls_preds (list[torch.Tensor]): Prediction score with the\n                shape of [N].\n            batch_reg_preds (list[torch.Tensor]): Prediction bbox with the\n                shape of [N, 9].\n            batch_cls_labels (list[torch.Tensor]): Prediction label with the\n                shape of [N].\n            img_metas (list[dict]): Meta information of each sample.\n\n        Returns:\n            list[dict[str: torch.Tensor]]: contains the following keys:\n\n                -bboxes (torch.Tensor): Prediction bboxes after nms with the\n                    shape of [N, 9].\n                -scores (torch.Tensor): Prediction scores after nms with the\n                    shape of [N].\n                -labels (torch.Tensor): Prediction labels after nms with the\n                    shape of [N].\n        \"\"\"\n        predictions_dicts = []\n        for i, (box_preds, cls_preds, cls_labels) in enumerate(\n                zip(batch_reg_preds, batch_cls_preds, batch_cls_labels)):\n            default_val = [1.0 for _ in range(len(self.task_heads))]\n            factor = self.test_cfg.get('nms_rescale_factor',\n                                       default_val)[task_id]\n            if isinstance(factor, list):\n                for cid in range(len(factor)):\n                    box_preds[cls_labels == cid, 3:6] = \\\n                        box_preds[cls_labels == cid, 3:6] * factor[cid]\n            else:\n                box_preds[:, 3:6] = box_preds[:, 3:6] * factor\n\n            # Apply NMS in birdeye view\n            top_labels = cls_labels.long()\n            top_scores = cls_preds.squeeze(-1) if cls_preds.shape[0]>1 \\\n                else cls_preds\n\n            if top_scores.shape[0] != 0:\n                boxes_for_nms = img_metas[i]['box_type_3d'](\n                    box_preds[:, :], self.bbox_coder.code_size).bev\n                # the nms in 3d detection just remove overlap boxes.\n                if isinstance(self.test_cfg['nms_thr'], list):\n                    nms_thresh = self.test_cfg['nms_thr'][task_id]\n                else:\n                    nms_thresh = self.test_cfg['nms_thr']\n                selected = nms_bev(\n                    boxes_for_nms,\n                    top_scores,\n                    thresh=nms_thresh,\n                    pre_max_size=self.test_cfg['pre_max_size'],\n                    post_max_size=self.test_cfg['post_max_size'],\n                    xyxyr2xywhr=False)\n            else:\n                selected = []\n\n            if isinstance(factor, list):\n                for cid in range(len(factor)):\n                    box_preds[top_labels == cid, 3:6] = \\\n                        box_preds[top_labels == cid, 3:6] / factor[cid]\n            else:\n                box_preds[:, 3:6] = box_preds[:, 3:6] / factor\n\n            # if selected is not None:\n            selected_boxes = box_preds[selected]\n            selected_labels = top_labels[selected]\n            selected_scores = top_scores[selected]\n\n            # finally generate predictions.\n            if selected_boxes.shape[0] != 0:\n                predictions_dict = dict(\n                    bboxes=selected_boxes,\n                    scores=selected_scores,\n                    labels=selected_labels)\n            else:\n                dtype = batch_reg_preds[0].dtype\n                device = batch_reg_preds[0].device\n                predictions_dict = dict(\n                    bboxes=torch.zeros([0, self.bbox_coder.code_size],\n                                       dtype=dtype,\n                                       device=device),\n                    scores=torch.zeros([0], dtype=dtype, device=device),\n                    labels=torch.zeros([0],\n                                       dtype=top_labels.dtype,\n                                       device=device))\n\n            predictions_dicts.append(predictions_dict)\n        return predictions_dicts\n"
        }
    ]
}