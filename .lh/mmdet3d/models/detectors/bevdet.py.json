{
    "sourceFile": "mmdet3d/models/detectors/bevdet.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 36,
            "patches": [
                {
                    "date": 1716004338617,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1716023875842,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -638,8 +638,10 @@\n                          img_metas,\n                          pred_prev=False,\n                          sequential=False,\n                          **kwargs):\n+        import pdb\n+        pdb.set_trace()\n         if sequential:\n             # Todo\n             assert False\n         imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, \\\n"
                },
                {
                    "date": 1716024002913,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,22 +23,29 @@\n             backbone.\n         img_bev_encoder_neck (dict): Configuration dict of the BEV encoder neck.\n     \"\"\"\n \n-    def __init__(self,\n-                 img_view_transformer,\n-                 img_bev_encoder_backbone=None,\n-                 img_bev_encoder_neck=None,\n-                 use_grid_mask=False,\n-                 **kwargs):\n+    def __init__(\n+        self,\n+        img_view_transformer,\n+        img_bev_encoder_backbone=None,\n+        img_bev_encoder_neck=None,\n+        use_grid_mask=False,\n+        **kwargs\n+    ):\n         super(BEVDet, self).__init__(**kwargs)\n-        self.grid_mask = None if not use_grid_mask else \\\n-            GridMask(True, True, rotate=1, offset=False, ratio=0.5, mode=1,\n-                     prob=0.7)\n+        self.grid_mask = (\n+            None\n+            if not use_grid_mask\n+            else GridMask(\n+                True, True, rotate=1, offset=False, ratio=0.5, mode=1, prob=0.7\n+            )\n+        )\n         self.img_view_transformer = builder.build_neck(img_view_transformer)\n         if img_bev_encoder_neck and img_bev_encoder_backbone:\n-            self.img_bev_encoder_backbone = \\\n-                builder.build_backbone(img_bev_encoder_backbone)\n+            self.img_bev_encoder_backbone = builder.build_backbone(\n+                img_bev_encoder_backbone\n+            )\n             self.img_bev_encoder_neck = builder.build_neck(img_bev_encoder_neck)\n \n     def image_encoder(self, img, stereo=False):\n         imgs = img\n@@ -70,23 +77,20 @@\n     def prepare_inputs(self, inputs):\n         # split the inputs into each frame\n         assert len(inputs) == 7\n         B, N, C, H, W = inputs[0].shape\n-        imgs, sensor2egos, ego2globals, intrins, post_rots, post_trans, bda = \\\n-            inputs\n+        imgs, sensor2egos, ego2globals, intrins, post_rots, post_trans, bda = inputs\n \n         sensor2egos = sensor2egos.view(B, N, 4, 4)\n         ego2globals = ego2globals.view(B, N, 4, 4)\n \n         # calculate the transformation from sweep sensor to key ego\n-        keyego2global = ego2globals[:, 0,  ...].unsqueeze(1)\n+        keyego2global = ego2globals[:, 0, ...].unsqueeze(1)\n         global2keyego = torch.inverse(keyego2global.double())\n-        sensor2keyegos = \\\n-            global2keyego @ ego2globals.double() @ sensor2egos.double()\n+        sensor2keyegos = global2keyego @ ego2globals.double() @ sensor2egos.double()\n         sensor2keyegos = sensor2keyegos.float()\n \n-        return [imgs, sensor2keyegos, ego2globals, intrins,\n-                post_rots, post_trans, bda]\n+        return [imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, bda]\n \n     def extract_img_feat(self, img, img_metas, **kwargs):\n         \"\"\"Extract features of images.\"\"\"\n         img = self.prepare_inputs(img)\n@@ -100,19 +104,21 @@\n         img_feats, depth = self.extract_img_feat(img, img_metas, **kwargs)\n         pts_feats = None\n         return (img_feats, pts_feats, depth)\n \n-    def forward_train(self,\n-                      points=None,\n-                      img_metas=None,\n-                      gt_bboxes_3d=None,\n-                      gt_labels_3d=None,\n-                      gt_labels=None,\n-                      gt_bboxes=None,\n-                      img_inputs=None,\n-                      proposals=None,\n-                      gt_bboxes_ignore=None,\n-                      **kwargs):\n+    def forward_train(\n+        self,\n+        points=None,\n+        img_metas=None,\n+        gt_bboxes_3d=None,\n+        gt_labels_3d=None,\n+        gt_labels=None,\n+        gt_bboxes=None,\n+        img_inputs=None,\n+        proposals=None,\n+        gt_bboxes_ignore=None,\n+        **kwargs\n+    ):\n         \"\"\"Forward training function.\n \n         Args:\n             points (list[torch.Tensor], optional): Points of each sample.\n@@ -137,21 +143,18 @@\n         Returns:\n             dict: Losses of different branches.\n         \"\"\"\n         img_feats, pts_feats, _ = self.extract_feat(\n-            points, img=img_inputs, img_metas=img_metas, **kwargs)\n+            points, img=img_inputs, img_metas=img_metas, **kwargs\n+        )\n         losses = dict()\n-        losses_pts = self.forward_pts_train(img_feats, gt_bboxes_3d,\n-                                            gt_labels_3d, img_metas,\n-                                            gt_bboxes_ignore)\n+        losses_pts = self.forward_pts_train(\n+            img_feats, gt_bboxes_3d, gt_labels_3d, img_metas, gt_bboxes_ignore\n+        )\n         losses.update(losses_pts)\n         return losses\n \n-    def forward_test(self,\n-                     points=None,\n-                     img_metas=None,\n-                     img_inputs=None,\n-                     **kwargs):\n+    def forward_test(self, points=None, img_metas=None, img_inputs=None, **kwargs):\n         \"\"\"\n         Args:\n             points (list[torch.Tensor]): the outer list indicates test-time\n                 augmentations and inner torch.Tensor should have a shape NxC,\n@@ -163,54 +166,46 @@\n                 list indicates test-time augmentations and inner\n                 torch.Tensor should have a shape NxCxHxW, which contains\n                 all images in the batch. Defaults to None.\n         \"\"\"\n-        for var, name in [(img_inputs, 'img_inputs'),\n-                          (img_metas, 'img_metas')]:\n+        for var, name in [(img_inputs, \"img_inputs\"), (img_metas, \"img_metas\")]:\n             if not isinstance(var, list):\n-                raise TypeError('{} must be a list, but got {}'.format(\n-                    name, type(var)))\n+                raise TypeError(\"{} must be a list, but got {}\".format(name, type(var)))\n \n         num_augs = len(img_inputs)\n         if num_augs != len(img_metas):\n             raise ValueError(\n-                'num of augmentations ({}) != num of image meta ({})'.format(\n-                    len(img_inputs), len(img_metas)))\n+                \"num of augmentations ({}) != num of image meta ({})\".format(\n+                    len(img_inputs), len(img_metas)\n+                )\n+            )\n \n         if not isinstance(img_inputs[0][0], list):\n             img_inputs = [img_inputs] if img_inputs is None else img_inputs\n             points = [points] if points is None else points\n-            return self.simple_test(points[0], img_metas[0], img_inputs[0],\n-                                    **kwargs)\n+            return self.simple_test(points[0], img_metas[0], img_inputs[0], **kwargs)\n         else:\n             return self.aug_test(None, img_metas[0], img_inputs[0], **kwargs)\n \n     def aug_test(self, points, img_metas, img=None, rescale=False):\n         \"\"\"Test function without augmentaiton.\"\"\"\n         assert False\n \n-    def simple_test(self,\n-                    points,\n-                    img_metas,\n-                    img=None,\n-                    rescale=False,\n-                    **kwargs):\n+    def simple_test(self, points, img_metas, img=None, rescale=False, **kwargs):\n         \"\"\"Test function without augmentaiton.\"\"\"\n         img_feats, _, _ = self.extract_feat(\n-            points, img=img, img_metas=img_metas, **kwargs)\n+            points, img=img, img_metas=img_metas, **kwargs\n+        )\n         bbox_list = [dict() for _ in range(len(img_metas))]\n         bbox_pts = self.simple_test_pts(img_feats, img_metas, rescale=rescale)\n         for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n-            result_dict['pts_bbox'] = pts_bbox\n+            result_dict[\"pts_bbox\"] = pts_bbox\n         return bbox_list\n \n-    def forward_dummy(self,\n-                      points=None,\n-                      img_metas=None,\n-                      img_inputs=None,\n-                      **kwargs):\n+    def forward_dummy(self, points=None, img_metas=None, img_inputs=None, **kwargs):\n         img_feats, _, _ = self.extract_feat(\n-            points, img=img_inputs, img_metas=img_metas, **kwargs)\n+            points, img=img_inputs, img_metas=img_metas, **kwargs\n+        )\n         assert self.with_pts_bbox\n         outs = self.pts_bbox_head(img_feats)\n         return outs\n \n@@ -220,15 +215,15 @@\n \n     def result_serialize(self, outs):\n         outs_ = []\n         for out in outs:\n-            for key in ['reg', 'height', 'dim', 'rot', 'vel', 'heatmap']:\n+            for key in [\"reg\", \"height\", \"dim\", \"rot\", \"vel\", \"heatmap\"]:\n                 outs_.append(out[0][key])\n         return outs_\n \n     def result_deserialize(self, outs):\n         outs_ = []\n-        keys = ['reg', 'height', 'dim', 'rot', 'vel', 'heatmap']\n+        keys = [\"reg\", \"height\", \"dim\", \"rot\", \"vel\", \"heatmap\"]\n         for head_id in range(len(outs) // 6):\n             outs_head = [dict()]\n             for kid, key in enumerate(keys):\n                 outs_head[0][key] = outs[head_id * 6 + kid]\n@@ -246,16 +241,25 @@\n     ):\n         x = self.img_backbone(img)\n         x = self.img_neck(x)\n         x = self.img_view_transformer.depth_net(x)\n-        depth = x[:, :self.img_view_transformer.D].softmax(dim=1)\n-        tran_feat = x[:, self.img_view_transformer.D:(\n-            self.img_view_transformer.D +\n-            self.img_view_transformer.out_channels)]\n+        depth = x[:, : self.img_view_transformer.D].softmax(dim=1)\n+        tran_feat = x[\n+            :,\n+            self.img_view_transformer.D : (\n+                self.img_view_transformer.D + self.img_view_transformer.out_channels\n+            ),\n+        ]\n         tran_feat = tran_feat.permute(0, 2, 3, 1)\n-        x = TRTBEVPoolv2.apply(depth.contiguous(), tran_feat.contiguous(),\n-                               ranks_depth, ranks_feat, ranks_bev,\n-                               interval_starts, interval_lengths)\n+        x = TRTBEVPoolv2.apply(\n+            depth.contiguous(),\n+            tran_feat.contiguous(),\n+            ranks_depth,\n+            ranks_feat,\n+            ranks_bev,\n+            interval_starts,\n+            interval_lengths,\n+        )\n         x = x.permute(0, 3, 1, 2).contiguous()\n         bev_feat = self.bev_encoder(x)\n         outs = self.pts_bbox_head([bev_feat])\n         outs = self.result_serialize(outs)\n@@ -281,14 +285,17 @@\n         num_adj (int): Number of adjacent frames.\n         with_prev (bool): Whether to set the BEV feature of previous frame as\n             all zero. By default, False.\n     \"\"\"\n-    def __init__(self,\n-                 pre_process=None,\n-                 align_after_view_transfromation=False,\n-                 num_adj=1,\n-                 with_prev=True,\n-                 **kwargs):\n+\n+    def __init__(\n+        self,\n+        pre_process=None,\n+        align_after_view_transfromation=False,\n+        num_adj=1,\n+        with_prev=True,\n+        **kwargs\n+    ):\n         super(BEVDet4D, self).__init__(**kwargs)\n         self.pre_process = pre_process is not None\n         if self.pre_process:\n             self.pre_process_net = builder.build_backbone(pre_process)\n@@ -302,14 +309,18 @@\n         n, c, h, w = input.shape\n         _, v, _, _ = sensor2keyegos[0].shape\n         if self.grid is None:\n             # generate grid\n-            xs = torch.linspace(\n-                0, w - 1, w, dtype=input.dtype,\n-                device=input.device).view(1, w).expand(h, w)\n-            ys = torch.linspace(\n-                0, h - 1, h, dtype=input.dtype,\n-                device=input.device).view(h, 1).expand(h, w)\n+            xs = (\n+                torch.linspace(0, w - 1, w, dtype=input.dtype, device=input.device)\n+                .view(1, w)\n+                .expand(h, w)\n+            )\n+            ys = (\n+                torch.linspace(0, h - 1, h, dtype=input.dtype, device=input.device)\n+                .view(h, 1)\n+                .expand(h, w)\n+            )\n             grid = torch.stack((xs, ys, torch.ones_like(xs)), -1)\n             self.grid = grid\n         else:\n             grid = self.grid\n@@ -333,20 +344,19 @@\n             bda_[:, :, 3, 3] = 1\n         c12l0 = bda_.matmul(c12l0)\n \n         # transformation from current ego frame to adjacent ego frame\n-        l02l1 = c02l0.matmul(torch.inverse(c12l0))[:, 0, :, :].view(\n-            n, 1, 1, 4, 4)\n-        '''\n+        l02l1 = c02l0.matmul(torch.inverse(c12l0))[:, 0, :, :].view(n, 1, 1, 4, 4)\n+        \"\"\"\n           c02l0 * inv(c12l0)\n         = c02l0 * inv(l12l0 * c12l1)\n         = c02l0 * inv(c12l1) * inv(l12l0)\n         = l02l1 # c02l0==c12l1\n-        '''\n+        \"\"\"\n \n-        l02l1 = l02l1[:, :, :,\n-                      [True, True, False, True], :][:, :, :, :,\n-                                                    [True, True, False, True]]\n+        l02l1 = l02l1[:, :, :, [True, True, False, True], :][\n+            :, :, :, :, [True, True, False, True]\n+        ]\n \n         feat2bev = torch.zeros((3, 3), dtype=grid.dtype).to(grid)\n         feat2bev[0, 0] = self.img_view_transformer.grid_interval[0]\n         feat2bev[1, 1] = self.img_view_transformer.grid_interval[1]\n@@ -357,26 +367,27 @@\n         tf = torch.inverse(feat2bev).matmul(l02l1).matmul(feat2bev)\n \n         # transform and normalize\n         grid = tf.matmul(grid)\n-        normalize_factor = torch.tensor([w - 1.0, h - 1.0],\n-                                        dtype=input.dtype,\n-                                        device=input.device)\n-        grid = grid[:, :, :, :2, 0] / normalize_factor.view(1, 1, 1,\n-                                                            2) * 2.0 - 1.0\n+        normalize_factor = torch.tensor(\n+            [w - 1.0, h - 1.0], dtype=input.dtype, device=input.device\n+        )\n+        grid = grid[:, :, :, :2, 0] / normalize_factor.view(1, 1, 1, 2) * 2.0 - 1.0\n         return grid\n \n     @force_fp32()\n     def shift_feature(self, input, sensor2keyegos, bda, bda_adj=None):\n         grid = self.gen_grid(input, sensor2keyegos, bda, bda_adj=bda_adj)\n         output = F.grid_sample(input, grid.to(input.dtype), align_corners=True)\n         return output\n \n-    def prepare_bev_feat(self, img, rot, tran, intrin, post_rot, post_tran,\n-                         bda, mlp_input):\n+    def prepare_bev_feat(\n+        self, img, rot, tran, intrin, post_rot, post_tran, bda, mlp_input\n+    ):\n         x, _ = self.image_encoder(img)\n         bev_feat, depth = self.img_view_transformer(\n-            [x, rot, tran, intrin, post_rot, post_tran, bda, mlp_input])\n+            [x, rot, tran, intrin, post_rot, post_tran, bda, mlp_input]\n+        )\n         if self.pre_process:\n             bev_feat = self.pre_process_net(bev_feat)[0]\n         return bev_feat, depth\n \n@@ -384,22 +395,33 @@\n         imgs, sensor2keyegos_curr, ego2globals_curr, intrins = inputs[:4]\n         sensor2keyegos_prev, _, post_rots, post_trans, bda = inputs[4:]\n         bev_feat_list = []\n         mlp_input = self.img_view_transformer.get_mlp_input(\n-            sensor2keyegos_curr[0:1, ...], ego2globals_curr[0:1, ...],\n-            intrins, post_rots, post_trans, bda[0:1, ...])\n-        inputs_curr = (imgs, sensor2keyegos_curr[0:1, ...],\n-                       ego2globals_curr[0:1, ...], intrins, post_rots,\n-                       post_trans, bda[0:1, ...], mlp_input)\n+            sensor2keyegos_curr[0:1, ...],\n+            ego2globals_curr[0:1, ...],\n+            intrins,\n+            post_rots,\n+            post_trans,\n+            bda[0:1, ...],\n+        )\n+        inputs_curr = (\n+            imgs,\n+            sensor2keyegos_curr[0:1, ...],\n+            ego2globals_curr[0:1, ...],\n+            intrins,\n+            post_rots,\n+            post_trans,\n+            bda[0:1, ...],\n+            mlp_input,\n+        )\n         bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n         bev_feat_list.append(bev_feat)\n \n         # align the feat_prev\n         _, C, H, W = feat_prev.shape\n-        feat_prev = \\\n-            self.shift_feature(feat_prev,\n-                               [sensor2keyegos_curr, sensor2keyegos_prev],\n-                               bda)\n+        feat_prev = self.shift_feature(\n+            feat_prev, [sensor2keyegos_curr, sensor2keyegos_prev], bda\n+        )\n         bev_feat_list.append(feat_prev.view(1, (self.num_frame - 1) * C, H, W))\n \n         bev_feat = torch.cat(bev_feat_list, dim=1)\n         x = self.bev_encoder(bev_feat)\n@@ -411,35 +433,35 @@\n         N = N // self.num_frame\n         imgs = inputs[0].view(B, N, self.num_frame, C, H, W)\n         imgs = torch.split(imgs, 1, 2)\n         imgs = [t.squeeze(2) for t in imgs]\n-        sensor2egos, ego2globals, intrins, post_rots, post_trans, bda = \\\n-            inputs[1:7]\n+        sensor2egos, ego2globals, intrins, post_rots, post_trans, bda = inputs[1:7]\n \n         sensor2egos = sensor2egos.view(B, self.num_frame, N, 4, 4)\n         ego2globals = ego2globals.view(B, self.num_frame, N, 4, 4)\n \n         # calculate the transformation from sweep sensor to key ego\n         keyego2global = ego2globals[:, 0, 0, ...].unsqueeze(1).unsqueeze(1)\n         global2keyego = torch.inverse(keyego2global.double())\n-        sensor2keyegos = \\\n-            global2keyego @ ego2globals.double() @ sensor2egos.double()\n+        sensor2keyegos = global2keyego @ ego2globals.double() @ sensor2egos.double()\n         sensor2keyegos = sensor2keyegos.float()\n \n         curr2adjsensor = None\n         if stereo:\n             sensor2egos_cv, ego2globals_cv = sensor2egos, ego2globals\n-            sensor2egos_curr = \\\n-                sensor2egos_cv[:, :self.temporal_frame, ...].double()\n-            ego2globals_curr = \\\n-                ego2globals_cv[:, :self.temporal_frame, ...].double()\n-            sensor2egos_adj = \\\n-                sensor2egos_cv[:, 1:self.temporal_frame + 1, ...].double()\n-            ego2globals_adj = \\\n-                ego2globals_cv[:, 1:self.temporal_frame + 1, ...].double()\n-            curr2adjsensor = \\\n-                torch.inverse(ego2globals_adj @ sensor2egos_adj) \\\n-                @ ego2globals_curr @ sensor2egos_curr\n+            sensor2egos_curr = sensor2egos_cv[:, : self.temporal_frame, ...].double()\n+            ego2globals_curr = ego2globals_cv[:, : self.temporal_frame, ...].double()\n+            sensor2egos_adj = sensor2egos_cv[\n+                :, 1 : self.temporal_frame + 1, ...\n+            ].double()\n+            ego2globals_adj = ego2globals_cv[\n+                :, 1 : self.temporal_frame + 1, ...\n+            ].double()\n+            curr2adjsensor = (\n+                torch.inverse(ego2globals_adj @ sensor2egos_adj)\n+                @ ego2globals_curr\n+                @ sensor2egos_curr\n+            )\n             curr2adjsensor = curr2adjsensor.float()\n             curr2adjsensor = torch.split(curr2adjsensor, 1, 1)\n             curr2adjsensor = [p.squeeze(1) for p in curr2adjsensor]\n             curr2adjsensor.extend([None for _ in range(self.extra_ref_frames)])\n@@ -449,39 +471,55 @@\n             sensor2keyegos,\n             ego2globals,\n             intrins.view(B, self.num_frame, N, 3, 3),\n             post_rots.view(B, self.num_frame, N, 3, 3),\n-            post_trans.view(B, self.num_frame, N, 3)\n+            post_trans.view(B, self.num_frame, N, 3),\n         ]\n         extra = [torch.split(t, 1, 1) for t in extra]\n         extra = [[p.squeeze(1) for p in t] for t in extra]\n         sensor2keyegos, ego2globals, intrins, post_rots, post_trans = extra\n-        return imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, \\\n-               bda, curr2adjsensor\n+        return (\n+            imgs,\n+            sensor2keyegos,\n+            ego2globals,\n+            intrins,\n+            post_rots,\n+            post_trans,\n+            bda,\n+            curr2adjsensor,\n+        )\n \n-    def extract_img_feat(self,\n-                         img,\n-                         img_metas,\n-                         pred_prev=False,\n-                         sequential=False,\n-                         **kwargs):\n+    def extract_img_feat(\n+        self, img, img_metas, pred_prev=False, sequential=False, **kwargs\n+    ):\n         if sequential:\n-            return self.extract_img_feat_sequential(img, kwargs['feat_prev'])\n-        imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, \\\n-        bda, _ = self.prepare_inputs(img)\n+            return self.extract_img_feat_sequential(img, kwargs[\"feat_prev\"])\n+        imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, bda, _ = (\n+            self.prepare_inputs(img)\n+        )\n         \"\"\"Extract features of images.\"\"\"\n         bev_feat_list = []\n         depth_list = []\n         key_frame = True  # back propagation for key frame only\n         for img, sensor2keyego, ego2global, intrin, post_rot, post_tran in zip(\n-                imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans):\n+            imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans\n+        ):\n             if key_frame or self.with_prev:\n                 if self.align_after_view_transfromation:\n                     sensor2keyego, ego2global = sensor2keyegos[0], ego2globals[0]\n                 mlp_input = self.img_view_transformer.get_mlp_input(\n-                    sensor2keyegos[0], ego2globals[0], intrin, post_rot, post_tran, bda)\n-                inputs_curr = (img, sensor2keyego, ego2global, intrin, post_rot,\n-                               post_tran, bda, mlp_input)\n+                    sensor2keyegos[0], ego2globals[0], intrin, post_rot, post_tran, bda\n+                )\n+                inputs_curr = (\n+                    img,\n+                    sensor2keyego,\n+                    ego2global,\n+                    intrin,\n+                    post_rot,\n+                    post_tran,\n+                    bda,\n+                    mlp_input,\n+                )\n                 if key_frame:\n                     bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n                 else:\n                     with torch.no_grad():\n@@ -495,47 +533,52 @@\n         if pred_prev:\n             assert self.align_after_view_transfromation\n             assert sensor2keyegos[0].shape[0] == 1\n             feat_prev = torch.cat(bev_feat_list[1:], dim=0)\n-            ego2globals_curr = \\\n-                ego2globals[0].repeat(self.num_frame - 1, 1, 1, 1)\n-            sensor2keyegos_curr = \\\n-                sensor2keyegos[0].repeat(self.num_frame - 1, 1, 1, 1)\n+            ego2globals_curr = ego2globals[0].repeat(self.num_frame - 1, 1, 1, 1)\n+            sensor2keyegos_curr = sensor2keyegos[0].repeat(self.num_frame - 1, 1, 1, 1)\n             ego2globals_prev = torch.cat(ego2globals[1:], dim=0)\n             sensor2keyegos_prev = torch.cat(sensor2keyegos[1:], dim=0)\n             bda_curr = bda.repeat(self.num_frame - 1, 1, 1)\n-            return feat_prev, [imgs[0],\n-                               sensor2keyegos_curr, ego2globals_curr,\n-                               intrins[0],\n-                               sensor2keyegos_prev, ego2globals_prev,\n-                               post_rots[0], post_trans[0],\n-                               bda_curr]\n+            return feat_prev, [\n+                imgs[0],\n+                sensor2keyegos_curr,\n+                ego2globals_curr,\n+                intrins[0],\n+                sensor2keyegos_prev,\n+                ego2globals_prev,\n+                post_rots[0],\n+                post_trans[0],\n+                bda_curr,\n+            ]\n         if self.align_after_view_transfromation:\n             for adj_id in range(1, self.num_frame):\n-                bev_feat_list[adj_id] = \\\n-                    self.shift_feature(bev_feat_list[adj_id],\n-                                       [sensor2keyegos[0],\n-                                        sensor2keyegos[adj_id]],\n-                                       bda)\n+                bev_feat_list[adj_id] = self.shift_feature(\n+                    bev_feat_list[adj_id],\n+                    [sensor2keyegos[0], sensor2keyegos[adj_id]],\n+                    bda,\n+                )\n         bev_feat = torch.cat(bev_feat_list, dim=1)\n         x = self.bev_encoder(bev_feat)\n         return [x], depth_list[0]\n \n \n @DETECTORS.register_module()\n class BEVDepth4D(BEVDet4D):\n \n-    def forward_train(self,\n-                      points=None,\n-                      img_metas=None,\n-                      gt_bboxes_3d=None,\n-                      gt_labels_3d=None,\n-                      gt_labels=None,\n-                      gt_bboxes=None,\n-                      img_inputs=None,\n-                      proposals=None,\n-                      gt_bboxes_ignore=None,\n-                      **kwargs):\n+    def forward_train(\n+        self,\n+        points=None,\n+        img_metas=None,\n+        gt_bboxes_3d=None,\n+        gt_labels_3d=None,\n+        gt_labels=None,\n+        gt_bboxes=None,\n+        img_inputs=None,\n+        proposals=None,\n+        gt_bboxes_ignore=None,\n+        **kwargs\n+    ):\n         \"\"\"Forward training function.\n \n         Args:\n             points (list[torch.Tensor], optional): Points of each sample.\n@@ -560,15 +603,16 @@\n         Returns:\n             dict: Losses of different branches.\n         \"\"\"\n         img_feats, pts_feats, depth = self.extract_feat(\n-            points, img=img_inputs, img_metas=img_metas, **kwargs)\n-        gt_depth = kwargs['gt_depth']\n+            points, img=img_inputs, img_metas=img_metas, **kwargs\n+        )\n+        gt_depth = kwargs[\"gt_depth\"]\n         loss_depth = self.img_view_transformer.get_depth_loss(gt_depth, depth)\n         losses = dict(loss_depth=loss_depth)\n-        losses_pts = self.forward_pts_train(img_feats, gt_bboxes_3d,\n-                                            gt_labels_3d, img_metas,\n-                                            gt_bboxes_ignore)\n+        losses_pts = self.forward_pts_train(\n+            img_feats, gt_bboxes_3d, gt_labels_3d, img_metas, gt_bboxes_ignore\n+        )\n         losses.update(losses_pts)\n         return losses\n \n \n@@ -582,9 +626,9 @@\n \n     def extract_stereo_ref_feat(self, x):\n         B, N, C, imH, imW = x.shape\n         x = x.view(B * N, C, imH, imW)\n-        if isinstance(self.img_backbone,ResNet):\n+        if isinstance(self.img_backbone, ResNet):\n             if self.img_backbone.deep_stem:\n                 x = self.img_backbone.stem(x)\n             else:\n                 x = self.img_backbone.conv1(x)\n@@ -596,113 +640,150 @@\n                 x = res_layer(x)\n                 return x\n         else:\n             x = self.img_backbone.patch_embed(x)\n-            hw_shape = (self.img_backbone.patch_embed.DH,\n-                        self.img_backbone.patch_embed.DW)\n+            hw_shape = (\n+                self.img_backbone.patch_embed.DH,\n+                self.img_backbone.patch_embed.DW,\n+            )\n             if self.img_backbone.use_abs_pos_embed:\n                 x = x + self.img_backbone.absolute_pos_embed\n             x = self.img_backbone.drop_after_pos(x)\n \n             for i, stage in enumerate(self.img_backbone.stages):\n                 x, hw_shape, out, out_hw_shape = stage(x, hw_shape)\n-                out = out.view(-1,  *out_hw_shape,\n-                               self.img_backbone.num_features[i])\n+                out = out.view(-1, *out_hw_shape, self.img_backbone.num_features[i])\n                 out = out.permute(0, 3, 1, 2).contiguous()\n                 return out\n \n-    def prepare_bev_feat(self, img, sensor2keyego, ego2global, intrin,\n-                         post_rot, post_tran, bda, mlp_input, feat_prev_iv,\n-                         k2s_sensor, extra_ref_frame):\n+    def prepare_bev_feat(\n+        self,\n+        img,\n+        sensor2keyego,\n+        ego2global,\n+        intrin,\n+        post_rot,\n+        post_tran,\n+        bda,\n+        mlp_input,\n+        feat_prev_iv,\n+        k2s_sensor,\n+        extra_ref_frame,\n+    ):\n         if extra_ref_frame:\n             stereo_feat = self.extract_stereo_ref_feat(img)\n             return None, None, stereo_feat\n         x, stereo_feat = self.image_encoder(img, stereo=True)\n-        metas = dict(k2s_sensor=k2s_sensor,\n-                     intrins=intrin,\n-                     post_rots=post_rot,\n-                     post_trans=post_tran,\n-                     frustum=self.img_view_transformer.cv_frustum.to(x),\n-                     cv_downsample=4,\n-                     downsample=self.img_view_transformer.downsample,\n-                     grid_config=self.img_view_transformer.grid_config,\n-                     cv_feat_list=[feat_prev_iv, stereo_feat])\n+        metas = dict(\n+            k2s_sensor=k2s_sensor,\n+            intrins=intrin,\n+            post_rots=post_rot,\n+            post_trans=post_tran,\n+            frustum=self.img_view_transformer.cv_frustum.to(x),\n+            cv_downsample=4,\n+            downsample=self.img_view_transformer.downsample,\n+            grid_config=self.img_view_transformer.grid_config,\n+            cv_feat_list=[feat_prev_iv, stereo_feat],\n+        )\n         bev_feat, depth = self.img_view_transformer(\n-            [x, sensor2keyego, ego2global, intrin, post_rot, post_tran, bda,\n-             mlp_input], metas)\n+            [x, sensor2keyego, ego2global, intrin, post_rot, post_tran, bda, mlp_input],\n+            metas,\n+        )\n         if self.pre_process:\n             bev_feat = self.pre_process_net(bev_feat)[0]\n         return bev_feat, depth, stereo_feat\n \n-    def extract_img_feat(self,\n-                         img,\n-                         img_metas,\n-                         pred_prev=False,\n-                         sequential=False,\n-                         **kwargs):\n+    def extract_img_feat(\n+        self, img, img_metas, pred_prev=False, sequential=False, **kwargs\n+    ):\n         import pdb\n+\n         pdb.set_trace()\n         if sequential:\n             # Todo\n             assert False\n-        imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, \\\n-        bda, curr2adjsensor = self.prepare_inputs(img, stereo=True)\n+        (\n+            imgs,\n+            sensor2keyegos,\n+            ego2globals,\n+            intrins,\n+            post_rots,\n+            post_trans,\n+            bda,\n+            curr2adjsensor,\n+        ) = self.prepare_inputs(img, stereo=True)\n         \"\"\"Extract features of images.\"\"\"\n         bev_feat_list = []\n         depth_key_frame = None\n         feat_prev_iv = None\n-        for fid in range(self.num_frame-1, -1, -1):\n-            img, sensor2keyego, ego2global, intrin, post_rot, post_tran = \\\n-                imgs[fid], sensor2keyegos[fid], ego2globals[fid], intrins[fid], \\\n-                post_rots[fid], post_trans[fid]\n+        for fid in range(self.num_frame - 1, -1, -1):\n+            img, sensor2keyego, ego2global, intrin, post_rot, post_tran = (\n+                imgs[fid],\n+                sensor2keyegos[fid],\n+                ego2globals[fid],\n\\ No newline at end of file\n+                intrins[fid],\n+                post_rots[fid],\n+                post_trans[fid],\n+            )\n             key_frame = fid == 0\n-            extra_ref_frame = fid == self.num_frame-self.extra_ref_frames\n+            extra_ref_frame = fid == self.num_frame - self.extra_ref_frames\n             if key_frame or self.with_prev:\n                 if self.align_after_view_transfromation:\n                     sensor2keyego, ego2global = sensor2keyegos[0], ego2globals[0]\n                 mlp_input = self.img_view_transformer.get_mlp_input(\n-                    sensor2keyegos[0], ego2globals[0], intrin,\n-                    post_rot, post_tran, bda)\n-                inputs_curr = (img, sensor2keyego, ego2global, intrin,\n-                               post_rot, post_tran, bda, mlp_input,\n-                               feat_prev_iv, curr2adjsensor[fid],\n-                               extra_ref_frame)\n+                    sensor2keyegos[0], ego2globals[0], intrin, post_rot, post_tran, bda\n+                )\n+                inputs_curr = (\n+                    img,\n+                    sensor2keyego,\n+                    ego2global,\n+                    intrin,\n+                    post_rot,\n+                    post_tran,\n+                    bda,\n+                    mlp_input,\n+                    feat_prev_iv,\n+                    curr2adjsensor[fid],\n+                    extra_ref_frame,\n+                )\n                 if key_frame:\n-                    bev_feat, depth, feat_curr_iv = \\\n-                        self.prepare_bev_feat(*inputs_curr)\n+                    bev_feat, depth, feat_curr_iv = self.prepare_bev_feat(*inputs_curr)\n                     depth_key_frame = depth\n                 else:\n                     with torch.no_grad():\n-                        bev_feat, depth, feat_curr_iv = \\\n-                            self.prepare_bev_feat(*inputs_curr)\n+                        bev_feat, depth, feat_curr_iv = self.prepare_bev_feat(\n+                            *inputs_curr\n+                        )\n                 if not extra_ref_frame:\n                     bev_feat_list.append(bev_feat)\n                 feat_prev_iv = feat_curr_iv\n         if pred_prev:\n             # Todo\n             assert False\n         if not self.with_prev:\n             bev_feat_key = bev_feat_list[0]\n-            if len(bev_feat_key.shape) ==4:\n-                b,c,h,w = bev_feat_key.shape\n-                bev_feat_list = \\\n-                    [torch.zeros([b,\n-                                  c * (self.num_frame -\n-                                       self.extra_ref_frames - 1),\n-                                  h, w]).to(bev_feat_key), bev_feat_key]\n+            if len(bev_feat_key.shape) == 4:\n+                b, c, h, w = bev_feat_key.shape\n+                bev_feat_list = [\n+                    torch.zeros(\n+                        [b, c * (self.num_frame - self.extra_ref_frames - 1), h, w]\n+                    ).to(bev_feat_key),\n+                    bev_feat_key,\n+                ]\n             else:\n                 b, c, z, h, w = bev_feat_key.shape\n-                bev_feat_list = \\\n-                    [torch.zeros([b,\n-                                  c * (self.num_frame -\n-                                       self.extra_ref_frames - 1), z,\n-                                  h, w]).to(bev_feat_key), bev_feat_key]\n+                bev_feat_list = [\n+                    torch.zeros(\n+                        [b, c * (self.num_frame - self.extra_ref_frames - 1), z, h, w]\n+                    ).to(bev_feat_key),\n+                    bev_feat_key,\n+                ]\n         if self.align_after_view_transfromation:\n-            for adj_id in range(self.num_frame-2):\n-                bev_feat_list[adj_id] = \\\n-                    self.shift_feature(bev_feat_list[adj_id],\n-                                       [sensor2keyegos[0],\n-                                        sensor2keyegos[self.num_frame-2-adj_id]],\n-                                       bda)\n+            for adj_id in range(self.num_frame - 2):\n+                bev_feat_list[adj_id] = self.shift_feature(\n+                    bev_feat_list[adj_id],\n+                    [sensor2keyegos[0], sensor2keyegos[self.num_frame - 2 - adj_id]],\n+                    bda,\n+                )\n         bev_feat = torch.cat(bev_feat_list, dim=1)\n         x = self.bev_encoder(bev_feat)\n-        return [x], depth_key_frame\n+        return [x], depth_key_frame\n"
                },
                {
                    "date": 1716024029835,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,8 +8,9 @@\n from .. import builder\n from .centerpoint import CenterPoint\n from mmdet3d.models.utils.grid_mask import GridMask\n from mmdet.models.backbones.resnet import ResNet\n+from termcolor import colored\n \n \n @DETECTORS.register_module()\n class BEVDet(CenterPoint):\n@@ -300,8 +301,9 @@\n         if self.pre_process:\n             self.pre_process_net = builder.build_backbone(pre_process)\n         self.align_after_view_transfromation = align_after_view_transfromation\n         self.num_frame = num_adj + 1\n+        print()\n \n         self.with_prev = with_prev\n         self.grid = None\n \n@@ -785,5 +787,5 @@\n                     bda,\n                 )\n         bev_feat = torch.cat(bev_feat_list, dim=1)\n         x = self.bev_encoder(bev_feat)\n-        return [x], depth_key_frame\n\\ No newline at end of file\n+        return [x], depth_key_frame\n"
                },
                {
                    "date": 1716024046888,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -696,11 +696,8 @@\n \n     def extract_img_feat(\n         self, img, img_metas, pred_prev=False, sequential=False, **kwargs\n     ):\n-        import pdb\n-\n-        pdb.set_trace()\n         if sequential:\n             # Todo\n             assert False\n         (\n"
                },
                {
                    "date": 1716024070947,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -301,9 +301,9 @@\n         if self.pre_process:\n             self.pre_process_net = builder.build_backbone(pre_process)\n         self.align_after_view_transfromation = align_after_view_transfromation\n         self.num_frame = num_adj + 1\n-        print()\n+        print(colored(f\"========{self.num_frame}===\",'green'))\n \n         self.with_prev = with_prev\n         self.grid = None\n \n"
                },
                {
                    "date": 1716024117420,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,9 +30,9 @@\n         img_view_transformer,\n         img_bev_encoder_backbone=None,\n         img_bev_encoder_neck=None,\n         use_grid_mask=False,\n-        **kwargs\n+        **kwargs,\n     ):\n         super(BEVDet, self).__init__(**kwargs)\n         self.grid_mask = (\n             None\n@@ -116,9 +116,9 @@\n         gt_bboxes=None,\n         img_inputs=None,\n         proposals=None,\n         gt_bboxes_ignore=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         \"\"\"Forward training function.\n \n         Args:\n@@ -293,17 +293,17 @@\n         pre_process=None,\n         align_after_view_transfromation=False,\n         num_adj=1,\n         with_prev=True,\n-        **kwargs\n+        **kwargs,\n     ):\n         super(BEVDet4D, self).__init__(**kwargs)\n         self.pre_process = pre_process is not None\n         if self.pre_process:\n             self.pre_process_net = builder.build_backbone(pre_process)\n         self.align_after_view_transfromation = align_after_view_transfromation\n         self.num_frame = num_adj + 1\n-        print(colored(f\"========{self.num_frame}===\",'green'))\n+        print(colored(f\"========{self.num_frame}===\", \"green\"))\n \n         self.with_prev = with_prev\n         self.grid = None\n \n@@ -577,9 +577,9 @@\n         gt_bboxes=None,\n         img_inputs=None,\n         proposals=None,\n         gt_bboxes_ignore=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         \"\"\"Forward training function.\n \n         Args:\n"
                },
                {
                    "date": 1716024134373,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -301,10 +301,10 @@\n         if self.pre_process:\n             self.pre_process_net = builder.build_backbone(pre_process)\n         self.align_after_view_transfromation = align_after_view_transfromation\n         self.num_frame = num_adj + 1\n-        print(colored(f\"========{self.num_frame}===\", \"green\"))\n \n+\n         self.with_prev = with_prev\n         self.grid = None\n \n     def gen_grid(self, input, sensor2keyegos, bda, bda_adj=None):\n"
                },
                {
                    "date": 1716024244485,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,8 +9,9 @@\n from .centerpoint import CenterPoint\n from mmdet3d.models.utils.grid_mask import GridMask\n from mmdet.models.backbones.resnet import ResNet\n from termcolor import colored\n+from mmdet.utils import get_root_logger\n \n \n @DETECTORS.register_module()\n class BEVDet(CenterPoint):\n@@ -302,9 +303,8 @@\n             self.pre_process_net = builder.build_backbone(pre_process)\n         self.align_after_view_transfromation = align_after_view_transfromation\n         self.num_frame = num_adj + 1\n \n-\n         self.with_prev = with_prev\n         self.grid = None\n \n     def gen_grid(self, input, sensor2keyegos, bda, bda_adj=None):\n@@ -624,8 +624,10 @@\n         super(BEVStereo4D, self).__init__(**kwargs)\n         self.extra_ref_frames = 1\n         self.temporal_frame = self.num_frame\n         self.num_frame += self.extra_ref_frames\n+        \n+        print(colored(f\"========{self.num_frame}===\", \"green\"))\n \n     def extract_stereo_ref_feat(self, x):\n         B, N, C, imH, imW = x.shape\n         x = x.view(B * N, C, imH, imW)\n"
                },
                {
                    "date": 1716024258005,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -625,8 +625,10 @@\n         self.extra_ref_frames = 1\n         self.temporal_frame = self.num_frame\n         self.num_frame += self.extra_ref_frames\n         \n+        \n+\n         print(colored(f\"========{self.num_frame}===\", \"green\"))\n \n     def extract_stereo_ref_feat(self, x):\n         B, N, C, imH, imW = x.shape\n"
                },
                {
                    "date": 1716024309736,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -624,13 +624,9 @@\n         super(BEVStereo4D, self).__init__(**kwargs)\n         self.extra_ref_frames = 1\n         self.temporal_frame = self.num_frame\n         self.num_frame += self.extra_ref_frames\n-        \n-        \n \n-        print(colored(f\"========{self.num_frame}===\", \"green\"))\n-\n     def extract_stereo_ref_feat(self, x):\n         B, N, C, imH, imW = x.shape\n         x = x.view(B * N, C, imH, imW)\n         if isinstance(self.img_backbone, ResNet):\n@@ -700,8 +696,11 @@\n \n     def extract_img_feat(\n         self, img, img_metas, pred_prev=False, sequential=False, **kwargs\n     ):\n+        \n+        print(colored(f\"========{self.num_frame}===\", \"green\"))\n+\n         if sequential:\n             # Todo\n             assert False\n         (\n"
                },
                {
                    "date": 1716024340975,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -696,11 +696,12 @@\n \n     def extract_img_feat(\n         self, img, img_metas, pred_prev=False, sequential=False, **kwargs\n     ):\n-        \n-        print(colored(f\"========{self.num_frame}===\", \"green\"))\n \n+        logger = get_root_logger()\n+        logger.warn(f\"========{self.num_frame}===\")\n+\n         if sequential:\n             # Todo\n             assert False\n         (\n"
                },
                {
                    "date": 1716025316744,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -697,11 +697,8 @@\n     def extract_img_feat(\n         self, img, img_metas, pred_prev=False, sequential=False, **kwargs\n     ):\n \n-        logger = get_root_logger()\n-        logger.warn(f\"========{self.num_frame}===\")\n-\n         if sequential:\n             # Todo\n             assert False\n         (\n"
                },
                {
                    "date": 1716025409283,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -55,8 +55,10 @@\n         imgs = imgs.view(B * N, C, imH, imW)\n         if self.grid_mask is not None:\n             imgs = self.grid_mask(imgs)\n         x = self.img_backbone(imgs)\n+        import pdb\n+        pdb.set_trace()\n         stereo_feat = None\n         if stereo:\n             stereo_feat = x[0]\n             x = x[1:]\n"
                },
                {
                    "date": 1716025500783,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -55,10 +55,8 @@\n         imgs = imgs.view(B * N, C, imH, imW)\n         if self.grid_mask is not None:\n             imgs = self.grid_mask(imgs)\n         x = self.img_backbone(imgs)\n-        import pdb\n-        pdb.set_trace()\n         stereo_feat = None\n         if stereo:\n             stereo_feat = x[0]\n             x = x[1:]\n"
                },
                {
                    "date": 1716367438286,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -673,8 +673,9 @@\n     ):\n         if extra_ref_frame:\n             stereo_feat = self.extract_stereo_ref_feat(img)\n             return None, None, stereo_feat\n+        \n         x, stereo_feat = self.image_encoder(img, stereo=True)\n         metas = dict(\n             k2s_sensor=k2s_sensor,\n             intrins=intrin,\n"
                },
                {
                    "date": 1716367449910,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -711,8 +711,9 @@\n             post_trans,\n             bda,\n             curr2adjsensor,\n         ) = self.prepare_inputs(img, stereo=True)\n+        \n         \"\"\"Extract features of images.\"\"\"\n         bev_feat_list = []\n         depth_key_frame = None\n         feat_prev_iv = None\n"
                },
                {
                    "date": 1716369754273,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -673,9 +673,9 @@\n     ):\n         if extra_ref_frame:\n             stereo_feat = self.extract_stereo_ref_feat(img)\n             return None, None, stereo_feat\n-        \n+\n         x, stereo_feat = self.image_encoder(img, stereo=True)\n         metas = dict(\n             k2s_sensor=k2s_sensor,\n             intrins=intrin,\n@@ -711,9 +711,9 @@\n             post_trans,\n             bda,\n             curr2adjsensor,\n         ) = self.prepare_inputs(img, stereo=True)\n-        \n+\n         \"\"\"Extract features of images.\"\"\"\n         bev_feat_list = []\n         depth_key_frame = None\n         feat_prev_iv = None\n"
                },
                {
                    "date": 1716437241016,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -336,9 +336,9 @@\n         c12l0 = sensor2keyegos[1][:, 0:1, :, :]\n \n         # add bev data augmentation\n         bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n-        bda_[:, :, :3, :3] = bda.unsqueeze(1)\n+        bda_[:, :, :3, :3] = bda.unsqueeze(1)[:,:,:3,:3]\n         bda_[:, :, 3, 3] = 1\n         c02l0 = bda_.matmul(c02l0)\n         if bda_adj is not None:\n             bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n"
                },
                {
                    "date": 1716437277104,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -336,9 +336,9 @@\n         c12l0 = sensor2keyegos[1][:, 0:1, :, :]\n \n         # add bev data augmentation\n         bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n-        bda_[:, :, :3, :3] = bda.unsqueeze(1)[:,:,:3,:3]\n+        bda_[:, :, :3, :3] = bda.unsqueeze(1)[:, :, :3, :3] # change for diff, before change is \"bda.unsqueeze(1\"\n         bda_[:, :, 3, 3] = 1\n         c02l0 = bda_.matmul(c02l0)\n         if bda_adj is not None:\n             bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n"
                },
                {
                    "date": 1716437398388,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -336,9 +336,9 @@\n         c12l0 = sensor2keyegos[1][:, 0:1, :, :]\n \n         # add bev data augmentation\n         bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n-        bda_[:, :, :3, :3] = bda.unsqueeze(1)[:, :, :3, :3] # change for diff, before change is \"bda.unsqueeze(1\"\n+        bda_[:, :, :3, :3] = bda.unsqueeze(1)[:, :, :3, :3] # change for diff, before change is \"bda.unsqueeze(1)\"\n         bda_[:, :, 3, 3] = 1\n         c02l0 = bda_.matmul(c02l0)\n         if bda_adj is not None:\n             bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n"
                },
                {
                    "date": 1716533363151,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -607,8 +607,11 @@\n         \"\"\"\n         img_feats, pts_feats, depth = self.extract_feat(\n             points, img=img_inputs, img_metas=img_metas, **kwargs\n         )\n+        \n+        # base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n+        \n         gt_depth = kwargs[\"gt_depth\"]\n         loss_depth = self.img_view_transformer.get_depth_loss(gt_depth, depth)\n         losses = dict(loss_depth=loss_depth)\n         losses_pts = self.forward_pts_train(\n"
                },
                {
                    "date": 1716533376485,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -608,9 +608,10 @@\n         img_feats, pts_feats, depth = self.extract_feat(\n             points, img=img_inputs, img_metas=img_metas, **kwargs\n         )\n         \n-        # base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n+        base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n+        feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n         \n         gt_depth = kwargs[\"gt_depth\"]\n         loss_depth = self.img_view_transformer.get_depth_loss(gt_depth, depth)\n         losses = dict(loss_depth=loss_depth)\n"
                },
                {
                    "date": 1716533384419,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,10 +10,10 @@\n from mmdet3d.models.utils.grid_mask import GridMask\n from mmdet.models.backbones.resnet import ResNet\n from termcolor import colored\n from mmdet.utils import get_root_logger\n+from mmdet3d.models.utils.self_print import feats_to_img\n \n-\n @DETECTORS.register_module()\n class BEVDet(CenterPoint):\n     r\"\"\"BEVDet paradigm for multi-camera 3D object detection.\n \n"
                },
                {
                    "date": 1716533511702,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -196,8 +196,13 @@\n         \"\"\"Test function without augmentaiton.\"\"\"\n         img_feats, _, _ = self.extract_feat(\n             points, img=img, img_metas=img_metas, **kwargs\n         )\n+        \n+                \n+        base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n+        feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n+        \n         bbox_list = [dict() for _ in range(len(img_metas))]\n         bbox_pts = self.simple_test_pts(img_feats, img_metas, rescale=rescale)\n         for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n             result_dict[\"pts_bbox\"] = pts_bbox\n@@ -607,12 +612,9 @@\n         \"\"\"\n         img_feats, pts_feats, depth = self.extract_feat(\n             points, img=img_inputs, img_metas=img_metas, **kwargs\n         )\n-        \n-        base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n-        feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n-        \n+\n         gt_depth = kwargs[\"gt_depth\"]\n         loss_depth = self.img_view_transformer.get_depth_loss(gt_depth, depth)\n         losses = dict(loss_depth=loss_depth)\n         losses_pts = self.forward_pts_train(\n"
                },
                {
                    "date": 1716533814836,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -198,10 +198,10 @@\n             points, img=img, img_metas=img_metas, **kwargs\n         )\n         \n                 \n-        base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n-        feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n+        # base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n+        # feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n         \n         bbox_list = [dict() for _ in range(len(img_metas))]\n         bbox_pts = self.simple_test_pts(img_feats, img_metas, rescale=rescale)\n         for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n"
                },
                {
                    "date": 1716533886138,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -707,8 +707,9 @@\n \n         if sequential:\n             # Todo\n             assert False\n+            \n         (\n             imgs,\n             sensor2keyegos,\n             ego2globals,\n"
                },
                {
                    "date": 1716533921711,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,8 +12,9 @@\n from termcolor import colored\n from mmdet.utils import get_root_logger\n from mmdet3d.models.utils.self_print import feats_to_img\n \n+\n @DETECTORS.register_module()\n class BEVDet(CenterPoint):\n     r\"\"\"BEVDet paradigm for multi-camera 3D object detection.\n \n@@ -196,13 +197,12 @@\n         \"\"\"Test function without augmentaiton.\"\"\"\n         img_feats, _, _ = self.extract_feat(\n             points, img=img, img_metas=img_metas, **kwargs\n         )\n-        \n-                \n+\n         # base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n         # feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n-        \n+\n         bbox_list = [dict() for _ in range(len(img_metas))]\n         bbox_pts = self.simple_test_pts(img_feats, img_metas, rescale=rescale)\n         for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n             result_dict[\"pts_bbox\"] = pts_bbox\n@@ -341,9 +341,11 @@\n         c12l0 = sensor2keyegos[1][:, 0:1, :, :]\n \n         # add bev data augmentation\n         bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n-        bda_[:, :, :3, :3] = bda.unsqueeze(1)[:, :, :3, :3] # change for diff, before change is \"bda.unsqueeze(1)\"\n+        bda_[:, :, :3, :3] = bda.unsqueeze(1)[\n+            :, :, :3, :3\n+        ]  # change for diff, before change is \"bda.unsqueeze(1)\"\n         bda_[:, :, 3, 3] = 1\n         c02l0 = bda_.matmul(c02l0)\n         if bda_adj is not None:\n             bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n@@ -707,9 +709,9 @@\n \n         if sequential:\n             # Todo\n             assert False\n-            \n+\n         (\n             imgs,\n             sensor2keyegos,\n             ego2globals,\n"
                },
                {
                    "date": 1716620943523,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -198,10 +198,10 @@\n         img_feats, _, _ = self.extract_feat(\n             points, img=img, img_metas=img_metas, **kwargs\n         )\n \n-        # base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n-        # feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n+        base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n+        feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n \n         bbox_list = [dict() for _ in range(len(img_metas))]\n         bbox_pts = self.simple_test_pts(img_feats, img_metas, rescale=rescale)\n         for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n"
                },
                {
                    "date": 1716621123089,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -198,10 +198,10 @@\n         img_feats, _, _ = self.extract_feat(\n             points, img=img, img_metas=img_metas, **kwargs\n         )\n \n-        base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n-        feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n+        # base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n+        # feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n \n         bbox_list = [dict() for _ in range(len(img_metas))]\n         bbox_pts = self.simple_test_pts(img_feats, img_metas, rescale=rescale)\n         for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n"
                },
                {
                    "date": 1716644656017,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,800 @@\n+# Copyright (c) Phigent Robotics. All rights reserved.\n+import torch\n+import torch.nn.functional as F\n+from mmcv.runner import force_fp32\n+\n+from mmdet3d.ops.bev_pool_v2.bev_pool import TRTBEVPoolv2\n+from mmdet.models import DETECTORS\n+from .. import builder\n+from .centerpoint import CenterPoint\n+from mmdet3d.models.utils.grid_mask import GridMask\n+from mmdet.models.backbones.resnet import ResNet\n+from termcolor import colored\n+from mmdet.utils import get_root_logger\n+from mmdet3d.models.utils.self_print import feats_to_img\n+\n+\n+@DETECTORS.register_module()\n+class BEVDet(CenterPoint):\n+    r\"\"\"BEVDet paradigm for multi-camera 3D object detection.\n+\n+    Please refer to the `paper <https://arxiv.org/abs/2112.11790>`_\n+\n+    Args:\n+        img_view_transformer (dict): Configuration dict of view transformer.\n+        img_bev_encoder_backbone (dict): Configuration dict of the BEV encoder\n+            backbone.\n+        img_bev_encoder_neck (dict): Configuration dict of the BEV encoder neck.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        img_view_transformer,\n+        img_bev_encoder_backbone=None,\n+        img_bev_encoder_neck=None,\n+        use_grid_mask=False,\n+        **kwargs,\n+    ):\n+        super(BEVDet, self).__init__(**kwargs)\n+        self.grid_mask = (\n+            None\n+            if not use_grid_mask\n+            else GridMask(\n+                True, True, rotate=1, offset=False, ratio=0.5, mode=1, prob=0.7\n+            )\n+        )\n+        self.img_view_transformer = builder.build_neck(img_view_transformer)\n+        if img_bev_encoder_neck and img_bev_encoder_backbone:\n+            self.img_bev_encoder_backbone = builder.build_backbone(\n+                img_bev_encoder_backbone\n+            )\n+            self.img_bev_encoder_neck = builder.build_neck(img_bev_encoder_neck)\n+\n+    def image_encoder(self, img, stereo=False):\n+        imgs = img\n+        B, N, C, imH, imW = imgs.shape\n+        imgs = imgs.view(B * N, C, imH, imW)\n+        if self.grid_mask is not None:\n+            imgs = self.grid_mask(imgs)\n+        x = self.img_backbone(imgs)\n+        stereo_feat = None\n+        if stereo:\n+            stereo_feat = x[0]\n+            x = x[1:]\n+        if self.with_img_neck:\n+            x = self.img_neck(x)\n+            if type(x) in [list, tuple]:\n+                x = x[0]\n+        _, output_dim, ouput_H, output_W = x.shape\n+        x = x.view(B, N, output_dim, ouput_H, output_W)\n+        return x, stereo_feat\n+\n+    @force_fp32()\n+    def bev_encoder(self, x):\n+        x = self.img_bev_encoder_backbone(x)\n+        x = self.img_bev_encoder_neck(x)\n+        if type(x) in [list, tuple]:\n+            x = x[0]\n+        return x\n+\n+    def prepare_inputs(self, inputs):\n+        # split the inputs into each frame\n+        assert len(inputs) == 7\n+        B, N, C, H, W = inputs[0].shape\n+        imgs, sensor2egos, ego2globals, intrins, post_rots, post_trans, bda = inputs\n+\n+        sensor2egos = sensor2egos.view(B, N, 4, 4)\n+        ego2globals = ego2globals.view(B, N, 4, 4)\n+\n+        # calculate the transformation from sweep sensor to key ego\n+        keyego2global = ego2globals[:, 0, ...].unsqueeze(1)\n+        global2keyego = torch.inverse(keyego2global.double())\n+        sensor2keyegos = global2keyego @ ego2globals.double() @ sensor2egos.double()\n+        sensor2keyegos = sensor2keyegos.float()\n+\n+        return [imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, bda]\n+\n+    def extract_img_feat(self, img, img_metas, **kwargs):\n+        \"\"\"Extract features of images.\"\"\"\n+        img = self.prepare_inputs(img)\n+        x, _ = self.image_encoder(img[0])\n+        x, depth = self.img_view_transformer([x] + img[1:7])\n+        x = self.bev_encoder(x)\n+        return [x], depth\n+\n+    def extract_feat(self, points, img, img_metas, **kwargs):\n+        \"\"\"Extract features from images and points.\"\"\"\n+        img_feats, depth = self.extract_img_feat(img, img_metas, **kwargs)\n+        pts_feats = None\n+        return (img_feats, pts_feats, depth)\n+\n+    def forward_train(\n+        self,\n+        points=None,\n+        img_metas=None,\n+        gt_bboxes_3d=None,\n+        gt_labels_3d=None,\n+        gt_labels=None,\n+        gt_bboxes=None,\n+        img_inputs=None,\n+        proposals=None,\n+        gt_bboxes_ignore=None,\n+        **kwargs,\n+    ):\n+        \"\"\"Forward training function.\n+\n+        Args:\n+            points (list[torch.Tensor], optional): Points of each sample.\n+                Defaults to None.\n+            img_metas (list[dict], optional): Meta information of each sample.\n+                Defaults to None.\n+            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`], optional):\n+                Ground truth 3D boxes. Defaults to None.\n+            gt_labels_3d (list[torch.Tensor], optional): Ground truth labels\n+                of 3D boxes. Defaults to None.\n+            gt_labels (list[torch.Tensor], optional): Ground truth labels\n+                of 2D boxes in images. Defaults to None.\n+            gt_bboxes (list[torch.Tensor], optional): Ground truth 2D boxes in\n+                images. Defaults to None.\n+            img (torch.Tensor optional): Images of each sample with shape\n+                (N, C, H, W). Defaults to None.\n+            proposals ([list[torch.Tensor], optional): Predicted proposals\n+                used for training Fast RCNN. Defaults to None.\n+            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n+                2D boxes in images to be ignored. Defaults to None.\n+\n+        Returns:\n+            dict: Losses of different branches.\n+        \"\"\"\n+        img_feats, pts_feats, _ = self.extract_feat(\n+            points, img=img_inputs, img_metas=img_metas, **kwargs\n+        )\n+        losses = dict()\n+        losses_pts = self.forward_pts_train(\n+            img_feats, gt_bboxes_3d, gt_labels_3d, img_metas, gt_bboxes_ignore\n+        )\n+        losses.update(losses_pts)\n+        return losses\n+\n+    def forward_test(self, points=None, img_metas=None, img_inputs=None, **kwargs):\n+        \"\"\"\n+        Args:\n+            points (list[torch.Tensor]): the outer list indicates test-time\n+                augmentations and inner torch.Tensor should have a shape NxC,\n+                which contains all points in the batch.\n+            img_metas (list[list[dict]]): the outer list indicates test-time\n+                augs (multiscale, flip, etc.) and the inner list indicates\n+                images in a batch\n+            img (list[torch.Tensor], optional): the outer\n+                list indicates test-time augmentations and inner\n+                torch.Tensor should have a shape NxCxHxW, which contains\n+                all images in the batch. Defaults to None.\n+        \"\"\"\n+        for var, name in [(img_inputs, \"img_inputs\"), (img_metas, \"img_metas\")]:\n+            if not isinstance(var, list):\n+                raise TypeError(\"{} must be a list, but got {}\".format(name, type(var)))\n+\n+        num_augs = len(img_inputs)\n+        if num_augs != len(img_metas):\n+            raise ValueError(\n+                \"num of augmentations ({}) != num of image meta ({})\".format(\n+                    len(img_inputs), len(img_metas)\n+                )\n+            )\n+\n+        if not isinstance(img_inputs[0][0], list):\n+            img_inputs = [img_inputs] if img_inputs is None else img_inputs\n+            points = [points] if points is None else points\n+            return self.simple_test(points[0], img_metas[0], img_inputs[0], **kwargs)\n+        else:\n+            return self.aug_test(None, img_metas[0], img_inputs[0], **kwargs)\n+\n+    def aug_test(self, points, img_metas, img=None, rescale=False):\n+        \"\"\"Test function without augmentaiton.\"\"\"\n+        assert False\n+\n+    def simple_test(self, points, img_metas, img=None, rescale=False, **kwargs):\n+        \"\"\"Test function without augmentaiton.\"\"\"\n+        img_feats, _, _ = self.extract_feat(\n+            points, img=img, img_metas=img_metas, **kwargs\n+        )\n+\n+        # base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n+        # feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n+\n+        bbox_list = [dict() for _ in range(len(img_metas))]\n+        bbox_pts = self.simple_test_pts(img_feats, img_metas, rescale=rescale)\n+        for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n+            result_dict[\"pts_bbox\"] = pts_bbox\n+        return bbox_list\n+\n+    def forward_dummy(self, points=None, img_metas=None, img_inputs=None, **kwargs):\n+        img_feats, _, _ = self.extract_feat(\n+            points, img=img_inputs, img_metas=img_metas, **kwargs\n+        )\n+        assert self.with_pts_bbox\n+        outs = self.pts_bbox_head(img_feats)\n+        return outs\n+\n+\n+@DETECTORS.register_module()\n+class BEVDetTRT(BEVDet):\n+\n+    def result_serialize(self, outs):\n+        outs_ = []\n+        for out in outs:\n+            for key in [\"reg\", \"height\", \"dim\", \"rot\", \"vel\", \"heatmap\"]:\n+                outs_.append(out[0][key])\n+        return outs_\n+\n+    def result_deserialize(self, outs):\n+        outs_ = []\n+        keys = [\"reg\", \"height\", \"dim\", \"rot\", \"vel\", \"heatmap\"]\n+        for head_id in range(len(outs) // 6):\n+            outs_head = [dict()]\n+            for kid, key in enumerate(keys):\n+                outs_head[0][key] = outs[head_id * 6 + kid]\n+            outs_.append(outs_head)\n+        return outs_\n+\n+    def forward(\n+        self,\n+        img,\n+        ranks_depth,\n+        ranks_feat,\n+        ranks_bev,\n+        interval_starts,\n+        interval_lengths,\n+    ):\n+        x = self.img_backbone(img)\n+        x = self.img_neck(x)\n+        x = self.img_view_transformer.depth_net(x)\n+        depth = x[:, : self.img_view_transformer.D].softmax(dim=1)\n+        tran_feat = x[\n+            :,\n+            self.img_view_transformer.D : (\n+                self.img_view_transformer.D + self.img_view_transformer.out_channels\n+            ),\n+        ]\n+        tran_feat = tran_feat.permute(0, 2, 3, 1)\n+        x = TRTBEVPoolv2.apply(\n+            depth.contiguous(),\n+            tran_feat.contiguous(),\n+            ranks_depth,\n+            ranks_feat,\n+            ranks_bev,\n+            interval_starts,\n+            interval_lengths,\n+        )\n+        x = x.permute(0, 3, 1, 2).contiguous()\n+        bev_feat = self.bev_encoder(x)\n+        outs = self.pts_bbox_head([bev_feat])\n+        outs = self.result_serialize(outs)\n+        return outs\n+\n+    def get_bev_pool_input(self, input):\n+        input = self.prepare_inputs(input)\n+        coor = self.img_view_transformer.get_lidar_coor(*input[1:7])\n+        return self.img_view_transformer.voxel_pooling_prepare_v2(coor)\n+\n+\n+@DETECTORS.register_module()\n+class BEVDet4D(BEVDet):\n+    r\"\"\"BEVDet4D paradigm for multi-camera 3D object detection.\n+\n+    Please refer to the `paper <https://arxiv.org/abs/2203.17054>`_\n+\n+    Args:\n+        pre_process (dict | None): Configuration dict of BEV pre-process net.\n+        align_after_view_transfromation (bool): Whether to align the BEV\n+            Feature after view transformation. By default, the BEV feature of\n+            the previous frame is aligned during the view transformation.\n+        num_adj (int): Number of adjacent frames.\n+        with_prev (bool): Whether to set the BEV feature of previous frame as\n+            all zero. By default, False.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        pre_process=None,\n+        align_after_view_transfromation=False,\n+        num_adj=1,\n+        with_prev=True,\n+        **kwargs,\n+    ):\n+        super(BEVDet4D, self).__init__(**kwargs)\n+        self.pre_process = pre_process is not None\n+        if self.pre_process:\n+            self.pre_process_net = builder.build_backbone(pre_process)\n+        self.align_after_view_transfromation = align_after_view_transfromation\n+        self.num_frame = num_adj + 1\n+\n+        self.with_prev = with_prev\n+        self.grid = None\n+\n+    def gen_grid(self, input, sensor2keyegos, bda, bda_adj=None):\n+        n, c, h, w = input.shape\n+        _, v, _, _ = sensor2keyegos[0].shape\n+        if self.grid is None:\n+            # generate grid\n+            xs = (\n+                torch.linspace(0, w - 1, w, dtype=input.dtype, device=input.device)\n+                .view(1, w)\n+                .expand(h, w)\n+            )\n+            ys = (\n+                torch.linspace(0, h - 1, h, dtype=input.dtype, device=input.device)\n+                .view(h, 1)\n+                .expand(h, w)\n+            )\n+            grid = torch.stack((xs, ys, torch.ones_like(xs)), -1)\n+            self.grid = grid\n+        else:\n+            grid = self.grid\n+        grid = grid.view(1, h, w, 3).expand(n, h, w, 3).view(n, h, w, 3, 1)\n+\n+        # get transformation from current ego frame to adjacent ego frame\n+        # transformation from current camera frame to current ego frame\n+        c02l0 = sensor2keyegos[0][:, 0:1, :, :]\n+\n+        # transformation from adjacent camera frame to current ego frame\n+        c12l0 = sensor2keyegos[1][:, 0:1, :, :]\n+\n+        # add bev data augmentation\n+        bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n+        bda_[:, :, :3, :3] = bda.unsqueeze(1)[\n+            :, :, :3, :3\n+        ]  # change for diff, before change is \"bda.unsqueeze(1)\"\n+        bda_[:, :, 3, 3] = 1\n+        c02l0 = bda_.matmul(c02l0)\n+        if bda_adj is not None:\n+            bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n+            bda_[:, :, :3, :3] = bda_adj.unsqueeze(1)\n+            bda_[:, :, 3, 3] = 1\n+        c12l0 = bda_.matmul(c12l0)\n+\n+        # transformation from current ego frame to adjacent ego frame\n+        l02l1 = c02l0.matmul(torch.inverse(c12l0))[:, 0, :, :].view(n, 1, 1, 4, 4)\n+        \"\"\"\n+          c02l0 * inv(c12l0)\n+        = c02l0 * inv(l12l0 * c12l1)\n+        = c02l0 * inv(c12l1) * inv(l12l0)\n+        = l02l1 # c02l0==c12l1\n+        \"\"\"\n+\n+        l02l1 = l02l1[:, :, :, [True, True, False, True], :][\n+            :, :, :, :, [True, True, False, True]\n+        ]\n+\n+        feat2bev = torch.zeros((3, 3), dtype=grid.dtype).to(grid)\n+        feat2bev[0, 0] = self.img_view_transformer.grid_interval[0]\n+        feat2bev[1, 1] = self.img_view_transformer.grid_interval[1]\n+        feat2bev[0, 2] = self.img_view_transformer.grid_lower_bound[0]\n+        feat2bev[1, 2] = self.img_view_transformer.grid_lower_bound[1]\n+        feat2bev[2, 2] = 1\n+        feat2bev = feat2bev.view(1, 3, 3)\n+        tf = torch.inverse(feat2bev).matmul(l02l1).matmul(feat2bev)\n+\n+        # transform and normalize\n+        grid = tf.matmul(grid)\n+        normalize_factor = torch.tensor(\n+            [w - 1.0, h - 1.0], dtype=input.dtype, device=input.device\n+        )\n+        grid = grid[:, :, :, :2, 0] / normalize_factor.view(1, 1, 1, 2) * 2.0 - 1.0\n+        return grid\n+\n+    @force_fp32()\n+    def shift_feature(self, input, sensor2keyegos, bda, bda_adj=None):\n+        grid = self.gen_grid(input, sensor2keyegos, bda, bda_adj=bda_adj)\n+        output = F.grid_sample(input, grid.to(input.dtype), align_corners=True)\n+        return output\n+\n+    def prepare_bev_feat(\n+        self, img, rot, tran, intrin, post_rot, post_tran, bda, mlp_input\n+    ):\n+        x, _ = self.image_encoder(img)\n+        bev_feat, depth = self.img_view_transformer(\n+            [x, rot, tran, intrin, post_rot, post_tran, bda, mlp_input]\n+        )\n+        if self.pre_process:\n+            bev_feat = self.pre_process_net(bev_feat)[0]\n+        return bev_feat, depth\n+\n+    def extract_img_feat_sequential(self, inputs, feat_prev):\n+        imgs, sensor2keyegos_curr, ego2globals_curr, intrins = inputs[:4]\n+        sensor2keyegos_prev, _, post_rots, post_trans, bda = inputs[4:]\n+        bev_feat_list = []\n+        mlp_input = self.img_view_transformer.get_mlp_input(\n+            sensor2keyegos_curr[0:1, ...],\n+            ego2globals_curr[0:1, ...],\n+            intrins,\n+            post_rots,\n+            post_trans,\n+            bda[0:1, ...],\n+        )\n+        inputs_curr = (\n+            imgs,\n+            sensor2keyegos_curr[0:1, ...],\n+            ego2globals_curr[0:1, ...],\n+            intrins,\n+            post_rots,\n+            post_trans,\n+            bda[0:1, ...],\n+            mlp_input,\n+        )\n+        bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n+        bev_feat_list.append(bev_feat)\n+\n+        # align the feat_prev\n+        _, C, H, W = feat_prev.shape\n+        feat_prev = self.shift_feature(\n+            feat_prev, [sensor2keyegos_curr, sensor2keyegos_prev], bda\n+        )\n+        bev_feat_list.append(feat_prev.view(1, (self.num_frame - 1) * C, H, W))\n+\n+        bev_feat = torch.cat(bev_feat_list, dim=1)\n+        x = self.bev_encoder(bev_feat)\n+        return [x], depth\n+\n+    def prepare_inputs(self, inputs, stereo=False):\n+        # split the inputs into each frame\n+        B, N, C, H, W = inputs[0].shape\n+        N = N // self.num_frame\n+        imgs = inputs[0].view(B, N, self.num_frame, C, H, W)\n+        imgs = torch.split(imgs, 1, 2)\n+        imgs = [t.squeeze(2) for t in imgs]\n+        sensor2egos, ego2globals, intrins, post_rots, post_trans, bda = inputs[1:7]\n+\n+        sensor2egos = sensor2egos.view(B, self.num_frame, N, 4, 4)\n+        ego2globals = ego2globals.view(B, self.num_frame, N, 4, 4)\n+\n+        # calculate the transformation from sweep sensor to key ego\n+        keyego2global = ego2globals[:, 0, 0, ...].unsqueeze(1).unsqueeze(1)\n+        global2keyego = torch.inverse(keyego2global.double())\n+        sensor2keyegos = global2keyego @ ego2globals.double() @ sensor2egos.double()\n+        sensor2keyegos = sensor2keyegos.float()\n+\n+        curr2adjsensor = None\n+        if stereo:\n+            sensor2egos_cv, ego2globals_cv = sensor2egos, ego2globals\n+            sensor2egos_curr = sensor2egos_cv[:, : self.temporal_frame, ...].double()\n+            ego2globals_curr = ego2globals_cv[:, : self.temporal_frame, ...].double()\n+            sensor2egos_adj = sensor2egos_cv[\n+                :, 1 : self.temporal_frame + 1, ...\n+            ].double()\n+            ego2globals_adj = ego2globals_cv[\n+                :, 1 : self.temporal_frame + 1, ...\n+            ].double()\n+            curr2adjsensor = (\n+                torch.inverse(ego2globals_adj @ sensor2egos_adj)\n+                @ ego2globals_curr\n+                @ sensor2egos_curr\n+            )\n+            curr2adjsensor = curr2adjsensor.float()\n+            curr2adjsensor = torch.split(curr2adjsensor, 1, 1)\n+            curr2adjsensor = [p.squeeze(1) for p in curr2adjsensor]\n+            curr2adjsensor.extend([None for _ in range(self.extra_ref_frames)])\n+            assert len(curr2adjsensor) == self.num_frame\n+\n+        extra = [\n+            sensor2keyegos,\n+            ego2globals,\n+            intrins.view(B, self.num_frame, N, 3, 3),\n+            post_rots.view(B, self.num_frame, N, 3, 3),\n+            post_trans.view(B, self.num_frame, N, 3),\n+        ]\n+        extra = [torch.split(t, 1, 1) for t in extra]\n+        extra = [[p.squeeze(1) for p in t] for t in extra]\n+        sensor2keyegos, ego2globals, intrins, post_rots, post_trans = extra\n+        return (\n+            imgs,\n+            sensor2keyegos,\n+            ego2globals,\n+            intrins,\n+            post_rots,\n+            post_trans,\n+            bda,\n+            curr2adjsensor,\n+        )\n+\n+    def extract_img_feat(\n+        self, img, img_metas, pred_prev=False, sequential=False, **kwargs\n+    ):\n+        if sequential:\n+            return self.extract_img_feat_sequential(img, kwargs[\"feat_prev\"])\n+        imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, bda, _ = (\n+            self.prepare_inputs(img)\n+        )\n+        \"\"\"Extract features of images.\"\"\"\n+        bev_feat_list = []\n+        depth_list = []\n+        key_frame = True  # back propagation for key frame only\n+        for img, sensor2keyego, ego2global, intrin, post_rot, post_tran in zip(\n+            imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans\n+        ):\n+            if key_frame or self.with_prev:\n+                if self.align_after_view_transfromation:\n+                    sensor2keyego, ego2global = sensor2keyegos[0], ego2globals[0]\n+                mlp_input = self.img_view_transformer.get_mlp_input(\n+                    sensor2keyegos[0], ego2globals[0], intrin, post_rot, post_tran, bda\n+                )\n+                inputs_curr = (\n+                    img,\n+                    sensor2keyego,\n+                    ego2global,\n+                    intrin,\n+                    post_rot,\n+                    post_tran,\n+                    bda,\n+                    mlp_input,\n+                )\n+                if key_frame:\n+                    bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n+                else:\n+                    with torch.no_grad():\n+                        bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n+            else:\n+                bev_feat = torch.zeros_like(bev_feat_list[0])\n+                depth = None\n+            bev_feat_list.append(bev_feat)\n+            depth_list.append(depth)\n+            key_frame = False\n+        if pred_prev:\n+            assert self.align_after_view_transfromation\n+            assert sensor2keyegos[0].shape[0] == 1\n+            feat_prev = torch.cat(bev_feat_list[1:], dim=0)\n+            ego2globals_curr = ego2globals[0].repeat(self.num_frame - 1, 1, 1, 1)\n+            sensor2keyegos_curr = sensor2keyegos[0].repeat(self.num_frame - 1, 1, 1, 1)\n+            ego2globals_prev = torch.cat(ego2globals[1:], dim=0)\n+            sensor2keyegos_prev = torch.cat(sensor2keyegos[1:], dim=0)\n+            bda_curr = bda.repeat(self.num_frame - 1, 1, 1)\n+            return feat_prev, [\n+                imgs[0],\n+                sensor2keyegos_curr,\n+                ego2globals_curr,\n+                intrins[0],\n+                sensor2keyegos_prev,\n+                ego2globals_prev,\n+                post_rots[0],\n+                post_trans[0],\n+                bda_curr,\n+            ]\n+        if self.align_after_view_transfromation:\n+            for adj_id in range(1, self.num_frame):\n+                bev_feat_list[adj_id] = self.shift_feature(\n+                    bev_feat_list[adj_id],\n+                    [sensor2keyegos[0], sensor2keyegos[adj_id]],\n+                    bda,\n+                )\n+        bev_feat = torch.cat(bev_feat_list, dim=1)\n+        x = self.bev_encoder(bev_feat)\n+        return [x], depth_list[0]\n+\n+\n+@DETECTORS.register_module()\n+class BEVDepth4D(BEVDet4D):\n+\n+    def forward_train(\n+        self,\n+        points=None,\n+        img_metas=None,\n+        gt_bboxes_3d=None,\n+        gt_labels_3d=None,\n+        gt_labels=None,\n+        gt_bboxes=None,\n+        img_inputs=None,\n+        proposals=None,\n+        gt_bboxes_ignore=None,\n+        **kwargs,\n+    ):\n+        \"\"\"Forward training function.\n+\n+        Args:\n+            points (list[torch.Tensor], optional): Points of each sample.\n+                Defaults to None.\n+            img_metas (list[dict], optional): Meta information of each sample.\n+                Defaults to None.\n+            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`], optional):\n+                Ground truth 3D boxes. Defaults to None.\n+            gt_labels_3d (list[torch.Tensor], optional): Ground truth labels\n+                of 3D boxes. Defaults to None.\n+            gt_labels (list[torch.Tensor], optional): Ground truth labels\n+                of 2D boxes in images. Defaults to None.\n+            gt_bboxes (list[torch.Tensor], optional): Ground truth 2D boxes in\n+                images. Defaults to None.\n+            img (torch.Tensor optional): Images of each sample with shape\n+                (N, C, H, W). Defaults to None.\n+            proposals ([list[torch.Tensor], optional): Predicted proposals\n+                used for training Fast RCNN. Defaults to None.\n+            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n+                2D boxes in images to be ignored. Defaults to None.\n+\n+        Returns:\n+            dict: Losses of different branches.\n+        \"\"\"\n+        img_feats, pts_feats, depth = self.extract_feat(\n+            points, img=img_inputs, img_metas=img_metas, **kwargs\n+        )\n+\n+        gt_depth = kwargs[\"gt_depth\"]\n+        loss_depth = self.img_view_transformer.get_depth_loss(gt_depth, depth)\n+        losses = dict(loss_depth=loss_depth)\n+        losses_pts = self.forward_pts_train(\n+            img_feats, gt_bboxes_3d, gt_labels_3d, img_metas, gt_bboxes_ignore\n+        )\n+        losses.update(losses_pts)\n+        return losses\n+\n+\n+@DETECTORS.register_module()\n+class BEVStereo4D(BEVDepth4D):\n+    def __init__(self, **kwargs):\n+        super(BEVStereo4D, self).__init__(**kwargs)\n+        self.extra_ref_frames = 1\n+        self.temporal_frame = self.num_frame\n+        self.num_frame += self.extra_ref_frames\n+\n+    def extract_stereo_ref_feat(self, x):\n+        B, N, C, imH, imW = x.shape\n+        x = x.view(B * N, C, imH, imW)\n+        if isinstance(self.img_backbone, ResNet):\n+            if self.img_backbone.deep_stem:\n+                x = self.img_backbone.stem(x)\n+            else:\n+                x = self.img_backbone.conv1(x)\n+                x = self.img_backbone.norm1(x)\n+                x = self.img_backbone.relu(x)\n+            x = self.img_backbone.maxpool(x)\n+            for i, layer_name in enumerate(self.img_backbone.res_layers):\n+                res_layer = getattr(self.img_backbone, layer_name)\n+                x = res_layer(x)\n+                return x\n+        else:\n+            x = self.img_backbone.patch_embed(x)\n+            hw_shape = (\n+                self.img_backbone.patch_embed.DH,\n+                self.img_backbone.patch_embed.DW,\n+            )\n+            if self.img_backbone.use_abs_pos_embed:\n+                x = x + self.img_backbone.absolute_pos_embed\n+            x = self.img_backbone.drop_after_pos(x)\n+\n+            for i, stage in enumerate(self.img_backbone.stages):\n+                x, hw_shape, out, out_hw_shape = stage(x, hw_shape)\n+                out = out.view(-1, *out_hw_shape, self.img_backbone.num_features[i])\n+                out = out.permute(0, 3, 1, 2).contiguous()\n+                return out\n+\n+    def prepare_bev_feat(\n+        self,\n+        img,\n+        sensor2keyego,\n+        ego2global,\n+        intrin,\n+        post_rot,\n+        post_tran,\n+        bda,\n+        mlp_input,\n+        feat_prev_iv,\n+        k2s_sensor,\n+        extra_ref_frame,\n+    ):\n+        if extra_ref_frame:\n+            stereo_feat = self.extract_stereo_ref_feat(img)\n+            return None, None, stereo_feat\n+\n+        x, stereo_feat = self.image_encoder(img, stereo=True)\n+        metas = dict(\n+            k2s_sensor=k2s_sensor,\n+            intrins=intrin,\n+            post_rots=post_rot,\n+            post_trans=post_tran,\n+            frustum=self.img_view_transformer.cv_frustum.to(x),\n+            cv_downsample=4,\n+            downsample=self.img_view_transformer.downsample,\n+            grid_config=self.img_view_transformer.grid_config,\n+            cv_feat_list=[feat_prev_iv, stereo_feat],\n+        )\n+        bev_feat, depth = self.img_view_transformer(\n+            [x, sensor2keyego, ego2global, intrin, post_rot, post_tran, bda, mlp_input],\n+            metas,\n+        )\n+        if self.pre_process:\n+            bev_feat = self.pre_process_net(bev_feat)[0]\n+        return bev_feat, depth, stereo_feat\n+\n+    def extract_img_feat(\n+        self, img, img_metas, pred_prev=False, sequential=False, **kwargs\n+    ):\n+\n+        if sequential:\n+            # Todo\n+            assert False\n+\n+        (\n+            imgs,\n+            sensor2keyegos,\n+            ego2globals,\n+            intrins,\n+            post_rots,\n+            post_trans,\n+            bda,\n+            curr2adjsensor,\n+        ) = self.prepare_inputs(img, stereo=True)\n+\n+        \"\"\"Extract features of images.\"\"\"\n+        bev_feat_list = []\n+        depth_key_frame = None\n+        feat_prev_iv = None\n+        for fid in range(self.num_frame - 1, -1, -1):\n+            img, sensor2keyego, ego2global, intrin, post_rot, post_tran = (\n+                imgs[fid],\n+                sensor2keyegos[fid],\n+                ego2globals[fid],\n+                intrins[fid],\n+                post_rots[fid],\n+                post_trans[fid],\n+            )\n+            key_frame = fid == 0\n+            extra_ref_frame = fid == self.num_frame - self.extra_ref_frames\n+            if key_frame or self.with_prev:\n+                if self.align_after_view_transfromation:\n+                    sensor2keyego, ego2global = sensor2keyegos[0], ego2globals[0]\n+                mlp_input = self.img_view_transformer.get_mlp_input(\n+                    sensor2keyegos[0], ego2globals[0], intrin, post_rot, post_tran, bda\n+                )\n+                inputs_curr = (\n+                    img,\n+                    sensor2keyego,\n+                    ego2global,\n+                    intrin,\n+                    post_rot,\n+                    post_tran,\n+                    bda,\n+                    mlp_input,\n+                    feat_prev_iv,\n+                    curr2adjsensor[fid],\n+                    extra_ref_frame,\n+                )\n+                if key_frame:\n+                    bev_feat, depth, feat_curr_iv = self.prepare_bev_feat(*inputs_curr)\n+                    depth_key_frame = depth\n+                else:\n+                    with torch.no_grad():\n+                        bev_feat, depth, feat_curr_iv = self.prepare_bev_feat(\n+                            *inputs_curr\n+                        )\n+                if not extra_ref_frame:\n+                    bev_feat_list.append(bev_feat)\n+                feat_prev_iv = feat_curr_iv\n+        if pred_prev:\n+            # Todo\n+            assert False\n+        if not self.with_prev:\n+            bev_feat_key = bev_feat_list[0]\n+            if len(bev_feat_key.shape) == 4:\n+                b, c, h, w = bev_feat_key.shape\n+                bev_feat_list = [\n+                    torch.zeros(\n+                        [b, c * (self.num_frame - self.extra_ref_frames - 1), h, w]\n+                    ).to(bev_feat_key),\n+                    bev_feat_key,\n+                ]\n+            else:\n+                b, c, z, h, w = bev_feat_key.shape\n+                bev_feat_list = [\n+                    torch.zeros(\n+                        [b, c * (self.num_frame - self.extra_ref_frames - 1), z, h, w]\n+                    ).to(bev_feat_key),\n+                    bev_feat_key,\n+                ]\n+        if self.align_after_view_transfromation:\n+            for adj_id in range(self.num_frame - 2):\n+                bev_feat_list[adj_id] = self.shift_feature(\n+                    bev_feat_list[adj_id],\n+                    [sensor2keyegos[0], sensor2keyegos[self.num_frame - 2 - adj_id]],\n+                    bda,\n+                )\n+        bev_feat = torch.cat(bev_feat_list, dim=1)\n+        x = self.bev_encoder(bev_feat)\n+        return [x], depth_key_frame\n"
                },
                {
                    "date": 1716644688634,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -797,804 +797,4 @@\n                 )\n         bev_feat = torch.cat(bev_feat_list, dim=1)\n         x = self.bev_encoder(bev_feat)\n         return [x], depth_key_frame\n-# Copyright (c) Phigent Robotics. All rights reserved.\n-import torch\n-import torch.nn.functional as F\n-from mmcv.runner import force_fp32\n-\n-from mmdet3d.ops.bev_pool_v2.bev_pool import TRTBEVPoolv2\n-from mmdet.models import DETECTORS\n-from .. import builder\n-from .centerpoint import CenterPoint\n-from mmdet3d.models.utils.grid_mask import GridMask\n-from mmdet.models.backbones.resnet import ResNet\n-from termcolor import colored\n-from mmdet.utils import get_root_logger\n-from mmdet3d.models.utils.self_print import feats_to_img\n-\n-\n-@DETECTORS.register_module()\n-class BEVDet(CenterPoint):\n-    r\"\"\"BEVDet paradigm for multi-camera 3D object detection.\n-\n-    Please refer to the `paper <https://arxiv.org/abs/2112.11790>`_\n-\n-    Args:\n-        img_view_transformer (dict): Configuration dict of view transformer.\n-        img_bev_encoder_backbone (dict): Configuration dict of the BEV encoder\n-            backbone.\n-        img_bev_encoder_neck (dict): Configuration dict of the BEV encoder neck.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        img_view_transformer,\n-        img_bev_encoder_backbone=None,\n-        img_bev_encoder_neck=None,\n-        use_grid_mask=False,\n-        **kwargs,\n-    ):\n-        super(BEVDet, self).__init__(**kwargs)\n-        self.grid_mask = (\n-            None\n-            if not use_grid_mask\n-            else GridMask(\n-                True, True, rotate=1, offset=False, ratio=0.5, mode=1, prob=0.7\n-            )\n-        )\n-        self.img_view_transformer = builder.build_neck(img_view_transformer)\n-        if img_bev_encoder_neck and img_bev_encoder_backbone:\n-            self.img_bev_encoder_backbone = builder.build_backbone(\n-                img_bev_encoder_backbone\n-            )\n-            self.img_bev_encoder_neck = builder.build_neck(img_bev_encoder_neck)\n-\n-    def image_encoder(self, img, stereo=False):\n-        imgs = img\n-        B, N, C, imH, imW = imgs.shape\n-        imgs = imgs.view(B * N, C, imH, imW)\n-        if self.grid_mask is not None:\n-            imgs = self.grid_mask(imgs)\n-        x = self.img_backbone(imgs)\n-        stereo_feat = None\n-        if stereo:\n-            stereo_feat = x[0]\n-            x = x[1:]\n-        if self.with_img_neck:\n-            x = self.img_neck(x)\n-            if type(x) in [list, tuple]:\n-                x = x[0]\n-        _, output_dim, ouput_H, output_W = x.shape\n-        x = x.view(B, N, output_dim, ouput_H, output_W)\n-        return x, stereo_feat\n-\n-    @force_fp32()\n-    def bev_encoder(self, x):\n-        x = self.img_bev_encoder_backbone(x)\n-        x = self.img_bev_encoder_neck(x)\n-        if type(x) in [list, tuple]:\n-            x = x[0]\n-        return x\n-\n-    def prepare_inputs(self, inputs):\n-        # split the inputs into each frame\n-        assert len(inputs) == 7\n-        B, N, C, H, W = inputs[0].shape\n-        imgs, sensor2egos, ego2globals, intrins, post_rots, post_trans, bda = inputs\n-\n-        sensor2egos = sensor2egos.view(B, N, 4, 4)\n-        ego2globals = ego2globals.view(B, N, 4, 4)\n-\n-        # calculate the transformation from sweep sensor to key ego\n-        keyego2global = ego2globals[:, 0, ...].unsqueeze(1)\n-        global2keyego = torch.inverse(keyego2global.double())\n-        sensor2keyegos = global2keyego @ ego2globals.double() @ sensor2egos.double()\n-        sensor2keyegos = sensor2keyegos.float()\n-\n-        return [imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, bda]\n-\n-    def extract_img_feat(self, img, img_metas, **kwargs):\n-        \"\"\"Extract features of images.\"\"\"\n-        img = self.prepare_inputs(img)\n-        x, _ = self.image_encoder(img[0])\n-        x, depth = self.img_view_transformer([x] + img[1:7])\n-        x = self.bev_encoder(x)\n-        return [x], depth\n-\n-    def extract_feat(self, points, img, img_metas, **kwargs):\n-        \"\"\"Extract features from images and points.\"\"\"\n-        img_feats, depth = self.extract_img_feat(img, img_metas, **kwargs)\n-        pts_feats = None\n-        return (img_feats, pts_feats, depth)\n-\n-    def forward_train(\n-        self,\n-        points=None,\n-        img_metas=None,\n-        gt_bboxes_3d=None,\n-        gt_labels_3d=None,\n-        gt_labels=None,\n-        gt_bboxes=None,\n-        img_inputs=None,\n-        proposals=None,\n-        gt_bboxes_ignore=None,\n-        **kwargs,\n-    ):\n-        \"\"\"Forward training function.\n-\n-        Args:\n-            points (list[torch.Tensor], optional): Points of each sample.\n-                Defaults to None.\n-            img_metas (list[dict], optional): Meta information of each sample.\n-                Defaults to None.\n-            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`], optional):\n-                Ground truth 3D boxes. Defaults to None.\n-            gt_labels_3d (list[torch.Tensor], optional): Ground truth labels\n-                of 3D boxes. Defaults to None.\n-            gt_labels (list[torch.Tensor], optional): Ground truth labels\n-                of 2D boxes in images. Defaults to None.\n-            gt_bboxes (list[torch.Tensor], optional): Ground truth 2D boxes in\n-                images. Defaults to None.\n-            img (torch.Tensor optional): Images of each sample with shape\n-                (N, C, H, W). Defaults to None.\n-            proposals ([list[torch.Tensor], optional): Predicted proposals\n-                used for training Fast RCNN. Defaults to None.\n-            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n-                2D boxes in images to be ignored. Defaults to None.\n-\n-        Returns:\n-            dict: Losses of different branches.\n-        \"\"\"\n-        img_feats, pts_feats, _ = self.extract_feat(\n-            points, img=img_inputs, img_metas=img_metas, **kwargs\n-        )\n-        losses = dict()\n-        losses_pts = self.forward_pts_train(\n-            img_feats, gt_bboxes_3d, gt_labels_3d, img_metas, gt_bboxes_ignore\n-        )\n-        losses.update(losses_pts)\n-        return losses\n-\n-    def forward_test(self, points=None, img_metas=None, img_inputs=None, **kwargs):\n-        \"\"\"\n-        Args:\n-            points (list[torch.Tensor]): the outer list indicates test-time\n-                augmentations and inner torch.Tensor should have a shape NxC,\n-                which contains all points in the batch.\n-            img_metas (list[list[dict]]): the outer list indicates test-time\n-                augs (multiscale, flip, etc.) and the inner list indicates\n-                images in a batch\n-            img (list[torch.Tensor], optional): the outer\n-                list indicates test-time augmentations and inner\n-                torch.Tensor should have a shape NxCxHxW, which contains\n-                all images in the batch. Defaults to None.\n-        \"\"\"\n-        for var, name in [(img_inputs, \"img_inputs\"), (img_metas, \"img_metas\")]:\n-            if not isinstance(var, list):\n-                raise TypeError(\"{} must be a list, but got {}\".format(name, type(var)))\n-\n-        num_augs = len(img_inputs)\n-        if num_augs != len(img_metas):\n-            raise ValueError(\n-                \"num of augmentations ({}) != num of image meta ({})\".format(\n-                    len(img_inputs), len(img_metas)\n-                )\n-            )\n-\n-        if not isinstance(img_inputs[0][0], list):\n-            img_inputs = [img_inputs] if img_inputs is None else img_inputs\n-            points = [points] if points is None else points\n-            return self.simple_test(points[0], img_metas[0], img_inputs[0], **kwargs)\n-        else:\n-            return self.aug_test(None, img_metas[0], img_inputs[0], **kwargs)\n-\n-    def aug_test(self, points, img_metas, img=None, rescale=False):\n-        \"\"\"Test function without augmentaiton.\"\"\"\n-        assert False\n-\n-    def simple_test(self, points, img_metas, img=None, rescale=False, **kwargs):\n-        \"\"\"Test function without augmentaiton.\"\"\"\n-        img_feats, _, _ = self.extract_feat(\n-            points, img=img, img_metas=img_metas, **kwargs\n-        )\n-\n-        # base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n-        # feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n-\n-        bbox_list = [dict() for _ in range(len(img_metas))]\n-        bbox_pts = self.simple_test_pts(img_feats, img_metas, rescale=rescale)\n-        for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n-            result_dict[\"pts_bbox\"] = pts_bbox\n-        return bbox_list\n-\n-    def forward_dummy(self, points=None, img_metas=None, img_inputs=None, **kwargs):\n-        img_feats, _, _ = self.extract_feat(\n-            points, img=img_inputs, img_metas=img_metas, **kwargs\n-        )\n-        assert self.with_pts_bbox\n-        outs = self.pts_bbox_head(img_feats)\n-        return outs\n-\n-\n-@DETECTORS.register_module()\n-class BEVDetTRT(BEVDet):\n-\n-    def result_serialize(self, outs):\n-        outs_ = []\n-        for out in outs:\n-            for key in [\"reg\", \"height\", \"dim\", \"rot\", \"vel\", \"heatmap\"]:\n-                outs_.append(out[0][key])\n-        return outs_\n-\n-    def result_deserialize(self, outs):\n-        outs_ = []\n-        keys = [\"reg\", \"height\", \"dim\", \"rot\", \"vel\", \"heatmap\"]\n-        for head_id in range(len(outs) // 6):\n-            outs_head = [dict()]\n-            for kid, key in enumerate(keys):\n-                outs_head[0][key] = outs[head_id * 6 + kid]\n-            outs_.append(outs_head)\n-        return outs_\n-\n-    def forward(\n-        self,\n-        img,\n-        ranks_depth,\n-        ranks_feat,\n-        ranks_bev,\n-        interval_starts,\n-        interval_lengths,\n-    ):\n-        x = self.img_backbone(img)\n-        x = self.img_neck(x)\n-        x = self.img_view_transformer.depth_net(x)\n-        depth = x[:, : self.img_view_transformer.D].softmax(dim=1)\n-        tran_feat = x[\n-            :,\n-            self.img_view_transformer.D : (\n-                self.img_view_transformer.D + self.img_view_transformer.out_channels\n-            ),\n-        ]\n-        tran_feat = tran_feat.permute(0, 2, 3, 1)\n-        x = TRTBEVPoolv2.apply(\n-            depth.contiguous(),\n-            tran_feat.contiguous(),\n-            ranks_depth,\n-            ranks_feat,\n-            ranks_bev,\n-            interval_starts,\n-            interval_lengths,\n-        )\n-        x = x.permute(0, 3, 1, 2).contiguous()\n-        bev_feat = self.bev_encoder(x)\n-        outs = self.pts_bbox_head([bev_feat])\n-        outs = self.result_serialize(outs)\n-        return outs\n-\n-    def get_bev_pool_input(self, input):\n-        input = self.prepare_inputs(input)\n-        coor = self.img_view_transformer.get_lidar_coor(*input[1:7])\n-        return self.img_view_transformer.voxel_pooling_prepare_v2(coor)\n-\n-\n-@DETECTORS.register_module()\n-class BEVDet4D(BEVDet):\n-    r\"\"\"BEVDet4D paradigm for multi-camera 3D object detection.\n-\n-    Please refer to the `paper <https://arxiv.org/abs/2203.17054>`_\n-\n-    Args:\n-        pre_process (dict | None): Configuration dict of BEV pre-process net.\n-        align_after_view_transfromation (bool): Whether to align the BEV\n-            Feature after view transformation. By default, the BEV feature of\n-            the previous frame is aligned during the view transformation.\n-        num_adj (int): Number of adjacent frames.\n-        with_prev (bool): Whether to set the BEV feature of previous frame as\n-            all zero. By default, False.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        pre_process=None,\n-        align_after_view_transfromation=False,\n-        num_adj=1,\n-        with_prev=True,\n-        **kwargs,\n-    ):\n-        super(BEVDet4D, self).__init__(**kwargs)\n-        self.pre_process = pre_process is not None\n-        if self.pre_process:\n-            self.pre_process_net = builder.build_backbone(pre_process)\n-        self.align_after_view_transfromation = align_after_view_transfromation\n-        self.num_frame = num_adj + 1\n-\n-        self.with_prev = with_prev\n-        self.grid = None\n-\n-    def gen_grid(self, input, sensor2keyegos, bda, bda_adj=None):\n-        n, c, h, w = input.shape\n-        _, v, _, _ = sensor2keyegos[0].shape\n-        if self.grid is None:\n-            # generate grid\n-            xs = (\n-                torch.linspace(0, w - 1, w, dtype=input.dtype, device=input.device)\n-                .view(1, w)\n-                .expand(h, w)\n-            )\n-            ys = (\n-                torch.linspace(0, h - 1, h, dtype=input.dtype, device=input.device)\n-                .view(h, 1)\n-                .expand(h, w)\n-            )\n-            grid = torch.stack((xs, ys, torch.ones_like(xs)), -1)\n-            self.grid = grid\n-        else:\n-            grid = self.grid\n-        grid = grid.view(1, h, w, 3).expand(n, h, w, 3).view(n, h, w, 3, 1)\n-\n-        # get transformation from current ego frame to adjacent ego frame\n-        # transformation from current camera frame to current ego frame\n-        c02l0 = sensor2keyegos[0][:, 0:1, :, :]\n-\n-        # transformation from adjacent camera frame to current ego frame\n-        c12l0 = sensor2keyegos[1][:, 0:1, :, :]\n-\n-        # add bev data augmentation\n-        bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n-        bda_[:, :, :3, :3] = bda.unsqueeze(1)[\n-            :, :, :3, :3\n-        ]  # change for diff, before change is \"bda.unsqueeze(1)\"\n-        bda_[:, :, 3, 3] = 1\n-        c02l0 = bda_.matmul(c02l0)\n-        if bda_adj is not None:\n-            bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n-            bda_[:, :, :3, :3] = bda_adj.unsqueeze(1)\n-            bda_[:, :, 3, 3] = 1\n-        c12l0 = bda_.matmul(c12l0)\n-\n-        # transformation from current ego frame to adjacent ego frame\n-        l02l1 = c02l0.matmul(torch.inverse(c12l0))[:, 0, :, :].view(n, 1, 1, 4, 4)\n-        \"\"\"\n-          c02l0 * inv(c12l0)\n-        = c02l0 * inv(l12l0 * c12l1)\n-        = c02l0 * inv(c12l1) * inv(l12l0)\n-        = l02l1 # c02l0==c12l1\n-        \"\"\"\n-\n-        l02l1 = l02l1[:, :, :, [True, True, False, True], :][\n-            :, :, :, :, [True, True, False, True]\n-        ]\n-\n-        feat2bev = torch.zeros((3, 3), dtype=grid.dtype).to(grid)\n-        feat2bev[0, 0] = self.img_view_transformer.grid_interval[0]\n-        feat2bev[1, 1] = self.img_view_transformer.grid_interval[1]\n-        feat2bev[0, 2] = self.img_view_transformer.grid_lower_bound[0]\n-        feat2bev[1, 2] = self.img_view_transformer.grid_lower_bound[1]\n-        feat2bev[2, 2] = 1\n-        feat2bev = feat2bev.view(1, 3, 3)\n-        tf = torch.inverse(feat2bev).matmul(l02l1).matmul(feat2bev)\n-\n-        # transform and normalize\n-        grid = tf.matmul(grid)\n-        normalize_factor = torch.tensor(\n-            [w - 1.0, h - 1.0], dtype=input.dtype, device=input.device\n-        )\n-        grid = grid[:, :, :, :2, 0] / normalize_factor.view(1, 1, 1, 2) * 2.0 - 1.0\n-        return grid\n-\n-    @force_fp32()\n-    def shift_feature(self, input, sensor2keyegos, bda, bda_adj=None):\n-        grid = self.gen_grid(input, sensor2keyegos, bda, bda_adj=bda_adj)\n-        output = F.grid_sample(input, grid.to(input.dtype), align_corners=True)\n-        return output\n-\n-    def prepare_bev_feat(\n-        self, img, rot, tran, intrin, post_rot, post_tran, bda, mlp_input\n-    ):\n-        x, _ = self.image_encoder(img)\n-        bev_feat, depth = self.img_view_transformer(\n-            [x, rot, tran, intrin, post_rot, post_tran, bda, mlp_input]\n-        )\n-        if self.pre_process:\n-            bev_feat = self.pre_process_net(bev_feat)[0]\n-        return bev_feat, depth\n-\n-    def extract_img_feat_sequential(self, inputs, feat_prev):\n-        imgs, sensor2keyegos_curr, ego2globals_curr, intrins = inputs[:4]\n-        sensor2keyegos_prev, _, post_rots, post_trans, bda = inputs[4:]\n-        bev_feat_list = []\n-        mlp_input = self.img_view_transformer.get_mlp_input(\n-            sensor2keyegos_curr[0:1, ...],\n-            ego2globals_curr[0:1, ...],\n-            intrins,\n-            post_rots,\n-            post_trans,\n-            bda[0:1, ...],\n-        )\n-        inputs_curr = (\n-            imgs,\n-            sensor2keyegos_curr[0:1, ...],\n-            ego2globals_curr[0:1, ...],\n-            intrins,\n-            post_rots,\n-            post_trans,\n-            bda[0:1, ...],\n-            mlp_input,\n-        )\n-        bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n-        bev_feat_list.append(bev_feat)\n-\n-        # align the feat_prev\n-        _, C, H, W = feat_prev.shape\n-        feat_prev = self.shift_feature(\n-            feat_prev, [sensor2keyegos_curr, sensor2keyegos_prev], bda\n-        )\n-        bev_feat_list.append(feat_prev.view(1, (self.num_frame - 1) * C, H, W))\n-\n-        bev_feat = torch.cat(bev_feat_list, dim=1)\n-        x = self.bev_encoder(bev_feat)\n-        return [x], depth\n-\n-    def prepare_inputs(self, inputs, stereo=False):\n-        # split the inputs into each frame\n-        B, N, C, H, W = inputs[0].shape\n-        N = N // self.num_frame\n-        imgs = inputs[0].view(B, N, self.num_frame, C, H, W)\n-        imgs = torch.split(imgs, 1, 2)\n-        imgs = [t.squeeze(2) for t in imgs]\n-        sensor2egos, ego2globals, intrins, post_rots, post_trans, bda = inputs[1:7]\n-\n-        sensor2egos = sensor2egos.view(B, self.num_frame, N, 4, 4)\n-        ego2globals = ego2globals.view(B, self.num_frame, N, 4, 4)\n-\n-        # calculate the transformation from sweep sensor to key ego\n-        keyego2global = ego2globals[:, 0, 0, ...].unsqueeze(1).unsqueeze(1)\n-        global2keyego = torch.inverse(keyego2global.double())\n-        sensor2keyegos = global2keyego @ ego2globals.double() @ sensor2egos.double()\n-        sensor2keyegos = sensor2keyegos.float()\n-\n-        curr2adjsensor = None\n-        if stereo:\n-            sensor2egos_cv, ego2globals_cv = sensor2egos, ego2globals\n-            sensor2egos_curr = sensor2egos_cv[:, : self.temporal_frame, ...].double()\n-            ego2globals_curr = ego2globals_cv[:, : self.temporal_frame, ...].double()\n-            sensor2egos_adj = sensor2egos_cv[\n-                :, 1 : self.temporal_frame + 1, ...\n-            ].double()\n-            ego2globals_adj = ego2globals_cv[\n-                :, 1 : self.temporal_frame + 1, ...\n-            ].double()\n-            curr2adjsensor = (\n-                torch.inverse(ego2globals_adj @ sensor2egos_adj)\n-                @ ego2globals_curr\n-                @ sensor2egos_curr\n-            )\n-            curr2adjsensor = curr2adjsensor.float()\n-            curr2adjsensor = torch.split(curr2adjsensor, 1, 1)\n-            curr2adjsensor = [p.squeeze(1) for p in curr2adjsensor]\n-            curr2adjsensor.extend([None for _ in range(self.extra_ref_frames)])\n-            assert len(curr2adjsensor) == self.num_frame\n-\n-        extra = [\n-            sensor2keyegos,\n-            ego2globals,\n-            intrins.view(B, self.num_frame, N, 3, 3),\n-            post_rots.view(B, self.num_frame, N, 3, 3),\n-            post_trans.view(B, self.num_frame, N, 3),\n-        ]\n-        extra = [torch.split(t, 1, 1) for t in extra]\n-        extra = [[p.squeeze(1) for p in t] for t in extra]\n-        sensor2keyegos, ego2globals, intrins, post_rots, post_trans = extra\n-        return (\n-            imgs,\n-            sensor2keyegos,\n-            ego2globals,\n-            intrins,\n-            post_rots,\n-            post_trans,\n-            bda,\n-            curr2adjsensor,\n-        )\n-\n-    def extract_img_feat(\n-        self, img, img_metas, pred_prev=False, sequential=False, **kwargs\n-    ):\n-        if sequential:\n-            return self.extract_img_feat_sequential(img, kwargs[\"feat_prev\"])\n-        imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, bda, _ = (\n-            self.prepare_inputs(img)\n-        )\n-        \"\"\"Extract features of images.\"\"\"\n-        bev_feat_list = []\n-        depth_list = []\n-        key_frame = True  # back propagation for key frame only\n-        for img, sensor2keyego, ego2global, intrin, post_rot, post_tran in zip(\n-            imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans\n-        ):\n-            if key_frame or self.with_prev:\n-                if self.align_after_view_transfromation:\n-                    sensor2keyego, ego2global = sensor2keyegos[0], ego2globals[0]\n-                mlp_input = self.img_view_transformer.get_mlp_input(\n-                    sensor2keyegos[0], ego2globals[0], intrin, post_rot, post_tran, bda\n-                )\n-                inputs_curr = (\n-                    img,\n-                    sensor2keyego,\n-                    ego2global,\n-                    intrin,\n-                    post_rot,\n-                    post_tran,\n-                    bda,\n-                    mlp_input,\n-                )\n-                if key_frame:\n-                    bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n-                else:\n-                    with torch.no_grad():\n-                        bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n-            else:\n-                bev_feat = torch.zeros_like(bev_feat_list[0])\n-                depth = None\n-            bev_feat_list.append(bev_feat)\n-            depth_list.append(depth)\n-            key_frame = False\n-        if pred_prev:\n-            assert self.align_after_view_transfromation\n-            assert sensor2keyegos[0].shape[0] == 1\n-            feat_prev = torch.cat(bev_feat_list[1:], dim=0)\n-            ego2globals_curr = ego2globals[0].repeat(self.num_frame - 1, 1, 1, 1)\n-            sensor2keyegos_curr = sensor2keyegos[0].repeat(self.num_frame - 1, 1, 1, 1)\n-            ego2globals_prev = torch.cat(ego2globals[1:], dim=0)\n-            sensor2keyegos_prev = torch.cat(sensor2keyegos[1:], dim=0)\n-            bda_curr = bda.repeat(self.num_frame - 1, 1, 1)\n-            return feat_prev, [\n-                imgs[0],\n-                sensor2keyegos_curr,\n-                ego2globals_curr,\n-                intrins[0],\n-                sensor2keyegos_prev,\n-                ego2globals_prev,\n-                post_rots[0],\n-                post_trans[0],\n-                bda_curr,\n-            ]\n-        if self.align_after_view_transfromation:\n-            for adj_id in range(1, self.num_frame):\n-                bev_feat_list[adj_id] = self.shift_feature(\n-                    bev_feat_list[adj_id],\n-                    [sensor2keyegos[0], sensor2keyegos[adj_id]],\n-                    bda,\n-                )\n-        bev_feat = torch.cat(bev_feat_list, dim=1)\n-        x = self.bev_encoder(bev_feat)\n-        return [x], depth_list[0]\n-\n-\n-@DETECTORS.register_module()\n-class BEVDepth4D(BEVDet4D):\n-\n-    def forward_train(\n-        self,\n-        points=None,\n-        img_metas=None,\n-        gt_bboxes_3d=None,\n-        gt_labels_3d=None,\n-        gt_labels=None,\n-        gt_bboxes=None,\n-        img_inputs=None,\n-        proposals=None,\n-        gt_bboxes_ignore=None,\n-        **kwargs,\n-    ):\n-        \"\"\"Forward training function.\n-\n-        Args:\n-            points (list[torch.Tensor], optional): Points of each sample.\n-                Defaults to None.\n-            img_metas (list[dict], optional): Meta information of each sample.\n-                Defaults to None.\n-            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`], optional):\n-                Ground truth 3D boxes. Defaults to None.\n-            gt_labels_3d (list[torch.Tensor], optional): Ground truth labels\n-                of 3D boxes. Defaults to None.\n-            gt_labels (list[torch.Tensor], optional): Ground truth labels\n-                of 2D boxes in images. Defaults to None.\n-            gt_bboxes (list[torch.Tensor], optional): Ground truth 2D boxes in\n-                images. Defaults to None.\n-            img (torch.Tensor optional): Images of each sample with shape\n-                (N, C, H, W). Defaults to None.\n-            proposals ([list[torch.Tensor], optional): Predicted proposals\n-                used for training Fast RCNN. Defaults to None.\n-            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n-                2D boxes in images to be ignored. Defaults to None.\n-\n-        Returns:\n-            dict: Losses of different branches.\n-        \"\"\"\n-        img_feats, pts_feats, depth = self.extract_feat(\n-            points, img=img_inputs, img_metas=img_metas, **kwargs\n-        )\n-\n-        gt_depth = kwargs[\"gt_depth\"]\n-        loss_depth = self.img_view_transformer.get_depth_loss(gt_depth, depth)\n-        losses = dict(loss_depth=loss_depth)\n-        losses_pts = self.forward_pts_train(\n-            img_feats, gt_bboxes_3d, gt_labels_3d, img_metas, gt_bboxes_ignore\n-        )\n-        losses.update(losses_pts)\n-        return losses\n-\n-\n-@DETECTORS.register_module()\n-class BEVStereo4D(BEVDepth4D):\n-    def __init__(self, **kwargs):\n-        super(BEVStereo4D, self).__init__(**kwargs)\n-        self.extra_ref_frames = 1\n-        self.temporal_frame = self.num_frame\n-        self.num_frame += self.extra_ref_frames\n-\n-    def extract_stereo_ref_feat(self, x):\n-        B, N, C, imH, imW = x.shape\n-        x = x.view(B * N, C, imH, imW)\n-        if isinstance(self.img_backbone, ResNet):\n-            if self.img_backbone.deep_stem:\n-                x = self.img_backbone.stem(x)\n-            else:\n-                x = self.img_backbone.conv1(x)\n-                x = self.img_backbone.norm1(x)\n-                x = self.img_backbone.relu(x)\n-            x = self.img_backbone.maxpool(x)\n-            for i, layer_name in enumerate(self.img_backbone.res_layers):\n-                res_layer = getattr(self.img_backbone, layer_name)\n-                x = res_layer(x)\n-                return x\n-        else:\n-            x = self.img_backbone.patch_embed(x)\n-            hw_shape = (\n-                self.img_backbone.patch_embed.DH,\n-                self.img_backbone.patch_embed.DW,\n-            )\n-            if self.img_backbone.use_abs_pos_embed:\n-                x = x + self.img_backbone.absolute_pos_embed\n-            x = self.img_backbone.drop_after_pos(x)\n-\n-            for i, stage in enumerate(self.img_backbone.stages):\n-                x, hw_shape, out, out_hw_shape = stage(x, hw_shape)\n-                out = out.view(-1, *out_hw_shape, self.img_backbone.num_features[i])\n-                out = out.permute(0, 3, 1, 2).contiguous()\n-                return out\n-\n-    def prepare_bev_feat(\n-        self,\n-        img,\n-        sensor2keyego,\n-        ego2global,\n-        intrin,\n-        post_rot,\n-        post_tran,\n-        bda,\n-        mlp_input,\n-        feat_prev_iv,\n-        k2s_sensor,\n-        extra_ref_frame,\n-    ):\n-        if extra_ref_frame:\n-            stereo_feat = self.extract_stereo_ref_feat(img)\n-            return None, None, stereo_feat\n-\n-        x, stereo_feat = self.image_encoder(img, stereo=True)\n-        metas = dict(\n-            k2s_sensor=k2s_sensor,\n-            intrins=intrin,\n-            post_rots=post_rot,\n-            post_trans=post_tran,\n-            frustum=self.img_view_transformer.cv_frustum.to(x),\n-            cv_downsample=4,\n-            downsample=self.img_view_transformer.downsample,\n-            grid_config=self.img_view_transformer.grid_config,\n-            cv_feat_list=[feat_prev_iv, stereo_feat],\n-        )\n-        bev_feat, depth = self.img_view_transformer(\n-            [x, sensor2keyego, ego2global, intrin, post_rot, post_tran, bda, mlp_input],\n-            metas,\n-        )\n-        if self.pre_process:\n-            bev_feat = self.pre_process_net(bev_feat)[0]\n-        return bev_feat, depth, stereo_feat\n-\n-    def extract_img_feat(\n-        self, img, img_metas, pred_prev=False, sequential=False, **kwargs\n-    ):\n-\n-        if sequential:\n-            # Todo\n-            assert False\n-\n-        (\n-            imgs,\n-            sensor2keyegos,\n-            ego2globals,\n-            intrins,\n-            post_rots,\n-            post_trans,\n-            bda,\n-            curr2adjsensor,\n-        ) = self.prepare_inputs(img, stereo=True)\n-\n-        \"\"\"Extract features of images.\"\"\"\n-        bev_feat_list = []\n-        depth_key_frame = None\n-        feat_prev_iv = None\n-        for fid in range(self.num_frame - 1, -1, -1):\n-            img, sensor2keyego, ego2global, intrin, post_rot, post_tran = (\n-                imgs[fid],\n-                sensor2keyegos[fid],\n-                ego2globals[fid],\n-                intrins[fid],\n-                post_rots[fid],\n-                post_trans[fid],\n-            )\n-            key_frame = fid == 0\n-            extra_ref_frame = fid == self.num_frame - self.extra_ref_frames\n-            if key_frame or self.with_prev:\n-                if self.align_after_view_transfromation:\n-                    sensor2keyego, ego2global = sensor2keyegos[0], ego2globals[0]\n-                mlp_input = self.img_view_transformer.get_mlp_input(\n-                    sensor2keyegos[0], ego2globals[0], intrin, post_rot, post_tran, bda\n-                )\n-                inputs_curr = (\n-                    img,\n-                    sensor2keyego,\n-                    ego2global,\n-                    intrin,\n-                    post_rot,\n-                    post_tran,\n-                    bda,\n-                    mlp_input,\n-                    feat_prev_iv,\n-                    curr2adjsensor[fid],\n-                    extra_ref_frame,\n-                )\n-                if key_frame:\n-                    bev_feat, depth, feat_curr_iv = self.prepare_bev_feat(*inputs_curr)\n-                    depth_key_frame = depth\n-                else:\n-                    with torch.no_grad():\n-                        bev_feat, depth, feat_curr_iv = self.prepare_bev_feat(\n-                            *inputs_curr\n-                        )\n-                if not extra_ref_frame:\n-                    bev_feat_list.append(bev_feat)\n-                feat_prev_iv = feat_curr_iv\n-        if pred_prev:\n-            # Todo\n-            assert False\n-        if not self.with_prev:\n-            bev_feat_key = bev_feat_list[0]\n-            if len(bev_feat_key.shape) == 4:\n-                b, c, h, w = bev_feat_key.shape\n-                bev_feat_list = [\n-                    torch.zeros(\n-                        [b, c * (self.num_frame - self.extra_ref_frames - 1), h, w]\n-                    ).to(bev_feat_key),\n-                    bev_feat_key,\n-                ]\n-            else:\n-                b, c, z, h, w = bev_feat_key.shape\n-                bev_feat_list = [\n-                    torch.zeros(\n-                        [b, c * (self.num_frame - self.extra_ref_frames - 1), z, h, w]\n-                    ).to(bev_feat_key),\n-                    bev_feat_key,\n-                ]\n-        if self.align_after_view_transfromation:\n-            for adj_id in range(self.num_frame - 2):\n-                bev_feat_list[adj_id] = self.shift_feature(\n-                    bev_feat_list[adj_id],\n-                    [sensor2keyegos[0], sensor2keyegos[self.num_frame - 2 - adj_id]],\n-                    bda,\n-                )\n-        bev_feat = torch.cat(bev_feat_list, dim=1)\n-        x = self.bev_encoder(bev_feat)\n-        return [x], depth_key_frame\n"
                },
                {
                    "date": 1716644829959,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -649,15 +649,34 @@\n                 res_layer = getattr(self.img_backbone, layer_name)\n                 x = res_layer(x)\n                 return x\n         else:\n-            x = self.img_backbone.patch_embed(x)\n-            hw_shape = (\n-                self.img_backbone.patch_embed.DH,\n-                self.img_backbone.patch_embed.DW,\n-            )\n+            # x = self.img_backbone.patch_embed(x)\n+            # hw_shape = (\n+            #     self.img_backbone.patch_embed.DH,\n+            #     self.img_backbone.patch_embed.DW,\n+            # )\n+            # if self.img_backbone.use_abs_pos_embed:\n+            #     x = x + self.img_backbone.absolute_pos_embed\n+            # x = self.img_backbone.drop_after_pos(x)\n+\n+            # for i, stage in enumerate(self.img_backbone.stages):\n+            #     x, hw_shape, out, out_hw_shape = stage(x, hw_shape)\n+            #     out = out.view(-1, *out_hw_shape, self.img_backbone.num_features[i])\n+            #     out = out.permute(0, 3, 1, 2).contiguous()\n+            #     return out\n+            \n+            #======================\n+                        # x = self.img_backbone.patch_embed(x)\n+            # hw_shape = (self.img_backbone.patch_embed.DH,\n+            #             self.img_backbone.patch_embed.DW)\n+            x, hw_shape = self.img_backbone.patch_embed(x)\n             if self.img_backbone.use_abs_pos_embed:\n-                x = x + self.img_backbone.absolute_pos_embed\n+                # x = x + self.img_backbone.absolute_pos_embed\n+                absolute_pos_embed = F.interpolate(\n+                    self.img_backbone.absolute_pos_embed, size=hw_shape, mode=\"bicubic\"\n+                )\n+                x = x + absolute_pos_embed.flatten(2).transpose(1, 2)\n             x = self.img_backbone.drop_after_pos(x)\n \n             for i, stage in enumerate(self.img_backbone.stages):\n                 x, hw_shape, out, out_hw_shape = stage(x, hw_shape)\n"
                },
                {
                    "date": 1716791513570,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,819 @@\n+# Copyright (c) Phigent Robotics. All rights reserved.\n+import torch\n+import torch.nn.functional as F\n+from mmcv.runner import force_fp32\n+\n+from mmdet3d.ops.bev_pool_v2.bev_pool import TRTBEVPoolv2\n+from mmdet.models import DETECTORS\n+from .. import builder\n+from .centerpoint import CenterPoint\n+from mmdet3d.models.utils.grid_mask import GridMask\n+from mmdet.models.backbones.resnet import ResNet\n+from termcolor import colored\n+from mmdet.utils import get_root_logger\n+from mmdet3d.models.utils.self_print import feats_to_img\n+\n+\n+@DETECTORS.register_module()\n+class BEVDet(CenterPoint):\n+    r\"\"\"BEVDet paradigm for multi-camera 3D object detection.\n+\n+    Please refer to the `paper <https://arxiv.org/abs/2112.11790>`_\n+\n+    Args:\n+        img_view_transformer (dict): Configuration dict of view transformer.\n+        img_bev_encoder_backbone (dict): Configuration dict of the BEV encoder\n+            backbone.\n+        img_bev_encoder_neck (dict): Configuration dict of the BEV encoder neck.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        img_view_transformer,\n+        img_bev_encoder_backbone=None,\n+        img_bev_encoder_neck=None,\n+        use_grid_mask=False,\n+        **kwargs,\n+    ):\n+        super(BEVDet, self).__init__(**kwargs)\n+        self.grid_mask = (\n+            None\n+            if not use_grid_mask\n+            else GridMask(\n+                True, True, rotate=1, offset=False, ratio=0.5, mode=1, prob=0.7\n+            )\n+        )\n+        self.img_view_transformer = builder.build_neck(img_view_transformer)\n+        if img_bev_encoder_neck and img_bev_encoder_backbone:\n+            self.img_bev_encoder_backbone = builder.build_backbone(\n+                img_bev_encoder_backbone\n+            )\n+            self.img_bev_encoder_neck = builder.build_neck(img_bev_encoder_neck)\n+\n+    def image_encoder(self, img, stereo=False):\n+        imgs = img\n+        B, N, C, imH, imW = imgs.shape\n+        imgs = imgs.view(B * N, C, imH, imW)\n+        if self.grid_mask is not None:\n+            imgs = self.grid_mask(imgs)\n+        x = self.img_backbone(imgs)\n+        stereo_feat = None\n+        if stereo:\n+            stereo_feat = x[0]\n+            x = x[1:]\n+        if self.with_img_neck:\n+            x = self.img_neck(x)\n+            if type(x) in [list, tuple]:\n+                x = x[0]\n+        _, output_dim, ouput_H, output_W = x.shape\n+        x = x.view(B, N, output_dim, ouput_H, output_W)\n+        return x, stereo_feat\n+\n+    @force_fp32()\n+    def bev_encoder(self, x):\n+        x = self.img_bev_encoder_backbone(x)\n+        x = self.img_bev_encoder_neck(x)\n+        if type(x) in [list, tuple]:\n+            x = x[0]\n+        return x\n+\n+    def prepare_inputs(self, inputs):\n+        # split the inputs into each frame\n+        assert len(inputs) == 7\n+        B, N, C, H, W = inputs[0].shape\n+        imgs, sensor2egos, ego2globals, intrins, post_rots, post_trans, bda = inputs\n+\n+        sensor2egos = sensor2egos.view(B, N, 4, 4)\n+        ego2globals = ego2globals.view(B, N, 4, 4)\n+\n+        # calculate the transformation from sweep sensor to key ego\n+        keyego2global = ego2globals[:, 0, ...].unsqueeze(1)\n+        global2keyego = torch.inverse(keyego2global.double())\n+        sensor2keyegos = global2keyego @ ego2globals.double() @ sensor2egos.double()\n+        sensor2keyegos = sensor2keyegos.float()\n+\n+        return [imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, bda]\n+\n+    def extract_img_feat(self, img, img_metas, **kwargs):\n+        \"\"\"Extract features of images.\"\"\"\n+        img = self.prepare_inputs(img)\n+        x, _ = self.image_encoder(img[0])\n+        x, depth = self.img_view_transformer([x] + img[1:7])\n+        x = self.bev_encoder(x)\n+        return [x], depth\n+\n+    def extract_feat(self, points, img, img_metas, **kwargs):\n+        \"\"\"Extract features from images and points.\"\"\"\n+        img_feats, depth = self.extract_img_feat(img, img_metas, **kwargs)\n+        pts_feats = None\n+        return (img_feats, pts_feats, depth)\n+\n+    def forward_train(\n+        self,\n+        points=None,\n+        img_metas=None,\n+        gt_bboxes_3d=None,\n+        gt_labels_3d=None,\n+        gt_labels=None,\n+        gt_bboxes=None,\n+        img_inputs=None,\n+        proposals=None,\n+        gt_bboxes_ignore=None,\n+        **kwargs,\n+    ):\n+        \"\"\"Forward training function.\n+\n+        Args:\n+            points (list[torch.Tensor], optional): Points of each sample.\n+                Defaults to None.\n+            img_metas (list[dict], optional): Meta information of each sample.\n+                Defaults to None.\n+            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`], optional):\n+                Ground truth 3D boxes. Defaults to None.\n+            gt_labels_3d (list[torch.Tensor], optional): Ground truth labels\n+                of 3D boxes. Defaults to None.\n+            gt_labels (list[torch.Tensor], optional): Ground truth labels\n+                of 2D boxes in images. Defaults to None.\n+            gt_bboxes (list[torch.Tensor], optional): Ground truth 2D boxes in\n+                images. Defaults to None.\n+            img (torch.Tensor optional): Images of each sample with shape\n+                (N, C, H, W). Defaults to None.\n+            proposals ([list[torch.Tensor], optional): Predicted proposals\n+                used for training Fast RCNN. Defaults to None.\n+            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n+                2D boxes in images to be ignored. Defaults to None.\n+\n+        Returns:\n+            dict: Losses of different branches.\n+        \"\"\"\n+        img_feats, pts_feats, _ = self.extract_feat(\n+            points, img=img_inputs, img_metas=img_metas, **kwargs\n+        )\n+        losses = dict()\n+        losses_pts = self.forward_pts_train(\n+            img_feats, gt_bboxes_3d, gt_labels_3d, img_metas, gt_bboxes_ignore\n+        )\n+        losses.update(losses_pts)\n+        return losses\n+\n+    def forward_test(self, points=None, img_metas=None, img_inputs=None, **kwargs):\n+        \"\"\"\n+        Args:\n+            points (list[torch.Tensor]): the outer list indicates test-time\n+                augmentations and inner torch.Tensor should have a shape NxC,\n+                which contains all points in the batch.\n+            img_metas (list[list[dict]]): the outer list indicates test-time\n+                augs (multiscale, flip, etc.) and the inner list indicates\n+                images in a batch\n+            img (list[torch.Tensor], optional): the outer\n+                list indicates test-time augmentations and inner\n+                torch.Tensor should have a shape NxCxHxW, which contains\n+                all images in the batch. Defaults to None.\n+        \"\"\"\n+        for var, name in [(img_inputs, \"img_inputs\"), (img_metas, \"img_metas\")]:\n+            if not isinstance(var, list):\n+                raise TypeError(\"{} must be a list, but got {}\".format(name, type(var)))\n+\n+        num_augs = len(img_inputs)\n+        if num_augs != len(img_metas):\n+            raise ValueError(\n+                \"num of augmentations ({}) != num of image meta ({})\".format(\n+                    len(img_inputs), len(img_metas)\n+                )\n+            )\n+\n+        if not isinstance(img_inputs[0][0], list):\n+            img_inputs = [img_inputs] if img_inputs is None else img_inputs\n+            points = [points] if points is None else points\n+            return self.simple_test(points[0], img_metas[0], img_inputs[0], **kwargs)\n+        else:\n+            return self.aug_test(None, img_metas[0], img_inputs[0], **kwargs)\n+\n+    def aug_test(self, points, img_metas, img=None, rescale=False):\n+        \"\"\"Test function without augmentaiton.\"\"\"\n+        assert False\n+\n+    def simple_test(self, points, img_metas, img=None, rescale=False, **kwargs):\n+        \"\"\"Test function without augmentaiton.\"\"\"\n+        img_feats, _, _ = self.extract_feat(\n+            points, img=img, img_metas=img_metas, **kwargs\n+        )\n+\n+        # base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n+        # feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n+\n+        bbox_list = [dict() for _ in range(len(img_metas))]\n+        bbox_pts = self.simple_test_pts(img_feats, img_metas, rescale=rescale)\n+        for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n+            result_dict[\"pts_bbox\"] = pts_bbox\n+        return bbox_list\n+\n+    def forward_dummy(self, points=None, img_metas=None, img_inputs=None, **kwargs):\n+        img_feats, _, _ = self.extract_feat(\n+            points, img=img_inputs, img_metas=img_metas, **kwargs\n+        )\n+        assert self.with_pts_bbox\n+        outs = self.pts_bbox_head(img_feats)\n+        return outs\n+\n+\n+@DETECTORS.register_module()\n+class BEVDetTRT(BEVDet):\n+\n+    def result_serialize(self, outs):\n+        outs_ = []\n+        for out in outs:\n+            for key in [\"reg\", \"height\", \"dim\", \"rot\", \"vel\", \"heatmap\"]:\n+                outs_.append(out[0][key])\n+        return outs_\n+\n+    def result_deserialize(self, outs):\n+        outs_ = []\n+        keys = [\"reg\", \"height\", \"dim\", \"rot\", \"vel\", \"heatmap\"]\n+        for head_id in range(len(outs) // 6):\n+            outs_head = [dict()]\n+            for kid, key in enumerate(keys):\n+                outs_head[0][key] = outs[head_id * 6 + kid]\n+            outs_.append(outs_head)\n+        return outs_\n+\n+    def forward(\n+        self,\n+        img,\n+        ranks_depth,\n+        ranks_feat,\n+        ranks_bev,\n+        interval_starts,\n+        interval_lengths,\n+    ):\n+        x = self.img_backbone(img)\n+        x = self.img_neck(x)\n+        x = self.img_view_transformer.depth_net(x)\n+        depth = x[:, : self.img_view_transformer.D].softmax(dim=1)\n+        tran_feat = x[\n+            :,\n+            self.img_view_transformer.D : (\n+                self.img_view_transformer.D + self.img_view_transformer.out_channels\n+            ),\n+        ]\n+        tran_feat = tran_feat.permute(0, 2, 3, 1)\n+        x = TRTBEVPoolv2.apply(\n+            depth.contiguous(),\n+            tran_feat.contiguous(),\n+            ranks_depth,\n+            ranks_feat,\n+            ranks_bev,\n+            interval_starts,\n+            interval_lengths,\n+        )\n+        x = x.permute(0, 3, 1, 2).contiguous()\n+        bev_feat = self.bev_encoder(x)\n+        outs = self.pts_bbox_head([bev_feat])\n+        outs = self.result_serialize(outs)\n+        return outs\n+\n+    def get_bev_pool_input(self, input):\n+        input = self.prepare_inputs(input)\n+        coor = self.img_view_transformer.get_lidar_coor(*input[1:7])\n+        return self.img_view_transformer.voxel_pooling_prepare_v2(coor)\n+\n+\n+@DETECTORS.register_module()\n+class BEVDet4D(BEVDet):\n+    r\"\"\"BEVDet4D paradigm for multi-camera 3D object detection.\n+\n+    Please refer to the `paper <https://arxiv.org/abs/2203.17054>`_\n+\n+    Args:\n+        pre_process (dict | None): Configuration dict of BEV pre-process net.\n+        align_after_view_transfromation (bool): Whether to align the BEV\n+            Feature after view transformation. By default, the BEV feature of\n+            the previous frame is aligned during the view transformation.\n+        num_adj (int): Number of adjacent frames.\n+        with_prev (bool): Whether to set the BEV feature of previous frame as\n+            all zero. By default, False.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        pre_process=None,\n+        align_after_view_transfromation=False,\n+        num_adj=1,\n+        with_prev=True,\n+        **kwargs,\n+    ):\n+        super(BEVDet4D, self).__init__(**kwargs)\n+        self.pre_process = pre_process is not None\n+        if self.pre_process:\n+            self.pre_process_net = builder.build_backbone(pre_process)\n+        self.align_after_view_transfromation = align_after_view_transfromation\n+        self.num_frame = num_adj + 1\n+\n+        self.with_prev = with_prev\n+        self.grid = None\n+\n+    def gen_grid(self, input, sensor2keyegos, bda, bda_adj=None):\n+        n, c, h, w = input.shape\n+        _, v, _, _ = sensor2keyegos[0].shape\n+        if self.grid is None:\n+            # generate grid\n+            xs = (\n+                torch.linspace(0, w - 1, w, dtype=input.dtype, device=input.device)\n+                .view(1, w)\n+                .expand(h, w)\n+            )\n+            ys = (\n+                torch.linspace(0, h - 1, h, dtype=input.dtype, device=input.device)\n+                .view(h, 1)\n+                .expand(h, w)\n+            )\n+            grid = torch.stack((xs, ys, torch.ones_like(xs)), -1)\n+            self.grid = grid\n+        else:\n+            grid = self.grid\n+        grid = grid.view(1, h, w, 3).expand(n, h, w, 3).view(n, h, w, 3, 1)\n+\n+        # get transformation from current ego frame to adjacent ego frame\n+        # transformation from current camera frame to current ego frame\n+        c02l0 = sensor2keyegos[0][:, 0:1, :, :]\n+\n+        # transformation from adjacent camera frame to current ego frame\n+        c12l0 = sensor2keyegos[1][:, 0:1, :, :]\n+\n+        # add bev data augmentation\n+        bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n+        bda_[:, :, :3, :3] = bda.unsqueeze(1)[\n+            :, :, :3, :3\n+        ]  # change for diff, before change is \"bda.unsqueeze(1)\"\n+        bda_[:, :, 3, 3] = 1\n+        c02l0 = bda_.matmul(c02l0)\n+        if bda_adj is not None:\n+            bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n+            bda_[:, :, :3, :3] = bda_adj.unsqueeze(1)\n+            bda_[:, :, 3, 3] = 1\n+        c12l0 = bda_.matmul(c12l0)\n+\n+        # transformation from current ego frame to adjacent ego frame\n+        l02l1 = c02l0.matmul(torch.inverse(c12l0))[:, 0, :, :].view(n, 1, 1, 4, 4)\n+        \"\"\"\n+          c02l0 * inv(c12l0)\n+        = c02l0 * inv(l12l0 * c12l1)\n+        = c02l0 * inv(c12l1) * inv(l12l0)\n+        = l02l1 # c02l0==c12l1\n+        \"\"\"\n+\n+        l02l1 = l02l1[:, :, :, [True, True, False, True], :][\n+            :, :, :, :, [True, True, False, True]\n+        ]\n+\n+        feat2bev = torch.zeros((3, 3), dtype=grid.dtype).to(grid)\n+        feat2bev[0, 0] = self.img_view_transformer.grid_interval[0]\n+        feat2bev[1, 1] = self.img_view_transformer.grid_interval[1]\n+        feat2bev[0, 2] = self.img_view_transformer.grid_lower_bound[0]\n+        feat2bev[1, 2] = self.img_view_transformer.grid_lower_bound[1]\n+        feat2bev[2, 2] = 1\n+        feat2bev = feat2bev.view(1, 3, 3)\n+        tf = torch.inverse(feat2bev).matmul(l02l1).matmul(feat2bev)\n+\n+        # transform and normalize\n+        grid = tf.matmul(grid)\n+        normalize_factor = torch.tensor(\n+            [w - 1.0, h - 1.0], dtype=input.dtype, device=input.device\n+        )\n+        grid = grid[:, :, :, :2, 0] / normalize_factor.view(1, 1, 1, 2) * 2.0 - 1.0\n+        return grid\n+\n+    @force_fp32()\n+    def shift_feature(self, input, sensor2keyegos, bda, bda_adj=None):\n+        grid = self.gen_grid(input, sensor2keyegos, bda, bda_adj=bda_adj)\n+        output = F.grid_sample(input, grid.to(input.dtype), align_corners=True)\n+        return output\n+\n+    def prepare_bev_feat(\n+        self, img, rot, tran, intrin, post_rot, post_tran, bda, mlp_input\n+    ):\n+        x, _ = self.image_encoder(img)\n+        bev_feat, depth = self.img_view_transformer(\n+            [x, rot, tran, intrin, post_rot, post_tran, bda, mlp_input]\n+        )\n+        if self.pre_process:\n+            bev_feat = self.pre_process_net(bev_feat)[0]\n+        return bev_feat, depth\n+\n+    def extract_img_feat_sequential(self, inputs, feat_prev):\n+        imgs, sensor2keyegos_curr, ego2globals_curr, intrins = inputs[:4]\n+        sensor2keyegos_prev, _, post_rots, post_trans, bda = inputs[4:]\n+        bev_feat_list = []\n+        mlp_input = self.img_view_transformer.get_mlp_input(\n+            sensor2keyegos_curr[0:1, ...],\n+            ego2globals_curr[0:1, ...],\n+            intrins,\n+            post_rots,\n+            post_trans,\n+            bda[0:1, ...],\n+        )\n+        inputs_curr = (\n+            imgs,\n+            sensor2keyegos_curr[0:1, ...],\n+            ego2globals_curr[0:1, ...],\n+            intrins,\n+            post_rots,\n+            post_trans,\n+            bda[0:1, ...],\n+            mlp_input,\n+        )\n+        bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n+        bev_feat_list.append(bev_feat)\n+\n+        # align the feat_prev\n+        _, C, H, W = feat_prev.shape\n+        feat_prev = self.shift_feature(\n+            feat_prev, [sensor2keyegos_curr, sensor2keyegos_prev], bda\n+        )\n+        bev_feat_list.append(feat_prev.view(1, (self.num_frame - 1) * C, H, W))\n+\n+        bev_feat = torch.cat(bev_feat_list, dim=1)\n+        x = self.bev_encoder(bev_feat)\n+        return [x], depth\n+\n+    def prepare_inputs(self, inputs, stereo=False):\n+        # split the inputs into each frame\n+        B, N, C, H, W = inputs[0].shape\n+        N = N // self.num_frame\n+        imgs = inputs[0].view(B, N, self.num_frame, C, H, W)\n+        imgs = torch.split(imgs, 1, 2)\n+        imgs = [t.squeeze(2) for t in imgs]\n+        sensor2egos, ego2globals, intrins, post_rots, post_trans, bda = inputs[1:7]\n+\n+        sensor2egos = sensor2egos.view(B, self.num_frame, N, 4, 4)\n+        ego2globals = ego2globals.view(B, self.num_frame, N, 4, 4)\n+\n+        # calculate the transformation from sweep sensor to key ego\n+        keyego2global = ego2globals[:, 0, 0, ...].unsqueeze(1).unsqueeze(1)\n+        global2keyego = torch.inverse(keyego2global.double())\n+        sensor2keyegos = global2keyego @ ego2globals.double() @ sensor2egos.double()\n+        sensor2keyegos = sensor2keyegos.float()\n+\n+        curr2adjsensor = None\n+        if stereo:\n+            sensor2egos_cv, ego2globals_cv = sensor2egos, ego2globals\n+            sensor2egos_curr = sensor2egos_cv[:, : self.temporal_frame, ...].double()\n+            ego2globals_curr = ego2globals_cv[:, : self.temporal_frame, ...].double()\n+            sensor2egos_adj = sensor2egos_cv[\n+                :, 1 : self.temporal_frame + 1, ...\n+            ].double()\n+            ego2globals_adj = ego2globals_cv[\n+                :, 1 : self.temporal_frame + 1, ...\n+            ].double()\n+            curr2adjsensor = (\n+                torch.inverse(ego2globals_adj @ sensor2egos_adj)\n+                @ ego2globals_curr\n+                @ sensor2egos_curr\n+            )\n+            curr2adjsensor = curr2adjsensor.float()\n+            curr2adjsensor = torch.split(curr2adjsensor, 1, 1)\n+            curr2adjsensor = [p.squeeze(1) for p in curr2adjsensor]\n+            curr2adjsensor.extend([None for _ in range(self.extra_ref_frames)])\n+            assert len(curr2adjsensor) == self.num_frame\n+\n+        extra = [\n+            sensor2keyegos,\n+            ego2globals,\n+            intrins.view(B, self.num_frame, N, 3, 3),\n+            post_rots.view(B, self.num_frame, N, 3, 3),\n+            post_trans.view(B, self.num_frame, N, 3),\n+        ]\n+        extra = [torch.split(t, 1, 1) for t in extra]\n+        extra = [[p.squeeze(1) for p in t] for t in extra]\n+        sensor2keyegos, ego2globals, intrins, post_rots, post_trans = extra\n+        return (\n+            imgs,\n+            sensor2keyegos,\n+            ego2globals,\n+            intrins,\n+            post_rots,\n+            post_trans,\n+            bda,\n+            curr2adjsensor,\n+        )\n+\n+    def extract_img_feat(\n+        self, img, img_metas, pred_prev=False, sequential=False, **kwargs\n+    ):\n+        if sequential:\n+            return self.extract_img_feat_sequential(img, kwargs[\"feat_prev\"])\n+        imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, bda, _ = (\n+            self.prepare_inputs(img)\n+        )\n+        \"\"\"Extract features of images.\"\"\"\n+        bev_feat_list = []\n+        depth_list = []\n+        key_frame = True  # back propagation for key frame only\n+        for img, sensor2keyego, ego2global, intrin, post_rot, post_tran in zip(\n+            imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans\n+        ):\n+            if key_frame or self.with_prev:\n+                if self.align_after_view_transfromation:\n+                    sensor2keyego, ego2global = sensor2keyegos[0], ego2globals[0]\n+                mlp_input = self.img_view_transformer.get_mlp_input(\n+                    sensor2keyegos[0], ego2globals[0], intrin, post_rot, post_tran, bda\n+                )\n+                inputs_curr = (\n+                    img,\n+                    sensor2keyego,\n+                    ego2global,\n+                    intrin,\n+                    post_rot,\n+                    post_tran,\n+                    bda,\n+                    mlp_input,\n+                )\n+                if key_frame:\n+                    bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n+                else:\n+                    with torch.no_grad():\n+                        bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n+            else:\n+                bev_feat = torch.zeros_like(bev_feat_list[0])\n+                depth = None\n+            bev_feat_list.append(bev_feat)\n+            depth_list.append(depth)\n+            key_frame = False\n+        if pred_prev:\n+            assert self.align_after_view_transfromation\n+            assert sensor2keyegos[0].shape[0] == 1\n+            feat_prev = torch.cat(bev_feat_list[1:], dim=0)\n+            ego2globals_curr = ego2globals[0].repeat(self.num_frame - 1, 1, 1, 1)\n+            sensor2keyegos_curr = sensor2keyegos[0].repeat(self.num_frame - 1, 1, 1, 1)\n+            ego2globals_prev = torch.cat(ego2globals[1:], dim=0)\n+            sensor2keyegos_prev = torch.cat(sensor2keyegos[1:], dim=0)\n+            bda_curr = bda.repeat(self.num_frame - 1, 1, 1)\n+            return feat_prev, [\n+                imgs[0],\n+                sensor2keyegos_curr,\n+                ego2globals_curr,\n+                intrins[0],\n+                sensor2keyegos_prev,\n+                ego2globals_prev,\n+                post_rots[0],\n+                post_trans[0],\n+                bda_curr,\n+            ]\n+        if self.align_after_view_transfromation:\n+            for adj_id in range(1, self.num_frame):\n+                bev_feat_list[adj_id] = self.shift_feature(\n+                    bev_feat_list[adj_id],\n+                    [sensor2keyegos[0], sensor2keyegos[adj_id]],\n+                    bda,\n+                )\n+        bev_feat = torch.cat(bev_feat_list, dim=1)\n+        x = self.bev_encoder(bev_feat)\n+        return [x], depth_list[0]\n+\n+\n+@DETECTORS.register_module()\n+class BEVDepth4D(BEVDet4D):\n+\n+    def forward_train(\n+        self,\n+        points=None,\n+        img_metas=None,\n+        gt_bboxes_3d=None,\n+        gt_labels_3d=None,\n+        gt_labels=None,\n+        gt_bboxes=None,\n+        img_inputs=None,\n+        proposals=None,\n+        gt_bboxes_ignore=None,\n+        **kwargs,\n+    ):\n+        \"\"\"Forward training function.\n+\n+        Args:\n+            points (list[torch.Tensor], optional): Points of each sample.\n+                Defaults to None.\n+            img_metas (list[dict], optional): Meta information of each sample.\n+                Defaults to None.\n+            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`], optional):\n+                Ground truth 3D boxes. Defaults to None.\n+            gt_labels_3d (list[torch.Tensor], optional): Ground truth labels\n+                of 3D boxes. Defaults to None.\n+            gt_labels (list[torch.Tensor], optional): Ground truth labels\n+                of 2D boxes in images. Defaults to None.\n+            gt_bboxes (list[torch.Tensor], optional): Ground truth 2D boxes in\n+                images. Defaults to None.\n+            img (torch.Tensor optional): Images of each sample with shape\n+                (N, C, H, W). Defaults to None.\n+            proposals ([list[torch.Tensor], optional): Predicted proposals\n+                used for training Fast RCNN. Defaults to None.\n+            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n+                2D boxes in images to be ignored. Defaults to None.\n+\n+        Returns:\n+            dict: Losses of different branches.\n+        \"\"\"\n+        img_feats, pts_feats, depth = self.extract_feat(\n+            points, img=img_inputs, img_metas=img_metas, **kwargs\n+        )\n+\n+        gt_depth = kwargs[\"gt_depth\"]\n+        loss_depth = self.img_view_transformer.get_depth_loss(gt_depth, depth)\n+        losses = dict(loss_depth=loss_depth)\n+        losses_pts = self.forward_pts_train(\n+            img_feats, gt_bboxes_3d, gt_labels_3d, img_metas, gt_bboxes_ignore\n+        )\n+        losses.update(losses_pts)\n+        return losses\n+\n+\n+@DETECTORS.register_module()\n+class BEVStereo4D(BEVDepth4D):\n+    def __init__(self, **kwargs):\n+        super(BEVStereo4D, self).__init__(**kwargs)\n+        self.extra_ref_frames = 1\n+        self.temporal_frame = self.num_frame\n+        self.num_frame += self.extra_ref_frames\n+\n+    def extract_stereo_ref_feat(self, x):\n+        B, N, C, imH, imW = x.shape\n+        x = x.view(B * N, C, imH, imW)\n+        if isinstance(self.img_backbone, ResNet):\n+            if self.img_backbone.deep_stem:\n+                x = self.img_backbone.stem(x)\n+            else:\n+                x = self.img_backbone.conv1(x)\n+                x = self.img_backbone.norm1(x)\n+                x = self.img_backbone.relu(x)\n+            x = self.img_backbone.maxpool(x)\n+            for i, layer_name in enumerate(self.img_backbone.res_layers):\n+                res_layer = getattr(self.img_backbone, layer_name)\n+                x = res_layer(x)\n+                return x\n+        else:\n+            # x = self.img_backbone.patch_embed(x)\n+            # hw_shape = (\n+            #     self.img_backbone.patch_embed.DH,\n+            #     self.img_backbone.patch_embed.DW,\n+            # )\n+            # if self.img_backbone.use_abs_pos_embed:\n+            #     x = x + self.img_backbone.absolute_pos_embed\n+            # x = self.img_backbone.drop_after_pos(x)\n+\n+            # for i, stage in enumerate(self.img_backbone.stages):\n+            #     x, hw_shape, out, out_hw_shape = stage(x, hw_shape)\n+            #     out = out.view(-1, *out_hw_shape, self.img_backbone.num_features[i])\n+            #     out = out.permute(0, 3, 1, 2).contiguous()\n+            #     return out\n+            \n+            #======================\n+            # x = self.img_backbone.patch_embed(x)\n+            # hw_shape = (self.img_backbone.patch_embed.DH,\n+            #             self.img_backbone.patch_embed.DW)\n+            x, hw_shape = self.img_backbone.patch_embed(x)\n+            if self.img_backbone.use_abs_pos_embed:\n+                # x = x + self.img_backbone.absolute_pos_embed\n+                absolute_pos_embed = F.interpolate(\n+                    self.img_backbone.absolute_pos_embed, size=hw_shape, mode=\"bicubic\"\n+                )\n+                x = x + absolute_pos_embed.flatten(2).transpose(1, 2)\n+            x = self.img_backbone.drop_after_pos(x)\n+\n+            for i, stage in enumerate(self.img_backbone.stages):\n+                x, hw_shape, out, out_hw_shape = stage(x, hw_shape)\n+                out = out.view(-1, *out_hw_shape, self.img_backbone.num_features[i])\n+                out = out.permute(0, 3, 1, 2).contiguous()\n+                return out\n+\n+    def prepare_bev_feat(\n+        self,\n+        img,\n+        sensor2keyego,\n+        ego2global,\n+        intrin,\n+        post_rot,\n+        post_tran,\n+        bda,\n+        mlp_input,\n+        feat_prev_iv,\n+        k2s_sensor,\n+        extra_ref_frame,\n+    ):\n+        if extra_ref_frame:\n+            stereo_feat = self.extract_stereo_ref_feat(img)\n+            return None, None, stereo_feat\n+\n+        x, stereo_feat = self.image_encoder(img, stereo=True)\n+        metas = dict(\n+            k2s_sensor=k2s_sensor,\n+            intrins=intrin,\n+            post_rots=post_rot,\n+            post_trans=post_tran,\n+            frustum=self.img_view_transformer.cv_frustum.to(x),\n+            cv_downsample=4,\n+            downsample=self.img_view_transformer.downsample,\n+            grid_config=self.img_view_transformer.grid_config,\n+            cv_feat_list=[feat_prev_iv, stereo_feat],\n+        )\n+        bev_feat, depth = self.img_view_transformer(\n+            [x, sensor2keyego, ego2global, intrin, post_rot, post_tran, bda, mlp_input],\n+            metas,\n+        )\n+        if self.pre_process:\n+            bev_feat = self.pre_process_net(bev_feat)[0]\n+        return bev_feat, depth, stereo_feat\n+\n+    def extract_img_feat(\n+        self, img, img_metas, pred_prev=False, sequential=False, **kwargs\n+    ):\n+\n+        if sequential:\n+            # Todo\n+            assert False\n+\n+        (\n+            imgs,\n+            sensor2keyegos,\n+            ego2globals,\n+            intrins,\n+            post_rots,\n+            post_trans,\n+            bda,\n+            curr2adjsensor,\n+        ) = self.prepare_inputs(img, stereo=True)\n+\n+        \"\"\"Extract features of images.\"\"\"\n+        bev_feat_list = []\n+        depth_key_frame = None\n+        feat_prev_iv = None\n+        for fid in range(self.num_frame - 1, -1, -1):\n+            img, sensor2keyego, ego2global, intrin, post_rot, post_tran = (\n+                imgs[fid],\n+                sensor2keyegos[fid],\n+                ego2globals[fid],\n+                intrins[fid],\n+                post_rots[fid],\n+                post_trans[fid],\n+            )\n+            key_frame = fid == 0\n+            extra_ref_frame = fid == self.num_frame - self.extra_ref_frames\n+            if key_frame or self.with_prev:\n+                if self.align_after_view_transfromation:\n+                    sensor2keyego, ego2global = sensor2keyegos[0], ego2globals[0]\n+                mlp_input = self.img_view_transformer.get_mlp_input(\n+                    sensor2keyegos[0], ego2globals[0], intrin, post_rot, post_tran, bda\n+                )\n+                inputs_curr = (\n+                    img,\n+                    sensor2keyego,\n+                    ego2global,\n+                    intrin,\n+                    post_rot,\n+                    post_tran,\n+                    bda,\n+                    mlp_input,\n+                    feat_prev_iv,\n+                    curr2adjsensor[fid],\n+                    extra_ref_frame,\n+                )\n+                if key_frame:\n+                    bev_feat, depth, feat_curr_iv = self.prepare_bev_feat(*inputs_curr)\n+                    depth_key_frame = depth\n+                else:\n+                    with torch.no_grad():\n+                        bev_feat, depth, feat_curr_iv = self.prepare_bev_feat(\n+                            *inputs_curr\n+                        )\n+                if not extra_ref_frame:\n+                    bev_feat_list.append(bev_feat)\n+                feat_prev_iv = feat_curr_iv\n+        if pred_prev:\n+            # Todo\n+            assert False\n+        if not self.with_prev:\n+            bev_feat_key = bev_feat_list[0]\n+            if len(bev_feat_key.shape) == 4:\n+                b, c, h, w = bev_feat_key.shape\n+                bev_feat_list = [\n+                    torch.zeros(\n+                        [b, c * (self.num_frame - self.extra_ref_frames - 1), h, w]\n+                    ).to(bev_feat_key),\n+                    bev_feat_key,\n+                ]\n+            else:\n+                b, c, z, h, w = bev_feat_key.shape\n+                bev_feat_list = [\n+                    torch.zeros(\n+                        [b, c * (self.num_frame - self.extra_ref_frames - 1), z, h, w]\n+                    ).to(bev_feat_key),\n+                    bev_feat_key,\n+                ]\n+        if self.align_after_view_transfromation:\n+            for adj_id in range(self.num_frame - 2):\n+                bev_feat_list[adj_id] = self.shift_feature(\n+                    bev_feat_list[adj_id],\n+                    [sensor2keyegos[0], sensor2keyegos[self.num_frame - 2 - adj_id]],\n+                    bda,\n+                )\n+        bev_feat = torch.cat(bev_feat_list, dim=1)\n+        x = self.bev_encoder(bev_feat)\n+        return [x], depth_key_frame\n"
                },
                {
                    "date": 1716791525170,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -816,823 +816,4 @@\n                 )\n         bev_feat = torch.cat(bev_feat_list, dim=1)\n         x = self.bev_encoder(bev_feat)\n         return [x], depth_key_frame\n-# Copyright (c) Phigent Robotics. All rights reserved.\n-import torch\n-import torch.nn.functional as F\n-from mmcv.runner import force_fp32\n-\n-from mmdet3d.ops.bev_pool_v2.bev_pool import TRTBEVPoolv2\n-from mmdet.models import DETECTORS\n-from .. import builder\n-from .centerpoint import CenterPoint\n-from mmdet3d.models.utils.grid_mask import GridMask\n-from mmdet.models.backbones.resnet import ResNet\n-from termcolor import colored\n-from mmdet.utils import get_root_logger\n-from mmdet3d.models.utils.self_print import feats_to_img\n-\n-\n-@DETECTORS.register_module()\n-class BEVDet(CenterPoint):\n-    r\"\"\"BEVDet paradigm for multi-camera 3D object detection.\n-\n-    Please refer to the `paper <https://arxiv.org/abs/2112.11790>`_\n-\n-    Args:\n-        img_view_transformer (dict): Configuration dict of view transformer.\n-        img_bev_encoder_backbone (dict): Configuration dict of the BEV encoder\n-            backbone.\n-        img_bev_encoder_neck (dict): Configuration dict of the BEV encoder neck.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        img_view_transformer,\n-        img_bev_encoder_backbone=None,\n-        img_bev_encoder_neck=None,\n-        use_grid_mask=False,\n-        **kwargs,\n-    ):\n-        super(BEVDet, self).__init__(**kwargs)\n-        self.grid_mask = (\n-            None\n-            if not use_grid_mask\n-            else GridMask(\n-                True, True, rotate=1, offset=False, ratio=0.5, mode=1, prob=0.7\n-            )\n-        )\n-        self.img_view_transformer = builder.build_neck(img_view_transformer)\n-        if img_bev_encoder_neck and img_bev_encoder_backbone:\n-            self.img_bev_encoder_backbone = builder.build_backbone(\n-                img_bev_encoder_backbone\n-            )\n-            self.img_bev_encoder_neck = builder.build_neck(img_bev_encoder_neck)\n-\n-    def image_encoder(self, img, stereo=False):\n-        imgs = img\n-        B, N, C, imH, imW = imgs.shape\n-        imgs = imgs.view(B * N, C, imH, imW)\n-        if self.grid_mask is not None:\n-            imgs = self.grid_mask(imgs)\n-        x = self.img_backbone(imgs)\n-        stereo_feat = None\n-        if stereo:\n-            stereo_feat = x[0]\n-            x = x[1:]\n-        if self.with_img_neck:\n-            x = self.img_neck(x)\n-            if type(x) in [list, tuple]:\n-                x = x[0]\n-        _, output_dim, ouput_H, output_W = x.shape\n-        x = x.view(B, N, output_dim, ouput_H, output_W)\n-        return x, stereo_feat\n-\n-    @force_fp32()\n-    def bev_encoder(self, x):\n-        x = self.img_bev_encoder_backbone(x)\n-        x = self.img_bev_encoder_neck(x)\n-        if type(x) in [list, tuple]:\n-            x = x[0]\n-        return x\n-\n-    def prepare_inputs(self, inputs):\n-        # split the inputs into each frame\n-        assert len(inputs) == 7\n-        B, N, C, H, W = inputs[0].shape\n-        imgs, sensor2egos, ego2globals, intrins, post_rots, post_trans, bda = inputs\n-\n-        sensor2egos = sensor2egos.view(B, N, 4, 4)\n-        ego2globals = ego2globals.view(B, N, 4, 4)\n-\n-        # calculate the transformation from sweep sensor to key ego\n-        keyego2global = ego2globals[:, 0, ...].unsqueeze(1)\n-        global2keyego = torch.inverse(keyego2global.double())\n-        sensor2keyegos = global2keyego @ ego2globals.double() @ sensor2egos.double()\n-        sensor2keyegos = sensor2keyegos.float()\n-\n-        return [imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, bda]\n-\n-    def extract_img_feat(self, img, img_metas, **kwargs):\n-        \"\"\"Extract features of images.\"\"\"\n-        img = self.prepare_inputs(img)\n-        x, _ = self.image_encoder(img[0])\n-        x, depth = self.img_view_transformer([x] + img[1:7])\n-        x = self.bev_encoder(x)\n-        return [x], depth\n-\n-    def extract_feat(self, points, img, img_metas, **kwargs):\n-        \"\"\"Extract features from images and points.\"\"\"\n-        img_feats, depth = self.extract_img_feat(img, img_metas, **kwargs)\n-        pts_feats = None\n-        return (img_feats, pts_feats, depth)\n-\n-    def forward_train(\n-        self,\n-        points=None,\n-        img_metas=None,\n-        gt_bboxes_3d=None,\n-        gt_labels_3d=None,\n-        gt_labels=None,\n-        gt_bboxes=None,\n-        img_inputs=None,\n-        proposals=None,\n-        gt_bboxes_ignore=None,\n-        **kwargs,\n-    ):\n-        \"\"\"Forward training function.\n-\n-        Args:\n-            points (list[torch.Tensor], optional): Points of each sample.\n-                Defaults to None.\n-            img_metas (list[dict], optional): Meta information of each sample.\n-                Defaults to None.\n-            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`], optional):\n-                Ground truth 3D boxes. Defaults to None.\n-            gt_labels_3d (list[torch.Tensor], optional): Ground truth labels\n-                of 3D boxes. Defaults to None.\n-            gt_labels (list[torch.Tensor], optional): Ground truth labels\n-                of 2D boxes in images. Defaults to None.\n-            gt_bboxes (list[torch.Tensor], optional): Ground truth 2D boxes in\n-                images. Defaults to None.\n-            img (torch.Tensor optional): Images of each sample with shape\n-                (N, C, H, W). Defaults to None.\n-            proposals ([list[torch.Tensor], optional): Predicted proposals\n-                used for training Fast RCNN. Defaults to None.\n-            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n-                2D boxes in images to be ignored. Defaults to None.\n-\n-        Returns:\n-            dict: Losses of different branches.\n-        \"\"\"\n-        img_feats, pts_feats, _ = self.extract_feat(\n-            points, img=img_inputs, img_metas=img_metas, **kwargs\n-        )\n-        losses = dict()\n-        losses_pts = self.forward_pts_train(\n-            img_feats, gt_bboxes_3d, gt_labels_3d, img_metas, gt_bboxes_ignore\n-        )\n-        losses.update(losses_pts)\n-        return losses\n-\n-    def forward_test(self, points=None, img_metas=None, img_inputs=None, **kwargs):\n-        \"\"\"\n-        Args:\n-            points (list[torch.Tensor]): the outer list indicates test-time\n-                augmentations and inner torch.Tensor should have a shape NxC,\n-                which contains all points in the batch.\n-            img_metas (list[list[dict]]): the outer list indicates test-time\n-                augs (multiscale, flip, etc.) and the inner list indicates\n-                images in a batch\n-            img (list[torch.Tensor], optional): the outer\n-                list indicates test-time augmentations and inner\n-                torch.Tensor should have a shape NxCxHxW, which contains\n-                all images in the batch. Defaults to None.\n-        \"\"\"\n-        for var, name in [(img_inputs, \"img_inputs\"), (img_metas, \"img_metas\")]:\n-            if not isinstance(var, list):\n-                raise TypeError(\"{} must be a list, but got {}\".format(name, type(var)))\n-\n-        num_augs = len(img_inputs)\n-        if num_augs != len(img_metas):\n-            raise ValueError(\n-                \"num of augmentations ({}) != num of image meta ({})\".format(\n-                    len(img_inputs), len(img_metas)\n-                )\n-            )\n-\n-        if not isinstance(img_inputs[0][0], list):\n-            img_inputs = [img_inputs] if img_inputs is None else img_inputs\n-            points = [points] if points is None else points\n-            return self.simple_test(points[0], img_metas[0], img_inputs[0], **kwargs)\n-        else:\n-            return self.aug_test(None, img_metas[0], img_inputs[0], **kwargs)\n-\n-    def aug_test(self, points, img_metas, img=None, rescale=False):\n-        \"\"\"Test function without augmentaiton.\"\"\"\n-        assert False\n-\n-    def simple_test(self, points, img_metas, img=None, rescale=False, **kwargs):\n-        \"\"\"Test function without augmentaiton.\"\"\"\n-        img_feats, _, _ = self.extract_feat(\n-            points, img=img, img_metas=img_metas, **kwargs\n-        )\n-\n-        # base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n-        # feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n-\n-        bbox_list = [dict() for _ in range(len(img_metas))]\n-        bbox_pts = self.simple_test_pts(img_feats, img_metas, rescale=rescale)\n-        for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n-            result_dict[\"pts_bbox\"] = pts_bbox\n-        return bbox_list\n-\n-    def forward_dummy(self, points=None, img_metas=None, img_inputs=None, **kwargs):\n-        img_feats, _, _ = self.extract_feat(\n-            points, img=img_inputs, img_metas=img_metas, **kwargs\n-        )\n-        assert self.with_pts_bbox\n-        outs = self.pts_bbox_head(img_feats)\n-        return outs\n-\n-\n-@DETECTORS.register_module()\n-class BEVDetTRT(BEVDet):\n-\n-    def result_serialize(self, outs):\n-        outs_ = []\n-        for out in outs:\n-            for key in [\"reg\", \"height\", \"dim\", \"rot\", \"vel\", \"heatmap\"]:\n-                outs_.append(out[0][key])\n-        return outs_\n-\n-    def result_deserialize(self, outs):\n-        outs_ = []\n-        keys = [\"reg\", \"height\", \"dim\", \"rot\", \"vel\", \"heatmap\"]\n-        for head_id in range(len(outs) // 6):\n-            outs_head = [dict()]\n-            for kid, key in enumerate(keys):\n-                outs_head[0][key] = outs[head_id * 6 + kid]\n-            outs_.append(outs_head)\n-        return outs_\n-\n-    def forward(\n-        self,\n-        img,\n-        ranks_depth,\n-        ranks_feat,\n-        ranks_bev,\n-        interval_starts,\n-        interval_lengths,\n-    ):\n-        x = self.img_backbone(img)\n-        x = self.img_neck(x)\n-        x = self.img_view_transformer.depth_net(x)\n-        depth = x[:, : self.img_view_transformer.D].softmax(dim=1)\n-        tran_feat = x[\n-            :,\n-            self.img_view_transformer.D : (\n-                self.img_view_transformer.D + self.img_view_transformer.out_channels\n-            ),\n-        ]\n-        tran_feat = tran_feat.permute(0, 2, 3, 1)\n-        x = TRTBEVPoolv2.apply(\n-            depth.contiguous(),\n-            tran_feat.contiguous(),\n-            ranks_depth,\n-            ranks_feat,\n-            ranks_bev,\n-            interval_starts,\n-            interval_lengths,\n-        )\n-        x = x.permute(0, 3, 1, 2).contiguous()\n-        bev_feat = self.bev_encoder(x)\n-        outs = self.pts_bbox_head([bev_feat])\n-        outs = self.result_serialize(outs)\n-        return outs\n-\n-    def get_bev_pool_input(self, input):\n-        input = self.prepare_inputs(input)\n-        coor = self.img_view_transformer.get_lidar_coor(*input[1:7])\n-        return self.img_view_transformer.voxel_pooling_prepare_v2(coor)\n-\n-\n-@DETECTORS.register_module()\n-class BEVDet4D(BEVDet):\n-    r\"\"\"BEVDet4D paradigm for multi-camera 3D object detection.\n-\n-    Please refer to the `paper <https://arxiv.org/abs/2203.17054>`_\n-\n-    Args:\n-        pre_process (dict | None): Configuration dict of BEV pre-process net.\n-        align_after_view_transfromation (bool): Whether to align the BEV\n-            Feature after view transformation. By default, the BEV feature of\n-            the previous frame is aligned during the view transformation.\n-        num_adj (int): Number of adjacent frames.\n-        with_prev (bool): Whether to set the BEV feature of previous frame as\n-            all zero. By default, False.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        pre_process=None,\n-        align_after_view_transfromation=False,\n-        num_adj=1,\n-        with_prev=True,\n-        **kwargs,\n-    ):\n-        super(BEVDet4D, self).__init__(**kwargs)\n-        self.pre_process = pre_process is not None\n-        if self.pre_process:\n-            self.pre_process_net = builder.build_backbone(pre_process)\n-        self.align_after_view_transfromation = align_after_view_transfromation\n-        self.num_frame = num_adj + 1\n-\n-        self.with_prev = with_prev\n-        self.grid = None\n-\n-    def gen_grid(self, input, sensor2keyegos, bda, bda_adj=None):\n-        n, c, h, w = input.shape\n-        _, v, _, _ = sensor2keyegos[0].shape\n-        if self.grid is None:\n-            # generate grid\n-            xs = (\n-                torch.linspace(0, w - 1, w, dtype=input.dtype, device=input.device)\n-                .view(1, w)\n-                .expand(h, w)\n-            )\n-            ys = (\n-                torch.linspace(0, h - 1, h, dtype=input.dtype, device=input.device)\n-                .view(h, 1)\n-                .expand(h, w)\n-            )\n-            grid = torch.stack((xs, ys, torch.ones_like(xs)), -1)\n-            self.grid = grid\n-        else:\n-            grid = self.grid\n-        grid = grid.view(1, h, w, 3).expand(n, h, w, 3).view(n, h, w, 3, 1)\n-\n-        # get transformation from current ego frame to adjacent ego frame\n-        # transformation from current camera frame to current ego frame\n-        c02l0 = sensor2keyegos[0][:, 0:1, :, :]\n-\n-        # transformation from adjacent camera frame to current ego frame\n-        c12l0 = sensor2keyegos[1][:, 0:1, :, :]\n-\n-        # add bev data augmentation\n-        bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n-        bda_[:, :, :3, :3] = bda.unsqueeze(1)[\n-            :, :, :3, :3\n-        ]  # change for diff, before change is \"bda.unsqueeze(1)\"\n-        bda_[:, :, 3, 3] = 1\n-        c02l0 = bda_.matmul(c02l0)\n-        if bda_adj is not None:\n-            bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n-            bda_[:, :, :3, :3] = bda_adj.unsqueeze(1)\n-            bda_[:, :, 3, 3] = 1\n-        c12l0 = bda_.matmul(c12l0)\n-\n-        # transformation from current ego frame to adjacent ego frame\n-        l02l1 = c02l0.matmul(torch.inverse(c12l0))[:, 0, :, :].view(n, 1, 1, 4, 4)\n-        \"\"\"\n-          c02l0 * inv(c12l0)\n-        = c02l0 * inv(l12l0 * c12l1)\n-        = c02l0 * inv(c12l1) * inv(l12l0)\n-        = l02l1 # c02l0==c12l1\n-        \"\"\"\n-\n-        l02l1 = l02l1[:, :, :, [True, True, False, True], :][\n-            :, :, :, :, [True, True, False, True]\n-        ]\n-\n-        feat2bev = torch.zeros((3, 3), dtype=grid.dtype).to(grid)\n-        feat2bev[0, 0] = self.img_view_transformer.grid_interval[0]\n-        feat2bev[1, 1] = self.img_view_transformer.grid_interval[1]\n-        feat2bev[0, 2] = self.img_view_transformer.grid_lower_bound[0]\n-        feat2bev[1, 2] = self.img_view_transformer.grid_lower_bound[1]\n-        feat2bev[2, 2] = 1\n-        feat2bev = feat2bev.view(1, 3, 3)\n-        tf = torch.inverse(feat2bev).matmul(l02l1).matmul(feat2bev)\n-\n-        # transform and normalize\n-        grid = tf.matmul(grid)\n-        normalize_factor = torch.tensor(\n-            [w - 1.0, h - 1.0], dtype=input.dtype, device=input.device\n-        )\n-        grid = grid[:, :, :, :2, 0] / normalize_factor.view(1, 1, 1, 2) * 2.0 - 1.0\n-        return grid\n-\n-    @force_fp32()\n-    def shift_feature(self, input, sensor2keyegos, bda, bda_adj=None):\n-        grid = self.gen_grid(input, sensor2keyegos, bda, bda_adj=bda_adj)\n-        output = F.grid_sample(input, grid.to(input.dtype), align_corners=True)\n-        return output\n-\n-    def prepare_bev_feat(\n-        self, img, rot, tran, intrin, post_rot, post_tran, bda, mlp_input\n-    ):\n-        x, _ = self.image_encoder(img)\n-        bev_feat, depth = self.img_view_transformer(\n-            [x, rot, tran, intrin, post_rot, post_tran, bda, mlp_input]\n-        )\n-        if self.pre_process:\n-            bev_feat = self.pre_process_net(bev_feat)[0]\n-        return bev_feat, depth\n-\n-    def extract_img_feat_sequential(self, inputs, feat_prev):\n-        imgs, sensor2keyegos_curr, ego2globals_curr, intrins = inputs[:4]\n-        sensor2keyegos_prev, _, post_rots, post_trans, bda = inputs[4:]\n-        bev_feat_list = []\n-        mlp_input = self.img_view_transformer.get_mlp_input(\n-            sensor2keyegos_curr[0:1, ...],\n-            ego2globals_curr[0:1, ...],\n-            intrins,\n-            post_rots,\n-            post_trans,\n-            bda[0:1, ...],\n-        )\n-        inputs_curr = (\n-            imgs,\n-            sensor2keyegos_curr[0:1, ...],\n-            ego2globals_curr[0:1, ...],\n-            intrins,\n-            post_rots,\n-            post_trans,\n-            bda[0:1, ...],\n-            mlp_input,\n-        )\n-        bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n-        bev_feat_list.append(bev_feat)\n-\n-        # align the feat_prev\n-        _, C, H, W = feat_prev.shape\n-        feat_prev = self.shift_feature(\n-            feat_prev, [sensor2keyegos_curr, sensor2keyegos_prev], bda\n-        )\n-        bev_feat_list.append(feat_prev.view(1, (self.num_frame - 1) * C, H, W))\n-\n-        bev_feat = torch.cat(bev_feat_list, dim=1)\n-        x = self.bev_encoder(bev_feat)\n-        return [x], depth\n-\n-    def prepare_inputs(self, inputs, stereo=False):\n-        # split the inputs into each frame\n-        B, N, C, H, W = inputs[0].shape\n-        N = N // self.num_frame\n-        imgs = inputs[0].view(B, N, self.num_frame, C, H, W)\n-        imgs = torch.split(imgs, 1, 2)\n-        imgs = [t.squeeze(2) for t in imgs]\n-        sensor2egos, ego2globals, intrins, post_rots, post_trans, bda = inputs[1:7]\n-\n-        sensor2egos = sensor2egos.view(B, self.num_frame, N, 4, 4)\n-        ego2globals = ego2globals.view(B, self.num_frame, N, 4, 4)\n-\n-        # calculate the transformation from sweep sensor to key ego\n-        keyego2global = ego2globals[:, 0, 0, ...].unsqueeze(1).unsqueeze(1)\n-        global2keyego = torch.inverse(keyego2global.double())\n-        sensor2keyegos = global2keyego @ ego2globals.double() @ sensor2egos.double()\n-        sensor2keyegos = sensor2keyegos.float()\n-\n-        curr2adjsensor = None\n-        if stereo:\n-            sensor2egos_cv, ego2globals_cv = sensor2egos, ego2globals\n-            sensor2egos_curr = sensor2egos_cv[:, : self.temporal_frame, ...].double()\n-            ego2globals_curr = ego2globals_cv[:, : self.temporal_frame, ...].double()\n-            sensor2egos_adj = sensor2egos_cv[\n-                :, 1 : self.temporal_frame + 1, ...\n-            ].double()\n-            ego2globals_adj = ego2globals_cv[\n-                :, 1 : self.temporal_frame + 1, ...\n-            ].double()\n-            curr2adjsensor = (\n-                torch.inverse(ego2globals_adj @ sensor2egos_adj)\n-                @ ego2globals_curr\n-                @ sensor2egos_curr\n-            )\n-            curr2adjsensor = curr2adjsensor.float()\n-            curr2adjsensor = torch.split(curr2adjsensor, 1, 1)\n-            curr2adjsensor = [p.squeeze(1) for p in curr2adjsensor]\n-            curr2adjsensor.extend([None for _ in range(self.extra_ref_frames)])\n-            assert len(curr2adjsensor) == self.num_frame\n-\n-        extra = [\n-            sensor2keyegos,\n-            ego2globals,\n-            intrins.view(B, self.num_frame, N, 3, 3),\n-            post_rots.view(B, self.num_frame, N, 3, 3),\n-            post_trans.view(B, self.num_frame, N, 3),\n-        ]\n-        extra = [torch.split(t, 1, 1) for t in extra]\n-        extra = [[p.squeeze(1) for p in t] for t in extra]\n-        sensor2keyegos, ego2globals, intrins, post_rots, post_trans = extra\n-        return (\n-            imgs,\n-            sensor2keyegos,\n-            ego2globals,\n-            intrins,\n-            post_rots,\n-            post_trans,\n-            bda,\n-            curr2adjsensor,\n-        )\n-\n-    def extract_img_feat(\n-        self, img, img_metas, pred_prev=False, sequential=False, **kwargs\n-    ):\n-        if sequential:\n-            return self.extract_img_feat_sequential(img, kwargs[\"feat_prev\"])\n-        imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, bda, _ = (\n-            self.prepare_inputs(img)\n-        )\n-        \"\"\"Extract features of images.\"\"\"\n-        bev_feat_list = []\n-        depth_list = []\n-        key_frame = True  # back propagation for key frame only\n-        for img, sensor2keyego, ego2global, intrin, post_rot, post_tran in zip(\n-            imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans\n-        ):\n-            if key_frame or self.with_prev:\n-                if self.align_after_view_transfromation:\n-                    sensor2keyego, ego2global = sensor2keyegos[0], ego2globals[0]\n-                mlp_input = self.img_view_transformer.get_mlp_input(\n-                    sensor2keyegos[0], ego2globals[0], intrin, post_rot, post_tran, bda\n-                )\n-                inputs_curr = (\n-                    img,\n-                    sensor2keyego,\n-                    ego2global,\n-                    intrin,\n-                    post_rot,\n-                    post_tran,\n-                    bda,\n-                    mlp_input,\n-                )\n-                if key_frame:\n-                    bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n-                else:\n-                    with torch.no_grad():\n-                        bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n-            else:\n-                bev_feat = torch.zeros_like(bev_feat_list[0])\n-                depth = None\n-            bev_feat_list.append(bev_feat)\n-            depth_list.append(depth)\n-            key_frame = False\n-        if pred_prev:\n-            assert self.align_after_view_transfromation\n-            assert sensor2keyegos[0].shape[0] == 1\n-            feat_prev = torch.cat(bev_feat_list[1:], dim=0)\n-            ego2globals_curr = ego2globals[0].repeat(self.num_frame - 1, 1, 1, 1)\n-            sensor2keyegos_curr = sensor2keyegos[0].repeat(self.num_frame - 1, 1, 1, 1)\n-            ego2globals_prev = torch.cat(ego2globals[1:], dim=0)\n-            sensor2keyegos_prev = torch.cat(sensor2keyegos[1:], dim=0)\n-            bda_curr = bda.repeat(self.num_frame - 1, 1, 1)\n-            return feat_prev, [\n-                imgs[0],\n-                sensor2keyegos_curr,\n-                ego2globals_curr,\n-                intrins[0],\n-                sensor2keyegos_prev,\n-                ego2globals_prev,\n-                post_rots[0],\n-                post_trans[0],\n-                bda_curr,\n-            ]\n-        if self.align_after_view_transfromation:\n-            for adj_id in range(1, self.num_frame):\n-                bev_feat_list[adj_id] = self.shift_feature(\n-                    bev_feat_list[adj_id],\n-                    [sensor2keyegos[0], sensor2keyegos[adj_id]],\n-                    bda,\n-                )\n-        bev_feat = torch.cat(bev_feat_list, dim=1)\n-        x = self.bev_encoder(bev_feat)\n-        return [x], depth_list[0]\n-\n-\n-@DETECTORS.register_module()\n-class BEVDepth4D(BEVDet4D):\n-\n-    def forward_train(\n-        self,\n-        points=None,\n-        img_metas=None,\n-        gt_bboxes_3d=None,\n-        gt_labels_3d=None,\n-        gt_labels=None,\n-        gt_bboxes=None,\n-        img_inputs=None,\n-        proposals=None,\n-        gt_bboxes_ignore=None,\n-        **kwargs,\n-    ):\n-        \"\"\"Forward training function.\n-\n-        Args:\n-            points (list[torch.Tensor], optional): Points of each sample.\n-                Defaults to None.\n-            img_metas (list[dict], optional): Meta information of each sample.\n-                Defaults to None.\n-            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`], optional):\n-                Ground truth 3D boxes. Defaults to None.\n-            gt_labels_3d (list[torch.Tensor], optional): Ground truth labels\n-                of 3D boxes. Defaults to None.\n-            gt_labels (list[torch.Tensor], optional): Ground truth labels\n-                of 2D boxes in images. Defaults to None.\n-            gt_bboxes (list[torch.Tensor], optional): Ground truth 2D boxes in\n-                images. Defaults to None.\n-            img (torch.Tensor optional): Images of each sample with shape\n-                (N, C, H, W). Defaults to None.\n-            proposals ([list[torch.Tensor], optional): Predicted proposals\n-                used for training Fast RCNN. Defaults to None.\n-            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n-                2D boxes in images to be ignored. Defaults to None.\n-\n-        Returns:\n-            dict: Losses of different branches.\n-        \"\"\"\n-        img_feats, pts_feats, depth = self.extract_feat(\n-            points, img=img_inputs, img_metas=img_metas, **kwargs\n-        )\n-\n-        gt_depth = kwargs[\"gt_depth\"]\n-        loss_depth = self.img_view_transformer.get_depth_loss(gt_depth, depth)\n-        losses = dict(loss_depth=loss_depth)\n-        losses_pts = self.forward_pts_train(\n-            img_feats, gt_bboxes_3d, gt_labels_3d, img_metas, gt_bboxes_ignore\n-        )\n-        losses.update(losses_pts)\n-        return losses\n-\n-\n-@DETECTORS.register_module()\n-class BEVStereo4D(BEVDepth4D):\n-    def __init__(self, **kwargs):\n-        super(BEVStereo4D, self).__init__(**kwargs)\n-        self.extra_ref_frames = 1\n-        self.temporal_frame = self.num_frame\n-        self.num_frame += self.extra_ref_frames\n-\n-    def extract_stereo_ref_feat(self, x):\n-        B, N, C, imH, imW = x.shape\n-        x = x.view(B * N, C, imH, imW)\n-        if isinstance(self.img_backbone, ResNet):\n-            if self.img_backbone.deep_stem:\n-                x = self.img_backbone.stem(x)\n-            else:\n-                x = self.img_backbone.conv1(x)\n-                x = self.img_backbone.norm1(x)\n-                x = self.img_backbone.relu(x)\n-            x = self.img_backbone.maxpool(x)\n-            for i, layer_name in enumerate(self.img_backbone.res_layers):\n-                res_layer = getattr(self.img_backbone, layer_name)\n-                x = res_layer(x)\n-                return x\n-        else:\n-            # x = self.img_backbone.patch_embed(x)\n-            # hw_shape = (\n-            #     self.img_backbone.patch_embed.DH,\n-            #     self.img_backbone.patch_embed.DW,\n-            # )\n-            # if self.img_backbone.use_abs_pos_embed:\n-            #     x = x + self.img_backbone.absolute_pos_embed\n-            # x = self.img_backbone.drop_after_pos(x)\n-\n-            # for i, stage in enumerate(self.img_backbone.stages):\n-            #     x, hw_shape, out, out_hw_shape = stage(x, hw_shape)\n-            #     out = out.view(-1, *out_hw_shape, self.img_backbone.num_features[i])\n-            #     out = out.permute(0, 3, 1, 2).contiguous()\n-            #     return out\n-            \n-            #======================\n-                        # x = self.img_backbone.patch_embed(x)\n-            # hw_shape = (self.img_backbone.patch_embed.DH,\n-            #             self.img_backbone.patch_embed.DW)\n-            x, hw_shape = self.img_backbone.patch_embed(x)\n-            if self.img_backbone.use_abs_pos_embed:\n-                # x = x + self.img_backbone.absolute_pos_embed\n-                absolute_pos_embed = F.interpolate(\n-                    self.img_backbone.absolute_pos_embed, size=hw_shape, mode=\"bicubic\"\n-                )\n-                x = x + absolute_pos_embed.flatten(2).transpose(1, 2)\n-            x = self.img_backbone.drop_after_pos(x)\n-\n-            for i, stage in enumerate(self.img_backbone.stages):\n-                x, hw_shape, out, out_hw_shape = stage(x, hw_shape)\n-                out = out.view(-1, *out_hw_shape, self.img_backbone.num_features[i])\n-                out = out.permute(0, 3, 1, 2).contiguous()\n-                return out\n-\n-    def prepare_bev_feat(\n-        self,\n-        img,\n-        sensor2keyego,\n-        ego2global,\n-        intrin,\n-        post_rot,\n-        post_tran,\n-        bda,\n-        mlp_input,\n-        feat_prev_iv,\n-        k2s_sensor,\n-        extra_ref_frame,\n-    ):\n-        if extra_ref_frame:\n-            stereo_feat = self.extract_stereo_ref_feat(img)\n-            return None, None, stereo_feat\n-\n-        x, stereo_feat = self.image_encoder(img, stereo=True)\n-        metas = dict(\n-            k2s_sensor=k2s_sensor,\n-            intrins=intrin,\n-            post_rots=post_rot,\n-            post_trans=post_tran,\n-            frustum=self.img_view_transformer.cv_frustum.to(x),\n-            cv_downsample=4,\n-            downsample=self.img_view_transformer.downsample,\n-            grid_config=self.img_view_transformer.grid_config,\n-            cv_feat_list=[feat_prev_iv, stereo_feat],\n-        )\n-        bev_feat, depth = self.img_view_transformer(\n-            [x, sensor2keyego, ego2global, intrin, post_rot, post_tran, bda, mlp_input],\n-            metas,\n-        )\n-        if self.pre_process:\n-            bev_feat = self.pre_process_net(bev_feat)[0]\n-        return bev_feat, depth, stereo_feat\n-\n-    def extract_img_feat(\n-        self, img, img_metas, pred_prev=False, sequential=False, **kwargs\n-    ):\n-\n-        if sequential:\n-            # Todo\n-            assert False\n-\n-        (\n-            imgs,\n-            sensor2keyegos,\n-            ego2globals,\n-            intrins,\n-            post_rots,\n-            post_trans,\n-            bda,\n-            curr2adjsensor,\n-        ) = self.prepare_inputs(img, stereo=True)\n-\n-        \"\"\"Extract features of images.\"\"\"\n-        bev_feat_list = []\n-        depth_key_frame = None\n-        feat_prev_iv = None\n-        for fid in range(self.num_frame - 1, -1, -1):\n-            img, sensor2keyego, ego2global, intrin, post_rot, post_tran = (\n-                imgs[fid],\n-                sensor2keyegos[fid],\n-                ego2globals[fid],\n-                intrins[fid],\n-                post_rots[fid],\n-                post_trans[fid],\n-            )\n-            key_frame = fid == 0\n-            extra_ref_frame = fid == self.num_frame - self.extra_ref_frames\n-            if key_frame or self.with_prev:\n-                if self.align_after_view_transfromation:\n-                    sensor2keyego, ego2global = sensor2keyegos[0], ego2globals[0]\n-                mlp_input = self.img_view_transformer.get_mlp_input(\n-                    sensor2keyegos[0], ego2globals[0], intrin, post_rot, post_tran, bda\n-                )\n-                inputs_curr = (\n-                    img,\n-                    sensor2keyego,\n-                    ego2global,\n-                    intrin,\n-                    post_rot,\n-                    post_tran,\n-                    bda,\n-                    mlp_input,\n-                    feat_prev_iv,\n-                    curr2adjsensor[fid],\n-                    extra_ref_frame,\n-                )\n-                if key_frame:\n-                    bev_feat, depth, feat_curr_iv = self.prepare_bev_feat(*inputs_curr)\n-                    depth_key_frame = depth\n-                else:\n-                    with torch.no_grad():\n-                        bev_feat, depth, feat_curr_iv = self.prepare_bev_feat(\n-                            *inputs_curr\n-                        )\n-                if not extra_ref_frame:\n-                    bev_feat_list.append(bev_feat)\n-                feat_prev_iv = feat_curr_iv\n-        if pred_prev:\n-            # Todo\n-            assert False\n-        if not self.with_prev:\n-            bev_feat_key = bev_feat_list[0]\n-            if len(bev_feat_key.shape) == 4:\n-                b, c, h, w = bev_feat_key.shape\n-                bev_feat_list = [\n-                    torch.zeros(\n-                        [b, c * (self.num_frame - self.extra_ref_frames - 1), h, w]\n-                    ).to(bev_feat_key),\n-                    bev_feat_key,\n-                ]\n-            else:\n-                b, c, z, h, w = bev_feat_key.shape\n-                bev_feat_list = [\n-                    torch.zeros(\n-                        [b, c * (self.num_frame - self.extra_ref_frames - 1), z, h, w]\n-                    ).to(bev_feat_key),\n-                    bev_feat_key,\n-                ]\n-        if self.align_after_view_transfromation:\n-            for adj_id in range(self.num_frame - 2):\n-                bev_feat_list[adj_id] = self.shift_feature(\n-                    bev_feat_list[adj_id],\n-                    [sensor2keyegos[0], sensor2keyegos[self.num_frame - 2 - adj_id]],\n-                    bda,\n-                )\n-        bev_feat = torch.cat(bev_feat_list, dim=1)\n-        x = self.bev_encoder(bev_feat)\n-        return [x], depth_key_frame\n"
                },
                {
                    "date": 1716815316013,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -501,9 +501,9 @@\n         self, img, img_metas, pred_prev=False, sequential=False, **kwargs\n     ):\n         if sequential:\n             return self.extract_img_feat_sequential(img, kwargs[\"feat_prev\"])\n-        \n+\n         imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, bda, _ = (\n             self.prepare_inputs(img)\n         )\n         \"\"\"Extract features of images.\"\"\"\n@@ -664,10 +664,10 @@\n             #     x, hw_shape, out, out_hw_shape = stage(x, hw_shape)\n             #     out = out.view(-1, *out_hw_shape, self.img_backbone.num_features[i])\n             #     out = out.permute(0, 3, 1, 2).contiguous()\n             #     return out\n-            \n-            #======================\n+\n+            # ======================\n             # x = self.img_backbone.patch_embed(x)\n             # hw_shape = (self.img_backbone.patch_embed.DH,\n             #             self.img_backbone.patch_embed.DW)\n             x, hw_shape = self.img_backbone.patch_embed(x)\n"
                },
                {
                    "date": 1717311361652,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -617,8 +617,9 @@\n         \n         \n         import pdb\n         pdb.set_trace()\n+        \n         img_feats, pts_feats, depth = self.extract_feat(\n             points, img=img_inputs, img_metas=img_metas, **kwargs\n         )\n \n"
                }
            ],
            "date": 1716004338617,
            "name": "Commit-0",
            "content": "# Copyright (c) Phigent Robotics. All rights reserved.\nimport torch\nimport torch.nn.functional as F\nfrom mmcv.runner import force_fp32\n\nfrom mmdet3d.ops.bev_pool_v2.bev_pool import TRTBEVPoolv2\nfrom mmdet.models import DETECTORS\nfrom .. import builder\nfrom .centerpoint import CenterPoint\nfrom mmdet3d.models.utils.grid_mask import GridMask\nfrom mmdet.models.backbones.resnet import ResNet\n\n\n@DETECTORS.register_module()\nclass BEVDet(CenterPoint):\n    r\"\"\"BEVDet paradigm for multi-camera 3D object detection.\n\n    Please refer to the `paper <https://arxiv.org/abs/2112.11790>`_\n\n    Args:\n        img_view_transformer (dict): Configuration dict of view transformer.\n        img_bev_encoder_backbone (dict): Configuration dict of the BEV encoder\n            backbone.\n        img_bev_encoder_neck (dict): Configuration dict of the BEV encoder neck.\n    \"\"\"\n\n    def __init__(self,\n                 img_view_transformer,\n                 img_bev_encoder_backbone=None,\n                 img_bev_encoder_neck=None,\n                 use_grid_mask=False,\n                 **kwargs):\n        super(BEVDet, self).__init__(**kwargs)\n        self.grid_mask = None if not use_grid_mask else \\\n            GridMask(True, True, rotate=1, offset=False, ratio=0.5, mode=1,\n                     prob=0.7)\n        self.img_view_transformer = builder.build_neck(img_view_transformer)\n        if img_bev_encoder_neck and img_bev_encoder_backbone:\n            self.img_bev_encoder_backbone = \\\n                builder.build_backbone(img_bev_encoder_backbone)\n            self.img_bev_encoder_neck = builder.build_neck(img_bev_encoder_neck)\n\n    def image_encoder(self, img, stereo=False):\n        imgs = img\n        B, N, C, imH, imW = imgs.shape\n        imgs = imgs.view(B * N, C, imH, imW)\n        if self.grid_mask is not None:\n            imgs = self.grid_mask(imgs)\n        x = self.img_backbone(imgs)\n        stereo_feat = None\n        if stereo:\n            stereo_feat = x[0]\n            x = x[1:]\n        if self.with_img_neck:\n            x = self.img_neck(x)\n            if type(x) in [list, tuple]:\n                x = x[0]\n        _, output_dim, ouput_H, output_W = x.shape\n        x = x.view(B, N, output_dim, ouput_H, output_W)\n        return x, stereo_feat\n\n    @force_fp32()\n    def bev_encoder(self, x):\n        x = self.img_bev_encoder_backbone(x)\n        x = self.img_bev_encoder_neck(x)\n        if type(x) in [list, tuple]:\n            x = x[0]\n        return x\n\n    def prepare_inputs(self, inputs):\n        # split the inputs into each frame\n        assert len(inputs) == 7\n        B, N, C, H, W = inputs[0].shape\n        imgs, sensor2egos, ego2globals, intrins, post_rots, post_trans, bda = \\\n            inputs\n\n        sensor2egos = sensor2egos.view(B, N, 4, 4)\n        ego2globals = ego2globals.view(B, N, 4, 4)\n\n        # calculate the transformation from sweep sensor to key ego\n        keyego2global = ego2globals[:, 0,  ...].unsqueeze(1)\n        global2keyego = torch.inverse(keyego2global.double())\n        sensor2keyegos = \\\n            global2keyego @ ego2globals.double() @ sensor2egos.double()\n        sensor2keyegos = sensor2keyegos.float()\n\n        return [imgs, sensor2keyegos, ego2globals, intrins,\n                post_rots, post_trans, bda]\n\n    def extract_img_feat(self, img, img_metas, **kwargs):\n        \"\"\"Extract features of images.\"\"\"\n        img = self.prepare_inputs(img)\n        x, _ = self.image_encoder(img[0])\n        x, depth = self.img_view_transformer([x] + img[1:7])\n        x = self.bev_encoder(x)\n        return [x], depth\n\n    def extract_feat(self, points, img, img_metas, **kwargs):\n        \"\"\"Extract features from images and points.\"\"\"\n        img_feats, depth = self.extract_img_feat(img, img_metas, **kwargs)\n        pts_feats = None\n        return (img_feats, pts_feats, depth)\n\n    def forward_train(self,\n                      points=None,\n                      img_metas=None,\n                      gt_bboxes_3d=None,\n                      gt_labels_3d=None,\n                      gt_labels=None,\n                      gt_bboxes=None,\n                      img_inputs=None,\n                      proposals=None,\n                      gt_bboxes_ignore=None,\n                      **kwargs):\n        \"\"\"Forward training function.\n\n        Args:\n            points (list[torch.Tensor], optional): Points of each sample.\n                Defaults to None.\n            img_metas (list[dict], optional): Meta information of each sample.\n                Defaults to None.\n            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`], optional):\n                Ground truth 3D boxes. Defaults to None.\n            gt_labels_3d (list[torch.Tensor], optional): Ground truth labels\n                of 3D boxes. Defaults to None.\n            gt_labels (list[torch.Tensor], optional): Ground truth labels\n                of 2D boxes in images. Defaults to None.\n            gt_bboxes (list[torch.Tensor], optional): Ground truth 2D boxes in\n                images. Defaults to None.\n            img (torch.Tensor optional): Images of each sample with shape\n                (N, C, H, W). Defaults to None.\n            proposals ([list[torch.Tensor], optional): Predicted proposals\n                used for training Fast RCNN. Defaults to None.\n            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n                2D boxes in images to be ignored. Defaults to None.\n\n        Returns:\n            dict: Losses of different branches.\n        \"\"\"\n        img_feats, pts_feats, _ = self.extract_feat(\n            points, img=img_inputs, img_metas=img_metas, **kwargs)\n        losses = dict()\n        losses_pts = self.forward_pts_train(img_feats, gt_bboxes_3d,\n                                            gt_labels_3d, img_metas,\n                                            gt_bboxes_ignore)\n        losses.update(losses_pts)\n        return losses\n\n    def forward_test(self,\n                     points=None,\n                     img_metas=None,\n                     img_inputs=None,\n                     **kwargs):\n        \"\"\"\n        Args:\n            points (list[torch.Tensor]): the outer list indicates test-time\n                augmentations and inner torch.Tensor should have a shape NxC,\n                which contains all points in the batch.\n            img_metas (list[list[dict]]): the outer list indicates test-time\n                augs (multiscale, flip, etc.) and the inner list indicates\n                images in a batch\n            img (list[torch.Tensor], optional): the outer\n                list indicates test-time augmentations and inner\n                torch.Tensor should have a shape NxCxHxW, which contains\n                all images in the batch. Defaults to None.\n        \"\"\"\n        for var, name in [(img_inputs, 'img_inputs'),\n                          (img_metas, 'img_metas')]:\n            if not isinstance(var, list):\n                raise TypeError('{} must be a list, but got {}'.format(\n                    name, type(var)))\n\n        num_augs = len(img_inputs)\n        if num_augs != len(img_metas):\n            raise ValueError(\n                'num of augmentations ({}) != num of image meta ({})'.format(\n                    len(img_inputs), len(img_metas)))\n\n        if not isinstance(img_inputs[0][0], list):\n            img_inputs = [img_inputs] if img_inputs is None else img_inputs\n            points = [points] if points is None else points\n            return self.simple_test(points[0], img_metas[0], img_inputs[0],\n                                    **kwargs)\n        else:\n            return self.aug_test(None, img_metas[0], img_inputs[0], **kwargs)\n\n    def aug_test(self, points, img_metas, img=None, rescale=False):\n        \"\"\"Test function without augmentaiton.\"\"\"\n        assert False\n\n    def simple_test(self,\n                    points,\n                    img_metas,\n                    img=None,\n                    rescale=False,\n                    **kwargs):\n        \"\"\"Test function without augmentaiton.\"\"\"\n        img_feats, _, _ = self.extract_feat(\n            points, img=img, img_metas=img_metas, **kwargs)\n        bbox_list = [dict() for _ in range(len(img_metas))]\n        bbox_pts = self.simple_test_pts(img_feats, img_metas, rescale=rescale)\n        for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n            result_dict['pts_bbox'] = pts_bbox\n        return bbox_list\n\n    def forward_dummy(self,\n                      points=None,\n                      img_metas=None,\n                      img_inputs=None,\n                      **kwargs):\n        img_feats, _, _ = self.extract_feat(\n            points, img=img_inputs, img_metas=img_metas, **kwargs)\n        assert self.with_pts_bbox\n        outs = self.pts_bbox_head(img_feats)\n        return outs\n\n\n@DETECTORS.register_module()\nclass BEVDetTRT(BEVDet):\n\n    def result_serialize(self, outs):\n        outs_ = []\n        for out in outs:\n            for key in ['reg', 'height', 'dim', 'rot', 'vel', 'heatmap']:\n                outs_.append(out[0][key])\n        return outs_\n\n    def result_deserialize(self, outs):\n        outs_ = []\n        keys = ['reg', 'height', 'dim', 'rot', 'vel', 'heatmap']\n        for head_id in range(len(outs) // 6):\n            outs_head = [dict()]\n            for kid, key in enumerate(keys):\n                outs_head[0][key] = outs[head_id * 6 + kid]\n            outs_.append(outs_head)\n        return outs_\n\n    def forward(\n        self,\n        img,\n        ranks_depth,\n        ranks_feat,\n        ranks_bev,\n        interval_starts,\n        interval_lengths,\n    ):\n        x = self.img_backbone(img)\n        x = self.img_neck(x)\n        x = self.img_view_transformer.depth_net(x)\n        depth = x[:, :self.img_view_transformer.D].softmax(dim=1)\n        tran_feat = x[:, self.img_view_transformer.D:(\n            self.img_view_transformer.D +\n            self.img_view_transformer.out_channels)]\n        tran_feat = tran_feat.permute(0, 2, 3, 1)\n        x = TRTBEVPoolv2.apply(depth.contiguous(), tran_feat.contiguous(),\n                               ranks_depth, ranks_feat, ranks_bev,\n                               interval_starts, interval_lengths)\n        x = x.permute(0, 3, 1, 2).contiguous()\n        bev_feat = self.bev_encoder(x)\n        outs = self.pts_bbox_head([bev_feat])\n        outs = self.result_serialize(outs)\n        return outs\n\n    def get_bev_pool_input(self, input):\n        input = self.prepare_inputs(input)\n        coor = self.img_view_transformer.get_lidar_coor(*input[1:7])\n        return self.img_view_transformer.voxel_pooling_prepare_v2(coor)\n\n\n@DETECTORS.register_module()\nclass BEVDet4D(BEVDet):\n    r\"\"\"BEVDet4D paradigm for multi-camera 3D object detection.\n\n    Please refer to the `paper <https://arxiv.org/abs/2203.17054>`_\n\n    Args:\n        pre_process (dict | None): Configuration dict of BEV pre-process net.\n        align_after_view_transfromation (bool): Whether to align the BEV\n            Feature after view transformation. By default, the BEV feature of\n            the previous frame is aligned during the view transformation.\n        num_adj (int): Number of adjacent frames.\n        with_prev (bool): Whether to set the BEV feature of previous frame as\n            all zero. By default, False.\n    \"\"\"\n    def __init__(self,\n                 pre_process=None,\n                 align_after_view_transfromation=False,\n                 num_adj=1,\n                 with_prev=True,\n                 **kwargs):\n        super(BEVDet4D, self).__init__(**kwargs)\n        self.pre_process = pre_process is not None\n        if self.pre_process:\n            self.pre_process_net = builder.build_backbone(pre_process)\n        self.align_after_view_transfromation = align_after_view_transfromation\n        self.num_frame = num_adj + 1\n\n        self.with_prev = with_prev\n        self.grid = None\n\n    def gen_grid(self, input, sensor2keyegos, bda, bda_adj=None):\n        n, c, h, w = input.shape\n        _, v, _, _ = sensor2keyegos[0].shape\n        if self.grid is None:\n            # generate grid\n            xs = torch.linspace(\n                0, w - 1, w, dtype=input.dtype,\n                device=input.device).view(1, w).expand(h, w)\n            ys = torch.linspace(\n                0, h - 1, h, dtype=input.dtype,\n                device=input.device).view(h, 1).expand(h, w)\n            grid = torch.stack((xs, ys, torch.ones_like(xs)), -1)\n            self.grid = grid\n        else:\n            grid = self.grid\n        grid = grid.view(1, h, w, 3).expand(n, h, w, 3).view(n, h, w, 3, 1)\n\n        # get transformation from current ego frame to adjacent ego frame\n        # transformation from current camera frame to current ego frame\n        c02l0 = sensor2keyegos[0][:, 0:1, :, :]\n\n        # transformation from adjacent camera frame to current ego frame\n        c12l0 = sensor2keyegos[1][:, 0:1, :, :]\n\n        # add bev data augmentation\n        bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n        bda_[:, :, :3, :3] = bda.unsqueeze(1)\n        bda_[:, :, 3, 3] = 1\n        c02l0 = bda_.matmul(c02l0)\n        if bda_adj is not None:\n            bda_ = torch.zeros((n, 1, 4, 4), dtype=grid.dtype).to(grid)\n            bda_[:, :, :3, :3] = bda_adj.unsqueeze(1)\n            bda_[:, :, 3, 3] = 1\n        c12l0 = bda_.matmul(c12l0)\n\n        # transformation from current ego frame to adjacent ego frame\n        l02l1 = c02l0.matmul(torch.inverse(c12l0))[:, 0, :, :].view(\n            n, 1, 1, 4, 4)\n        '''\n          c02l0 * inv(c12l0)\n        = c02l0 * inv(l12l0 * c12l1)\n        = c02l0 * inv(c12l1) * inv(l12l0)\n        = l02l1 # c02l0==c12l1\n        '''\n\n        l02l1 = l02l1[:, :, :,\n                      [True, True, False, True], :][:, :, :, :,\n                                                    [True, True, False, True]]\n\n        feat2bev = torch.zeros((3, 3), dtype=grid.dtype).to(grid)\n        feat2bev[0, 0] = self.img_view_transformer.grid_interval[0]\n        feat2bev[1, 1] = self.img_view_transformer.grid_interval[1]\n        feat2bev[0, 2] = self.img_view_transformer.grid_lower_bound[0]\n        feat2bev[1, 2] = self.img_view_transformer.grid_lower_bound[1]\n        feat2bev[2, 2] = 1\n        feat2bev = feat2bev.view(1, 3, 3)\n        tf = torch.inverse(feat2bev).matmul(l02l1).matmul(feat2bev)\n\n        # transform and normalize\n        grid = tf.matmul(grid)\n        normalize_factor = torch.tensor([w - 1.0, h - 1.0],\n                                        dtype=input.dtype,\n                                        device=input.device)\n        grid = grid[:, :, :, :2, 0] / normalize_factor.view(1, 1, 1,\n                                                            2) * 2.0 - 1.0\n        return grid\n\n    @force_fp32()\n    def shift_feature(self, input, sensor2keyegos, bda, bda_adj=None):\n        grid = self.gen_grid(input, sensor2keyegos, bda, bda_adj=bda_adj)\n        output = F.grid_sample(input, grid.to(input.dtype), align_corners=True)\n        return output\n\n    def prepare_bev_feat(self, img, rot, tran, intrin, post_rot, post_tran,\n                         bda, mlp_input):\n        x, _ = self.image_encoder(img)\n        bev_feat, depth = self.img_view_transformer(\n            [x, rot, tran, intrin, post_rot, post_tran, bda, mlp_input])\n        if self.pre_process:\n            bev_feat = self.pre_process_net(bev_feat)[0]\n        return bev_feat, depth\n\n    def extract_img_feat_sequential(self, inputs, feat_prev):\n        imgs, sensor2keyegos_curr, ego2globals_curr, intrins = inputs[:4]\n        sensor2keyegos_prev, _, post_rots, post_trans, bda = inputs[4:]\n        bev_feat_list = []\n        mlp_input = self.img_view_transformer.get_mlp_input(\n            sensor2keyegos_curr[0:1, ...], ego2globals_curr[0:1, ...],\n            intrins, post_rots, post_trans, bda[0:1, ...])\n        inputs_curr = (imgs, sensor2keyegos_curr[0:1, ...],\n                       ego2globals_curr[0:1, ...], intrins, post_rots,\n                       post_trans, bda[0:1, ...], mlp_input)\n        bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n        bev_feat_list.append(bev_feat)\n\n        # align the feat_prev\n        _, C, H, W = feat_prev.shape\n        feat_prev = \\\n            self.shift_feature(feat_prev,\n                               [sensor2keyegos_curr, sensor2keyegos_prev],\n                               bda)\n        bev_feat_list.append(feat_prev.view(1, (self.num_frame - 1) * C, H, W))\n\n        bev_feat = torch.cat(bev_feat_list, dim=1)\n        x = self.bev_encoder(bev_feat)\n        return [x], depth\n\n    def prepare_inputs(self, inputs, stereo=False):\n        # split the inputs into each frame\n        B, N, C, H, W = inputs[0].shape\n        N = N // self.num_frame\n        imgs = inputs[0].view(B, N, self.num_frame, C, H, W)\n        imgs = torch.split(imgs, 1, 2)\n        imgs = [t.squeeze(2) for t in imgs]\n        sensor2egos, ego2globals, intrins, post_rots, post_trans, bda = \\\n            inputs[1:7]\n\n        sensor2egos = sensor2egos.view(B, self.num_frame, N, 4, 4)\n        ego2globals = ego2globals.view(B, self.num_frame, N, 4, 4)\n\n        # calculate the transformation from sweep sensor to key ego\n        keyego2global = ego2globals[:, 0, 0, ...].unsqueeze(1).unsqueeze(1)\n        global2keyego = torch.inverse(keyego2global.double())\n        sensor2keyegos = \\\n            global2keyego @ ego2globals.double() @ sensor2egos.double()\n        sensor2keyegos = sensor2keyegos.float()\n\n        curr2adjsensor = None\n        if stereo:\n            sensor2egos_cv, ego2globals_cv = sensor2egos, ego2globals\n            sensor2egos_curr = \\\n                sensor2egos_cv[:, :self.temporal_frame, ...].double()\n            ego2globals_curr = \\\n                ego2globals_cv[:, :self.temporal_frame, ...].double()\n            sensor2egos_adj = \\\n                sensor2egos_cv[:, 1:self.temporal_frame + 1, ...].double()\n            ego2globals_adj = \\\n                ego2globals_cv[:, 1:self.temporal_frame + 1, ...].double()\n            curr2adjsensor = \\\n                torch.inverse(ego2globals_adj @ sensor2egos_adj) \\\n                @ ego2globals_curr @ sensor2egos_curr\n            curr2adjsensor = curr2adjsensor.float()\n            curr2adjsensor = torch.split(curr2adjsensor, 1, 1)\n            curr2adjsensor = [p.squeeze(1) for p in curr2adjsensor]\n            curr2adjsensor.extend([None for _ in range(self.extra_ref_frames)])\n            assert len(curr2adjsensor) == self.num_frame\n\n        extra = [\n            sensor2keyegos,\n            ego2globals,\n            intrins.view(B, self.num_frame, N, 3, 3),\n            post_rots.view(B, self.num_frame, N, 3, 3),\n            post_trans.view(B, self.num_frame, N, 3)\n        ]\n        extra = [torch.split(t, 1, 1) for t in extra]\n        extra = [[p.squeeze(1) for p in t] for t in extra]\n        sensor2keyegos, ego2globals, intrins, post_rots, post_trans = extra\n        return imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, \\\n               bda, curr2adjsensor\n\n    def extract_img_feat(self,\n                         img,\n                         img_metas,\n                         pred_prev=False,\n                         sequential=False,\n                         **kwargs):\n        if sequential:\n            return self.extract_img_feat_sequential(img, kwargs['feat_prev'])\n        imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, \\\n        bda, _ = self.prepare_inputs(img)\n        \"\"\"Extract features of images.\"\"\"\n        bev_feat_list = []\n        depth_list = []\n        key_frame = True  # back propagation for key frame only\n        for img, sensor2keyego, ego2global, intrin, post_rot, post_tran in zip(\n                imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans):\n            if key_frame or self.with_prev:\n                if self.align_after_view_transfromation:\n                    sensor2keyego, ego2global = sensor2keyegos[0], ego2globals[0]\n                mlp_input = self.img_view_transformer.get_mlp_input(\n                    sensor2keyegos[0], ego2globals[0], intrin, post_rot, post_tran, bda)\n                inputs_curr = (img, sensor2keyego, ego2global, intrin, post_rot,\n                               post_tran, bda, mlp_input)\n                if key_frame:\n                    bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n                else:\n                    with torch.no_grad():\n                        bev_feat, depth = self.prepare_bev_feat(*inputs_curr)\n            else:\n                bev_feat = torch.zeros_like(bev_feat_list[0])\n                depth = None\n            bev_feat_list.append(bev_feat)\n            depth_list.append(depth)\n            key_frame = False\n        if pred_prev:\n            assert self.align_after_view_transfromation\n            assert sensor2keyegos[0].shape[0] == 1\n            feat_prev = torch.cat(bev_feat_list[1:], dim=0)\n            ego2globals_curr = \\\n                ego2globals[0].repeat(self.num_frame - 1, 1, 1, 1)\n            sensor2keyegos_curr = \\\n                sensor2keyegos[0].repeat(self.num_frame - 1, 1, 1, 1)\n            ego2globals_prev = torch.cat(ego2globals[1:], dim=0)\n            sensor2keyegos_prev = torch.cat(sensor2keyegos[1:], dim=0)\n            bda_curr = bda.repeat(self.num_frame - 1, 1, 1)\n            return feat_prev, [imgs[0],\n                               sensor2keyegos_curr, ego2globals_curr,\n                               intrins[0],\n                               sensor2keyegos_prev, ego2globals_prev,\n                               post_rots[0], post_trans[0],\n                               bda_curr]\n        if self.align_after_view_transfromation:\n            for adj_id in range(1, self.num_frame):\n                bev_feat_list[adj_id] = \\\n                    self.shift_feature(bev_feat_list[adj_id],\n                                       [sensor2keyegos[0],\n                                        sensor2keyegos[adj_id]],\n                                       bda)\n        bev_feat = torch.cat(bev_feat_list, dim=1)\n        x = self.bev_encoder(bev_feat)\n        return [x], depth_list[0]\n\n\n@DETECTORS.register_module()\nclass BEVDepth4D(BEVDet4D):\n\n    def forward_train(self,\n                      points=None,\n                      img_metas=None,\n                      gt_bboxes_3d=None,\n                      gt_labels_3d=None,\n                      gt_labels=None,\n                      gt_bboxes=None,\n                      img_inputs=None,\n                      proposals=None,\n                      gt_bboxes_ignore=None,\n                      **kwargs):\n        \"\"\"Forward training function.\n\n        Args:\n            points (list[torch.Tensor], optional): Points of each sample.\n                Defaults to None.\n            img_metas (list[dict], optional): Meta information of each sample.\n                Defaults to None.\n            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`], optional):\n                Ground truth 3D boxes. Defaults to None.\n            gt_labels_3d (list[torch.Tensor], optional): Ground truth labels\n                of 3D boxes. Defaults to None.\n            gt_labels (list[torch.Tensor], optional): Ground truth labels\n                of 2D boxes in images. Defaults to None.\n            gt_bboxes (list[torch.Tensor], optional): Ground truth 2D boxes in\n                images. Defaults to None.\n            img (torch.Tensor optional): Images of each sample with shape\n                (N, C, H, W). Defaults to None.\n            proposals ([list[torch.Tensor], optional): Predicted proposals\n                used for training Fast RCNN. Defaults to None.\n            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n                2D boxes in images to be ignored. Defaults to None.\n\n        Returns:\n            dict: Losses of different branches.\n        \"\"\"\n        img_feats, pts_feats, depth = self.extract_feat(\n            points, img=img_inputs, img_metas=img_metas, **kwargs)\n        gt_depth = kwargs['gt_depth']\n        loss_depth = self.img_view_transformer.get_depth_loss(gt_depth, depth)\n        losses = dict(loss_depth=loss_depth)\n        losses_pts = self.forward_pts_train(img_feats, gt_bboxes_3d,\n                                            gt_labels_3d, img_metas,\n                                            gt_bboxes_ignore)\n        losses.update(losses_pts)\n        return losses\n\n\n@DETECTORS.register_module()\nclass BEVStereo4D(BEVDepth4D):\n    def __init__(self, **kwargs):\n        super(BEVStereo4D, self).__init__(**kwargs)\n        self.extra_ref_frames = 1\n        self.temporal_frame = self.num_frame\n        self.num_frame += self.extra_ref_frames\n\n    def extract_stereo_ref_feat(self, x):\n        B, N, C, imH, imW = x.shape\n        x = x.view(B * N, C, imH, imW)\n        if isinstance(self.img_backbone,ResNet):\n            if self.img_backbone.deep_stem:\n                x = self.img_backbone.stem(x)\n            else:\n                x = self.img_backbone.conv1(x)\n                x = self.img_backbone.norm1(x)\n                x = self.img_backbone.relu(x)\n            x = self.img_backbone.maxpool(x)\n            for i, layer_name in enumerate(self.img_backbone.res_layers):\n                res_layer = getattr(self.img_backbone, layer_name)\n                x = res_layer(x)\n                return x\n        else:\n            x = self.img_backbone.patch_embed(x)\n            hw_shape = (self.img_backbone.patch_embed.DH,\n                        self.img_backbone.patch_embed.DW)\n            if self.img_backbone.use_abs_pos_embed:\n                x = x + self.img_backbone.absolute_pos_embed\n            x = self.img_backbone.drop_after_pos(x)\n\n            for i, stage in enumerate(self.img_backbone.stages):\n                x, hw_shape, out, out_hw_shape = stage(x, hw_shape)\n                out = out.view(-1,  *out_hw_shape,\n                               self.img_backbone.num_features[i])\n                out = out.permute(0, 3, 1, 2).contiguous()\n                return out\n\n    def prepare_bev_feat(self, img, sensor2keyego, ego2global, intrin,\n                         post_rot, post_tran, bda, mlp_input, feat_prev_iv,\n                         k2s_sensor, extra_ref_frame):\n        if extra_ref_frame:\n            stereo_feat = self.extract_stereo_ref_feat(img)\n            return None, None, stereo_feat\n        x, stereo_feat = self.image_encoder(img, stereo=True)\n        metas = dict(k2s_sensor=k2s_sensor,\n                     intrins=intrin,\n                     post_rots=post_rot,\n                     post_trans=post_tran,\n                     frustum=self.img_view_transformer.cv_frustum.to(x),\n                     cv_downsample=4,\n                     downsample=self.img_view_transformer.downsample,\n                     grid_config=self.img_view_transformer.grid_config,\n                     cv_feat_list=[feat_prev_iv, stereo_feat])\n        bev_feat, depth = self.img_view_transformer(\n            [x, sensor2keyego, ego2global, intrin, post_rot, post_tran, bda,\n             mlp_input], metas)\n        if self.pre_process:\n            bev_feat = self.pre_process_net(bev_feat)[0]\n        return bev_feat, depth, stereo_feat\n\n    def extract_img_feat(self,\n                         img,\n                         img_metas,\n                         pred_prev=False,\n                         sequential=False,\n                         **kwargs):\n        if sequential:\n            # Todo\n            assert False\n        imgs, sensor2keyegos, ego2globals, intrins, post_rots, post_trans, \\\n        bda, curr2adjsensor = self.prepare_inputs(img, stereo=True)\n        \"\"\"Extract features of images.\"\"\"\n        bev_feat_list = []\n        depth_key_frame = None\n        feat_prev_iv = None\n        for fid in range(self.num_frame-1, -1, -1):\n            img, sensor2keyego, ego2global, intrin, post_rot, post_tran = \\\n                imgs[fid], sensor2keyegos[fid], ego2globals[fid], intrins[fid], \\\n                post_rots[fid], post_trans[fid]\n            key_frame = fid == 0\n            extra_ref_frame = fid == self.num_frame-self.extra_ref_frames\n            if key_frame or self.with_prev:\n                if self.align_after_view_transfromation:\n                    sensor2keyego, ego2global = sensor2keyegos[0], ego2globals[0]\n                mlp_input = self.img_view_transformer.get_mlp_input(\n                    sensor2keyegos[0], ego2globals[0], intrin,\n                    post_rot, post_tran, bda)\n                inputs_curr = (img, sensor2keyego, ego2global, intrin,\n                               post_rot, post_tran, bda, mlp_input,\n                               feat_prev_iv, curr2adjsensor[fid],\n                               extra_ref_frame)\n                if key_frame:\n                    bev_feat, depth, feat_curr_iv = \\\n                        self.prepare_bev_feat(*inputs_curr)\n                    depth_key_frame = depth\n                else:\n                    with torch.no_grad():\n                        bev_feat, depth, feat_curr_iv = \\\n                            self.prepare_bev_feat(*inputs_curr)\n                if not extra_ref_frame:\n                    bev_feat_list.append(bev_feat)\n                feat_prev_iv = feat_curr_iv\n        if pred_prev:\n            # Todo\n            assert False\n        if not self.with_prev:\n            bev_feat_key = bev_feat_list[0]\n            if len(bev_feat_key.shape) ==4:\n                b,c,h,w = bev_feat_key.shape\n                bev_feat_list = \\\n                    [torch.zeros([b,\n                                  c * (self.num_frame -\n                                       self.extra_ref_frames - 1),\n                                  h, w]).to(bev_feat_key), bev_feat_key]\n            else:\n                b, c, z, h, w = bev_feat_key.shape\n                bev_feat_list = \\\n                    [torch.zeros([b,\n                                  c * (self.num_frame -\n                                       self.extra_ref_frames - 1), z,\n                                  h, w]).to(bev_feat_key), bev_feat_key]\n        if self.align_after_view_transfromation:\n            for adj_id in range(self.num_frame-2):\n                bev_feat_list[adj_id] = \\\n                    self.shift_feature(bev_feat_list[adj_id],\n                                       [sensor2keyegos[0],\n                                        sensor2keyegos[self.num_frame-2-adj_id]],\n                                       bda)\n        bev_feat = torch.cat(bev_feat_list, dim=1)\n        x = self.bev_encoder(bev_feat)\n        return [x], depth_key_frame"
        }
    ]
}