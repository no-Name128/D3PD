{
    "sourceFile": "mmdet3d/models/detectors/LetOCC.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 2,
            "patches": [
                {
                    "date": 1717379414993,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1717379422522,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,9 +8,9 @@\n import numpy as np\n \n \n @DETECTORS.register_module()\n-class BEVStereo4DOCC(BEVStereo4D):\n+class LetOCC(BEVStereo4D):\n \n     def __init__(\n         self,\n         loss_occ=None,\n"
                },
                {
                    "date": 1717379432181,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,9 @@\n         use_predicter=True,\n         class_wise=False,\n         **kwargs\n     ):\n-        super(BEVStereo4DOCC, self).__init__(**kwargs)\n+        super(LetOCC, self).__init__(**kwargs)\n         self.out_dim = out_dim\n         out_channels = out_dim if use_predicter else num_classes\n         self.final_conv = ConvModule(\n             self.img_view_transformer.out_channels,\n"
                }
            ],
            "date": 1717379414993,
            "name": "Commit-0",
            "content": "from .bevdet import BEVStereo4D\n\nimport torch\nfrom mmdet.models import DETECTORS\nfrom mmdet.models.builder import build_loss\nfrom mmcv.cnn.bricks.conv_module import ConvModule\nfrom torch import nn\nimport numpy as np\n\n\n@DETECTORS.register_module()\nclass BEVStereo4DOCC(BEVStereo4D):\n\n    def __init__(\n        self,\n        loss_occ=None,\n        out_dim=32,\n        use_mask=False,\n        num_classes=18,\n        use_predicter=True,\n        class_wise=False,\n        **kwargs\n    ):\n        super(BEVStereo4DOCC, self).__init__(**kwargs)\n        self.out_dim = out_dim\n        out_channels = out_dim if use_predicter else num_classes\n        self.final_conv = ConvModule(\n            self.img_view_transformer.out_channels,\n            out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=True,\n            conv_cfg=dict(type=\"Conv3d\"),\n        )\n        self.use_predicter = use_predicter\n        if use_predicter:\n            self.predicter = nn.Sequential(\n                nn.Linear(self.out_dim, self.out_dim * 2),\n                nn.Softplus(),\n                nn.Linear(self.out_dim * 2, num_classes),\n            )\n        self.pts_bbox_head = None\n        self.use_mask = use_mask\n        self.num_classes = num_classes\n        self.loss_occ = build_loss(loss_occ)\n        self.class_wise = class_wise\n        self.align_after_view_transfromation = False\n\n    def loss_single(self, voxel_semantics, mask_camera, preds):\n        loss_ = dict()\n        voxel_semantics = voxel_semantics.long()\n        if self.use_mask:\n            mask_camera = mask_camera.to(torch.int32)\n            voxel_semantics = voxel_semantics.reshape(-1)\n            preds = preds.reshape(-1, self.num_classes)\n            mask_camera = mask_camera.reshape(-1)\n            num_total_samples = mask_camera.sum()\n            loss_occ = self.loss_occ(\n                preds, voxel_semantics, mask_camera, avg_factor=num_total_samples\n            )\n            loss_[\"loss_occ\"] = loss_occ\n        else:\n            voxel_semantics = voxel_semantics.reshape(-1)\n            preds = preds.reshape(-1, self.num_classes)\n            loss_occ = self.loss_occ(\n                preds,\n                voxel_semantics,\n            )\n            loss_[\"loss_occ\"] = loss_occ\n        return loss_\n\n    def simple_test(self, points, img_metas, img=None, rescale=False, **kwargs):\n        \"\"\"Test function without augmentaiton.\"\"\"\n        img_feats, _, _ = self.extract_feat(\n            points, img=img, img_metas=img_metas, **kwargs\n        )\n        occ_pred = self.final_conv(img_feats[0]).permute(0, 4, 3, 2, 1)\n        # bncdhw->bnwhdc\n        if self.use_predicter:\n            occ_pred = self.predicter(occ_pred)\n        occ_score = occ_pred.softmax(-1)\n        occ_res = occ_score.argmax(-1)\n        occ_res = occ_res.squeeze(dim=0).cpu().numpy().astype(np.uint8)\n        return [occ_res]\n\n    def forward_train(\n        self,\n        points=None,\n        img_metas=None,\n        gt_bboxes_3d=None,\n        gt_labels_3d=None,\n        gt_labels=None,\n        gt_bboxes=None,\n        img_inputs=None,\n        proposals=None,\n        gt_bboxes_ignore=None,\n        **kwargs\n    ):\n        \"\"\"Forward training function.\n\n        Args:\n            points (list[torch.Tensor], optional): Points of each sample.\n                Defaults to None.\n            img_metas (list[dict], optional): Meta information of each sample.\n                Defaults to None.\n            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`], optional):\n                Ground truth 3D boxes. Defaults to None.\n            gt_labels_3d (list[torch.Tensor], optional): Ground truth labels\n                of 3D boxes. Defaults to None.\n            gt_labels (list[torch.Tensor], optional): Ground truth labels\n                of 2D boxes in images. Defaults to None.\n            gt_bboxes (list[torch.Tensor], optional): Ground truth 2D boxes in\n                images. Defaults to None.\n            img (torch.Tensor optional): Images of each sample with shape\n                (N, C, H, W). Defaults to None.\n            proposals ([list[torch.Tensor], optional): Predicted proposals\n                used for training Fast RCNN. Defaults to None.\n            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n                2D boxes in images to be ignored. Defaults to None.\n\n        Returns:\n            dict: Losses of different branches.\n        \"\"\"\n        img_feats, pts_feats, depth = self.extract_feat(\n            points, img=img_inputs, img_metas=img_metas, **kwargs\n        )\n        gt_depth = kwargs[\"gt_depth\"]\n        losses = dict()\n        loss_depth = self.img_view_transformer.get_depth_loss(gt_depth, depth)\n        losses[\"loss_depth\"] = loss_depth\n\n        occ_pred = self.final_conv(img_feats[0]).permute(\n            0, 4, 3, 2, 1\n        )  # bncdhw->bnwhdc\n        if self.use_predicter:\n            occ_pred = self.predicter(occ_pred)\n        voxel_semantics = kwargs[\"voxel_semantics\"]\n        mask_camera = kwargs[\"mask_camera\"]\n        assert voxel_semantics.min() >= 0 and voxel_semantics.max() <= 17\n        loss_occ = self.loss_single(voxel_semantics, mask_camera, occ_pred)\n        losses.update(loss_occ)\n        return losses\n"
        }
    ]
}