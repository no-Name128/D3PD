{
    "sourceFile": "mmdet3d/models/detectors/bevdet_occ.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 10,
            "patches": [
                {
                    "date": 1717379014636,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1717379401097,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,5 +1,4 @@\n-# Copyright (c) Phigent Robotics. All rights reserved.\n from .bevdet import BEVStereo4D\n \n import torch\n from mmdet.models import DETECTORS\n@@ -11,88 +10,94 @@\n \n @DETECTORS.register_module()\n class BEVStereo4DOCC(BEVStereo4D):\n \n-    def __init__(self,\n-                 loss_occ=None,\n-                 out_dim=32,\n-                 use_mask=False,\n-                 num_classes=18,\n-                 use_predicter=True,\n-                 class_wise=False,\n-                 **kwargs):\n+    def __init__(\n+        self,\n+        loss_occ=None,\n+        out_dim=32,\n+        use_mask=False,\n+        num_classes=18,\n+        use_predicter=True,\n+        class_wise=False,\n+        **kwargs\n+    ):\n         super(BEVStereo4DOCC, self).__init__(**kwargs)\n         self.out_dim = out_dim\n         out_channels = out_dim if use_predicter else num_classes\n         self.final_conv = ConvModule(\n-                        self.img_view_transformer.out_channels,\n-                        out_channels,\n-                        kernel_size=3,\n-                        stride=1,\n-                        padding=1,\n-                        bias=True,\n-                        conv_cfg=dict(type='Conv3d'))\n-        self.use_predicter =use_predicter\n+            self.img_view_transformer.out_channels,\n+            out_channels,\n+            kernel_size=3,\n+            stride=1,\n+            padding=1,\n+            bias=True,\n+            conv_cfg=dict(type=\"Conv3d\"),\n+        )\n+        self.use_predicter = use_predicter\n         if use_predicter:\n             self.predicter = nn.Sequential(\n-                nn.Linear(self.out_dim, self.out_dim*2),\n+                nn.Linear(self.out_dim, self.out_dim * 2),\n                 nn.Softplus(),\n-                nn.Linear(self.out_dim*2, num_classes),\n+                nn.Linear(self.out_dim * 2, num_classes),\n             )\n         self.pts_bbox_head = None\n         self.use_mask = use_mask\n         self.num_classes = num_classes\n         self.loss_occ = build_loss(loss_occ)\n         self.class_wise = class_wise\n         self.align_after_view_transfromation = False\n \n-    def loss_single(self,voxel_semantics,mask_camera,preds):\n+    def loss_single(self, voxel_semantics, mask_camera, preds):\n         loss_ = dict()\n-        voxel_semantics=voxel_semantics.long()\n+        voxel_semantics = voxel_semantics.long()\n         if self.use_mask:\n             mask_camera = mask_camera.to(torch.int32)\n-            voxel_semantics=voxel_semantics.reshape(-1)\n-            preds=preds.reshape(-1,self.num_classes)\n+            voxel_semantics = voxel_semantics.reshape(-1)\n+            preds = preds.reshape(-1, self.num_classes)\n             mask_camera = mask_camera.reshape(-1)\n-            num_total_samples=mask_camera.sum()\n-            loss_occ=self.loss_occ(preds,voxel_semantics,mask_camera, avg_factor=num_total_samples)\n-            loss_['loss_occ'] = loss_occ\n+            num_total_samples = mask_camera.sum()\n+            loss_occ = self.loss_occ(\n+                preds, voxel_semantics, mask_camera, avg_factor=num_total_samples\n+            )\n+            loss_[\"loss_occ\"] = loss_occ\n         else:\n             voxel_semantics = voxel_semantics.reshape(-1)\n             preds = preds.reshape(-1, self.num_classes)\n-            loss_occ = self.loss_occ(preds, voxel_semantics,)\n-            loss_['loss_occ'] = loss_occ\n+            loss_occ = self.loss_occ(\n+                preds,\n+                voxel_semantics,\n+            )\n+            loss_[\"loss_occ\"] = loss_occ\n         return loss_\n \n-    def simple_test(self,\n-                    points,\n-                    img_metas,\n-                    img=None,\n-                    rescale=False,\n-                    **kwargs):\n+    def simple_test(self, points, img_metas, img=None, rescale=False, **kwargs):\n         \"\"\"Test function without augmentaiton.\"\"\"\n         img_feats, _, _ = self.extract_feat(\n-            points, img=img, img_metas=img_metas, **kwargs)\n+            points, img=img, img_metas=img_metas, **kwargs\n+        )\n         occ_pred = self.final_conv(img_feats[0]).permute(0, 4, 3, 2, 1)\n         # bncdhw->bnwhdc\n         if self.use_predicter:\n             occ_pred = self.predicter(occ_pred)\n-        occ_score=occ_pred.softmax(-1)\n-        occ_res=occ_score.argmax(-1)\n+        occ_score = occ_pred.softmax(-1)\n+        occ_res = occ_score.argmax(-1)\n         occ_res = occ_res.squeeze(dim=0).cpu().numpy().astype(np.uint8)\n         return [occ_res]\n \n-    def forward_train(self,\n-                      points=None,\n-                      img_metas=None,\n-                      gt_bboxes_3d=None,\n-                      gt_labels_3d=None,\n-                      gt_labels=None,\n-                      gt_bboxes=None,\n-                      img_inputs=None,\n-                      proposals=None,\n-                      gt_bboxes_ignore=None,\n-                      **kwargs):\n+    def forward_train(\n+        self,\n+        points=None,\n+        img_metas=None,\n+        gt_bboxes_3d=None,\n+        gt_labels_3d=None,\n+        gt_labels=None,\n+        gt_bboxes=None,\n+        img_inputs=None,\n+        proposals=None,\n+        gt_bboxes_ignore=None,\n+        **kwargs\n+    ):\n         \"\"\"Forward training function.\n \n         Args:\n             points (list[torch.Tensor], optional): Points of each sample.\n@@ -117,19 +122,22 @@\n         Returns:\n             dict: Losses of different branches.\n         \"\"\"\n         img_feats, pts_feats, depth = self.extract_feat(\n-            points, img=img_inputs, img_metas=img_metas, **kwargs)\n-        gt_depth = kwargs['gt_depth']\n+            points, img=img_inputs, img_metas=img_metas, **kwargs\n+        )\n+        gt_depth = kwargs[\"gt_depth\"]\n         losses = dict()\n         loss_depth = self.img_view_transformer.get_depth_loss(gt_depth, depth)\n-        losses['loss_depth'] = loss_depth\n+        losses[\"loss_depth\"] = loss_depth\n \n-        occ_pred = self.final_conv(img_feats[0]).permute(0, 4, 3, 2, 1) # bncdhw->bnwhdc\n+        occ_pred = self.final_conv(img_feats[0]).permute(\n+            0, 4, 3, 2, 1\n+        )  # bncdhw->bnwhdc\n         if self.use_predicter:\n             occ_pred = self.predicter(occ_pred)\n-        voxel_semantics = kwargs['voxel_semantics']\n-        mask_camera = kwargs['mask_camera']\n+        voxel_semantics = kwargs[\"voxel_semantics\"]\n+        mask_camera = kwargs[\"mask_camera\"]\n         assert voxel_semantics.min() >= 0 and voxel_semantics.max() <= 17\n         loss_occ = self.loss_single(voxel_semantics, mask_camera, occ_pred)\n         losses.update(loss_occ)\n         return losses\n"
                },
                {
                    "date": 1720509057700,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -121,8 +121,9 @@\n \n         Returns:\n             dict: Losses of different branches.\n         \"\"\"\n+        \n         img_feats, pts_feats, depth = self.extract_feat(\n             points, img=img_inputs, img_metas=img_metas, **kwargs\n         )\n         gt_depth = kwargs[\"gt_depth\"]\n"
                },
                {
                    "date": 1720511942277,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -121,9 +121,9 @@\n \n         Returns:\n             dict: Losses of different branches.\n         \"\"\"\n-        \n+\n         img_feats, pts_feats, depth = self.extract_feat(\n             points, img=img_inputs, img_metas=img_metas, **kwargs\n         )\n         gt_depth = kwargs[\"gt_depth\"]\n"
                },
                {
                    "date": 1720513210132,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -121,8 +121,10 @@\n \n         Returns:\n             dict: Losses of different branches.\n         \"\"\"\n+        \n+        raise RuntimeError\n \n         img_feats, pts_feats, depth = self.extract_feat(\n             points, img=img_inputs, img_metas=img_metas, **kwargs\n         )\n"
                },
                {
                    "date": 1720689551546,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -121,10 +121,8 @@\n \n         Returns:\n             dict: Losses of different branches.\n         \"\"\"\n-        \n-        raise RuntimeError\n \n         img_feats, pts_feats, depth = self.extract_feat(\n             points, img=img_inputs, img_metas=img_metas, **kwargs\n         )\n"
                },
                {
                    "date": 1720689755975,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -121,8 +121,11 @@\n \n         Returns:\n             dict: Losses of different branches.\n         \"\"\"\n+        \n+        import pdb\n+        pdb.set_trace()\n \n         img_feats, pts_feats, depth = self.extract_feat(\n             points, img=img_inputs, img_metas=img_metas, **kwargs\n         )\n"
                },
                {
                    "date": 1720771505249,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -121,11 +121,8 @@\n \n         Returns:\n             dict: Losses of different branches.\n         \"\"\"\n-        \n-        import pdb\n-        pdb.set_trace()\n \n         img_feats, pts_feats, depth = self.extract_feat(\n             points, img=img_inputs, img_metas=img_metas, **kwargs\n         )\n"
                },
                {
                    "date": 1721012182096,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -69,8 +69,16 @@\n             )\n             loss_[\"loss_occ\"] = loss_occ\n         return loss_\n \n+    def extract_img_feat(self, img, img_metas, **kwargs):\n+        \"\"\"Extract features of images.\"\"\"\n+        img = self.prepare_inputs(img)\n+        x, _ = self.image_encoder(img[0])\n+        x, depth = self.img_view_transformer([x] + img[1:7])\n+        x = self.bev_encoder(x)\n+        return [x], depth\n+\n     def simple_test(self, points, img_metas, img=None, rescale=False, **kwargs):\n         \"\"\"Test function without augmentaiton.\"\"\"\n         img_feats, _, _ = self.extract_feat(\n             points, img=img, img_metas=img_metas, **kwargs\n"
                },
                {
                    "date": 1721012203211,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -75,10 +75,10 @@\n         img = self.prepare_inputs(img)\n         x, _ = self.image_encoder(img[0])\n         \n         # fast voxel gen processing\n+        s\n         \n-        \n         # slow voxel gen processing\n         x, depth = self.img_view_transformer([x] + img[1:7])\n         x = self.bev_encoder(x)\n         return [x], depth\n"
                },
                {
                    "date": 1721012255306,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -69,9 +69,8 @@\n             )\n             loss_[\"loss_occ\"] = loss_occ\n         return loss_\n \n-\n     def simple_test(self, points, img_metas, img=None, rescale=False, **kwargs):\n         \"\"\"Test function without augmentaiton.\"\"\"\n         img_feats, _, _ = self.extract_feat(\n             points, img=img, img_metas=img_metas, **kwargs\n"
                }
            ],
            "date": 1717379014636,
            "name": "Commit-0",
            "content": "# Copyright (c) Phigent Robotics. All rights reserved.\nfrom .bevdet import BEVStereo4D\n\nimport torch\nfrom mmdet.models import DETECTORS\nfrom mmdet.models.builder import build_loss\nfrom mmcv.cnn.bricks.conv_module import ConvModule\nfrom torch import nn\nimport numpy as np\n\n\n@DETECTORS.register_module()\nclass BEVStereo4DOCC(BEVStereo4D):\n\n    def __init__(self,\n                 loss_occ=None,\n                 out_dim=32,\n                 use_mask=False,\n                 num_classes=18,\n                 use_predicter=True,\n                 class_wise=False,\n                 **kwargs):\n        super(BEVStereo4DOCC, self).__init__(**kwargs)\n        self.out_dim = out_dim\n        out_channels = out_dim if use_predicter else num_classes\n        self.final_conv = ConvModule(\n                        self.img_view_transformer.out_channels,\n                        out_channels,\n                        kernel_size=3,\n                        stride=1,\n                        padding=1,\n                        bias=True,\n                        conv_cfg=dict(type='Conv3d'))\n        self.use_predicter =use_predicter\n        if use_predicter:\n            self.predicter = nn.Sequential(\n                nn.Linear(self.out_dim, self.out_dim*2),\n                nn.Softplus(),\n                nn.Linear(self.out_dim*2, num_classes),\n            )\n        self.pts_bbox_head = None\n        self.use_mask = use_mask\n        self.num_classes = num_classes\n        self.loss_occ = build_loss(loss_occ)\n        self.class_wise = class_wise\n        self.align_after_view_transfromation = False\n\n    def loss_single(self,voxel_semantics,mask_camera,preds):\n        loss_ = dict()\n        voxel_semantics=voxel_semantics.long()\n        if self.use_mask:\n            mask_camera = mask_camera.to(torch.int32)\n            voxel_semantics=voxel_semantics.reshape(-1)\n            preds=preds.reshape(-1,self.num_classes)\n            mask_camera = mask_camera.reshape(-1)\n            num_total_samples=mask_camera.sum()\n            loss_occ=self.loss_occ(preds,voxel_semantics,mask_camera, avg_factor=num_total_samples)\n            loss_['loss_occ'] = loss_occ\n        else:\n            voxel_semantics = voxel_semantics.reshape(-1)\n            preds = preds.reshape(-1, self.num_classes)\n            loss_occ = self.loss_occ(preds, voxel_semantics,)\n            loss_['loss_occ'] = loss_occ\n        return loss_\n\n    def simple_test(self,\n                    points,\n                    img_metas,\n                    img=None,\n                    rescale=False,\n                    **kwargs):\n        \"\"\"Test function without augmentaiton.\"\"\"\n        img_feats, _, _ = self.extract_feat(\n            points, img=img, img_metas=img_metas, **kwargs)\n        occ_pred = self.final_conv(img_feats[0]).permute(0, 4, 3, 2, 1)\n        # bncdhw->bnwhdc\n        if self.use_predicter:\n            occ_pred = self.predicter(occ_pred)\n        occ_score=occ_pred.softmax(-1)\n        occ_res=occ_score.argmax(-1)\n        occ_res = occ_res.squeeze(dim=0).cpu().numpy().astype(np.uint8)\n        return [occ_res]\n\n    def forward_train(self,\n                      points=None,\n                      img_metas=None,\n                      gt_bboxes_3d=None,\n                      gt_labels_3d=None,\n                      gt_labels=None,\n                      gt_bboxes=None,\n                      img_inputs=None,\n                      proposals=None,\n                      gt_bboxes_ignore=None,\n                      **kwargs):\n        \"\"\"Forward training function.\n\n        Args:\n            points (list[torch.Tensor], optional): Points of each sample.\n                Defaults to None.\n            img_metas (list[dict], optional): Meta information of each sample.\n                Defaults to None.\n            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`], optional):\n                Ground truth 3D boxes. Defaults to None.\n            gt_labels_3d (list[torch.Tensor], optional): Ground truth labels\n                of 3D boxes. Defaults to None.\n            gt_labels (list[torch.Tensor], optional): Ground truth labels\n                of 2D boxes in images. Defaults to None.\n            gt_bboxes (list[torch.Tensor], optional): Ground truth 2D boxes in\n                images. Defaults to None.\n            img (torch.Tensor optional): Images of each sample with shape\n                (N, C, H, W). Defaults to None.\n            proposals ([list[torch.Tensor], optional): Predicted proposals\n                used for training Fast RCNN. Defaults to None.\n            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n                2D boxes in images to be ignored. Defaults to None.\n\n        Returns:\n            dict: Losses of different branches.\n        \"\"\"\n        img_feats, pts_feats, depth = self.extract_feat(\n            points, img=img_inputs, img_metas=img_metas, **kwargs)\n        gt_depth = kwargs['gt_depth']\n        losses = dict()\n        loss_depth = self.img_view_transformer.get_depth_loss(gt_depth, depth)\n        losses['loss_depth'] = loss_depth\n\n        occ_pred = self.final_conv(img_feats[0]).permute(0, 4, 3, 2, 1) # bncdhw->bnwhdc\n        if self.use_predicter:\n            occ_pred = self.predicter(occ_pred)\n        voxel_semantics = kwargs['voxel_semantics']\n        mask_camera = kwargs['mask_camera']\n        assert voxel_semantics.min() >= 0 and voxel_semantics.max() <= 17\n        loss_occ = self.loss_single(voxel_semantics, mask_camera, occ_pred)\n        losses.update(loss_occ)\n        return losses\n"
        }
    ]
}