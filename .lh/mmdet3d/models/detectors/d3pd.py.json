{
    "sourceFile": "mmdet3d/models/detectors/d3pd.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 6,
            "patches": [
                {
                    "date": 1716470317625,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1716470449933,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -58,11 +58,8 @@\n         self._init_fusion_module()\n \n         self._init_teacher_model(pts_bbox_head_tea, **kwargs)\n         self._init_student_model()\n-        # self._init_img_transformation(\n-        #     img_view_transformer, img_bev_encoder_backbone, img_bev_encoder_neck\n-        # )\n         self._init_radar_net(\n             radar_voxel_layer,\n             radar_pillar_encoder,\n             radar_middle_encoder,\n"
                },
                {
                    "date": 1716536566321,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,735 +18,175 @@\n from mmdet3d.models.utils.self_print import feats_to_img\n \n \n @DETECTORS.register_module()\n-class D3PD(BEVStereo4D):\n-    def __init__(\n-        self,\n-        teacher_pretrained=None,\n-        student_pretrained=None,\n-        vovnet_pretrained=None,\n-        load_with_teacher_head=None,\n-        freeze_img_backbone=False,\n-        # img_view_transformer=None,\n-        # img_bev_encoder_backbone=None,\n-        # img_bev_encoder_neck=None,\n-        radar_voxel_layer=None,\n-        radar_pillar_encoder=None,\n-        radar_middle_encoder=None,\n-        radar_backbone=None,\n-        radar_neck=None,\n-        bi_dire_fusion=None,\n-        middle_radar_aug=None,\n-        rc_bev_fusion=None,\n-        pts_bbox_head_tea=None,\n-        distillation=None,\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-        self.student_pretrained = student_pretrained\n-        self.teacher_pretrained = teacher_pretrained\n-        self.vovnet_pretrained = vovnet_pretrained\n-        self.load_with_teacher_head = load_with_teacher_head\n-        self.freeze_img_backbone = freeze_img_backbone\n+class D3PD(BEVDepth4D):\n+    def __init__(self, **kwargs):\n+        super(BEVStereo4D, self).__init__(**kwargs)\n+        self.extra_ref_frames = 1\n+        self.temporal_frame = self.num_frame\n+        self.num_frame += self.extra_ref_frames\n \n-        self.bi_dire_fusion = bi_dire_fusion\n-        self.middle_radar_aug = middle_radar_aug\n-        self.rc_bev_fusion = rc_bev_fusion\n+    def extract_stereo_ref_feat(self, x):\n+        B, N, C, imH, imW = x.shape\n+        x = x.view(B * N, C, imH, imW)\n+        if isinstance(self.img_backbone, ResNet):\n+            if self.img_backbone.deep_stem:\n+                x = self.img_backbone.stem(x)\n+            else:\n+                x = self.img_backbone.conv1(x)\n+                x = self.img_backbone.norm1(x)\n+                x = self.img_backbone.relu(x)\n+            x = self.img_backbone.maxpool(x)\n+            for i, layer_name in enumerate(self.img_backbone.res_layers):\n+                res_layer = getattr(self.img_backbone, layer_name)\n+                x = res_layer(x)\n+                return x\n+        else:\n+            x = self.img_backbone.patch_embed(x)\n+            hw_shape = (\n+                self.img_backbone.patch_embed.DH,\n+                self.img_backbone.patch_embed.DW,\n+            )\n+            if self.img_backbone.use_abs_pos_embed:\n+                x = x + self.img_backbone.absolute_pos_embed\n+            x = self.img_backbone.drop_after_pos(x)\n \n-        self.distillation = distillation\n+            for i, stage in enumerate(self.img_backbone.stages):\n+                x, hw_shape, out, out_hw_shape = stage(x, hw_shape)\n+                out = out.view(-1, *out_hw_shape, self.img_backbone.num_features[i])\n+                out = out.permute(0, 3, 1, 2).contiguous()\n+                return out\n \n-        self._init_fusion_module()\n-\n-        self._init_teacher_model(pts_bbox_head_tea, **kwargs)\n-        self._init_student_model()\n-        self._init_radar_net(\n-            radar_voxel_layer,\n-            radar_pillar_encoder,\n-            radar_middle_encoder,\n-            radar_backbone,\n-            radar_neck,\n-        )\n-        # self._init_img_backbone(close=student_pretrained is not None)\n-        self._init_img_backbone(close=self.freeze_img_backbone)\n-        self._init_distill_module()\n-\n-    def _init_fusion_module(self):\n-        self.bi_dire_fusion = (\n-            builder.build_neck(self.bi_dire_fusion) if self.bi_dire_fusion else None\n-        )\n-        self.middle_radar_aug = (\n-            builder.build_neck(self.middle_radar_aug) if self.middle_radar_aug else None\n-        )\n-        self.rc_bev_fusion = (\n-            builder.build_neck(self.rc_bev_fusion) if self.rc_bev_fusion else None\n-        )\n-\n-    def _init_teacher_model(self, pts_bbox_head_tea=None, **kwargs):\n-\n-        if self.pts_middle_encoder:\n-            for param in self.pts_middle_encoder.parameters():\n-                param.requires_grad = False\n-            self.pts_middle_encoder.eval()\n-\n-        if self.pts_backbone:\n-            for param in self.pts_backbone.parameters():\n-                param.requires_grad = False\n-            self.pts_backbone.eval()\n-\n-        if self.pts_neck:\n-            for param in self.pts_neck.parameters():\n-                param.requires_grad = False\n-            self.pts_neck.eval()\n-\n-        if pts_bbox_head_tea:\n-            train_cfg = kwargs[\"train_cfg\"]\n-            test_cfg = kwargs[\"test_cfg\"]\n-            pts_train_cfg = train_cfg.pts if train_cfg else None\n-            pts_bbox_head_tea.update(train_cfg=pts_train_cfg)\n-            pts_test_cfg = test_cfg.pts if test_cfg else None\n-            pts_bbox_head_tea.update(test_cfg=pts_test_cfg)\n-            self.pts_bbox_head_tea = builder.build_head(pts_bbox_head_tea)\n-\n-            teacher_weight = torch.load(\n-                self.teacher_pretrained,\n-                map_location=\"cuda:{}\".format(torch.cuda.current_device()),\n-            )[\"state_dict\"]\n-\n-            dict_load = {\n-                _key.replace(\"pts_bbox_head.\", \"\"): teacher_weight[_key]\n-                for _key in teacher_weight\n-                if \"pts_bbox_head\" in _key\n-            }\n-\n-            self.pts_bbox_head_tea.load_state_dict(dict_load, strict=False)\n-\n-            for param in self.pts_bbox_head_tea.parameters():\n-                param.requires_grad = False\n-            self.pts_bbox_head_tea.eval()\n-\n-    def _init_student_model(self):\n-\n-        if self.student_pretrained is not None:\n-\n-            logger = get_root_logger()\n-            student_pretrained_load = [\n-                \"img_neck\",\n-                \"img_view_transformer\",\n-                \"img_bev_encoder_backbone\",\n-                \"img_bev_encoder_neck\",\n-                \"pre_process\",\n-            ]\n-\n-            sutdent_model = torch.load(\n-                self.student_pretrained,\n-                map_location=\"cuda:{}\".format(torch.cuda.current_device()),\n-            )[\"state_dict\"]\n-\n-            for load_key in student_pretrained_load:\n-\n-                dict_load = {\n-                    _key.replace(load_key + \".\", \"\"): sutdent_model[_key]\n-                    for _key in sutdent_model\n-                    if load_key in _key\n-                }\n-\n-                if \"pre_process\" in load_key:\n-                    load_key = \"pre_process_net\"\n-\n-                getattr(self, load_key).load_state_dict(dict_load, strict=False)\n-\n-                print(\"Loaded pretrained {}\".format(load_key))\n-                logger.info(\"Loaded pretrained {}\".format(load_key))\n-                assert len(dict_load) > 0\n-\n-            # for module in student_pretrained_load:\n-\n-            #     for param in getattr(self, module).parameters():\n-            #         param.requires_grad = False\n-            #     getattr(self, module).eval()\n-\n-        if self.load_with_teacher_head:\n-            teacher_weight = torch.load(\n-                self.teacher_pretrained,\n-                map_location=\"cuda:{}\".format(torch.cuda.current_device()),\n-            )[\"state_dict\"]\n-\n-            dict_load = {\n-                _key.replace(\"pts_bbox_head.task_heads.\", \"\"): teacher_weight[_key]\n-                for _key in teacher_weight\n-                if \"pts_bbox_head.task_heads\" in _key\n-            }\n-\n-            self.pts_bbox_head.task_heads.load_state_dict(dict_load, strict=False)\n-\n-    # def _init_img_transformation(self):\n-    #     pass\n-\n-    def _init_radar_net(\n+    def prepare_bev_feat(\n         self,\n-        radar_voxel_layer,\n-        radar_pillar_encoder,\n-        radar_middle_encoder,\n-        radar_backbone,\n-        radar_neck,\n+        img,\n+        sensor2keyego,\n+        ego2global,\n+        intrin,\n+        post_rot,\n+        post_tran,\n+        bda,\n+        mlp_input,\n+        feat_prev_iv,\n+        k2s_sensor,\n+        extra_ref_frame,\n     ):\n-        if radar_voxel_layer:\n-            self.radar_voxel_layer = Voxelization(**radar_voxel_layer)\n-        else:\n-            self.radar_voxel_layer = None\n+        if extra_ref_frame:\n+            stereo_feat = self.extract_stereo_ref_feat(img)\n+            return None, None, stereo_feat\n \n-        if radar_pillar_encoder:\n-            self.radar_pillar_encoder = builder.build_voxel_encoder(\n-                radar_pillar_encoder\n-            )\n-\n-        if radar_middle_encoder:\n-            self.radar_middle_cfg = radar_middle_encoder\n-            self.radar_middle_encoder = builder.build_middle_encoder(\n-                radar_middle_encoder\n-            )\n-\n-        if radar_backbone is not None:\n-            self.with_radar_backbone = True\n-            self.radar_backbone = builder.build_backbone(radar_backbone)\n-        else:\n-            self.with_radar_backbone = False\n-\n-        if radar_neck is not None:\n-            self.with_radar_neck = True\n-            self.radar_neck = builder.build_neck(radar_neck)\n-        else:\n-            self.with_radar_neck = False\n-\n-    def _init_img_backbone(self, close=False):\n-\n-        if self.img_backbone.__class__.__name__ == \"VoVNetCP\":\n-            img_backbone_weight = self.vovnet_pretrained\n-\n-            img_backbone_load = torch.load(\n-                img_backbone_weight,\n-                map_location=\"cuda:{}\".format(torch.cuda.current_device()),\n-            )[\"model\"]\n-\n-            img_backbone_dict_load = {\n-                _key.replace(\"backbone.bottom_up.\", \"\"): img_backbone_load[_key]\n-                for _key in img_backbone_load\n-                if \"backbone.bottom_up\" in _key\n-            }\n-\n-            self.img_backbone.load_state_dict(img_backbone_dict_load, strict=False)\n-            logger = get_root_logger()\n-            logger.warn(\"Loaded pretrained {}\".format(\"backbone.bottom_up\"))\n-            # print(colored(\"Loaded pretrained {}\".format(\"backbone.bottom_up\"), \"green\"))\n-            assert len(img_backbone_dict_load) > 0\n-\n-        if False:\n-            for param in self.img_backbone.parameters():\n-                param.requires_grad = False\n-            self.img_backbone.eval()\n-\n-    def _init_distill_module(self):\n-\n-        if self.distillation.get(\"sparse_feats_distill\"):\n-            # Construct sparse feature distillation, including knowledge transfer of image bev and radar bev two view features\n-            self.sparse_feats_distill_img_bev = (\n-                build_loss(self.distillation.sparse_feats_distill.img_bev_distill)\n-                if \"img_bev_distill\" in self.distillation.sparse_feats_distill.keys()\n-                else None\n-            )\n-\n-            self.sparse_feats_distill_radar_bev = (\n-                build_loss(self.distillation.sparse_feats_distill.radar_bev_distill)\n-                if \"radar_bev_distill\" in self.distillation.sparse_feats_distill.keys()\n-                else None\n-            )\n-            self.radar_ms_feats_distill = (\n-                build_loss(self.distillation.sparse_feats_distill.radar_ms_distill)\n-                if \"sparse_feats_distill\" in self.distillation.keys()\n-                and \"radar_ms_distill\" in self.distillation.sparse_feats_distill.keys()\n-                else None\n-            )\n-        else:\n-            self.sparse_feats_distill_img_bev = None\n-            self.sparse_feats_distill_radar_bev = None\n-            self.radar_ms_feats_distill = None\n-\n-        self.sampling_pos_distill = (\n-            build_loss(self.distillation.sampling_pos_distill)\n-            if \"sampling_pos_distill\" in self.distillation.keys()\n-            else None\n+        x, stereo_feat = self.image_encoder(img, stereo=True)\n+        metas = dict(\n+            k2s_sensor=k2s_sensor,\n+            intrins=intrin,\n+            post_rots=post_rot,\n+            post_trans=post_tran,\n+            frustum=self.img_view_transformer.cv_frustum.to(x),\n+            cv_downsample=4,\n+            downsample=self.img_view_transformer.downsample,\n+            grid_config=self.img_view_transformer.grid_config,\n+            cv_feat_list=[feat_prev_iv, stereo_feat],\n         )\n-        self.sampling_feats_distill = (\n-            build_loss(self.distillation.sampling_feats_distill)\n-            if \"sampling_feats_distill\" in self.distillation.keys()\n-            else None\n+        bev_feat, depth = self.img_view_transformer(\n+            [x, sensor2keyego, ego2global, intrin, post_rot, post_tran, bda, mlp_input],\n+            metas,\n         )\n+        if self.pre_process:\n+            bev_feat = self.pre_process_net(bev_feat)[0]\n+        return bev_feat, depth, stereo_feat\n \n-        self.det_feats_distill = (\n-            build_loss(self.distillation.det_feats_distill)\n-            if \"det_feats_distill\" in self.distillation.keys()\n-            else None\n-        )\n-\n-        if \"det_result_distill\" in self.distillation.keys():\n-            self.ret_sum = self.distillation.det_result_distill[\"ret_sum\"]\n-            self.dcdistill_loss = build_loss(self.distillation.det_result_distill)\n-        else:\n-            self.ret_sum = None\n-            self.dcdistill_loss = None\n-\n-        self.smfd_distill_loss = (\n-            build_loss(self.distillation.mask_bev_feats_distill)\n-            if \"mask_bev_feats_distill\" in self.distillation.keys()\n-            else None\n-        )\n-\n-        self.heatmap_aug_distill_loss = (\n-            build_loss(self.distillation.heatmap_aug_distill)\n-            if \"heatmap_aug_distill\" in self.distillation.keys()\n-            else None\n-        )\n-\n-    @torch.no_grad()\n-    @force_fp32()\n-    def radar_voxelization(self, points):\n-        \"\"\"Apply dynamic voxelization to points.\n-\n-        Args:\n-            points (list[torch.Tensor]): Points of each sample.\n-\n-        Returns:\n-            tuple[torch.Tensor]: Concatenated points, number of points\n-                per voxel, and coordinates.\n-        \"\"\"\n-        voxels, coors, num_points = [], [], []\n-        for res in points:\n-            res_voxels, res_coors, res_num_points = self.radar_voxel_layer(res)\n-            voxels.append(res_voxels)\n-            coors.append(res_coors)\n-            num_points.append(res_num_points)\n-        voxels = torch.cat(voxels, dim=0)\n-        num_points = torch.cat(num_points, dim=0)\n-        coors_batch = []\n-        for i, coor in enumerate(coors):\n-            coor_pad = F.pad(coor, (1, 0), mode=\"constant\", value=i)\n-            coors_batch.append(coor_pad)\n-        coors_batch = torch.cat(coors_batch, dim=0)\n-        return voxels, num_points, coors_batch\n-\n-    def extract_pts_feat(self, pts, img_feats, img_metas):\n-        \"\"\"Extract features of points.\"\"\"\n-        if not self.pts_middle_encoder:\n-            return None\n-        if not self.with_pts_bbox:\n-            return None\n-\n-        voxels, num_points, coors = self.voxelize(pts)\n-\n-        voxel_features = self.pts_voxel_encoder(voxels, num_points, coors)\n-        batch_size = coors[-1, 0] + 1\n-        x = self.pts_middle_encoder(voxel_features, coors, batch_size)\n-        x = self.pts_backbone(x)\n-\n-        if self.radar_ms_feats_distill:\n-            pts_ms_feats = [data for data in x]\n-\n-        if self.with_pts_neck:\n-            x = self.pts_neck(x)\n-\n-        if self.radar_ms_feats_distill:\n-            return x, pts_ms_feats\n-        else:\n-            return x, None\n-\n-    def extract_radar_feat(self, radar_points, ret_coords=False):\n-        \"\"\"Extract features of points.\"\"\"\n-        if not self.with_pts_bbox:\n-            return None\n-\n-        # radar visualization\n-        # random_sava_num = np.random.randint(0, 10000000000)\n-        # radar_points[0].cpu().numpy().tofile(\n-        #     f\"./radar_vis/radapoints_{random_sava_num}.bin\"\n-        # )\n-        voxels, num_points, coors = self.radar_voxelization(radar_points)\n-\n-        radar_pillar_features = self.radar_pillar_encoder(voxels, num_points, coors)\n-        batch_size = coors[-1, 0] + 1\n-        x = self.radar_middle_encoder(radar_pillar_features, coors, batch_size)\n-\n-        if self.with_radar_backbone:\n-            x = self.radar_backbone(x)\n-\n-            if self.radar_ms_feats_distill:\n-                radar_ms_feas = [data for data in x]\n-\n-        if self.with_radar_neck:\n-            x = self.radar_neck(x)\n-\n-        if self.radar_ms_feats_distill:\n-            if ret_coords:\n-                return x, coors, radar_ms_feas\n-            else:\n-                return x, radar_ms_feas\n-        else:\n-            if ret_coords:\n-                return x, coors, None\n-            else:\n-                return x, None\n-\n-    # def feats_post_process(self, img_feats, radar_feats, pts_feats=None):\n-    #     if self.bi_dire_fusion:\n-    #         img_feats, radar_feats = self.bi_dire_fusion(img_feats[0], radar_feats[0])\n-\n-    #     if self.middle_radar_aug:\n-    #         radar_feats = self.middle_radar_aug(img_feats, radar_feats)\n-\n-    #     det_input_feats = (\n-    #         self.rc_bev_fusion(img_feats, radar_feats)\n-    #         if self.rc_bev_fusion is not None\n-    #         else dict()\n-    #     )\n-\n-    #     return [img_feats], radar_feats, det_input_feats\n-\n-    def fusion_img_radar_bev(self, img_bev, radar_bev, **kwargs) -> list:\n-        img_bev = (\n-            img_bev[0]\n-            if isinstance(img_bev, list) or isinstance(img_bev, tuple)\n-            else img_bev\n-        )\n-        radar_bev = radar_bev[0] if isinstance(radar_bev, list) else radar_bev\n-\n-        radar_points = kwargs.get(\"radar_points\")\n-        # output = self.reduce_conv(torch.cat([img_bev, radar_bev], dim=1))\n-\n-        det_input_dict = {}\n-\n-        if self.rc_bev_fusion:\n-            output = self.rc_bev_fusion(img_bev, radar_bev)\n-            if not isinstance(output, dict):\n-                det_input_dict.update({\"det_feats\": [output]})\n-            else:\n-                det_input_dict.update(output)\n-\n-        return det_input_dict\n-\n-    def extract_feat(self, points, radar, img, img_metas):\n-        \"\"\"Extract features from images and points.\"\"\"\n-\n-        img_feats, depth = self.extract_img_feat(img, img_metas)\n-        # img_feats, depth=super(BEVStereo4D)\n-        radar_feats, radar_ms_feats = self.extract_radar_feat(radar)\n-\n-        pts_feats, pts_ms_feats = self.extract_pts_feat(points, img_feats, img_metas)\n-\n-        det_input_feats = self.fusion_img_radar_bev(\n-            img_feats, radar_feats, pts_feats=pts_feats\n-        )\n-\n-        # base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n-        # feats_to_img(\n-        #     det_input_feats[\"det_feats\"], base_path=base_path, suffix=\"fusion\"\n-        # )\n-        # feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n-\n-        # feats_to_img(\n-        #     det_input_feats[\"student_sampling_feats\"],\n-        #     base_path=base_path,\n-        #     suffix=\"img_feats_sampling\",\n-        # )\n-        # feats_to_img(radar_feats, base_path=base_path, suffix=\"radar_feats\")\n-        # feats_to_img(pts_feats, base_path=base_path, suffix=\"pts_feats\")\n-\n-        # raise RuntimeError\n-\n-        # feats_to_img(radar_feats, base_path=base_path, suffix=\"radar_feats_aug\")\n-\n-        if radar_ms_feats and pts_ms_feats:\n-            det_input_feats.update(dict(radar_ms_feats=radar_ms_feats))\n-            det_input_feats.update(dict(pts_ms_feats=pts_ms_feats))\n-\n-        return (img_feats, pts_feats, radar_feats, depth, det_input_feats)\n-\n-    def forward_pts_train(\n-        self,\n-        pts_feats,\n-        gt_bboxes_3d,\n-        gt_labels_3d,\n-        img_metas,\n-        gt_bboxes_ignore=None,\n-    ) -> tuple:\n-        \"\"\"Forward function for point cloud branch.\n-\n-        Args:\n-            pts_feats (_type_): Features of point cloud branch\n-            gt_bboxes_3d (_type_): Ground truth\n-                boxes for each sample.\n-            gt_labels_3d (_type_): Ground truth labels for\n-                boxes of each sampole\n-            img_metas (_type_): Meta information of samples.\n-            gt_bboxes_ignore (_type_, optional): Ground truth\n-                boxes to be ignored. Defaults to None.. Defaults to None.\n-\n-        Returns:\n-            tuple: _description_\n-        \"\"\"\n-\n-        outs = self.pts_bbox_head(pts_feats)\n-        loss_inputs = [gt_bboxes_3d, gt_labels_3d, outs]\n-        losses = self.pts_bbox_head.loss(*loss_inputs)\n-\n-        return losses, outs\n-\n-    def forward_distill_loss(\n-        self,\n-        teacher_bev_feats=None,\n-        teacher_sampling_pos=None,\n-        teacher_sampling_feats=None,\n-        tea_resp_bboxes=None,\n-        student_bev_feats=None,\n-        student_samping_pos=None,\n-        student_samping_feats=None,\n-        stu_resp_bboxes=None,\n-        pts_ms_feats=None,\n-        radar_ms_feats=None,\n-        radar_feats=None,\n-        rc_fusion_feats=None,\n-        gt_bboxes_3d=None,\n-        bda_mat=None,\n-        **kwargs,\n+    def extract_img_feat(\n+        self, img, img_metas, pred_prev=False, sequential=False, **kwargs\n     ):\n-        losses = {}\n-        ivt_cfg = {}\n-        # ivt_cfg.update(\n-        #     {\n-        #         \"dx\": self.img_view_transformer.dx,\n-        #         \"bx\": self.img_view_transformer.bx,\n-        #         \"nx\": self.img_view_transformer.nx,\n-        #     }\n-        # )\n \n-        ivt_cfg.update(\n-            {\n-                \"dx\": self.img_view_transformer.grid_interval,\n-                \"bx\": (\n-                    self.img_view_transformer.grid_lower_bound\n-                    + self.img_view_transformer.grid_interval\n-                )\n-                / 2.0,\n-                \"nx\": self.img_view_transformer.grid_size,\n-            }\n-        )\n+        if sequential:\n+            # Todo\n+            assert False\n \n-        if self.sparse_feats_distill_img_bev is not None:\n+        (\n+            imgs,\n+            sensor2keyegos,\n+            ego2globals,\n+            intrins,\n+            post_rots,\n+            post_trans,\n+            bda,\n+            curr2adjsensor,\n+        ) = self.prepare_inputs(img, stereo=True)\n \n-            # ================lidar distill img-bev================\n-\n-            loss_inter_channel = self.sparse_feats_distill_img_bev(\n-                student_bev_feats,\n-                teacher_bev_feats,\n-                gt_bboxes_list=gt_bboxes_3d,\n-                ivt_cfg=ivt_cfg,\n+        \"\"\"Extract features of images.\"\"\"\n+        bev_feat_list = []\n+        depth_key_frame = None\n+        feat_prev_iv = None\n+        for fid in range(self.num_frame - 1, -1, -1):\n+            img, sensor2keyego, ego2global, intrin, post_rot, post_tran = (\n+                imgs[fid],\n+                sensor2keyegos[fid],\n+                ego2globals[fid],\n+                intrins[fid],\n+                post_rots[fid],\n+                post_trans[fid],\n             )\n-\n-            if isinstance(loss_inter_channel, tuple):\n-                losses.update({\"loss_inter_channel_img_bev\": loss_inter_channel[0]})\n-            else:\n-                losses.update({\"loss_inter_channel_img_bev\": loss_inter_channel})\n-\n-        if self.sparse_feats_distill_radar_bev is not None:\n-\n-            # ================lidar distill radar-bev================\n-            if self.sparse_feats_distill_radar_bev is not None:\n-                loss_inter_channel = self.sparse_feats_distill_radar_bev(\n-                    radar_feats, teacher_bev_feats, gt_bboxes_3d, ivt_cfg=ivt_cfg\n-                )  # A self-distillation scheme is used here to migrate the features of the img-bev network to the network's own radar-bev\n-\n-            if isinstance(loss_inter_channel, tuple):\n-                losses.update({\"loss_inter_channel_radar_bev\": loss_inter_channel[0]})\n-            else:\n-                losses.update({\"loss_inter_channel_radar_bev\": loss_inter_channel})\n-\n-        if rc_fusion_feats is not None and self.det_feats_distill is not None:\n-            loss_det_feats_distill = self.det_feats_distill(\n-                rc_fusion_feats, teacher_bev_feats\n-            )\n-            losses.update({\"loss_det_feats_distill\": loss_det_feats_distill})\n-\n-        if self.sampling_pos_distill is not None and teacher_sampling_pos is not None:\n-            loss_sampling_pos_distill = self.sampling_pos_distill(\n-                student_samping_pos, teacher_sampling_pos\n-            )\n-            losses.update({\"loss_sampling_pos_distill\": loss_sampling_pos_distill})\n-\n-        if (\n-            teacher_sampling_feats is not None\n-            and self.sampling_feats_distill is not None\n-        ):\n-            loss_sampling_warp_distill = self.sampling_feats_distill(\n-                student_samping_feats, teacher_sampling_feats\n-            )\n-            losses.update({\"loss_sampling_feats_distill\": loss_sampling_warp_distill})\n-\n-        if self.dcdistill_loss is not None:\n-\n-            if self.ret_sum:\n-                loss_dc_distill = self.dcdistill_loss(\n-                    tea_resp_bboxes, stu_resp_bboxes, gt_bboxes_3d\n+            key_frame = fid == 0\n+            extra_ref_frame = fid == self.num_frame - self.extra_ref_frames\n+            if key_frame or self.with_prev:\n+                if self.align_after_view_transfromation:\n+                    sensor2keyego, ego2global = sensor2keyegos[0], ego2globals[0]\n+                mlp_input = self.img_view_transformer.get_mlp_input(\n+                    sensor2keyegos[0], ego2globals[0], intrin, post_rot, post_tran, bda\n                 )\n-                losses.update({\"loss_dc_distill\": loss_dc_distill})\n+                inputs_curr = (\n+                    img,\n+                    sensor2keyego,\n+                    ego2global,\n+                    intrin,\n+                    post_rot,\n+                    post_tran,\n+                    bda,\n+                    mlp_input,\n+                    feat_prev_iv,\n+                    curr2adjsensor[fid],\n+                    extra_ref_frame,\n+                )\n+                if key_frame:\n+                    bev_feat, depth, feat_curr_iv = self.prepare_bev_feat(*inputs_curr)\n+                    depth_key_frame = depth\n+                else:\n+                    with torch.no_grad():\n+                        bev_feat, depth, feat_curr_iv = self.prepare_bev_feat(\n+                            *inputs_curr\n+                        )\n+                if not extra_ref_frame:\n+                    bev_feat_list.append(bev_feat)\n+                feat_prev_iv = feat_curr_iv\n+        if pred_prev:\n+            # Todo\n+            assert False\n+        if not self.with_prev:\n+            bev_feat_key = bev_feat_list[0]\n+            if len(bev_feat_key.shape) == 4:\n+                b, c, h, w = bev_feat_key.shape\n+                bev_feat_list = [\n+                    torch.zeros(\n+                        [b, c * (self.num_frame - self.extra_ref_frames - 1), h, w]\n+                    ).to(bev_feat_key),\n+                    bev_feat_key,\n+                ]\n             else:\n-                loss_dc_reg_distill, loss_dc_cls_distill = self.dcdistill_loss(\n-                    tea_resp_bboxes, stu_resp_bboxes, gt_bboxes_3d\n+                b, c, z, h, w = bev_feat_key.shape\n+                bev_feat_list = [\n+                    torch.zeros(\n+                        [b, c * (self.num_frame - self.extra_ref_frames - 1), z, h, w]\n+                    ).to(bev_feat_key),\n+                    bev_feat_key,\n+                ]\n+        if self.align_after_view_transfromation:\n+            for adj_id in range(self.num_frame - 2):\n+                bev_feat_list[adj_id] = self.shift_feature(\n+                    bev_feat_list[adj_id],\n+                    [sensor2keyegos[0], sensor2keyegos[self.num_frame - 2 - adj_id]],\n+                    bda,\n                 )\n-                losses.update({\"loss_dc_reg_distill\": loss_dc_reg_distill})\n-                losses.update({\"loss_dc_cls_distill\": loss_dc_cls_distill})\n-\n-        # Self-learning mask focused distillation\n-        if self.smfd_distill_loss is not None:\n-            loss_smfd_distill, auto_mask = self.smfd_distill_loss(\n-                rc_fusion_feats,\n-                teacher_bev_feats,\n-                gt_bboxes_3d,\n-                stu_resp_bboxes,\n-                bda_mat=bda_mat,\n-            )\n-            losses.update(dict(loss_smfd_distill=loss_smfd_distill))\n-\n-            # forward_distill_loss_input_cfg=\n-        if self.radar_ms_feats_distill:\n-            radar_ms_distill_loss = self.radar_ms_feats_distill(\n-                radar_ms_feats, pts_ms_feats, gt_bboxes_3d=gt_bboxes_3d\n-            )\n-            losses.update(dict(loss_radar_ms=radar_ms_distill_loss))\n-\n-        if self.heatmap_aug_distill_loss is not None:\n-            loss_heatmap_aug = self.heatmap_aug_distill_loss(\n-                stu_resp_bboxes, tea_resp_bboxes, auto_mask\n-            )\n-\n-            losses.update(dict(loss_heatmap_aug=loss_heatmap_aug))\n-\n-        return losses\n-\n-    def forward_train(\n-        self,\n-        points=None,\n-        img_metas=None,\n-        gt_bboxes_3d=None,\n-        gt_labels_3d=None,\n-        radar=None,\n-        gt_labels=None,\n-        gt_bboxes=None,\n-        img_inputs=None,\n-        proposals=None,\n-        gt_bboxes_ignore=None,\n-        **kwargs,\n-    ):\n-\n-        img_feats, pts_feats, radar_feats, depth, feats_wrap = self.extract_feat(\n-            points, radar=radar, img=img_inputs, img_metas=img_metas\n-        )\n-\n-        teacher_outs = (\n-            self.pts_bbox_head_tea(pts_feats) if self.distillation is not None else None\n-        )\n-\n-        teacher_sampling_pos = feats_wrap.get(\"teacher_sampling_pos\")\n-        teacher_sampling_feats = feats_wrap.get(\"teacher_sampling_feats\")\n-        student_sampling_pos = feats_wrap.get(\"student_sampling_pos\")\n-        student_sampling_feats = feats_wrap.get(\"student_sampling_feats\")\n-        radar_ms_feats = feats_wrap.get(\"radar_ms_feats\")\n-        pts_ms_feats = feats_wrap.get(\"pts_ms_feats\")\n-        det_feats = feats_wrap.get(\"det_feats\")\n-\n-        # base_path = \"/mnt/data/exps/D3PD/d3pd/out\"\n-        # feats_to_img(det_feats[0], base_path, \"fusion\")\n-\n-        # raise RuntimeError\n-\n-        losses = dict()\n-\n-        gt_depth = kwargs[\"gt_depth\"]\n-        loss_depth = self.img_view_transformer.get_depth_loss(gt_depth, depth)\n-        losses.update(dict(loss_depth=loss_depth))\n-\n-        img_feats_kd = img_feats[0] if isinstance(img_feats, list) else img_feats\n-        if pts_feats:\n-            pts_feats_kd = pts_feats[0]\n-\n-        # assert det_input_feats[\"det_feats\"].size(1) == 64\n-        losses_pts, student_outs = self.forward_pts_train(\n-            det_feats,\n-            gt_bboxes_3d,\n-            gt_labels_3d,\n-            img_metas,\n-            gt_bboxes_ignore,\n-        )\n-\n-        losses.update(losses_pts)\n-\n-        ret_losses_dict = self.forward_distill_loss(\n-            teacher_bev_feats=pts_feats_kd,\n-            student_bev_feats=img_feats_kd,\n-            teacher_sampling_pos=teacher_sampling_pos,\n-            student_samping_pos=student_sampling_pos,\n-            teacher_sampling_feats=teacher_sampling_feats,\n-            student_samping_feats=student_sampling_feats,\n-            tea_resp_bboxes=teacher_outs,\n-            stu_resp_bboxes=student_outs,\n-            pts_ms_feats=pts_ms_feats,\n-            radar_ms_feats=radar_ms_feats,\n-            radar_feats=radar_feats[0],\n-            rc_fusion_feats=det_feats[0],\n-            gt_bboxes_3d=gt_bboxes_3d,\n-        )\n-\n-        losses.update(ret_losses_dict)\n-\n-        # adaptive_weight_list = [\n-        #     \"loss_depth\",\n-        #     \"loss_radar_ms\",\n-        #     \"loss_dc_reg_distill\",\n-        #     \"loss_dc_cls_distill\",\n-        #     \"loss_smfd_distill\",\n-        # ]\n-        # for key in adaptive_weight_list:\n-        #     adaptive_weight = (\n-        #         losses[key] / losses[\"loss_inter_channel_img_bev\"]\n-        #     ).detach()\n-        #     losses.update({key: losses[key] / adaptive_weight})\n-\n-        return losses\n-\n-    def simple_test(self, points, radar, img_metas, img=None, rescale=False):\n-        \"\"\"Test function without augmentaiton.\"\"\"\n-        _, _, _, _, det_input_feats = self.extract_feat(\n-            points, radar, img=img, img_metas=img_metas\n-        )\n-\n-        bbox_list = [dict() for _ in range(len(img_metas))]\n-        bbox_pts = self.simple_test_pts(\n-            det_input_feats[\"det_feats\"], img_metas, rescale=rescale\n-        )\n-        for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n-            result_dict[\"pts_bbox\"] = pts_bbox\n-        return bbox_list\n-\n-    def forward_test(\n-        self, points=None, radar=None, img_metas=None, img_inputs=None, **kwargs\n-    ):\n-        for var, name in [(img_inputs, \"img_inputs\"), (img_metas, \"img_metas\")]:\n-            if not isinstance(var, list):\n-                raise TypeError(\"{} must be a list, but got {}\".format(name, type(var)))\n-\n-        if not isinstance(img_inputs[0][0], list):\n-            img_inputs = [img_inputs] if img_inputs is None else img_inputs\n-            points = [points] if points is None else points\n-            radar = [radar] if radar is None else radar\n-            return self.simple_test(\n-                points[0], radar[0], img_metas[0], img_inputs[0], **kwargs\n-            )\n-        else:\n-            return self.aug_test(None, img_metas[0], img_inputs[0], **kwargs)\n+        bev_feat = torch.cat(bev_feat_list, dim=1)\n+        x = self.bev_encoder(bev_feat)\n+        return [x], depth_key_frame\n"
                },
                {
                    "date": 1716536590057,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,8 +15,9 @@\n from mmdet.models.backbones.resnet import ResNet\n from mmdet3d.models.detectors.bevdet import BEVStereo4D\n from mmdet.utils import get_root_logger\n from mmdet3d.models.utils.self_print import feats_to_img\n+from mmdet3d.models.detectors.bevdet import BEVDepth4D\n \n \n @DETECTORS.register_module()\n class D3PD(BEVDepth4D):\n"
                },
                {
                    "date": 1716537445306,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -190,4 +190,19 @@\n                 )\n         bev_feat = torch.cat(bev_feat_list, dim=1)\n         x = self.bev_encoder(bev_feat)\n         return [x], depth_key_frame\n+\n+    def simple_test(self, points, img_metas, img=None, rescale=False, **kwargs):\n+        \"\"\"Test function without augmentaiton.\"\"\"\n+        img_feats, _, _ = self.extract_feat(\n+            points, img=img, img_metas=img_metas, **kwargs\n+        )\n+\n+        # base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n+        # feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n+\n+        bbox_list = [dict() for _ in range(len(img_metas))]\n+        bbox_pts = self.simple_test_pts(img_feats, img_metas, rescale=rescale)\n+        for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n+            result_dict[\"pts_bbox\"] = pts_bbox\n+        return bbox_list\n"
                },
                {
                    "date": 1716537458995,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -225,8 +225,9 @@\n             det_input_feats.update(dict(radar_ms_feats=radar_ms_feats))\n             det_input_feats.update(dict(pts_ms_feats=pts_ms_feats))\n \n         return img_feats, pts_feats, radar_feats, depth, det_input_feats\n+\n     def simple_test(self, points, img_metas, img=None, rescale=False, **kwargs):\n         \"\"\"Test function without augmentaiton.\"\"\"\n         img_feats, _, _ = self.extract_feat(\n             points, img=img, img_metas=img_metas, **kwargs\n"
                },
                {
                    "date": 1716537581213,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,10 +25,9 @@\n         super(BEVStereo4D, self).__init__(**kwargs)\n         self.extra_ref_frames = 1\n         self.temporal_frame = self.num_frame\n         self.num_frame += self.extra_ref_frames\n-        \n-    \n+\n     def _init_fusion_module(self):\n         self.bi_dire_fusion = (\n             builder.build_neck(self.bi_dire_fusion) if self.bi_dire_fusion else None\n         )\n"
                }
            ],
            "date": 1716470317625,
            "name": "Commit-0",
            "content": "import torch\nfrom torch.nn import functional as F\nfrom mmcv.runner import force_fp32\nfrom mmcv.cnn import ConvModule\nfrom mmdet3d.models.utils.self_print import feats_to_img\nfrom .. import builder\nfrom ..builder import DETECTORS, build_loss\nfrom .centerpoint import CenterPoint\nimport torch.nn as nn\nimport numpy as np\nfrom mmdet3d.ops import Voxelization\nfrom mmdet3d.core import bbox3d2result\nimport matplotlib.pyplot as plt\nfrom termcolor import colored\nfrom mmdet.models.backbones.resnet import ResNet\nfrom mmdet3d.models.detectors.bevdet import BEVStereo4D\nfrom mmdet.utils import get_root_logger\nfrom mmdet3d.models.utils.self_print import feats_to_img\n\n\n@DETECTORS.register_module()\nclass D3PD(BEVStereo4D):\n    def __init__(\n        self,\n        teacher_pretrained=None,\n        student_pretrained=None,\n        vovnet_pretrained=None,\n        load_with_teacher_head=None,\n        freeze_img_backbone=False,\n        # img_view_transformer=None,\n        # img_bev_encoder_backbone=None,\n        # img_bev_encoder_neck=None,\n        radar_voxel_layer=None,\n        radar_pillar_encoder=None,\n        radar_middle_encoder=None,\n        radar_backbone=None,\n        radar_neck=None,\n        bi_dire_fusion=None,\n        middle_radar_aug=None,\n        rc_bev_fusion=None,\n        pts_bbox_head_tea=None,\n        distillation=None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.student_pretrained = student_pretrained\n        self.teacher_pretrained = teacher_pretrained\n        self.vovnet_pretrained = vovnet_pretrained\n        self.load_with_teacher_head = load_with_teacher_head\n        self.freeze_img_backbone = freeze_img_backbone\n\n        self.bi_dire_fusion = bi_dire_fusion\n        self.middle_radar_aug = middle_radar_aug\n        self.rc_bev_fusion = rc_bev_fusion\n\n        self.distillation = distillation\n\n        self._init_fusion_module()\n\n        self._init_teacher_model(pts_bbox_head_tea, **kwargs)\n        self._init_student_model()\n        # self._init_img_transformation(\n        #     img_view_transformer, img_bev_encoder_backbone, img_bev_encoder_neck\n        # )\n        self._init_radar_net(\n            radar_voxel_layer,\n            radar_pillar_encoder,\n            radar_middle_encoder,\n            radar_backbone,\n            radar_neck,\n        )\n        # self._init_img_backbone(close=student_pretrained is not None)\n        self._init_img_backbone(close=self.freeze_img_backbone)\n        self._init_distill_module()\n\n    def _init_fusion_module(self):\n        self.bi_dire_fusion = (\n            builder.build_neck(self.bi_dire_fusion) if self.bi_dire_fusion else None\n        )\n        self.middle_radar_aug = (\n            builder.build_neck(self.middle_radar_aug) if self.middle_radar_aug else None\n        )\n        self.rc_bev_fusion = (\n            builder.build_neck(self.rc_bev_fusion) if self.rc_bev_fusion else None\n        )\n\n    def _init_teacher_model(self, pts_bbox_head_tea=None, **kwargs):\n\n        if self.pts_middle_encoder:\n            for param in self.pts_middle_encoder.parameters():\n                param.requires_grad = False\n            self.pts_middle_encoder.eval()\n\n        if self.pts_backbone:\n            for param in self.pts_backbone.parameters():\n                param.requires_grad = False\n            self.pts_backbone.eval()\n\n        if self.pts_neck:\n            for param in self.pts_neck.parameters():\n                param.requires_grad = False\n            self.pts_neck.eval()\n\n        if pts_bbox_head_tea:\n            train_cfg = kwargs[\"train_cfg\"]\n            test_cfg = kwargs[\"test_cfg\"]\n            pts_train_cfg = train_cfg.pts if train_cfg else None\n            pts_bbox_head_tea.update(train_cfg=pts_train_cfg)\n            pts_test_cfg = test_cfg.pts if test_cfg else None\n            pts_bbox_head_tea.update(test_cfg=pts_test_cfg)\n            self.pts_bbox_head_tea = builder.build_head(pts_bbox_head_tea)\n\n            teacher_weight = torch.load(\n                self.teacher_pretrained,\n                map_location=\"cuda:{}\".format(torch.cuda.current_device()),\n            )[\"state_dict\"]\n\n            dict_load = {\n                _key.replace(\"pts_bbox_head.\", \"\"): teacher_weight[_key]\n                for _key in teacher_weight\n                if \"pts_bbox_head\" in _key\n            }\n\n            self.pts_bbox_head_tea.load_state_dict(dict_load, strict=False)\n\n            for param in self.pts_bbox_head_tea.parameters():\n                param.requires_grad = False\n            self.pts_bbox_head_tea.eval()\n\n    def _init_student_model(self):\n\n        if self.student_pretrained is not None:\n\n            logger = get_root_logger()\n            student_pretrained_load = [\n                \"img_neck\",\n                \"img_view_transformer\",\n                \"img_bev_encoder_backbone\",\n                \"img_bev_encoder_neck\",\n                \"pre_process\",\n            ]\n\n            sutdent_model = torch.load(\n                self.student_pretrained,\n                map_location=\"cuda:{}\".format(torch.cuda.current_device()),\n            )[\"state_dict\"]\n\n            for load_key in student_pretrained_load:\n\n                dict_load = {\n                    _key.replace(load_key + \".\", \"\"): sutdent_model[_key]\n                    for _key in sutdent_model\n                    if load_key in _key\n                }\n\n                if \"pre_process\" in load_key:\n                    load_key = \"pre_process_net\"\n\n                getattr(self, load_key).load_state_dict(dict_load, strict=False)\n\n                print(\"Loaded pretrained {}\".format(load_key))\n                logger.info(\"Loaded pretrained {}\".format(load_key))\n                assert len(dict_load) > 0\n\n            # for module in student_pretrained_load:\n\n            #     for param in getattr(self, module).parameters():\n            #         param.requires_grad = False\n            #     getattr(self, module).eval()\n\n        if self.load_with_teacher_head:\n            teacher_weight = torch.load(\n                self.teacher_pretrained,\n                map_location=\"cuda:{}\".format(torch.cuda.current_device()),\n            )[\"state_dict\"]\n\n            dict_load = {\n                _key.replace(\"pts_bbox_head.task_heads.\", \"\"): teacher_weight[_key]\n                for _key in teacher_weight\n                if \"pts_bbox_head.task_heads\" in _key\n            }\n\n            self.pts_bbox_head.task_heads.load_state_dict(dict_load, strict=False)\n\n    # def _init_img_transformation(self):\n    #     pass\n\n    def _init_radar_net(\n        self,\n        radar_voxel_layer,\n        radar_pillar_encoder,\n        radar_middle_encoder,\n        radar_backbone,\n        radar_neck,\n    ):\n        if radar_voxel_layer:\n            self.radar_voxel_layer = Voxelization(**radar_voxel_layer)\n        else:\n            self.radar_voxel_layer = None\n\n        if radar_pillar_encoder:\n            self.radar_pillar_encoder = builder.build_voxel_encoder(\n                radar_pillar_encoder\n            )\n\n        if radar_middle_encoder:\n            self.radar_middle_cfg = radar_middle_encoder\n            self.radar_middle_encoder = builder.build_middle_encoder(\n                radar_middle_encoder\n            )\n\n        if radar_backbone is not None:\n            self.with_radar_backbone = True\n            self.radar_backbone = builder.build_backbone(radar_backbone)\n        else:\n            self.with_radar_backbone = False\n\n        if radar_neck is not None:\n            self.with_radar_neck = True\n            self.radar_neck = builder.build_neck(radar_neck)\n        else:\n            self.with_radar_neck = False\n\n    def _init_img_backbone(self, close=False):\n\n        if self.img_backbone.__class__.__name__ == \"VoVNetCP\":\n            img_backbone_weight = self.vovnet_pretrained\n\n            img_backbone_load = torch.load(\n                img_backbone_weight,\n                map_location=\"cuda:{}\".format(torch.cuda.current_device()),\n            )[\"model\"]\n\n            img_backbone_dict_load = {\n                _key.replace(\"backbone.bottom_up.\", \"\"): img_backbone_load[_key]\n                for _key in img_backbone_load\n                if \"backbone.bottom_up\" in _key\n            }\n\n            self.img_backbone.load_state_dict(img_backbone_dict_load, strict=False)\n            logger = get_root_logger()\n            logger.warn(\"Loaded pretrained {}\".format(\"backbone.bottom_up\"))\n            # print(colored(\"Loaded pretrained {}\".format(\"backbone.bottom_up\"), \"green\"))\n            assert len(img_backbone_dict_load) > 0\n\n        if False:\n            for param in self.img_backbone.parameters():\n                param.requires_grad = False\n            self.img_backbone.eval()\n\n    def _init_distill_module(self):\n\n        if self.distillation.get(\"sparse_feats_distill\"):\n            # Construct sparse feature distillation, including knowledge transfer of image bev and radar bev two view features\n            self.sparse_feats_distill_img_bev = (\n                build_loss(self.distillation.sparse_feats_distill.img_bev_distill)\n                if \"img_bev_distill\" in self.distillation.sparse_feats_distill.keys()\n                else None\n            )\n\n            self.sparse_feats_distill_radar_bev = (\n                build_loss(self.distillation.sparse_feats_distill.radar_bev_distill)\n                if \"radar_bev_distill\" in self.distillation.sparse_feats_distill.keys()\n                else None\n            )\n            self.radar_ms_feats_distill = (\n                build_loss(self.distillation.sparse_feats_distill.radar_ms_distill)\n                if \"sparse_feats_distill\" in self.distillation.keys()\n                and \"radar_ms_distill\" in self.distillation.sparse_feats_distill.keys()\n                else None\n            )\n        else:\n            self.sparse_feats_distill_img_bev = None\n            self.sparse_feats_distill_radar_bev = None\n            self.radar_ms_feats_distill = None\n\n        self.sampling_pos_distill = (\n            build_loss(self.distillation.sampling_pos_distill)\n            if \"sampling_pos_distill\" in self.distillation.keys()\n            else None\n        )\n        self.sampling_feats_distill = (\n            build_loss(self.distillation.sampling_feats_distill)\n            if \"sampling_feats_distill\" in self.distillation.keys()\n            else None\n        )\n\n        self.det_feats_distill = (\n            build_loss(self.distillation.det_feats_distill)\n            if \"det_feats_distill\" in self.distillation.keys()\n            else None\n        )\n\n        if \"det_result_distill\" in self.distillation.keys():\n            self.ret_sum = self.distillation.det_result_distill[\"ret_sum\"]\n            self.dcdistill_loss = build_loss(self.distillation.det_result_distill)\n        else:\n            self.ret_sum = None\n            self.dcdistill_loss = None\n\n        self.smfd_distill_loss = (\n            build_loss(self.distillation.mask_bev_feats_distill)\n            if \"mask_bev_feats_distill\" in self.distillation.keys()\n            else None\n        )\n\n        self.heatmap_aug_distill_loss = (\n            build_loss(self.distillation.heatmap_aug_distill)\n            if \"heatmap_aug_distill\" in self.distillation.keys()\n            else None\n        )\n\n    @torch.no_grad()\n    @force_fp32()\n    def radar_voxelization(self, points):\n        \"\"\"Apply dynamic voxelization to points.\n\n        Args:\n            points (list[torch.Tensor]): Points of each sample.\n\n        Returns:\n            tuple[torch.Tensor]: Concatenated points, number of points\n                per voxel, and coordinates.\n        \"\"\"\n        voxels, coors, num_points = [], [], []\n        for res in points:\n            res_voxels, res_coors, res_num_points = self.radar_voxel_layer(res)\n            voxels.append(res_voxels)\n            coors.append(res_coors)\n            num_points.append(res_num_points)\n        voxels = torch.cat(voxels, dim=0)\n        num_points = torch.cat(num_points, dim=0)\n        coors_batch = []\n        for i, coor in enumerate(coors):\n            coor_pad = F.pad(coor, (1, 0), mode=\"constant\", value=i)\n            coors_batch.append(coor_pad)\n        coors_batch = torch.cat(coors_batch, dim=0)\n        return voxels, num_points, coors_batch\n\n    def extract_pts_feat(self, pts, img_feats, img_metas):\n        \"\"\"Extract features of points.\"\"\"\n        if not self.pts_middle_encoder:\n            return None\n        if not self.with_pts_bbox:\n            return None\n\n        voxels, num_points, coors = self.voxelize(pts)\n\n        voxel_features = self.pts_voxel_encoder(voxels, num_points, coors)\n        batch_size = coors[-1, 0] + 1\n        x = self.pts_middle_encoder(voxel_features, coors, batch_size)\n        x = self.pts_backbone(x)\n\n        if self.radar_ms_feats_distill:\n            pts_ms_feats = [data for data in x]\n\n        if self.with_pts_neck:\n            x = self.pts_neck(x)\n\n        if self.radar_ms_feats_distill:\n            return x, pts_ms_feats\n        else:\n            return x, None\n\n    def extract_radar_feat(self, radar_points, ret_coords=False):\n        \"\"\"Extract features of points.\"\"\"\n        if not self.with_pts_bbox:\n            return None\n\n        # radar visualization\n        # random_sava_num = np.random.randint(0, 10000000000)\n        # radar_points[0].cpu().numpy().tofile(\n        #     f\"./radar_vis/radapoints_{random_sava_num}.bin\"\n        # )\n        voxels, num_points, coors = self.radar_voxelization(radar_points)\n\n        radar_pillar_features = self.radar_pillar_encoder(voxels, num_points, coors)\n        batch_size = coors[-1, 0] + 1\n        x = self.radar_middle_encoder(radar_pillar_features, coors, batch_size)\n\n        if self.with_radar_backbone:\n            x = self.radar_backbone(x)\n\n            if self.radar_ms_feats_distill:\n                radar_ms_feas = [data for data in x]\n\n        if self.with_radar_neck:\n            x = self.radar_neck(x)\n\n        if self.radar_ms_feats_distill:\n            if ret_coords:\n                return x, coors, radar_ms_feas\n            else:\n                return x, radar_ms_feas\n        else:\n            if ret_coords:\n                return x, coors, None\n            else:\n                return x, None\n\n    # def feats_post_process(self, img_feats, radar_feats, pts_feats=None):\n    #     if self.bi_dire_fusion:\n    #         img_feats, radar_feats = self.bi_dire_fusion(img_feats[0], radar_feats[0])\n\n    #     if self.middle_radar_aug:\n    #         radar_feats = self.middle_radar_aug(img_feats, radar_feats)\n\n    #     det_input_feats = (\n    #         self.rc_bev_fusion(img_feats, radar_feats)\n    #         if self.rc_bev_fusion is not None\n    #         else dict()\n    #     )\n\n    #     return [img_feats], radar_feats, det_input_feats\n\n    def fusion_img_radar_bev(self, img_bev, radar_bev, **kwargs) -> list:\n        img_bev = (\n            img_bev[0]\n            if isinstance(img_bev, list) or isinstance(img_bev, tuple)\n            else img_bev\n        )\n        radar_bev = radar_bev[0] if isinstance(radar_bev, list) else radar_bev\n\n        radar_points = kwargs.get(\"radar_points\")\n        # output = self.reduce_conv(torch.cat([img_bev, radar_bev], dim=1))\n\n        det_input_dict = {}\n\n        if self.rc_bev_fusion:\n            output = self.rc_bev_fusion(img_bev, radar_bev)\n            if not isinstance(output, dict):\n                det_input_dict.update({\"det_feats\": [output]})\n            else:\n                det_input_dict.update(output)\n\n        return det_input_dict\n\n    def extract_feat(self, points, radar, img, img_metas):\n        \"\"\"Extract features from images and points.\"\"\"\n\n        img_feats, depth = self.extract_img_feat(img, img_metas)\n        # img_feats, depth=super(BEVStereo4D)\n        radar_feats, radar_ms_feats = self.extract_radar_feat(radar)\n\n        pts_feats, pts_ms_feats = self.extract_pts_feat(points, img_feats, img_metas)\n\n        det_input_feats = self.fusion_img_radar_bev(\n            img_feats, radar_feats, pts_feats=pts_feats\n        )\n\n        # base_path = \"/mnt/data/exps/D3PD/d3pd/out/v3-feats-out\"\n        # feats_to_img(\n        #     det_input_feats[\"det_feats\"], base_path=base_path, suffix=\"fusion\"\n        # )\n        # feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats\")\n\n        # feats_to_img(\n        #     det_input_feats[\"student_sampling_feats\"],\n        #     base_path=base_path,\n        #     suffix=\"img_feats_sampling\",\n        # )\n        # feats_to_img(radar_feats, base_path=base_path, suffix=\"radar_feats\")\n        # feats_to_img(pts_feats, base_path=base_path, suffix=\"pts_feats\")\n\n        # raise RuntimeError\n\n        # feats_to_img(radar_feats, base_path=base_path, suffix=\"radar_feats_aug\")\n\n        if radar_ms_feats and pts_ms_feats:\n            det_input_feats.update(dict(radar_ms_feats=radar_ms_feats))\n            det_input_feats.update(dict(pts_ms_feats=pts_ms_feats))\n\n        return (img_feats, pts_feats, radar_feats, depth, det_input_feats)\n\n    def forward_pts_train(\n        self,\n        pts_feats,\n        gt_bboxes_3d,\n        gt_labels_3d,\n        img_metas,\n        gt_bboxes_ignore=None,\n    ) -> tuple:\n        \"\"\"Forward function for point cloud branch.\n\n        Args:\n            pts_feats (_type_): Features of point cloud branch\n            gt_bboxes_3d (_type_): Ground truth\n                boxes for each sample.\n            gt_labels_3d (_type_): Ground truth labels for\n                boxes of each sampole\n            img_metas (_type_): Meta information of samples.\n            gt_bboxes_ignore (_type_, optional): Ground truth\n                boxes to be ignored. Defaults to None.. Defaults to None.\n\n        Returns:\n            tuple: _description_\n        \"\"\"\n\n        outs = self.pts_bbox_head(pts_feats)\n        loss_inputs = [gt_bboxes_3d, gt_labels_3d, outs]\n        losses = self.pts_bbox_head.loss(*loss_inputs)\n\n        return losses, outs\n\n    def forward_distill_loss(\n        self,\n        teacher_bev_feats=None,\n        teacher_sampling_pos=None,\n        teacher_sampling_feats=None,\n        tea_resp_bboxes=None,\n        student_bev_feats=None,\n        student_samping_pos=None,\n        student_samping_feats=None,\n        stu_resp_bboxes=None,\n        pts_ms_feats=None,\n        radar_ms_feats=None,\n        radar_feats=None,\n        rc_fusion_feats=None,\n        gt_bboxes_3d=None,\n        bda_mat=None,\n        **kwargs,\n    ):\n        losses = {}\n        ivt_cfg = {}\n        # ivt_cfg.update(\n        #     {\n        #         \"dx\": self.img_view_transformer.dx,\n        #         \"bx\": self.img_view_transformer.bx,\n        #         \"nx\": self.img_view_transformer.nx,\n        #     }\n        # )\n\n        ivt_cfg.update(\n            {\n                \"dx\": self.img_view_transformer.grid_interval,\n                \"bx\": (\n                    self.img_view_transformer.grid_lower_bound\n                    + self.img_view_transformer.grid_interval\n                )\n                / 2.0,\n                \"nx\": self.img_view_transformer.grid_size,\n            }\n        )\n\n        if self.sparse_feats_distill_img_bev is not None:\n\n            # ================lidar distill img-bev================\n\n            loss_inter_channel = self.sparse_feats_distill_img_bev(\n                student_bev_feats,\n                teacher_bev_feats,\n                gt_bboxes_list=gt_bboxes_3d,\n                ivt_cfg=ivt_cfg,\n            )\n\n            if isinstance(loss_inter_channel, tuple):\n                losses.update({\"loss_inter_channel_img_bev\": loss_inter_channel[0]})\n            else:\n                losses.update({\"loss_inter_channel_img_bev\": loss_inter_channel})\n\n        if self.sparse_feats_distill_radar_bev is not None:\n\n            # ================lidar distill radar-bev================\n            if self.sparse_feats_distill_radar_bev is not None:\n                loss_inter_channel = self.sparse_feats_distill_radar_bev(\n                    radar_feats, teacher_bev_feats, gt_bboxes_3d, ivt_cfg=ivt_cfg\n                )  # A self-distillation scheme is used here to migrate the features of the img-bev network to the network's own radar-bev\n\n            if isinstance(loss_inter_channel, tuple):\n                losses.update({\"loss_inter_channel_radar_bev\": loss_inter_channel[0]})\n            else:\n                losses.update({\"loss_inter_channel_radar_bev\": loss_inter_channel})\n\n        if rc_fusion_feats is not None and self.det_feats_distill is not None:\n            loss_det_feats_distill = self.det_feats_distill(\n                rc_fusion_feats, teacher_bev_feats\n            )\n            losses.update({\"loss_det_feats_distill\": loss_det_feats_distill})\n\n        if self.sampling_pos_distill is not None and teacher_sampling_pos is not None:\n            loss_sampling_pos_distill = self.sampling_pos_distill(\n                student_samping_pos, teacher_sampling_pos\n            )\n            losses.update({\"loss_sampling_pos_distill\": loss_sampling_pos_distill})\n\n        if (\n            teacher_sampling_feats is not None\n            and self.sampling_feats_distill is not None\n        ):\n            loss_sampling_warp_distill = self.sampling_feats_distill(\n                student_samping_feats, teacher_sampling_feats\n            )\n            losses.update({\"loss_sampling_feats_distill\": loss_sampling_warp_distill})\n\n        if self.dcdistill_loss is not None:\n\n            if self.ret_sum:\n                loss_dc_distill = self.dcdistill_loss(\n                    tea_resp_bboxes, stu_resp_bboxes, gt_bboxes_3d\n                )\n                losses.update({\"loss_dc_distill\": loss_dc_distill})\n            else:\n                loss_dc_reg_distill, loss_dc_cls_distill = self.dcdistill_loss(\n                    tea_resp_bboxes, stu_resp_bboxes, gt_bboxes_3d\n                )\n                losses.update({\"loss_dc_reg_distill\": loss_dc_reg_distill})\n                losses.update({\"loss_dc_cls_distill\": loss_dc_cls_distill})\n\n        # Self-learning mask focused distillation\n        if self.smfd_distill_loss is not None:\n            loss_smfd_distill, auto_mask = self.smfd_distill_loss(\n                rc_fusion_feats,\n                teacher_bev_feats,\n                gt_bboxes_3d,\n                stu_resp_bboxes,\n                bda_mat=bda_mat,\n            )\n            losses.update(dict(loss_smfd_distill=loss_smfd_distill))\n\n            # forward_distill_loss_input_cfg=\n        if self.radar_ms_feats_distill:\n            radar_ms_distill_loss = self.radar_ms_feats_distill(\n                radar_ms_feats, pts_ms_feats, gt_bboxes_3d=gt_bboxes_3d\n            )\n            losses.update(dict(loss_radar_ms=radar_ms_distill_loss))\n\n        if self.heatmap_aug_distill_loss is not None:\n            loss_heatmap_aug = self.heatmap_aug_distill_loss(\n                stu_resp_bboxes, tea_resp_bboxes, auto_mask\n            )\n\n            losses.update(dict(loss_heatmap_aug=loss_heatmap_aug))\n\n        return losses\n\n    def forward_train(\n        self,\n        points=None,\n        img_metas=None,\n        gt_bboxes_3d=None,\n        gt_labels_3d=None,\n        radar=None,\n        gt_labels=None,\n        gt_bboxes=None,\n        img_inputs=None,\n        proposals=None,\n        gt_bboxes_ignore=None,\n        **kwargs,\n    ):\n\n        img_feats, pts_feats, radar_feats, depth, feats_wrap = self.extract_feat(\n            points, radar=radar, img=img_inputs, img_metas=img_metas\n        )\n\n        teacher_outs = (\n            self.pts_bbox_head_tea(pts_feats) if self.distillation is not None else None\n        )\n\n        teacher_sampling_pos = feats_wrap.get(\"teacher_sampling_pos\")\n        teacher_sampling_feats = feats_wrap.get(\"teacher_sampling_feats\")\n        student_sampling_pos = feats_wrap.get(\"student_sampling_pos\")\n        student_sampling_feats = feats_wrap.get(\"student_sampling_feats\")\n        radar_ms_feats = feats_wrap.get(\"radar_ms_feats\")\n        pts_ms_feats = feats_wrap.get(\"pts_ms_feats\")\n        det_feats = feats_wrap.get(\"det_feats\")\n\n        # base_path = \"/mnt/data/exps/D3PD/d3pd/out\"\n        # feats_to_img(det_feats[0], base_path, \"fusion\")\n\n        # raise RuntimeError\n\n        losses = dict()\n\n        gt_depth = kwargs[\"gt_depth\"]\n        loss_depth = self.img_view_transformer.get_depth_loss(gt_depth, depth)\n        losses.update(dict(loss_depth=loss_depth))\n\n        img_feats_kd = img_feats[0] if isinstance(img_feats, list) else img_feats\n        if pts_feats:\n            pts_feats_kd = pts_feats[0]\n\n        # assert det_input_feats[\"det_feats\"].size(1) == 64\n        losses_pts, student_outs = self.forward_pts_train(\n            det_feats,\n            gt_bboxes_3d,\n            gt_labels_3d,\n            img_metas,\n            gt_bboxes_ignore,\n        )\n\n        losses.update(losses_pts)\n\n        ret_losses_dict = self.forward_distill_loss(\n            teacher_bev_feats=pts_feats_kd,\n            student_bev_feats=img_feats_kd,\n            teacher_sampling_pos=teacher_sampling_pos,\n            student_samping_pos=student_sampling_pos,\n            teacher_sampling_feats=teacher_sampling_feats,\n            student_samping_feats=student_sampling_feats,\n            tea_resp_bboxes=teacher_outs,\n            stu_resp_bboxes=student_outs,\n            pts_ms_feats=pts_ms_feats,\n            radar_ms_feats=radar_ms_feats,\n            radar_feats=radar_feats[0],\n            rc_fusion_feats=det_feats[0],\n            gt_bboxes_3d=gt_bboxes_3d,\n        )\n\n        losses.update(ret_losses_dict)\n\n        # adaptive_weight_list = [\n        #     \"loss_depth\",\n        #     \"loss_radar_ms\",\n        #     \"loss_dc_reg_distill\",\n        #     \"loss_dc_cls_distill\",\n        #     \"loss_smfd_distill\",\n        # ]\n        # for key in adaptive_weight_list:\n        #     adaptive_weight = (\n        #         losses[key] / losses[\"loss_inter_channel_img_bev\"]\n        #     ).detach()\n        #     losses.update({key: losses[key] / adaptive_weight})\n\n        return losses\n\n    def simple_test(self, points, radar, img_metas, img=None, rescale=False):\n        \"\"\"Test function without augmentaiton.\"\"\"\n        _, _, _, _, det_input_feats = self.extract_feat(\n            points, radar, img=img, img_metas=img_metas\n        )\n\n        bbox_list = [dict() for _ in range(len(img_metas))]\n        bbox_pts = self.simple_test_pts(\n            det_input_feats[\"det_feats\"], img_metas, rescale=rescale\n        )\n        for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n            result_dict[\"pts_bbox\"] = pts_bbox\n        return bbox_list\n\n    def forward_test(\n        self, points=None, radar=None, img_metas=None, img_inputs=None, **kwargs\n    ):\n        for var, name in [(img_inputs, \"img_inputs\"), (img_metas, \"img_metas\")]:\n            if not isinstance(var, list):\n                raise TypeError(\"{} must be a list, but got {}\".format(name, type(var)))\n\n        if not isinstance(img_inputs[0][0], list):\n            img_inputs = [img_inputs] if img_inputs is None else img_inputs\n            points = [points] if points is None else points\n            radar = [radar] if radar is None else radar\n            return self.simple_test(\n                points[0], radar[0], img_metas[0], img_inputs[0], **kwargs\n            )\n        else:\n            return self.aug_test(None, img_metas[0], img_inputs[0], **kwargs)\n"
        }
    ]
}