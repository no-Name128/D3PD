{
    "sourceFile": "mmdet3d/models/detectors/mvx_two_stage.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1716025348559,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1716528881106,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,503 @@\n+# Copyright (c) OpenMMLab. All rights reserved.\n+import warnings\n+from os import path as osp\n+\n+import mmcv\n+import torch\n+from mmcv.ops import Voxelization\n+from mmcv.parallel import DataContainer as DC\n+from mmcv.runner import force_fp32\n+from torch.nn import functional as F\n+\n+from mmdet3d.core import (Box3DMode, Coord3DMode, bbox3d2result,\n+                          merge_aug_bboxes_3d, show_result)\n+from mmdet.core import multi_apply\n+from .. import builder\n+from ..builder import DETECTORS\n+from .base import Base3DDetector\n+\n+\n+@DETECTORS.register_module()\n+class MVXTwoStageDetector(Base3DDetector):\n+    \"\"\"Base class of Multi-modality VoxelNet.\"\"\"\n+\n+    def __init__(self,\n+                 pts_voxel_layer=None,\n+                 pts_voxel_encoder=None,\n+                 pts_middle_encoder=None,\n+                 pts_fusion_layer=None,\n+                 img_backbone=None,\n+                 pts_backbone=None,\n+                 img_neck=None,\n+                 pts_neck=None,\n+                 pts_bbox_head=None,\n+                 img_roi_head=None,\n+                 img_rpn_head=None,\n+                 train_cfg=None,\n+                 test_cfg=None,\n+                 pretrained=None,\n+                 init_cfg=None):\n+        super(MVXTwoStageDetector, self).__init__(init_cfg=init_cfg)\n+\n+        if pts_voxel_layer:\n+            self.pts_voxel_layer = Voxelization(**pts_voxel_layer)\n+        if pts_voxel_encoder:\n+            self.pts_voxel_encoder = builder.build_voxel_encoder(\n+                pts_voxel_encoder)\n+        if pts_middle_encoder:\n+            self.pts_middle_encoder = builder.build_middle_encoder(\n+                pts_middle_encoder)\n+        if pts_backbone:\n+            self.pts_backbone = builder.build_backbone(pts_backbone)\n+        if pts_fusion_layer:\n+            self.pts_fusion_layer = builder.build_fusion_layer(\n+                pts_fusion_layer)\n+        if pts_neck is not None:\n+            self.pts_neck = builder.build_neck(pts_neck)\n+        if pts_bbox_head:\n+            pts_train_cfg = train_cfg.pts if train_cfg else None\n+            pts_bbox_head.update(train_cfg=pts_train_cfg)\n+            pts_test_cfg = test_cfg.pts if test_cfg else None\n+            pts_bbox_head.update(test_cfg=pts_test_cfg)\n+            self.pts_bbox_head = builder.build_head(pts_bbox_head)\n+\n+        if img_backbone:\n+            self.img_backbone = builder.build_backbone(img_backbone)\n+        if img_neck is not None:\n+            self.img_neck = builder.build_neck(img_neck)\n+        if img_rpn_head is not None:\n+            self.img_rpn_head = builder.build_head(img_rpn_head)\n+        if img_roi_head is not None:\n+            self.img_roi_head = builder.build_head(img_roi_head)\n+\n+        self.train_cfg = train_cfg\n+        self.test_cfg = test_cfg\n+\n+        if pretrained is None:\n+            img_pretrained = None\n+            pts_pretrained = None\n+        elif isinstance(pretrained, dict):\n+            img_pretrained = pretrained.get('img', None)\n+            pts_pretrained = pretrained.get('pts', None)\n+        else:\n+            raise ValueError(\n+                f'pretrained should be a dict, got {type(pretrained)}')\n+\n+        if self.with_img_backbone:\n+            if img_pretrained is not None:\n+                warnings.warn('DeprecationWarning: pretrained is a deprecated '\n+                              'key, please consider using init_cfg.')\n+                self.img_backbone.init_cfg = dict(\n+                    type='Pretrained', checkpoint=img_pretrained)\n+        if self.with_img_roi_head:\n+            if img_pretrained is not None:\n+                warnings.warn('DeprecationWarning: pretrained is a deprecated '\n+                              'key, please consider using init_cfg.')\n+                self.img_roi_head.init_cfg = dict(\n+                    type='Pretrained', checkpoint=img_pretrained)\n+        if self.with_pts_backbone:\n+            if pts_pretrained is not None:\n+                warnings.warn('DeprecationWarning: pretrained is a deprecated '\n+                              'key, please consider using init_cfg')\n+                self.pts_backbone.init_cfg = dict(\n+                    type='Pretrained', checkpoint=pts_pretrained)\n+\n+    @property\n+    def with_img_shared_head(self):\n+        \"\"\"bool: Whether the detector has a shared head in image branch.\"\"\"\n+        return hasattr(self,\n+                       'img_shared_head') and self.img_shared_head is not None\n+\n+    @property\n+    def with_pts_bbox(self):\n+        \"\"\"bool: Whether the detector has a 3D box head.\"\"\"\n+        return hasattr(self,\n+                       'pts_bbox_head') and self.pts_bbox_head is not None\n+\n+    @property\n+    def with_img_bbox(self):\n+        \"\"\"bool: Whether the detector has a 2D image box head.\"\"\"\n+        return hasattr(self,\n+                       'img_bbox_head') and self.img_bbox_head is not None\n+\n+    @property\n+    def with_img_backbone(self):\n+        \"\"\"bool: Whether the detector has a 2D image backbone.\"\"\"\n+        return hasattr(self, 'img_backbone') and self.img_backbone is not None\n+\n+    @property\n+    def with_pts_backbone(self):\n+        \"\"\"bool: Whether the detector has a 3D backbone.\"\"\"\n+        return hasattr(self, 'pts_backbone') and self.pts_backbone is not None\n+\n+    @property\n+    def with_fusion(self):\n+        \"\"\"bool: Whether the detector has a fusion layer.\"\"\"\n+        return hasattr(self,\n+                       'pts_fusion_layer') and self.fusion_layer is not None\n+\n+    @property\n+    def with_img_neck(self):\n+        \"\"\"bool: Whether the detector has a neck in image branch.\"\"\"\n+        return hasattr(self, 'img_neck') and self.img_neck is not None\n+\n+    @property\n+    def with_pts_neck(self):\n+        \"\"\"bool: Whether the detector has a neck in 3D detector branch.\"\"\"\n+        return hasattr(self, 'pts_neck') and self.pts_neck is not None\n+\n+    @property\n+    def with_img_rpn(self):\n+        \"\"\"bool: Whether the detector has a 2D RPN in image detector branch.\"\"\"\n+        return hasattr(self, 'img_rpn_head') and self.img_rpn_head is not None\n+\n+    @property\n+    def with_img_roi_head(self):\n+        \"\"\"bool: Whether the detector has a RoI Head in image branch.\"\"\"\n+        return hasattr(self, 'img_roi_head') and self.img_roi_head is not None\n+\n+    @property\n+    def with_voxel_encoder(self):\n+        \"\"\"bool: Whether the detector has a voxel encoder.\"\"\"\n+        return hasattr(self,\n+                       'voxel_encoder') and self.voxel_encoder is not None\n+\n+    @property\n+    def with_middle_encoder(self):\n+        \"\"\"bool: Whether the detector has a middle encoder.\"\"\"\n+        return hasattr(self,\n+                       'middle_encoder') and self.middle_encoder is not None\n+\n+    def extract_img_feat(self, img, img_metas):\n+        \"\"\"Extract features of images.\"\"\"\n+        if self.with_img_backbone and img is not None:\n+            input_shape = img.shape[-2:]\n+            # update real input shape of each single img\n+            for img_meta in img_metas:\n+                img_meta.update(input_shape=input_shape)\n+\n+            if img.dim() == 5 and img.size(0) == 1:\n+                img.squeeze_()\n+            elif img.dim() == 5 and img.size(0) > 1:\n+                B, N, C, H, W = img.size()\n+                img = img.view(B * N, C, H, W)\n+            img_feats = self.img_backbone(img)\n+        else:\n+            return None\n+        if self.with_img_neck:\n+            img_feats = self.img_neck(img_feats)\n+        return img_feats\n+\n+    def extract_pts_feat(self, pts, img_feats, img_metas):\n+        \"\"\"Extract features of points.\"\"\"\n+        if not self.with_pts_bbox:\n+            return None\n+        voxels, num_points, coors = self.voxelize(pts)\n+        voxel_features = self.pts_voxel_encoder(voxels, num_points, coors,\n+                                                img_feats, img_metas)\n+        batch_size = coors[-1, 0] + 1\n+        x = self.pts_middle_encoder(voxel_features, coors, batch_size)\n+        x = self.pts_backbone(x)\n+        if self.with_pts_neck:\n+            x = self.pts_neck(x)\n+        return x\n+\n+    def extract_feat(self, points, img, img_metas):\n+        \"\"\"Extract features from images and points.\"\"\"\n+        img_feats = self.extract_img_feat(img, img_metas)\n+        pts_feats = self.extract_pts_feat(points, img_feats, img_metas)\n+        return (img_feats, pts_feats)\n+\n+    @torch.no_grad()\n+    @force_fp32()\n+    def voxelize(self, points):\n+        \"\"\"Apply dynamic voxelization to points.\n+\n+        Args:\n+            points (list[torch.Tensor]): Points of each sample.\n+\n+        Returns:\n+            tuple[torch.Tensor]: Concatenated points, number of points\n+                per voxel, and coordinates.\n+        \"\"\"\n+        voxels, coors, num_points = [], [], []\n+        for res in points:\n+            res_voxels, res_coors, res_num_points = self.pts_voxel_layer(res)\n+            voxels.append(res_voxels)\n+            coors.append(res_coors)\n+            num_points.append(res_num_points)\n+        voxels = torch.cat(voxels, dim=0)\n+        num_points = torch.cat(num_points, dim=0)\n+        coors_batch = []\n+        for i, coor in enumerate(coors):\n+            coor_pad = F.pad(coor, (1, 0), mode='constant', value=i)\n+            coors_batch.append(coor_pad)\n+        coors_batch = torch.cat(coors_batch, dim=0)\n+        return voxels, num_points, coors_batch\n+\n+    def forward_train(self,\n+                      points=None,\n+                      img_metas=None,\n+                      gt_bboxes_3d=None,\n+                      gt_labels_3d=None,\n+                      gt_labels=None,\n+                      gt_bboxes=None,\n+                      img=None,\n+                      proposals=None,\n+                      gt_bboxes_ignore=None):\n+        \"\"\"Forward training function.\n+\n+        Args:\n+            points (list[torch.Tensor], optional): Points of each sample.\n+                Defaults to None.\n+            img_metas (list[dict], optional): Meta information of each sample.\n+                Defaults to None.\n+            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`], optional):\n+                Ground truth 3D boxes. Defaults to None.\n+            gt_labels_3d (list[torch.Tensor], optional): Ground truth labels\n+                of 3D boxes. Defaults to None.\n+            gt_labels (list[torch.Tensor], optional): Ground truth labels\n+                of 2D boxes in images. Defaults to None.\n+            gt_bboxes (list[torch.Tensor], optional): Ground truth 2D boxes in\n+                images. Defaults to None.\n+            img (torch.Tensor, optional): Images of each sample with shape\n+                (N, C, H, W). Defaults to None.\n+            proposals ([list[torch.Tensor], optional): Predicted proposals\n+                used for training Fast RCNN. Defaults to None.\n+            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n+                2D boxes in images to be ignored. Defaults to None.\n+\n+        Returns:\n+            dict: Losses of different branches.\n+        \"\"\"\n+        img_feats, pts_feats = self.extract_feat(\n+            points, img=img, img_metas=img_metas)\n+        losses = dict()\n+        if pts_feats:\n+            losses_pts = self.forward_pts_train(pts_feats, gt_bboxes_3d,\n+                                                gt_labels_3d, img_metas,\n+                                                gt_bboxes_ignore)\n+            losses.update(losses_pts)\n+        if img_feats:\n+            losses_img = self.forward_img_train(\n+                img_feats,\n+                img_metas=img_metas,\n+                gt_bboxes=gt_bboxes,\n+                gt_labels=gt_labels,\n+                gt_bboxes_ignore=gt_bboxes_ignore,\n+                proposals=proposals)\n+            losses.update(losses_img)\n+        return losses\n+\n+    def forward_pts_train(self,\n+                          pts_feats,\n+                          gt_bboxes_3d,\n+                          gt_labels_3d,\n+                          img_metas,\n+                          gt_bboxes_ignore=None):\n+        \"\"\"Forward function for point cloud branch.\n+\n+        Args:\n+            pts_feats (list[torch.Tensor]): Features of point cloud branch\n+            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`]): Ground truth\n+                boxes for each sample.\n+            gt_labels_3d (list[torch.Tensor]): Ground truth labels for\n+                boxes of each sampole\n+            img_metas (list[dict]): Meta information of samples.\n+            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n+                boxes to be ignored. Defaults to None.\n+\n+        Returns:\n+            dict: Losses of each branch.\n+        \"\"\"\n+        outs = self.pts_bbox_head(pts_feats)\n+        loss_inputs = outs + (gt_bboxes_3d, gt_labels_3d, img_metas)\n+        losses = self.pts_bbox_head.loss(\n+            *loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n+        return losses\n+\n+    def forward_img_train(self,\n+                          x,\n+                          img_metas,\n+                          gt_bboxes,\n+                          gt_labels,\n+                          gt_bboxes_ignore=None,\n+                          proposals=None,\n+                          **kwargs):\n+        \"\"\"Forward function for image branch.\n+\n+        This function works similar to the forward function of Faster R-CNN.\n+\n+        Args:\n+            x (list[torch.Tensor]): Image features of shape (B, C, H, W)\n+                of multiple levels.\n+            img_metas (list[dict]): Meta information of images.\n+            gt_bboxes (list[torch.Tensor]): Ground truth boxes of each image\n+                sample.\n+            gt_labels (list[torch.Tensor]): Ground truth labels of boxes.\n+            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n+                boxes to be ignored. Defaults to None.\n+            proposals (list[torch.Tensor], optional): Proposals of each sample.\n+                Defaults to None.\n+\n+        Returns:\n+            dict: Losses of each branch.\n+        \"\"\"\n+        losses = dict()\n+        # RPN forward and loss\n+        if self.with_img_rpn:\n+            rpn_outs = self.img_rpn_head(x)\n+            rpn_loss_inputs = rpn_outs + (gt_bboxes, img_metas,\n+                                          self.train_cfg.img_rpn)\n+            rpn_losses = self.img_rpn_head.loss(\n+                *rpn_loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n+            losses.update(rpn_losses)\n+\n+            proposal_cfg = self.train_cfg.get('img_rpn_proposal',\n+                                              self.test_cfg.img_rpn)\n+            proposal_inputs = rpn_outs + (img_metas, proposal_cfg)\n+            proposal_list = self.img_rpn_head.get_bboxes(*proposal_inputs)\n+        else:\n+            proposal_list = proposals\n+\n+        # bbox head forward and loss\n+        if self.with_img_bbox:\n+            # bbox head forward and loss\n+            img_roi_losses = self.img_roi_head.forward_train(\n+                x, img_metas, proposal_list, gt_bboxes, gt_labels,\n+                gt_bboxes_ignore, **kwargs)\n+            losses.update(img_roi_losses)\n+\n+        return losses\n+\n+    def simple_test_img(self, x, img_metas, proposals=None, rescale=False):\n+        \"\"\"Test without augmentation.\"\"\"\n+        if proposals is None:\n+            proposal_list = self.simple_test_rpn(x, img_metas,\n+                                                 self.test_cfg.img_rpn)\n+        else:\n+            proposal_list = proposals\n+\n+        return self.img_roi_head.simple_test(\n+            x, proposal_list, img_metas, rescale=rescale)\n+\n+    def simple_test_rpn(self, x, img_metas, rpn_test_cfg):\n+        \"\"\"RPN test function.\"\"\"\n+        rpn_outs = self.img_rpn_head(x)\n+        proposal_inputs = rpn_outs + (img_metas, rpn_test_cfg)\n+        proposal_list = self.img_rpn_head.get_bboxes(*proposal_inputs)\n+        return proposal_list\n+\n+    def simple_test_pts(self, x, img_metas, rescale=False):\n+        \"\"\"Test function of point cloud branch.\"\"\"\n+        outs = self.pts_bbox_head(x)\n+        bbox_list = self.pts_bbox_head.get_bboxes(\n+            *outs, img_metas, rescale=rescale)\n+        bbox_results = [\n+            bbox3d2result(bboxes, scores, labels)\n+            for bboxes, scores, labels in bbox_list\n+        ]\n+        return bbox_results\n+\n+    def simple_test(self, points, img_metas, img=None, rescale=False):\n+        \"\"\"Test function without augmentaiton.\"\"\"\n+        img_feats, pts_feats = self.extract_feat(\n+            points, img=img, img_metas=img_metas)\n+\n+        bbox_list = [dict() for i in range(len(img_metas))]\n+        if pts_feats and self.with_pts_bbox:\n+            bbox_pts = self.simple_test_pts(\n+                pts_feats, img_metas, rescale=rescale)\n+            for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n+                result_dict['pts_bbox'] = pts_bbox\n+        if img_feats and self.with_img_bbox:\n+            bbox_img = self.simple_test_img(\n+                img_feats, img_metas, rescale=rescale)\n+            for result_dict, img_bbox in zip(bbox_list, bbox_img):\n+                result_dict['img_bbox'] = img_bbox\n+        return bbox_list\n+\n+    def aug_test(self, points, img_metas, imgs=None, rescale=False):\n+        \"\"\"Test function with augmentaiton.\"\"\"\n+        img_feats, pts_feats = self.extract_feats(points, img_metas, imgs)\n+\n+        bbox_list = dict()\n+        if pts_feats and self.with_pts_bbox:\n+            bbox_pts = self.aug_test_pts(pts_feats, img_metas, rescale)\n+            bbox_list.update(pts_bbox=bbox_pts)\n+        return [bbox_list]\n+\n+    def extract_feats(self, points, img_metas, imgs=None):\n+        \"\"\"Extract point and image features of multiple samples.\"\"\"\n+        if imgs is None:\n+            imgs = [None] * len(img_metas)\n+        img_feats, pts_feats = multi_apply(self.extract_feat, points, imgs,\n+                                           img_metas)\n+        return img_feats, pts_feats\n+\n+    def aug_test_pts(self, feats, img_metas, rescale=False):\n+        \"\"\"Test function of point cloud branch with augmentaiton.\"\"\"\n+        # only support aug_test for one sample\n+        aug_bboxes = []\n+        for x, img_meta in zip(feats, img_metas):\n+            outs = self.pts_bbox_head(x)\n+            bbox_list = self.pts_bbox_head.get_bboxes(\n+                *outs, img_meta, rescale=rescale)\n+            bbox_list = [\n+                dict(boxes_3d=bboxes, scores_3d=scores, labels_3d=labels)\n+                for bboxes, scores, labels in bbox_list\n+            ]\n+            aug_bboxes.append(bbox_list[0])\n+\n+        # after merging, bboxes will be rescaled to the original image size\n+        merged_bboxes = merge_aug_bboxes_3d(aug_bboxes, img_metas,\n+                                            self.pts_bbox_head.test_cfg)\n+        return merged_bboxes\n+\n+    def show_results(self, data, result, out_dir):\n+        \"\"\"Results visualization.\n+\n+        Args:\n+            data (dict): Input points and the information of the sample.\n+            result (dict): Prediction results.\n+            out_dir (str): Output directory of visualization result.\n+        \"\"\"\n+        for batch_id in range(len(result)):\n+            if isinstance(data['points'][0], DC):\n+                points = data['points'][0]._data[0][batch_id].numpy()\n+            elif mmcv.is_list_of(data['points'][0], torch.Tensor):\n+                points = data['points'][0][batch_id]\n+            else:\n+                ValueError(f\"Unsupported data type {type(data['points'][0])} \"\n+                           f'for visualization!')\n+            if isinstance(data['img_metas'][0], DC):\n+                pts_filename = data['img_metas'][0]._data[0][batch_id][\n+                    'pts_filename']\n+                box_mode_3d = data['img_metas'][0]._data[0][batch_id][\n+                    'box_mode_3d']\n+            elif mmcv.is_list_of(data['img_metas'][0], dict):\n+                pts_filename = data['img_metas'][0][batch_id]['pts_filename']\n+                box_mode_3d = data['img_metas'][0][batch_id]['box_mode_3d']\n+            else:\n+                ValueError(\n+                    f\"Unsupported data type {type(data['img_metas'][0])} \"\n+                    f'for visualization!')\n+            file_name = osp.split(pts_filename)[-1].split('.')[0]\n+\n+            assert out_dir is not None, 'Expect out_dir, got none.'\n+            inds = result[batch_id]['pts_bbox']['scores_3d'] > 0.1\n+            pred_bboxes = result[batch_id]['pts_bbox']['boxes_3d'][inds]\n+\n+            # for now we convert points and bbox into depth mode\n+            if (box_mode_3d == Box3DMode.CAM) or (box_mode_3d\n+                                                  == Box3DMode.LIDAR):\n+                points = Coord3DMode.convert_point(points, Coord3DMode.LIDAR,\n+                                                   Coord3DMode.DEPTH)\n+                pred_bboxes = Box3DMode.convert(pred_bboxes, box_mode_3d,\n+                                                Box3DMode.DEPTH)\n+            elif box_mode_3d != Box3DMode.DEPTH:\n+                ValueError(\n+                    f'Unsupported box_mode_3d {box_mode_3d} for conversion!')\n+\n+            pred_bboxes = pred_bboxes.tensor.cpu().numpy()\n+            show_result(points, None, pred_bboxes, out_dir, file_name)\n"
                }
            ],
            "date": 1716025348559,
            "name": "Commit-0",
            "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport warnings\nfrom os import path as osp\n\nimport mmcv\nimport torch\nfrom mmcv.ops import Voxelization\nfrom mmcv.parallel import DataContainer as DC\nfrom mmcv.runner import force_fp32\nfrom torch.nn import functional as F\n\nfrom mmdet3d.core import (Box3DMode, Coord3DMode, bbox3d2result,\n                          merge_aug_bboxes_3d, show_result)\nfrom mmdet.core import multi_apply\nfrom .. import builder\nfrom ..builder import DETECTORS\nfrom .base import Base3DDetector\n\n\n@DETECTORS.register_module()\nclass MVXTwoStageDetector(Base3DDetector):\n    \"\"\"Base class of Multi-modality VoxelNet.\"\"\"\n\n    def __init__(self,\n                 pts_voxel_layer=None,\n                 pts_voxel_encoder=None,\n                 pts_middle_encoder=None,\n                 pts_fusion_layer=None,\n                 img_backbone=None,\n                 pts_backbone=None,\n                 img_neck=None,\n                 pts_neck=None,\n                 pts_bbox_head=None,\n                 img_roi_head=None,\n                 img_rpn_head=None,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None,\n                 init_cfg=None):\n        super(MVXTwoStageDetector, self).__init__(init_cfg=init_cfg)\n\n        if pts_voxel_layer:\n            self.pts_voxel_layer = Voxelization(**pts_voxel_layer)\n        if pts_voxel_encoder:\n            self.pts_voxel_encoder = builder.build_voxel_encoder(\n                pts_voxel_encoder)\n        if pts_middle_encoder:\n            self.pts_middle_encoder = builder.build_middle_encoder(\n                pts_middle_encoder)\n        if pts_backbone:\n            self.pts_backbone = builder.build_backbone(pts_backbone)\n        if pts_fusion_layer:\n            self.pts_fusion_layer = builder.build_fusion_layer(\n                pts_fusion_layer)\n        if pts_neck is not None:\n            self.pts_neck = builder.build_neck(pts_neck)\n        if pts_bbox_head:\n            pts_train_cfg = train_cfg.pts if train_cfg else None\n            pts_bbox_head.update(train_cfg=pts_train_cfg)\n            pts_test_cfg = test_cfg.pts if test_cfg else None\n            pts_bbox_head.update(test_cfg=pts_test_cfg)\n            self.pts_bbox_head = builder.build_head(pts_bbox_head)\n\n        if img_backbone:\n            self.img_backbone = builder.build_backbone(img_backbone)\n        if img_neck is not None:\n            self.img_neck = builder.build_neck(img_neck)\n        if img_rpn_head is not None:\n            self.img_rpn_head = builder.build_head(img_rpn_head)\n        if img_roi_head is not None:\n            self.img_roi_head = builder.build_head(img_roi_head)\n\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n        if pretrained is None:\n            img_pretrained = None\n            pts_pretrained = None\n        elif isinstance(pretrained, dict):\n            img_pretrained = pretrained.get('img', None)\n            pts_pretrained = pretrained.get('pts', None)\n        else:\n            raise ValueError(\n                f'pretrained should be a dict, got {type(pretrained)}')\n\n        if self.with_img_backbone:\n            if img_pretrained is not None:\n                warnings.warn('DeprecationWarning: pretrained is a deprecated '\n                              'key, please consider using init_cfg.')\n                self.img_backbone.init_cfg = dict(\n                    type='Pretrained', checkpoint=img_pretrained)\n        if self.with_img_roi_head:\n            if img_pretrained is not None:\n                warnings.warn('DeprecationWarning: pretrained is a deprecated '\n                              'key, please consider using init_cfg.')\n                self.img_roi_head.init_cfg = dict(\n                    type='Pretrained', checkpoint=img_pretrained)\n        if self.with_pts_backbone:\n            if pts_pretrained is not None:\n                warnings.warn('DeprecationWarning: pretrained is a deprecated '\n                              'key, please consider using init_cfg')\n                self.pts_backbone.init_cfg = dict(\n                    type='Pretrained', checkpoint=pts_pretrained)\n\n    @property\n    def with_img_shared_head(self):\n        \"\"\"bool: Whether the detector has a shared head in image branch.\"\"\"\n        return hasattr(self,\n                       'img_shared_head') and self.img_shared_head is not None\n\n    @property\n    def with_pts_bbox(self):\n        \"\"\"bool: Whether the detector has a 3D box head.\"\"\"\n        return hasattr(self,\n                       'pts_bbox_head') and self.pts_bbox_head is not None\n\n    @property\n    def with_img_bbox(self):\n        \"\"\"bool: Whether the detector has a 2D image box head.\"\"\"\n        return hasattr(self,\n                       'img_bbox_head') and self.img_bbox_head is not None\n\n    @property\n    def with_img_backbone(self):\n        \"\"\"bool: Whether the detector has a 2D image backbone.\"\"\"\n        return hasattr(self, 'img_backbone') and self.img_backbone is not None\n\n    @property\n    def with_pts_backbone(self):\n        \"\"\"bool: Whether the detector has a 3D backbone.\"\"\"\n        return hasattr(self, 'pts_backbone') and self.pts_backbone is not None\n\n    @property\n    def with_fusion(self):\n        \"\"\"bool: Whether the detector has a fusion layer.\"\"\"\n        return hasattr(self,\n                       'pts_fusion_layer') and self.fusion_layer is not None\n\n    @property\n    def with_img_neck(self):\n        \"\"\"bool: Whether the detector has a neck in image branch.\"\"\"\n        return hasattr(self, 'img_neck') and self.img_neck is not None\n\n    @property\n    def with_pts_neck(self):\n        \"\"\"bool: Whether the detector has a neck in 3D detector branch.\"\"\"\n        return hasattr(self, 'pts_neck') and self.pts_neck is not None\n\n    @property\n    def with_img_rpn(self):\n        \"\"\"bool: Whether the detector has a 2D RPN in image detector branch.\"\"\"\n        return hasattr(self, 'img_rpn_head') and self.img_rpn_head is not None\n\n    @property\n    def with_img_roi_head(self):\n        \"\"\"bool: Whether the detector has a RoI Head in image branch.\"\"\"\n        return hasattr(self, 'img_roi_head') and self.img_roi_head is not None\n\n    @property\n    def with_voxel_encoder(self):\n        \"\"\"bool: Whether the detector has a voxel encoder.\"\"\"\n        return hasattr(self,\n                       'voxel_encoder') and self.voxel_encoder is not None\n\n    @property\n    def with_middle_encoder(self):\n        \"\"\"bool: Whether the detector has a middle encoder.\"\"\"\n        return hasattr(self,\n                       'middle_encoder') and self.middle_encoder is not None\n\n    def extract_img_feat(self, img, img_metas):\n        \"\"\"Extract features of images.\"\"\"\n        if self.with_img_backbone and img is not None:\n            input_shape = img.shape[-2:]\n            # update real input shape of each single img\n            for img_meta in img_metas:\n                img_meta.update(input_shape=input_shape)\n\n            if img.dim() == 5 and img.size(0) == 1:\n                img.squeeze_()\n            elif img.dim() == 5 and img.size(0) > 1:\n                B, N, C, H, W = img.size()\n                img = img.view(B * N, C, H, W)\n            img_feats = self.img_backbone(img)\n        else:\n            return None\n        if self.with_img_neck:\n            img_feats = self.img_neck(img_feats)\n        return img_feats\n\n    def extract_pts_feat(self, pts, img_feats, img_metas):\n        \"\"\"Extract features of points.\"\"\"\n        if not self.with_pts_bbox:\n            return None\n        voxels, num_points, coors = self.voxelize(pts)\n        voxel_features = self.pts_voxel_encoder(voxels, num_points, coors,\n                                                img_feats, img_metas)\n        batch_size = coors[-1, 0] + 1\n        x = self.pts_middle_encoder(voxel_features, coors, batch_size)\n        x = self.pts_backbone(x)\n        if self.with_pts_neck:\n            x = self.pts_neck(x)\n        return x\n\n    def extract_feat(self, points, img, img_metas):\n        \"\"\"Extract features from images and points.\"\"\"\n        img_feats = self.extract_img_feat(img, img_metas)\n        pts_feats = self.extract_pts_feat(points, img_feats, img_metas)\n        return (img_feats, pts_feats)\n\n    @torch.no_grad()\n    @force_fp32()\n    def voxelize(self, points):\n        \"\"\"Apply dynamic voxelization to points.\n\n        Args:\n            points (list[torch.Tensor]): Points of each sample.\n\n        Returns:\n            tuple[torch.Tensor]: Concatenated points, number of points\n                per voxel, and coordinates.\n        \"\"\"\n        voxels, coors, num_points = [], [], []\n        for res in points:\n            res_voxels, res_coors, res_num_points = self.pts_voxel_layer(res)\n            voxels.append(res_voxels)\n            coors.append(res_coors)\n            num_points.append(res_num_points)\n        voxels = torch.cat(voxels, dim=0)\n        num_points = torch.cat(num_points, dim=0)\n        coors_batch = []\n        for i, coor in enumerate(coors):\n            coor_pad = F.pad(coor, (1, 0), mode='constant', value=i)\n            coors_batch.append(coor_pad)\n        coors_batch = torch.cat(coors_batch, dim=0)\n        return voxels, num_points, coors_batch\n\n    def forward_train(self,\n                      points=None,\n                      img_metas=None,\n                      gt_bboxes_3d=None,\n                      gt_labels_3d=None,\n                      gt_labels=None,\n                      gt_bboxes=None,\n                      img=None,\n                      proposals=None,\n                      gt_bboxes_ignore=None):\n        \"\"\"Forward training function.\n\n        Args:\n            points (list[torch.Tensor], optional): Points of each sample.\n                Defaults to None.\n            img_metas (list[dict], optional): Meta information of each sample.\n                Defaults to None.\n            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`], optional):\n                Ground truth 3D boxes. Defaults to None.\n            gt_labels_3d (list[torch.Tensor], optional): Ground truth labels\n                of 3D boxes. Defaults to None.\n            gt_labels (list[torch.Tensor], optional): Ground truth labels\n                of 2D boxes in images. Defaults to None.\n            gt_bboxes (list[torch.Tensor], optional): Ground truth 2D boxes in\n                images. Defaults to None.\n            img (torch.Tensor, optional): Images of each sample with shape\n                (N, C, H, W). Defaults to None.\n            proposals ([list[torch.Tensor], optional): Predicted proposals\n                used for training Fast RCNN. Defaults to None.\n            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n                2D boxes in images to be ignored. Defaults to None.\n\n        Returns:\n            dict: Losses of different branches.\n        \"\"\"\n        img_feats, pts_feats = self.extract_feat(\n            points, img=img, img_metas=img_metas)\n        losses = dict()\n        if pts_feats:\n            losses_pts = self.forward_pts_train(pts_feats, gt_bboxes_3d,\n                                                gt_labels_3d, img_metas,\n                                                gt_bboxes_ignore)\n            losses.update(losses_pts)\n        if img_feats:\n            losses_img = self.forward_img_train(\n                img_feats,\n                img_metas=img_metas,\n                gt_bboxes=gt_bboxes,\n                gt_labels=gt_labels,\n                gt_bboxes_ignore=gt_bboxes_ignore,\n                proposals=proposals)\n            losses.update(losses_img)\n        return losses\n\n    def forward_pts_train(self,\n                          pts_feats,\n                          gt_bboxes_3d,\n                          gt_labels_3d,\n                          img_metas,\n                          gt_bboxes_ignore=None):\n        \"\"\"Forward function for point cloud branch.\n\n        Args:\n            pts_feats (list[torch.Tensor]): Features of point cloud branch\n            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`]): Ground truth\n                boxes for each sample.\n            gt_labels_3d (list[torch.Tensor]): Ground truth labels for\n                boxes of each sampole\n            img_metas (list[dict]): Meta information of samples.\n            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n                boxes to be ignored. Defaults to None.\n\n        Returns:\n            dict: Losses of each branch.\n        \"\"\"\n        outs = self.pts_bbox_head(pts_feats)\n        loss_inputs = outs + (gt_bboxes_3d, gt_labels_3d, img_metas)\n        losses = self.pts_bbox_head.loss(\n            *loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n        return losses\n\n    def forward_img_train(self,\n                          x,\n                          img_metas,\n                          gt_bboxes,\n                          gt_labels,\n                          gt_bboxes_ignore=None,\n                          proposals=None,\n                          **kwargs):\n        \"\"\"Forward function for image branch.\n\n        This function works similar to the forward function of Faster R-CNN.\n\n        Args:\n            x (list[torch.Tensor]): Image features of shape (B, C, H, W)\n                of multiple levels.\n            img_metas (list[dict]): Meta information of images.\n            gt_bboxes (list[torch.Tensor]): Ground truth boxes of each image\n                sample.\n            gt_labels (list[torch.Tensor]): Ground truth labels of boxes.\n            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth\n                boxes to be ignored. Defaults to None.\n            proposals (list[torch.Tensor], optional): Proposals of each sample.\n                Defaults to None.\n\n        Returns:\n            dict: Losses of each branch.\n        \"\"\"\n        losses = dict()\n        # RPN forward and loss\n        if self.with_img_rpn:\n            rpn_outs = self.img_rpn_head(x)\n            rpn_loss_inputs = rpn_outs + (gt_bboxes, img_metas,\n                                          self.train_cfg.img_rpn)\n            rpn_losses = self.img_rpn_head.loss(\n                *rpn_loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n            losses.update(rpn_losses)\n\n            proposal_cfg = self.train_cfg.get('img_rpn_proposal',\n                                              self.test_cfg.img_rpn)\n            proposal_inputs = rpn_outs + (img_metas, proposal_cfg)\n            proposal_list = self.img_rpn_head.get_bboxes(*proposal_inputs)\n        else:\n            proposal_list = proposals\n\n        # bbox head forward and loss\n        if self.with_img_bbox:\n            # bbox head forward and loss\n            img_roi_losses = self.img_roi_head.forward_train(\n                x, img_metas, proposal_list, gt_bboxes, gt_labels,\n                gt_bboxes_ignore, **kwargs)\n            losses.update(img_roi_losses)\n\n        return losses\n\n    def simple_test_img(self, x, img_metas, proposals=None, rescale=False):\n        \"\"\"Test without augmentation.\"\"\"\n        if proposals is None:\n            proposal_list = self.simple_test_rpn(x, img_metas,\n                                                 self.test_cfg.img_rpn)\n        else:\n            proposal_list = proposals\n\n        return self.img_roi_head.simple_test(\n            x, proposal_list, img_metas, rescale=rescale)\n\n    def simple_test_rpn(self, x, img_metas, rpn_test_cfg):\n        \"\"\"RPN test function.\"\"\"\n        rpn_outs = self.img_rpn_head(x)\n        proposal_inputs = rpn_outs + (img_metas, rpn_test_cfg)\n        proposal_list = self.img_rpn_head.get_bboxes(*proposal_inputs)\n        return proposal_list\n\n    def simple_test_pts(self, x, img_metas, rescale=False):\n        \"\"\"Test function of point cloud branch.\"\"\"\n        outs = self.pts_bbox_head(x)\n        bbox_list = self.pts_bbox_head.get_bboxes(\n            *outs, img_metas, rescale=rescale)\n        bbox_results = [\n            bbox3d2result(bboxes, scores, labels)\n            for bboxes, scores, labels in bbox_list\n        ]\n        return bbox_results\n\n    def simple_test(self, points, img_metas, img=None, rescale=False):\n        \"\"\"Test function without augmentaiton.\"\"\"\n        img_feats, pts_feats = self.extract_feat(\n            points, img=img, img_metas=img_metas)\n\n        bbox_list = [dict() for i in range(len(img_metas))]\n        if pts_feats and self.with_pts_bbox:\n            bbox_pts = self.simple_test_pts(\n                pts_feats, img_metas, rescale=rescale)\n            for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n                result_dict['pts_bbox'] = pts_bbox\n        if img_feats and self.with_img_bbox:\n            bbox_img = self.simple_test_img(\n                img_feats, img_metas, rescale=rescale)\n            for result_dict, img_bbox in zip(bbox_list, bbox_img):\n                result_dict['img_bbox'] = img_bbox\n        return bbox_list\n\n    def aug_test(self, points, img_metas, imgs=None, rescale=False):\n        \"\"\"Test function with augmentaiton.\"\"\"\n        img_feats, pts_feats = self.extract_feats(points, img_metas, imgs)\n\n        bbox_list = dict()\n        if pts_feats and self.with_pts_bbox:\n            bbox_pts = self.aug_test_pts(pts_feats, img_metas, rescale)\n            bbox_list.update(pts_bbox=bbox_pts)\n        return [bbox_list]\n\n    def extract_feats(self, points, img_metas, imgs=None):\n        \"\"\"Extract point and image features of multiple samples.\"\"\"\n        if imgs is None:\n            imgs = [None] * len(img_metas)\n        img_feats, pts_feats = multi_apply(self.extract_feat, points, imgs,\n                                           img_metas)\n        return img_feats, pts_feats\n\n    def aug_test_pts(self, feats, img_metas, rescale=False):\n        \"\"\"Test function of point cloud branch with augmentaiton.\"\"\"\n        # only support aug_test for one sample\n        aug_bboxes = []\n        for x, img_meta in zip(feats, img_metas):\n            outs = self.pts_bbox_head(x)\n            bbox_list = self.pts_bbox_head.get_bboxes(\n                *outs, img_meta, rescale=rescale)\n            bbox_list = [\n                dict(boxes_3d=bboxes, scores_3d=scores, labels_3d=labels)\n                for bboxes, scores, labels in bbox_list\n            ]\n            aug_bboxes.append(bbox_list[0])\n\n        # after merging, bboxes will be rescaled to the original image size\n        merged_bboxes = merge_aug_bboxes_3d(aug_bboxes, img_metas,\n                                            self.pts_bbox_head.test_cfg)\n        return merged_bboxes\n\n    def show_results(self, data, result, out_dir):\n        \"\"\"Results visualization.\n\n        Args:\n            data (dict): Input points and the information of the sample.\n            result (dict): Prediction results.\n            out_dir (str): Output directory of visualization result.\n        \"\"\"\n        for batch_id in range(len(result)):\n            if isinstance(data['points'][0], DC):\n                points = data['points'][0]._data[0][batch_id].numpy()\n            elif mmcv.is_list_of(data['points'][0], torch.Tensor):\n                points = data['points'][0][batch_id]\n            else:\n                ValueError(f\"Unsupported data type {type(data['points'][0])} \"\n                           f'for visualization!')\n            if isinstance(data['img_metas'][0], DC):\n                pts_filename = data['img_metas'][0]._data[0][batch_id][\n                    'pts_filename']\n                box_mode_3d = data['img_metas'][0]._data[0][batch_id][\n                    'box_mode_3d']\n            elif mmcv.is_list_of(data['img_metas'][0], dict):\n                pts_filename = data['img_metas'][0][batch_id]['pts_filename']\n                box_mode_3d = data['img_metas'][0][batch_id]['box_mode_3d']\n            else:\n                ValueError(\n                    f\"Unsupported data type {type(data['img_metas'][0])} \"\n                    f'for visualization!')\n            file_name = osp.split(pts_filename)[-1].split('.')[0]\n\n            assert out_dir is not None, 'Expect out_dir, got none.'\n            inds = result[batch_id]['pts_bbox']['scores_3d'] > 0.1\n            pred_bboxes = result[batch_id]['pts_bbox']['boxes_3d'][inds]\n\n            # for now we convert points and bbox into depth mode\n            if (box_mode_3d == Box3DMode.CAM) or (box_mode_3d\n                                                  == Box3DMode.LIDAR):\n                points = Coord3DMode.convert_point(points, Coord3DMode.LIDAR,\n                                                   Coord3DMode.DEPTH)\n                pred_bboxes = Box3DMode.convert(pred_bboxes, box_mode_3d,\n                                                Box3DMode.DEPTH)\n            elif box_mode_3d != Box3DMode.DEPTH:\n                ValueError(\n                    f'Unsupported box_mode_3d {box_mode_3d} for conversion!')\n\n            pred_bboxes = pred_bboxes.tensor.cpu().numpy()\n            show_result(points, None, pred_bboxes, out_dir, file_name)\n"
        }
    ]
}