{
    "sourceFile": "mmdet3d/models/detectors/d3pd_v3.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 9,
            "patches": [
                {
                    "date": 1716001431622,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1716004335545,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n from mmdet3d.core import bbox3d2result\n import matplotlib.pyplot as plt\n from termcolor import colored\n from mmdet.models.backbones.resnet import ResNet\n-from mmdet3d.models.detectors.bevdet_v3 import BEVStereo4D\n+from mmdet3d.models.detectors.bevdet import BEVStereo4D\n from mmdet.utils import get_root_logger\n from mmdet3d.models.detectors import D3PD\n \n \n"
                },
                {
                    "date": 1716015338522,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -432,22 +432,22 @@\n         pts_feats, pts_ms_feats = self.extract_pts_feat(points, img_feats, img_metas)\n \n         det_input_dict = self.fusion_img_radar_bev(img_feats, radar_feats)\n \n-        base_path = \"/mnt/data/exps/DenseRadar/out/v3-feats-out\"\n-        feats_to_img(\n-            det_input_dict[\"det_feats\"], base_path=base_path, suffix=\"img_feats_d3pd\"\n-        )\n+        # base_path = \"/mnt/data/exps/DenseRadar/out/v3-feats-out\"\n+        # feats_to_img(\n+        #     det_input_dict[\"det_feats\"], base_path=base_path, suffix=\"img_feats_d3pd\"\n+        # )\n \n-        feats_to_img(\n-            det_input_dict[\"student_sampling_feats\"],\n-            base_path=base_path,\n-            suffix=\"img_feats_sampling\",\n-        )\n-        feats_to_img(radar_feats, base_path=base_path, suffix=\"radar_feats\")\n-        feats_to_img(pts_feats, base_path=base_path, suffix=\"pts_feats\")\n+        # feats_to_img(\n+        #     det_input_dict[\"student_sampling_feats\"],\n+        #     base_path=base_path,\n+        #     suffix=\"img_feats_sampling\",\n+        # )\n+        # feats_to_img(radar_feats, base_path=base_path, suffix=\"radar_feats\")\n+        # feats_to_img(pts_feats, base_path=base_path, suffix=\"pts_feats\")\n \n-        raise RuntimeError\n+        # raise RuntimeError\n \n         # feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats_aug\")\n         # feats_to_img(radar_feats, base_path=base_path, suffix=\"radar_feats_aug\")\n \n"
                },
                {
                    "date": 1716015370765,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -631,21 +631,10 @@\n         proposals=None,\n         gt_bboxes_ignore=None,\n         **kwargs,\n     ):\n-        (\n-            img_feats,\n-            pts_feats,\n-            radar_feats,\n-            depth,\n-            det_input_feats,\n-            radar_ms_feats,\n-            pts_ms_feats,\n-        ) = self.extract_feat(\n-            points,\n-            radar=radar,\n-            img=img_inputs,\n-            img_metas=img_metas,\n+        img_feats, pts_feats, radar_feats, depth, feats_wrap = self.extract_feat(\n+            points, radar=radar, img=img_inputs, img_metas=img_metas\n         )\n \n         teacher_outs = self.pts_bbox_head_tea(pts_feats)\n \n"
                },
                {
                    "date": 1716015400381,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -637,13 +637,13 @@\n         )\n \n         teacher_outs = self.pts_bbox_head_tea(pts_feats)\n \n-        teacher_sampling_pos = det_input_feats.get(\"teacher_sampling_pos\")\n-        teacher_sampling_feats = det_input_feats.get(\"teacher_sampling_feats\")\n-        student_sampling_pos = det_input_feats.get(\"student_sampling_pos\")\n-        student_sampling_feats = det_input_feats.get(\"student_sampling_feats\")\n-        det_feats = det_input_feats.get(\"det_feats\")\n+        teacher_sampling_pos = feats_wrap.get(\"teacher_sampling_pos\")\n+        teacher_sampling_feats = feats_wrap.get(\"teacher_sampling_feats\")\n+        student_sampling_pos = feats_wrap.get(\"student_sampling_pos\")\n+        student_sampling_feats = feats_wrap.get(\"student_sampling_feats\")\n+        det_feats = feats_wrap.get(\"det_feats\")\n \n         losses = dict()\n \n         gt_depth = kwargs[\"gt_depth\"]\n"
                },
                {
                    "date": 1716015436209,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -450,21 +450,15 @@\n \n         # feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats_aug\")\n         # feats_to_img(radar_feats, base_path=base_path, suffix=\"radar_feats_aug\")\n \n-        if self.training:\n-            return (\n-                img_feats,\n-                pts_feats,\n-                radar_feats,\n-                depth,\n-                det_input_feats,\n-                radar_ms_feats,\n-                pts_ms_feats,\n-            )\n-        else:\n-            return img_feats[0], pts_feats, radar_feats, depth, det_input_feats\n+        det_input_feats = self.fusion_img_radar_bev(\n+            img_feats, radar_feats, pts_feats=pts_feats\n+        )\n \n+        return (img_feats, pts_feats, radar_feats, depth, det_input_feats)\n+\n+\n     def forward_pts_train(\n         self,\n         pts_feats,\n         gt_bboxes_3d,\n"
                },
                {
                    "date": 1716015436863,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -456,9 +456,8 @@\n         )\n \n         return (img_feats, pts_feats, radar_feats, depth, det_input_feats)\n \n-\n     def forward_pts_train(\n         self,\n         pts_feats,\n         gt_bboxes_3d,\n"
                },
                {
                    "date": 1716015466199,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -406,21 +406,21 @@\n \n     #     return [img_feats], radar_feats, det_input_feats\n \n     def fusion_img_radar_bev(self, img_bev, radar_bev, **kwargs) -> list:\n-        img_bev = img_bev[0] if isinstance(img_bev, list) else img_bev\n+\n         radar_bev = radar_bev[0] if isinstance(radar_bev, list) else radar_bev\n \n         # output = self.reduce_conv(torch.cat([img_bev, radar_bev], dim=1))\n \n         det_input_dict = {}\n \n         if self.rc_bev_fusion:\n             output = self.rc_bev_fusion(img_bev, radar_bev)\n-            if isinstance(output, dict):\n+            if not isinstance(output, dict):\n+                det_input_dict.update({\"det_feats\": [output]})\n+            else:\n                 det_input_dict.update(output)\n-            else:\n-                det_input_dict.update({\"det_feats\": output})\n \n         return det_input_dict\n \n     def extract_feat(self, points, radar, img, img_metas):\n"
                },
                {
                    "date": 1716015487500,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -430,9 +430,8 @@\n         # img_feats, depth=super(BEVStereo4D)\n         radar_feats, radar_ms_feats = self.extract_radar_feat(radar)\n         pts_feats, pts_ms_feats = self.extract_pts_feat(points, img_feats, img_metas)\n \n-\n         # base_path = \"/mnt/data/exps/DenseRadar/out/v3-feats-out\"\n         # feats_to_img(\n         #     det_input_dict[\"det_feats\"], base_path=base_path, suffix=\"img_feats_d3pd\"\n         # )\n"
                },
                {
                    "date": 1716019717354,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -407,9 +407,8 @@\n \n     def fusion_img_radar_bev(self, img_bev, radar_bev, **kwargs) -> list:\n         radar_bev = img_bev[0] if isinstance(img_bev, list) else img_bev\n         radar_bev = radar_bev[0] if isinstance(radar_bev, list) else radar_bev\n-        \n \n         # output = self.reduce_conv(torch.cat([img_bev, radar_bev], dim=1))\n \n         det_input_dict = {}\n"
                }
            ],
            "date": 1716001431622,
            "name": "Commit-0",
            "content": "import torch\nfrom torch.nn import functional as F\nfrom mmcv.runner import force_fp32\nfrom mmcv.cnn import ConvModule\nfrom mmdet3d.models.utils.self_print import feats_to_img\nfrom .. import builder\nfrom ..builder import DETECTORS, build_loss\nfrom .centerpoint import CenterPoint\nimport torch.nn as nn\nimport numpy as np\nfrom mmdet3d.ops import Voxelization\nfrom mmdet3d.core import bbox3d2result\nimport matplotlib.pyplot as plt\nfrom termcolor import colored\nfrom mmdet.models.backbones.resnet import ResNet\nfrom mmdet3d.models.detectors.bevdet_v3 import BEVStereo4D\nfrom mmdet.utils import get_root_logger\nfrom mmdet3d.models.detectors import D3PD\n\n\n@DETECTORS.register_module()\nclass D3PD_V3(BEVStereo4D):\n    def __init__(\n        self,\n        teacher_pretrained=None,\n        student_pretrained=None,\n        vovnet_pretrained=None,\n        load_stu_with_tea=None,\n        # img_view_transformer=None,\n        # img_bev_encoder_backbone=None,\n        # img_bev_encoder_neck=None,\n        radar_voxel_layer=None,\n        radar_pillar_encoder=None,\n        radar_middle_encoder=None,\n        radar_backbone=None,\n        radar_neck=None,\n        bi_dire_fusion=None,\n        middle_radar_aug=None,\n        rc_bev_fusion=None,\n        pts_bbox_head_tea=None,\n        distillation=None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.student_pretrained = student_pretrained\n        self.teacher_pretrained = teacher_pretrained\n        self.vovnet_pretrained = vovnet_pretrained\n        self.load_stu_with_tea = load_stu_with_tea\n\n        self.bi_dire_fusion = bi_dire_fusion\n        self.middle_radar_aug = middle_radar_aug\n        self.rc_bev_fusion = rc_bev_fusion\n\n        self.distillation = distillation\n\n        self._init_fusion_module()\n\n        self._init_teacher_model(pts_bbox_head_tea, **kwargs)\n        self._init_student_model()\n        # self._init_img_transformation(\n        #     img_view_transformer, img_bev_encoder_backbone, img_bev_encoder_neck\n        # )\n        self._init_radar_net(\n            radar_voxel_layer,\n            radar_pillar_encoder,\n            radar_middle_encoder,\n            radar_backbone,\n            radar_neck,\n        )\n        # self._init_img_backbone(close=student_pretrained is not None)\n        self._init_img_backbone(close=False)\n        self._init_distill_module()\n\n    def _init_fusion_module(self):\n        self.bi_dire_fusion = (\n            builder.build_neck(self.bi_dire_fusion) if self.bi_dire_fusion else None\n        )\n        self.middle_radar_aug = (\n            builder.build_neck(self.middle_radar_aug) if self.middle_radar_aug else None\n        )\n        self.rc_bev_fusion = (\n            builder.build_neck(self.rc_bev_fusion) if self.rc_bev_fusion else None\n        )\n\n    def _init_teacher_model(self, pts_bbox_head_tea=None, **kwargs):\n\n        if self.pts_middle_encoder:\n            for param in self.pts_middle_encoder.parameters():\n                param.requires_grad = False\n            self.pts_middle_encoder.eval()\n\n        if self.pts_backbone:\n            for param in self.pts_backbone.parameters():\n                param.requires_grad = False\n            self.pts_backbone.eval()\n\n        if self.pts_neck:\n            for param in self.pts_neck.parameters():\n                param.requires_grad = False\n            self.pts_neck.eval()\n\n        if pts_bbox_head_tea:\n            train_cfg = kwargs[\"train_cfg\"]\n            test_cfg = kwargs[\"test_cfg\"]\n            pts_train_cfg = train_cfg.pts if train_cfg else None\n            pts_bbox_head_tea.update(train_cfg=pts_train_cfg)\n            pts_test_cfg = test_cfg.pts if test_cfg else None\n            pts_bbox_head_tea.update(test_cfg=pts_test_cfg)\n            self.pts_bbox_head_tea = builder.build_head(pts_bbox_head_tea)\n\n            teacher_weight = torch.load(\n                self.teacher_pretrained,\n                map_location=\"cuda:{}\".format(torch.cuda.current_device()),\n            )[\"state_dict\"]\n\n            dict_load = {\n                _key.replace(\"pts_bbox_head.\", \"\"): teacher_weight[_key]\n                for _key in teacher_weight\n                if \"pts_bbox_head\" in _key\n            }\n\n            self.pts_bbox_head_tea.load_state_dict(dict_load, strict=False)\n\n            for param in self.pts_bbox_head_tea.parameters():\n                param.requires_grad = False\n            self.pts_bbox_head_tea.eval()\n\n    def _init_student_model(self):\n\n        if self.student_pretrained is not None:\n\n            logger = get_root_logger()\n            student_pretrained_load = [\n                \"img_neck\",\n                \"img_view_transformer\",\n                \"img_bev_encoder_backbone\",\n                \"img_bev_encoder_neck\",\n                \"pre_process\",\n            ]\n\n            sutdent_model = torch.load(\n                self.student_pretrained,\n                map_location=\"cuda:{}\".format(torch.cuda.current_device()),\n            )[\"state_dict\"]\n\n            for load_key in student_pretrained_load:\n\n                dict_load = {\n                    _key.replace(load_key + \".\", \"\"): sutdent_model[_key]\n                    for _key in sutdent_model\n                    if load_key in _key\n                }\n\n                if \"pre_process\" in load_key:\n                    load_key = \"pre_process_net\"\n\n                getattr(self, load_key).load_state_dict(dict_load, strict=False)\n\n                print(\"Loaded pretrained {}\".format(load_key))\n                logger.info(\"Loaded pretrained {}\".format(load_key))\n                assert len(dict_load) > 0\n\n            # for module in student_pretrained_load:\n\n            #     for param in getattr(self, module).parameters():\n            #         param.requires_grad = False\n            #     getattr(self, module).eval()\n\n        # if self.load_stu_with_tea:\n        #     teacher_weight = torch.load(\n        #         self.teacher_pretrained,\n        #         map_location=\"cuda:{}\".format(torch.cuda.current_device()),\n        #     )[\"state_dict\"]\n\n        #     dict_load = {\n        #         _key.replace(\"pts_bbox_head.task_heads.\", \"\"): teacher_weight[_key]\n        #         for _key in teacher_weight\n        #         if \"pts_bbox_head.task_heads\" in _key\n        #     }\n\n        #     self.pts_bbox_head_stu.task_heads.load_state_dict(dict_load, strict=False)\n\n    # def _init_img_transformation(self):\n    #     pass\n\n    def _init_radar_net(\n        self,\n        radar_voxel_layer,\n        radar_pillar_encoder,\n        radar_middle_encoder,\n        radar_backbone,\n        radar_neck,\n    ):\n        if radar_voxel_layer:\n            self.radar_voxel_layer = Voxelization(**radar_voxel_layer)\n        else:\n            self.radar_voxel_layer = None\n\n        if radar_pillar_encoder:\n            self.radar_pillar_encoder = builder.build_voxel_encoder(\n                radar_pillar_encoder\n            )\n\n        if radar_middle_encoder:\n            self.radar_middle_cfg = radar_middle_encoder\n            self.radar_middle_encoder = builder.build_middle_encoder(\n                radar_middle_encoder\n            )\n\n        if radar_backbone is not None:\n            self.with_radar_backbone = True\n            self.radar_backbone = builder.build_backbone(radar_backbone)\n        else:\n            self.with_radar_backbone = False\n\n        if radar_neck is not None:\n            self.with_radar_neck = True\n            self.radar_neck = builder.build_neck(radar_neck)\n        else:\n            self.with_radar_neck = False\n\n    def _init_img_backbone(self, close=False):\n\n        if self.img_backbone.__class__.__name__ == \"VoVNetCP\":\n            img_backbone_weight = self.vovnet_pretrained\n\n            img_backbone_load = torch.load(\n                img_backbone_weight,\n                map_location=\"cuda:{}\".format(torch.cuda.current_device()),\n            )[\"model\"]\n\n            img_backbone_dict_load = {\n                _key.replace(\"backbone.bottom_up.\", \"\"): img_backbone_load[_key]\n                for _key in img_backbone_load\n                if \"backbone.bottom_up\" in _key\n            }\n\n            self.img_backbone.load_state_dict(img_backbone_dict_load, strict=False)\n            print(colored(\"Loaded pretrained {}\".format(\"backbone.bottom_up\"), \"green\"))\n            assert len(img_backbone_dict_load) > 0\n\n        if close:\n            for param in self.img_backbone.parameters():\n                param.requires_grad = False\n            self.img_backbone.eval()\n\n    def _init_distill_module(self):\n\n        if self.training:\n\n            # Construct sparse feature distillation, including knowledge transfer of image bev and radar bev two view features\n            self.sparse_feats_distill_img_bev = (\n                build_loss(self.distillation.sparse_feats_distill.img_bev_distill)\n                if \"img_bev_distill\" in self.distillation.sparse_feats_distill.keys()\n                else None\n            )\n\n            self.sparse_feats_distill_radar_bev = (\n                build_loss(self.distillation.sparse_feats_distill.radar_bev_distill)\n                if \"radar_bev_distill\" in self.distillation.sparse_feats_distill.keys()\n                else None\n            )\n            self.radar_ms_feats_distill = (\n                build_loss(self.distillation.sparse_feats_distill.radar_ms_distill)\n                if \"sparse_feats_distill\" in self.distillation.keys()\n                and \"radar_ms_distill\" in self.distillation.sparse_feats_distill.keys()\n                else None\n            )\n\n            self.sampling_pos_distill = (\n                build_loss(self.distillation.sampling_pos_distill)\n                if \"sampling_pos_distill\" in self.distillation.keys()\n                else None\n            )\n            self.sampling_feats_distill = (\n                build_loss(self.distillation.sampling_feats_distill)\n                if \"sampling_feats_distill\" in self.distillation.keys()\n                else None\n            )\n\n            self.det_feats_distill = (\n                build_loss(self.distillation.det_feats_distill)\n                if \"det_feats_distill\" in self.distillation.keys()\n                else None\n            )\n\n            if \"det_result_distill\" in self.distillation.keys():\n                self.ret_sum = self.distillation.det_result_distill[\"ret_sum\"]\n                self.dcdistill_loss = build_loss(self.distillation.det_result_distill)\n            else:\n                self.ret_sum = None\n                self.dcdistill_loss = None\n\n            self.smfd_distill_loss = (\n                build_loss(self.distillation.mask_bev_feats_distill)\n                if \"mask_bev_feats_distill\" in self.distillation.keys()\n                else None\n            )\n\n            self.heatmap_aug_distill_loss = (\n                build_loss(self.distillation.heatmap_aug_distill)\n                if \"heatmap_aug_distill\" in self.distillation.keys()\n                else None\n            )\n\n    @torch.no_grad()\n    @force_fp32()\n    def radar_voxelization(self, points):\n        \"\"\"Apply dynamic voxelization to points.\n\n        Args:\n            points (list[torch.Tensor]): Points of each sample.\n\n        Returns:\n            tuple[torch.Tensor]: Concatenated points, number of points\n                per voxel, and coordinates.\n        \"\"\"\n        voxels, coors, num_points = [], [], []\n        for res in points:\n            res_voxels, res_coors, res_num_points = self.radar_voxel_layer(res)\n            voxels.append(res_voxels)\n            coors.append(res_coors)\n            num_points.append(res_num_points)\n        voxels = torch.cat(voxels, dim=0)\n        num_points = torch.cat(num_points, dim=0)\n        coors_batch = []\n        for i, coor in enumerate(coors):\n            coor_pad = F.pad(coor, (1, 0), mode=\"constant\", value=i)\n            coors_batch.append(coor_pad)\n        coors_batch = torch.cat(coors_batch, dim=0)\n        return voxels, num_points, coors_batch\n\n    def extract_pts_feat(self, pts, img_feats, img_metas):\n        \"\"\"Extract features of points.\"\"\"\n        if not self.pts_middle_encoder:\n            return None\n        if not self.with_pts_bbox:\n            return None\n\n        voxels, num_points, coors = self.voxelize(pts)\n\n        voxel_features = self.pts_voxel_encoder(voxels, num_points, coors)\n        batch_size = coors[-1, 0] + 1\n        x = self.pts_middle_encoder(voxel_features, coors, batch_size)\n        x = self.pts_backbone(x)\n\n        if self.radar_ms_feats_distill:\n            pts_ms_feats = [data for data in x]\n\n        if self.with_pts_neck:\n            x = self.pts_neck(x)\n\n        if self.radar_ms_feats_distill:\n            return x, pts_ms_feats\n        else:\n            return x, None\n\n    def extract_radar_feat(self, radar_points, ret_coords=False):\n        \"\"\"Extract features of points.\"\"\"\n        if not self.with_pts_bbox:\n            return None\n\n        # radar visualization\n        # random_sava_num = np.random.randint(0, 10000000000)\n        # radar_points[0].cpu().numpy().tofile(\n        #     f\"./radar_vis/radapoints_{random_sava_num}.bin\"\n        # )\n        voxels, num_points, coors = self.radar_voxelization(radar_points)\n\n        radar_pillar_features = self.radar_pillar_encoder(voxels, num_points, coors)\n        batch_size = coors[-1, 0] + 1\n        x = self.radar_middle_encoder(radar_pillar_features, coors, batch_size)\n\n        if self.with_radar_backbone:\n            x = self.radar_backbone(x)\n\n            if self.radar_ms_feats_distill:\n                radar_ms_feas = [data for data in x]\n\n        if self.with_radar_neck:\n            x = self.radar_neck(x)\n\n        if self.radar_ms_feats_distill:\n            if ret_coords:\n                return x, coors, radar_ms_feas\n            else:\n                return x, radar_ms_feas\n        else:\n            if ret_coords:\n                return x, coors\n            else:\n                return x, None\n\n    # def feats_post_process(self, img_feats, radar_feats, pts_feats=None):\n    #     if self.bi_dire_fusion:\n    #         img_feats, radar_feats = self.bi_dire_fusion(img_feats[0], radar_feats[0])\n\n    #     if self.middle_radar_aug:\n    #         radar_feats = self.middle_radar_aug(img_feats, radar_feats)\n\n    #     det_input_feats = (\n    #         self.rc_bev_fusion(img_feats, radar_feats)\n    #         if self.rc_bev_fusion is not None\n    #         else dict()\n    #     )\n\n    #     return [img_feats], radar_feats, det_input_feats\n\n    def fusion_img_radar_bev(self, img_bev, radar_bev, **kwargs) -> list:\n        img_bev = img_bev[0] if isinstance(img_bev, list) else img_bev\n        radar_bev = radar_bev[0] if isinstance(radar_bev, list) else radar_bev\n\n        # output = self.reduce_conv(torch.cat([img_bev, radar_bev], dim=1))\n\n        det_input_dict = {}\n\n        if self.rc_bev_fusion:\n            output = self.rc_bev_fusion(img_bev, radar_bev)\n            if isinstance(output, dict):\n                det_input_dict.update(output)\n            else:\n                det_input_dict.update({\"det_feats\": output})\n\n        return det_input_dict\n\n    def extract_feat(self, points, radar, img, img_metas):\n        \"\"\"Extract features from images and points.\"\"\"\n\n        img_feats, depth = self.extract_img_feat(img, img_metas)\n        # img_feats, depth=super(BEVStereo4D)\n        radar_feats, radar_ms_feats = self.extract_radar_feat(radar)\n        pts_feats, pts_ms_feats = self.extract_pts_feat(points, img_feats, img_metas)\n\n        det_input_dict = self.fusion_img_radar_bev(img_feats, radar_feats)\n\n        base_path = \"/mnt/data/exps/DenseRadar/out/v3-feats-out\"\n        feats_to_img(\n            det_input_dict[\"det_feats\"], base_path=base_path, suffix=\"img_feats_d3pd\"\n        )\n\n        feats_to_img(\n            det_input_dict[\"student_sampling_feats\"],\n            base_path=base_path,\n            suffix=\"img_feats_sampling\",\n        )\n        feats_to_img(radar_feats, base_path=base_path, suffix=\"radar_feats\")\n        feats_to_img(pts_feats, base_path=base_path, suffix=\"pts_feats\")\n\n        raise RuntimeError\n\n        # feats_to_img(img_feats, base_path=base_path, suffix=\"img_feats_aug\")\n        # feats_to_img(radar_feats, base_path=base_path, suffix=\"radar_feats_aug\")\n\n        if self.training:\n            return (\n                img_feats,\n                pts_feats,\n                radar_feats,\n                depth,\n                det_input_feats,\n                radar_ms_feats,\n                pts_ms_feats,\n            )\n        else:\n            return img_feats[0], pts_feats, radar_feats, depth, det_input_feats\n\n    def forward_pts_train(\n        self,\n        pts_feats,\n        gt_bboxes_3d,\n        gt_labels_3d,\n        img_metas,\n        gt_bboxes_ignore=None,\n    ) -> tuple:\n        \"\"\"Forward function for point cloud branch.\n\n        Args:\n            pts_feats (_type_): Features of point cloud branch\n            gt_bboxes_3d (_type_): Ground truth\n                boxes for each sample.\n            gt_labels_3d (_type_): Ground truth labels for\n                boxes of each sampole\n            img_metas (_type_): Meta information of samples.\n            gt_bboxes_ignore (_type_, optional): Ground truth\n                boxes to be ignored. Defaults to None.. Defaults to None.\n\n        Returns:\n            tuple: _description_\n        \"\"\"\n\n        outs = self.pts_bbox_head(pts_feats)\n        loss_inputs = [gt_bboxes_3d, gt_labels_3d, outs]\n        losses = self.pts_bbox_head.loss(*loss_inputs)\n\n        return losses, outs\n\n    def forward_distill_loss(\n        self,\n        teacher_bev_feats=None,\n        teacher_sampling_pos=None,\n        teacher_sampling_feats=None,\n        tea_resp_bboxes=None,\n        student_bev_feats=None,\n        student_samping_pos=None,\n        student_samping_feats=None,\n        stu_resp_bboxes=None,\n        radar_feats=None,\n        rc_fusion_feats=None,\n        gt_bboxes_3d=None,\n        bda_mat=None,\n        **kwargs,\n    ):\n        losses = {}\n        ivt_cfg = {}\n        # ivt_cfg.update(\n        #     {\n        #         \"dx\": self.img_view_transformer.dx,\n        #         \"bx\": self.img_view_transformer.bx,\n        #         \"nx\": self.img_view_transformer.nx,\n        #     }\n        # )\n\n        ivt_cfg.update(\n            {\n                \"dx\": self.img_view_transformer.grid_interval,\n                \"bx\": (\n                    self.img_view_transformer.grid_lower_bound\n                    + self.img_view_transformer.grid_interval\n                )\n                / 2.0,\n                \"nx\": self.img_view_transformer.grid_size,\n            }\n        )\n\n        if self.sparse_feats_distill_img_bev is not None:\n\n            # ================lidar distill img-bev================\n\n            loss_inter_channel = self.sparse_feats_distill_img_bev(\n                student_bev_feats,\n                teacher_bev_feats,\n                gt_bboxes_list=gt_bboxes_3d,\n                ivt_cfg=ivt_cfg,\n            )\n\n            if isinstance(loss_inter_channel, tuple):\n                losses.update({\"loss_inter_channel_img_bev\": loss_inter_channel[0]})\n            else:\n                losses.update({\"loss_inter_channel_img_bev\": loss_inter_channel})\n\n        if self.sparse_feats_distill_radar_bev is not None:\n\n            # ================lidar distill radar-bev================\n            if self.sparse_feats_distill_radar_bev is not None:\n                loss_inter_channel = self.sparse_feats_distill_radar_bev(\n                    radar_feats, teacher_bev_feats, gt_bboxes_3d, ivt_cfg=ivt_cfg\n                )  # A self-distillation scheme is used here to migrate the features of the img-bev network to the network's own radar-bev\n\n            if isinstance(loss_inter_channel, tuple):\n                losses.update({\"loss_inter_channel_radar_bev\": loss_inter_channel[0]})\n            else:\n                losses.update({\"loss_inter_channel_radar_bev\": loss_inter_channel})\n\n        if rc_fusion_feats is not None and self.det_feats_distill is not None:\n            loss_det_feats_distill = self.det_feats_distill(\n                rc_fusion_feats, teacher_bev_feats\n            )\n            losses.update({\"loss_det_feats_distill\": loss_det_feats_distill})\n\n        if self.sampling_pos_distill is not None and teacher_sampling_pos is not None:\n            loss_sampling_pos_distill = self.sampling_pos_distill(\n                student_samping_pos, teacher_sampling_pos\n            )\n            losses.update({\"loss_sampling_pos_distill\": loss_sampling_pos_distill})\n\n        if (\n            teacher_sampling_feats is not None\n            and self.sampling_feats_distill is not None\n        ):\n            # import pdb\n            # pdb.set_trace()\n            loss_sampling_warp_distill = self.sampling_feats_distill(\n                student_samping_feats, teacher_sampling_feats\n            )\n            losses.update({\"loss_sampling_feats_distill\": loss_sampling_warp_distill})\n\n        if self.dcdistill_loss is not None:\n\n            if self.ret_sum:\n                loss_dc_distill = self.dcdistill_loss(\n                    tea_resp_bboxes, stu_resp_bboxes, gt_bboxes_3d\n                )\n                losses.update({\"loss_dc_distill\": loss_dc_distill})\n            else:\n                loss_dc_reg_distill, loss_dc_cls_distill = self.dcdistill_loss(\n                    tea_resp_bboxes, stu_resp_bboxes, gt_bboxes_3d\n                )\n                losses.update({\"loss_dc_reg_distill\": loss_dc_reg_distill})\n                losses.update({\"loss_dc_cls_distill\": loss_dc_cls_distill})\n\n        # Self-learning mask focused distillation\n        if self.smfd_distill_loss is not None:\n            loss_smfd_distill, auto_mask = self.smfd_distill_loss(\n                rc_fusion_feats,\n                teacher_bev_feats,\n                gt_bboxes_3d,\n                stu_resp_bboxes,\n                bda_mat=bda_mat,\n            )\n            losses.update(dict(loss_smfd_distill=loss_smfd_distill))\n\n        if self.heatmap_aug_distill_loss is not None:\n            loss_heatmap_aug = self.heatmap_aug_distill_loss(\n                stu_resp_bboxes, tea_resp_bboxes, auto_mask\n            )\n\n            losses.update(dict(loss_heatmap_aug=loss_heatmap_aug))\n\n        return losses\n\n    def forward_train(\n        self,\n        points=None,\n        img_metas=None,\n        gt_bboxes_3d=None,\n        gt_labels_3d=None,\n        radar=None,\n        gt_labels=None,\n        gt_bboxes=None,\n        img_inputs=None,\n        proposals=None,\n        gt_bboxes_ignore=None,\n        **kwargs,\n    ):\n        (\n            img_feats,\n            pts_feats,\n            radar_feats,\n            depth,\n            det_input_feats,\n            radar_ms_feats,\n            pts_ms_feats,\n        ) = self.extract_feat(\n            points,\n            radar=radar,\n            img=img_inputs,\n            img_metas=img_metas,\n        )\n\n        teacher_outs = self.pts_bbox_head_tea(pts_feats)\n\n        teacher_sampling_pos = det_input_feats.get(\"teacher_sampling_pos\")\n        teacher_sampling_feats = det_input_feats.get(\"teacher_sampling_feats\")\n        student_sampling_pos = det_input_feats.get(\"student_sampling_pos\")\n        student_sampling_feats = det_input_feats.get(\"student_sampling_feats\")\n        det_feats = det_input_feats.get(\"det_feats\")\n\n        losses = dict()\n\n        gt_depth = kwargs[\"gt_depth\"]\n        loss_depth = self.img_view_transformer.get_depth_loss(gt_depth, depth)\n        losses.update(dict(loss_depth=loss_depth))\n\n        img_feats_kd = img_feats\n        if pts_feats:\n            pts_feats_kd = pts_feats[0]\n\n        # assert det_input_feats[\"det_feats\"].size(1) == 64\n        losses_pts, student_outs = self.forward_pts_train(\n            [det_feats],\n            gt_bboxes_3d,\n            gt_labels_3d,\n            img_metas,\n            gt_bboxes_ignore,\n        )\n\n        losses.update(losses_pts)\n\n        # forward_distill_loss_input_cfg=\n        if self.radar_ms_feats_distill:\n            radar_ms_distill_loss = self.radar_ms_feats_distill(\n                radar_ms_feats, pts_ms_feats\n            )\n            losses.update(dict(loss_radar_ms=radar_ms_distill_loss))\n\n        ret_losses_dict = self.forward_distill_loss(\n            teacher_bev_feats=pts_feats_kd,\n            teacher_sampling_pos=teacher_sampling_pos,\n            teacher_sampling_feats=teacher_sampling_feats,\n            tea_resp_bboxes=teacher_outs,\n            student_bev_feats=img_feats_kd,\n            student_samping_pos=student_sampling_pos,\n            student_samping_feats=student_sampling_feats,\n            stu_resp_bboxes=student_outs,\n            radar_feats=radar_feats[0],\n            rc_fusion_feats=det_feats,\n            gt_bboxes_3d=gt_bboxes_3d,\n        )\n\n        losses.update(ret_losses_dict)\n\n        # adaptive_weight_list = [\n        #     \"loss_depth\",\n        #     \"loss_radar_ms\",\n        #     \"loss_dc_reg_distill\",\n        #     \"loss_dc_cls_distill\",\n        #     \"loss_smfd_distill\",\n        # ]\n        # for key in adaptive_weight_list:\n        #     adaptive_weight = (\n        #         losses[key] / losses[\"loss_inter_channel_img_bev\"]\n        #     ).detach()\n        #     losses.update({key: losses[key] / adaptive_weight})\n\n        return losses\n\n    def simple_test_pts(self, x, img_metas, rescale=False):\n        \"\"\"Test function of point cloud branch.\"\"\"\n        outs = self.pts_bbox_head(x)\n        bbox_list = self.pts_bbox_head.get_bboxes(outs, img_metas, rescale=rescale)\n        bbox_results = [\n            bbox3d2result(bboxes, scores, labels)\n            for bboxes, scores, labels in bbox_list\n        ]\n        return bbox_results\n\n    def simple_test(self, points, radar, img_metas, img=None, rescale=False):\n        \"\"\"Test function without augmentaiton.\"\"\"\n        _, _, _, _, det_input_feats = self.extract_feat(\n            points, radar, img=img, img_metas=img_metas\n        )\n\n        bbox_list = [dict() for _ in range(len(img_metas))]\n        bbox_pts = self.simple_test_pts(\n            [det_input_feats[\"det_feats\"]], img_metas, rescale=rescale\n        )\n        for result_dict, pts_bbox in zip(bbox_list, bbox_pts):\n            result_dict[\"pts_bbox\"] = pts_bbox\n        return bbox_list\n\n    def forward_test(\n        self, points=None, radar=None, img_metas=None, img_inputs=None, **kwargs\n    ):\n        for var, name in [(img_inputs, \"img_inputs\"), (img_metas, \"img_metas\")]:\n            if not isinstance(var, list):\n                raise TypeError(\"{} must be a list, but got {}\".format(name, type(var)))\n\n        if not isinstance(img_inputs[0][0], list):\n            img_inputs = [img_inputs] if img_inputs is None else img_inputs\n            points = [points] if points is None else points\n            radar = [radar] if radar is None else radar\n            return self.simple_test(\n                points[0], radar[0], img_metas[0], img_inputs[0], **kwargs\n            )\n        else:\n            return self.aug_test(None, img_metas[0], img_inputs[0], **kwargs)\n"
        }
    ]
}