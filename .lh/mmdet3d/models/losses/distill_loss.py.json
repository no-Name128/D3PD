{
    "sourceFile": "mmdet3d/models/losses/distill_loss.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 24,
            "patches": [
                {
                    "date": 1716015764361,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1716021739977,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,1011 @@\n+import torch.nn as nn\n+import torch.nn.functional as F\n+import torch\n+from ..builder import LOSSES, build_loss\n+from ..utils.gaussian_utils import calculate_box_mask_gaussian\n+from torch import distributed as dist\n+from mmdet.core import multi_apply, build_bbox_coder\n+from mmdet3d.core import draw_heatmap_gaussian, gaussian_radius\n+from mmdet3d.core.bbox.coders.centerpoint_bbox_coders import (\n+    CenterPointBBoxCoder as bboxcoder,\n+)\n+from mmdet.models.losses import QualityFocalLoss\n+from mmcv.runner import force_fp32\n+import torchsort\n+import torchvision\n+import numpy as np\n+\n+\n+def get_world_size() -> int:\n+    if not dist.is_available():\n+        return 1\n+    if not dist.is_initialized():\n+        return 1\n+\n+    return dist.get_world_size()\n+\n+\n+def reduce_sum(tensor):\n+    world_size = get_world_size()\n+    if world_size < 2:\n+        return tensor\n+    tensor = tensor.clone()\n+    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n+    return tensor\n+\n+\n+def reduce_mean(tensor):\n+    return reduce_sum(tensor) / float(get_world_size())\n+\n+\n+def _sigmoid(x):\n+    # y = torch.clamp(x.sigmoid(), min=1e-3, max=1 - 1e-3)\n+    y = x.sigmoid()\n+    return y\n+\n+\n+def off_diagonal(x):\n+    # return a flattened view of the off-diagonal elements of a square matrix\n+    n, m = x.shape\n+    assert n == m\n+    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n+\n+\n+def response_list_wrap(response_lists: list):\n+    \"\"\"response_list_wrap\n+\n+    Args:\n+        response_lists (list): Pack list content\n+\n+    Returns:\n+        list: Return the new list of packaged integration\n+    \"\"\"\n+    assert len(response_lists) == 1\n+\n+    tmp_resp = []\n+    for response in response_lists:\n+        tmp_resp.append(response[0])\n+\n+    return tmp_resp\n+\n+\n+@LOSSES.register_module()\n+class QualityFocalLoss_(nn.Module):\n+    \"\"\"\n+    input[B,M,C] not sigmoid\n+    target[B,M,C], sigmoid\n+    \"\"\"\n+\n+    def __init__(self, beta=2.0):\n+\n+        super(QualityFocalLoss_, self).__init__()\n+        self.beta = beta\n+\n+    def forward(\n+        self,\n+        input: torch.Tensor,\n+        target: torch.Tensor,\n+        pos_normalizer=torch.tensor(1.0),\n+    ):\n+\n+        pred_sigmoid = torch.sigmoid(input)\n+        scale_factor = pred_sigmoid - target\n+\n+        # pred_sigmoid = torch.sigmoid(input)\n+        # scale_factor = input - target\n+        loss = F.binary_cross_entropy_with_logits(input, target, reduction=\"none\") * (\n+            scale_factor.abs().pow(self.beta)\n+        )\n+        loss /= torch.clamp(pos_normalizer, min=1.0)\n+        return loss\n+\n+\n+@LOSSES.register_module()\n+class SimpleL1(nn.Module):\n+    def __init__(self, criterion=\"L1\", student_ch=256, teacher_ch=512):\n+        super().__init__()\n+        self.criterion = criterion\n+        if criterion == \"L1\":\n+            self.criterion_loss = nn.L1Loss(reduce=False)\n+        elif criterion == \"SmoothL1\":\n+            self.criterion_loss = nn.SmoothL1Loss(reduce=False)\n+        elif criterion == \"MSE\":\n+            self.criterion_loss = nn.MSELoss(reduction=\"none\")\n+\n+        if student_ch != teacher_ch:\n+            self.align = nn.Conv2d(student_ch, teacher_ch, kernel_size=1)\n+\n+    def forward(self, feats1, feats2, *args, **kwargs):\n+        if self.criterion == \"MSE\":\n+            feats1 = self.align(feats1) if getattr(self, \"align\", None) else feats1\n+            losses = self.criterion_loss(feats1, feats2).mean()\n+            return losses\n+        else:\n+            return self.criterion_loss(feats1, feats2)\n+\n+\n+@LOSSES.register_module()\n+class Relevance_Distillation(nn.Module):\n+    def __init__(self, bs=2, bn_dim=512, lambd=0.0051):\n+        super().__init__()\n+        self.bs = bs\n+        self.bn_dim = bn_dim\n+        self.lambd = lambd\n+\n+        # normalization layer for the representations z1 and z2\n+        self.bn = nn.BatchNorm1d(self.bn_dim, affine=False)\n+\n+        self.align = nn.Conv2d(256, 512, kernel_size=1)\n+\n+    def forward(self, student_bev, teacher_bev, *args, **kwargs):\n+        student_bev = self.align(student_bev)\n+\n+        student_bev = student_bev.flatten(2)\n+        teacher_bev = teacher_bev.flatten(2)\n+\n+        # empirical cross-correlation matrix\n+        c = self.bn(student_bev).flatten(1).T @ self.bn(teacher_bev).flatten(1)\n+\n+        # sum the cross-correlation matrix between all gpus\n+        c.div_(self.bs)\n+        if self.bs == 1:\n+            pass\n+        else:\n+            torch.distributed.all_reduce(c)\n+\n+        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n+        off_diag = off_diagonal(c).pow_(2).sum()\n+        loss = on_diag + self.lambd * off_diag\n+        return loss\n+\n+\n+@LOSSES.register_module()\n+class FeatureLoss(nn.Module):\n+    \"\"\"PyTorch version of `Masked Generative Distillation`\n+\n+    Args:\n+        student_channels(int): Number of channels in the student's feature map.\n+        teacher_channels(int): Number of channels in the teacher's feature map.\n+        name (str): the loss name of the layer\n+        alpha_mgd (float, optional): Weight of dis_loss. Defaults to 0.00002\n+        lambda_mgd (float, optional): masked ratio. Defaults to 0.65\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        student_channels,\n+        teacher_channels,\n+        name,\n+        alpha_mgd=0.00002,\n+        lambda_mgd=0.65,\n+    ):\n+        super(FeatureLoss, self).__init__()\n+        self.alpha_mgd = alpha_mgd\n+        self.lambda_mgd = lambda_mgd\n+        self.name = name\n+\n+        if student_channels != teacher_channels:\n+            self.align = nn.Conv2d(\n+                student_channels, teacher_channels, kernel_size=1, stride=1, padding=0\n+            )\n+        else:\n+            self.align = None\n+\n+        self.generation = nn.Sequential(\n+            nn.Conv2d(teacher_channels, teacher_channels, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.Conv2d(teacher_channels, teacher_channels, kernel_size=3, padding=1),\n+        )\n+\n+    def forward(self, preds_S, preds_T, **kwargs):\n+        \"\"\"Forward function.\n+        Args:\n+            preds_S(Tensor): Bs*C*H*W, student's feature map\n+            preds_T(Tensor): Bs*C*H*W, teacher's feature map\n+        \"\"\"\n+        assert preds_S.shape[-2:] == preds_T.shape[-2:]\n+\n+        if self.align is not None:\n+            preds_S = self.align(preds_S)\n+\n+        loss = self.get_dis_loss(preds_S, preds_T) * self.alpha_mgd\n+\n+        return loss\n+\n+    def get_dis_loss(self, preds_S, preds_T):\n+        loss_mse = nn.MSELoss(reduction=\"sum\")\n+        N, C, H, W = preds_T.shape\n+\n+        device = preds_S.device\n+        mat = torch.rand((N, 1, H, W)).to(device)\n+        mat = torch.where(mat > 1 - self.lambda_mgd, 0, 1).to(device)\n+\n+        masked_fea = torch.mul(preds_S, mat)\n+        new_fea = self.generation(masked_fea)\n+\n+        dis_loss = loss_mse(new_fea, preds_T) / N\n+\n+        return dis_loss\n+\n+\n+@LOSSES.register_module()\n+class FeatureLoss_InnerClip(nn.Module):\n+    def __init__(\n+        self,\n+        x_sample_num=24,\n+        y_sample_num=24,\n+        inter_keypoint_weight=1,\n+        inter_channel_weight=10,\n+        enlarge_width=1.6,\n+        embed_channels=[256, 512],\n+        inner_feats_distill=None,\n+    ):\n+        super().__init__()\n+        self.x_sample_num = x_sample_num\n+        self.y_sample_num = y_sample_num\n+        self.inter_keypoint_weight = inter_keypoint_weight\n+        self.inter_channel_weight = inter_channel_weight\n+        self.enlarge_width = enlarge_width\n+\n+        self.img_view_transformer = None\n+\n+        self.embed_channels = embed_channels\n+\n+        self.imgbev_embed = nn.Sequential(\n+            nn.Conv2d(\n+                embed_channels[0],\n+                embed_channels[1],\n+                kernel_size=1,\n+                stride=1,\n+                padding=0,\n+                bias=False,\n+            ),\n+            nn.BatchNorm2d(embed_channels[1]),\n+        )\n+\n+        self.inner_feats_loss = (\n+            build_loss(inner_feats_distill) if inner_feats_distill is not None else None\n+        )\n+\n+    def get_gt_sample_grid(self, corner_points2d):\n+        dH_x, dH_y = corner_points2d[0] - corner_points2d[1]\n+        dW_x, dW_y = corner_points2d[0] - corner_points2d[2]\n+        raw_grid_x = (\n+            torch.linspace(\n+                corner_points2d[0][0], corner_points2d[1][0], self.x_sample_num\n+            )\n+            .view(1, -1)\n+            .repeat(self.y_sample_num, 1)\n+        )\n+        raw_grid_y = (\n+            torch.linspace(\n+                corner_points2d[0][1], corner_points2d[2][1], self.y_sample_num\n+            )\n+            .view(-1, 1)\n+            .repeat(1, self.x_sample_num)\n+        )\n+        raw_grid = torch.cat((raw_grid_x.unsqueeze(2), raw_grid_y.unsqueeze(2)), dim=2)\n+        raw_grid_x_offset = (\n+            torch.linspace(0, -dW_x, self.x_sample_num)\n+            .view(-1, 1)\n+            .repeat(1, self.y_sample_num)\n+        )\n+        raw_grid_y_offset = (\n+            torch.linspace(0, -dH_y, self.y_sample_num)\n+            .view(1, -1)\n+            .repeat(self.x_sample_num, 1)\n+        )\n+        raw_grid_offset = torch.cat(\n+            (raw_grid_x_offset.unsqueeze(2), raw_grid_y_offset.unsqueeze(2)), dim=2\n+        )\n+        grid = raw_grid + raw_grid_offset  # X_sample,Y_sample,2\n+        grid[:, :, 0] = torch.clip(\n+            (\n+                (\n+                    grid[:, :, 0]\n+                    - (\n+                        self.img_view_transformer[\"bx\"][0].to(grid.device)\n+                        - self.img_view_transformer[\"dx\"][0].to(grid.device) / 2.0\n+                    )\n+                )\n+                / self.img_view_transformer[\"dx\"][0].to(grid.device)\n+                / (self.img_view_transformer[\"nx\"][0].to(grid.device) - 1)\n+            )\n+            * 2.0\n+            - 1.0,\n+            min=-1.0,\n+            max=1.0,\n+        )\n+        grid[:, :, 1] = torch.clip(\n+            (\n+                (\n+                    grid[:, :, 1]\n+                    - (\n+                        self.img_view_transformer[\"bx\"][1].to(grid.device)\n+                        - self.img_view_transformer[\"dx\"][1].to(grid.device) / 2.0\n+                    )\n+                )\n+                / self.img_view_transformer[\"dx\"][1].to(grid.device)\n+                / (self.img_view_transformer[\"nx\"][1].to(grid.device) - 1)\n+            )\n+            * 2.0\n+            - 1.0,\n+            min=-1.0,\n+            max=1.0,\n+        )\n+\n+        return grid.unsqueeze(0)\n+\n+    def get_inner_feat(self, gt_bboxes_3d, img_feats, pts_feats):\n+        \"\"\"Use grid to sample features of key points\"\"\"\n+        device = img_feats.device\n+        dtype = img_feats[0].dtype\n+\n+        img_feats_sampled_list = []\n+        pts_feats_sampled_list = []\n+\n+        for sample_ind in torch.arange(len(gt_bboxes_3d)):\n+            img_feat = img_feats[sample_ind].unsqueeze(0)  # 1,C,H,W\n+            pts_feat = pts_feats[sample_ind].unsqueeze(0)  # 1,C,H,W\n+\n+            bbox_num, corner_num, point_num = gt_bboxes_3d[sample_ind].corners.shape\n+\n+            for bbox_ind in torch.arange(bbox_num):\n+                if self.enlarge_width > 0:\n+                    gt_sample_grid = self.get_gt_sample_grid(\n+                        gt_bboxes_3d[sample_ind]\n+                        .enlarged_box(self.enlarge_width)\n+                        .corners[bbox_ind][[0, 2, 4, 6], :-1]\n+                    ).to(device)\n+                else:\n+                    gt_sample_grid = self.get_gt_sample_grid(\n+                        gt_bboxes_3d[sample_ind].corners[bbox_ind][[0, 2, 4, 6], :-1]\n+                    ).to(\n+                        device\n+                    )  # 1,sample_y,sample_x,2\n+\n+                img_feats_sampled_list.append(\n+                    F.grid_sample(\n+                        img_feat,\n+                        grid=gt_sample_grid,\n+                        align_corners=False,\n+                        mode=\"bilinear\",\n+                    )\n+                )  # 'bilinear')) #all_bbox_num,C,y_sample,x_sample\n+                pts_feats_sampled_list.append(\n+                    F.grid_sample(\n+                        pts_feat,\n+                        grid=gt_sample_grid,\n+                        align_corners=False,\n+                        mode=\"bilinear\",\n+                    )\n+                )  # 'bilinear')) #all_bbox_num,C,y_sample,x_sample\n+\n+        return torch.cat(img_feats_sampled_list, dim=0), torch.cat(\n+            pts_feats_sampled_list, dim=0\n+        )\n+\n+    @force_fp32(apply_to=(\"img_feats_kd\", \"pts_feats_kd\"))\n+    def get_inter_channel_loss(self, img_feats_kd, pts_feats_kd):\n+        \"\"\"Calculate the inter-channel similarities, guide the student keypoint features to mimic the channel-wise relationships of the teacher`s\"\"\"\n+\n+        C_img = img_feats_kd.shape[1]\n+        C_pts = pts_feats_kd.shape[1]\n+        N = self.x_sample_num * self.y_sample_num\n+\n+        img_feats_kd = img_feats_kd.view(-1, C_img, N).matmul(\n+            img_feats_kd.view(-1, C_img, N).permute(0, 2, 1)\n+        )  # -1,N,N\n+        pts_feats_kd = pts_feats_kd.view(-1, C_pts, N).matmul(\n+            pts_feats_kd.view(-1, C_pts, N).permute(0, 2, 1)\n+        )\n+\n+        img_feats_kd = F.normalize(img_feats_kd, dim=2)\n+        pts_feats_kd = F.normalize(pts_feats_kd, dim=2)\n+\n+        loss_inter_channel = F.mse_loss(img_feats_kd, pts_feats_kd, reduction=\"none\")\n+        loss_inter_channel = loss_inter_channel.sum(-1)\n+        loss_inter_channel = loss_inter_channel.mean()\n+        loss_inter_channel = self.inter_channel_weight * loss_inter_channel\n+        return loss_inter_channel\n+\n+    def forward(self, student_feats, teacher_feats, gt_bboxes_list, **kwargs):\n+\n+        self.img_view_transformer = kwargs.get(\"ivt_cfg\")\n+\n+        if student_feats.size(1) != teacher_feats.size(1):\n+            student_feats = self.imgbev_embed(student_feats)\n+\n+        if self.inter_keypoint_weight > 0 or self.inter_channel_weight > 0:\n+            img_feats_kd, pts_feats_kd = self.get_inner_feat(\n+                gt_bboxes_list, student_feats, teacher_feats\n+            )\n+\n+        if self.inner_feats_loss:\n+            return self.inner_feats_loss(img_feats_kd, pts_feats_kd)\n+\n+        # if self.inter_keypoint_weight > 0:\n+        #     loss_inter_keypoint = self.get_inter_keypoint_loss(\n+        #         img_feats_kd, pts_feats_kd\n+        #     )\n+        #     losses.update({\"loss_inter_keypoint_img_bev\": loss_inter_keypoint})\n+\n+        if self.inter_channel_weight > 0:\n+            loss_inter_channel = self.get_inter_channel_loss(img_feats_kd, pts_feats_kd)\n+\n+        return loss_inter_channel\n+\n+\n+@LOSSES.register_module()\n+class FeatureLoss_Affinity(nn.Module):\n+    def __init__(self):\n+        super().__init__()\n+\n+    def forward(self, student_feats, teacher_feats, *args, **kwargs):\n+        student_feats = [student_feats]\n+        teacher_feats = [teacher_feats]\n+\n+        feature_ditill_loss = 0.0\n+\n+        resize_shape = student_feats[-1].shape[-2:]\n+        if isinstance(student_feats, list):\n+            for i in range(len(student_feats)):\n+                feature_target = teacher_feats[i].detach()\n+                feature_pred = student_feats[i]\n+\n+                B, C, H, W = student_feats[-1].shape\n+\n+                if student_feats[-1].size(-1) != teacher_feats[-1].size(-1):\n+                    feature_pred_down = F.interpolate(\n+                        feature_pred, size=resize_shape, mode=\"bilinear\"\n+                    )\n+                    feature_target_down = F.interpolate(\n+                        feature_target, size=resize_shape, mode=\"bilinear\"\n+                    )\n+                else:\n+                    feature_pred_down = feature_pred\n+                    feature_target_down = feature_target\n+\n+                feature_target_down = feature_target_down.reshape(B, C, -1)\n+                depth_affinity = torch.bmm(\n+                    feature_target_down.permute(0, 2, 1), feature_target_down\n+                )\n+\n+                feature_pred_down = feature_pred_down.reshape(B, C, -1)\n+                rgb_affinity = torch.bmm(\n+                    feature_pred_down.permute(0, 2, 1), feature_pred_down\n+                )\n+\n+                feature_ditill_loss = (\n+                    feature_ditill_loss\n+                    + F.l1_loss(rgb_affinity, depth_affinity, reduction=\"mean\") / B\n+                )\n+\n+        else:\n+            raise NotImplementedError\n+\n+        return feature_ditill_loss\n+\n+\n+@LOSSES.register_module()\n+class FeatureLoss_Coefficient(nn.Module):\n+    def __init__(self, **kwargs):\n+        super().__init__()\n+\n+    def corrcoef(self, target, pred):\n+        pred_n = pred - pred.mean()\n+        target_n = target - target.mean()\n+        pred_n = pred_n / pred_n.norm()\n+        target_n = target_n / target_n.norm()\n+        return (pred_n * target_n).sum()\n+\n+    def forward(\n+        self,\n+        pred,\n+        target,\n+        regularization=\"l2\",\n+        regularization_strength=1.0,\n+    ):\n+        pred = [pred.clone()]\n+        target = [target.clone()]\n+        spearman_loss = 0.0\n+\n+        resize_shape = pred[-1].shape[-2:]  # save training time\n+\n+        if isinstance(pred, list):\n+            for i in range(len(pred)):\n+                feature_target = target[i]\n+                feature_pred = pred[i]\n+                B, C, H, W = feature_pred.shape\n+\n+                feature_pred_down = F.interpolate(\n+                    feature_pred, size=resize_shape, mode=\"bilinear\"\n+                )\n+                feature_target_down = F.interpolate(\n+                    feature_target, size=resize_shape, mode=\"bilinear\"\n+                )\n+\n+                # if feature_pred.size(-1) != feature_target.size(-1):\n+\n+                #     feature_pred_down = F.interpolate(\n+                #         feature_pred, size=resize_shape, mode=\"bilinear\"\n+                #     )\n+                #     feature_target_down = F.interpolate(\n+                #         feature_target, size=resize_shape, mode=\"bilinear\"\n+                #     )\n+                # else:\n+                #     feature_pred_down = feature_pred\n+                #     feature_target_down = feature_target\n+\n+                feature_pred_down = feature_pred_down.reshape(B, -1)\n+                feature_target_down = feature_target_down.reshape(B, -1)\n+\n+                feature_pred_down = torchsort.soft_rank(\n+                    feature_pred_down,\n+                    regularization=regularization,\n+                    regularization_strength=regularization_strength,\n+                )\n+                spearman_loss += 1 - self.corrcoef(\n+                    feature_target_down, feature_pred_down / feature_pred_down.shape[-1]\n+                )\n+\n+        return spearman_loss\n+\n+\n+@LOSSES.register_module()\n+class Radar_MSDistilll(nn.Module):\n+    def __init__(self, num_layers=2, each_layer_loss_cfg=[]):\n+        super().__init__()\n+        self.num_layers = num_layers\n+        assert num_layers == len(each_layer_loss_cfg)\n+        for idx, cfg_dict in enumerate(each_layer_loss_cfg):\n+            setattr(self, f\"layer_loss_{idx}\", build_loss(cfg_dict))\n+\n+    def forward(self, radar_ms_feats, pts_ms_feats):\n+        assert isinstance(radar_ms_feats, list)\n+        losses = 0.0\n+\n+        for idx in range(self.num_layers):\n+            losses += getattr(self, f\"layer_loss_{idx}\")(\n+                radar_ms_feats[idx], pts_ms_feats[idx]\n+            )\n+\n+        return losses / self.num_layers\n+\n+\n+@LOSSES.register_module()\n+class InfoMax(nn.Module):\n+    def __init__(self, **kwargs):\n+        super().__init__()\n+\n+    def forward(self, x1, x2):\n+        import pdb\n+\n+        pdb.set_trace()\n+        x1 = x1 / (torch.norm(x1, p=2, dim=1, keepdim=True) + 1e-10)\n+        x2 = x2 / (torch.norm(x2, p=2, dim=1, keepdim=True) + 1e-10)\n+        bs = x1.size(0)\n+        s = torch.matmul(x1, x2.permute(1, 0))\n+        mask_joint = torch.eye(bs).cuda()\n+        mask_marginal = 1 - mask_joint\n+\n+        Ej = (s * mask_joint).mean()\n+        Em = torch.exp(s * mask_marginal).mean()\n+        # decoupled comtrastive learning?!!!!\n+        # infomax_loss = - (Ej - torch.log(Em)) * self.alpha\n+        infomax_loss = -(Ej - torch.log(Em))  # / Em\n+        return infomax_loss\n+\n+\n+@LOSSES.register_module()\n+class HeatMapAug(nn.Module):\n+    def __init__(self):\n+        super().__init__()\n+\n+    @force_fp32(apply_to=(\"stu_pred\", \"tea_pred\"))\n+    def forward(self, stu_pred, tea_pred, fg_map):\n+\n+        num_task = len(stu_pred)\n+        kl_loss = 0\n+        for task_id in range(num_task):\n+            student_pred = stu_pred[task_id][0][\"heatmap\"].sigmoid()\n+            teacher_pred = tea_pred[task_id][0][\"heatmap\"].sigmoid()\n+            fg_map = fg_map.unsqueeze(1)\n+            task_kl_loss = F.binary_cross_entropy(student_pred, teacher_pred.detach())\n+            task_kl_loss = torch.sum(task_kl_loss * fg_map) / torch.sum(fg_map)\n+            kl_loss += task_kl_loss\n+\n+        return kl_loss\n+\n+\n+@LOSSES.register_module()\n+class Dc_ResultDistill(nn.Module):\n+    def __init__(\n+        self,\n+        pc_range=[],\n+        voxel_size=[],\n+        out_size_scale=8,\n+        ret_sum=False,\n+        loss_weight_reg=10,\n+        loss_weight_cls=10,\n+        max_cls=True,\n+    ):\n+        super().__init__()\n+\n+        self.pc_range = pc_range\n+        self.voxel_size = voxel_size\n+        self.out_size_scale = out_size_scale\n+        self.ret_sum = ret_sum\n+        self.loss_weight_reg = loss_weight_reg\n+        self.loss_weight_cls = loss_weight_cls\n+        self.max_cls = max_cls\n+\n+    def forward(self, resp_lidar, resp_fuse, gt_boxes):\n+        \"\"\"Dc_ResultDistill forward.\n+\n+        Args:\n+            resp_lidar (_type_):\n+            resp_fuse (_type_):\n+            gt_boxes (_type_):\n+\n+        Returns:\n+            _type_: _description_\n+        \"\"\"\n+\n+        tmp_resp_lidar = []\n+        tmp_resp_fuse = []\n+        for res_lidar, res_fuse in zip(resp_lidar, resp_fuse):\n+            tmp_resp_lidar.append(res_lidar[0])\n+            tmp_resp_fuse.append(res_fuse[0])\n+\n+        tmp_gt_boxes = []\n+        for bs_idx in range(len(gt_boxes)):\n+\n+            gt_bboxes_3d = torch.cat(\n+                (gt_boxes[bs_idx].gravity_center, gt_boxes[bs_idx].tensor[:, 3:]), dim=1\n+            )\n+\n+            tmp_gt_boxes.append(gt_bboxes_3d)\n+\n+        cls_lidar = []\n+        reg_lidar = []\n+        cls_fuse = []\n+        reg_fuse = []\n+\n+        # criterion = nn.L1Loss(reduce=False)\n+        # criterion_cls = QualityFocalLoss_()\n+\n+        criterion = nn.SmoothL1Loss(reduce=False)\n+        criterion_cls = nn.L1Loss(reduce=False)\n+\n+        for task_id, task_out in enumerate(tmp_resp_lidar):\n+            cls_lidar.append(task_out[\"heatmap\"])\n+            cls_fuse.append(_sigmoid(tmp_resp_fuse[task_id][\"heatmap\"] / 2))\n+            reg_lidar.append(\n+                torch.cat(\n+                    [\n+                        task_out[\"reg\"],\n+                        task_out[\"height\"],\n+                        task_out[\"dim\"],\n+                        task_out[\"rot\"],\n+                        task_out[\"vel\"],\n+                        # task_out[\"iou\"],\n+                    ],\n+                    dim=1,\n+                )\n+            )\n+            reg_fuse.append(\n+                torch.cat(\n+                    [\n+                        tmp_resp_fuse[task_id][\"reg\"],\n+                        tmp_resp_fuse[task_id][\"height\"],\n+                        tmp_resp_fuse[task_id][\"dim\"],\n+                        tmp_resp_fuse[task_id][\"rot\"],\n+                        tmp_resp_fuse[task_id][\"vel\"],\n+                        # resp_fuse[task_id][\"iou\"],\n+                    ],\n+                    dim=1,\n+                )\n+            )\n+        cls_lidar = torch.cat(cls_lidar, dim=1)\n+        reg_lidar = torch.cat(reg_lidar, dim=1)\n+        cls_fuse = torch.cat(cls_fuse, dim=1)\n+        reg_fuse = torch.cat(reg_fuse, dim=1)\n+\n+        if self.max_cls:\n+            cls_lidar_max, _ = torch.max(cls_lidar, dim=1)\n+            cls_fuse_max, _ = torch.max(cls_fuse, dim=1)\n+        else:\n+            _, _, ht_h, ht_w = cls_fuse.shape\n+            cls_lidar_max = cls_lidar.flatten(2).permute(0, 2, 1)\n+            cls_fuse_max = cls_fuse.flatten(2).permute(0, 2, 1)\n+\n+        gaussian_mask = calculate_box_mask_gaussian(\n+            reg_lidar.shape,\n+            tmp_gt_boxes,\n+            self.pc_range,\n+            self.voxel_size,\n+            self.out_size_scale,\n+        )\n+\n+        # # diff_reg = criterion(reg_lidar, reg_fuse)\n+        # # diff_cls = criterion(cls_lidar_max, cls_fuse_max)\n+        # diff_reg = criterion_reg(reg_lidar, reg_fuse)\n+\n+        # cls_lidar_max = torch.sigmoid(cls_lidar_max)\n+        # diff_cls = criterion_cls(\n+        #     cls_fuse_max, cls_lidar_max\n+        # )  # Compared with directly using the L1 loss constraint, replace it with bce focal loss and change the position?\n+\n+        # weight = gaussian_mask.sum()\n+        # weight = reduce_mean(weight)\n+\n+        # # diff_cls = criterion_cls(cls_lidar_max, cls_fuse_max)\n+        # diff_reg = torch.mean(diff_reg, dim=1)\n+        # diff_reg = diff_reg * gaussian_mask\n+        # loss_reg_distill = torch.sum(diff_reg) / (weight + 1e-4)\n+\n+        # if not self.max_cls:\n+        #     diff_cls = diff_cls\n+        #     loss_cls_distill = diff_cls.sum() * 2\n+        # else:\n+        #     diff_cls = diff_cls * gaussian_mask\n+        #     loss_cls_distill = torch.sum(diff_cls) / (weight + 1e-4)\n+\n+        diff_reg = criterion(reg_lidar, reg_fuse)\n+\n+        diff_cls = criterion_cls(cls_lidar_max, cls_fuse_max)\n+        diff_reg = torch.mean(diff_reg, dim=1)\n+        diff_reg = diff_reg * gaussian_mask\n+        diff_cls = diff_cls * gaussian_mask\n+        weight = gaussian_mask.sum()\n+        weight = reduce_mean(weight)\n+        loss_reg_distill = torch.sum(diff_reg) / (weight + 1e-4)\n+        loss_cls_distill = torch.sum(diff_cls) / (weight + 1e-4)\n+\n+        if self.ret_sum:\n+\n+            loss_det_distill = self.loss_weight * (loss_reg_distill + loss_cls_distill)\n+\n+            return loss_det_distill\n+        else:\n+\n+            return (\n+                self.loss_weight_reg * loss_reg_distill,\n+                self.loss_weight_cls * loss_cls_distill,\n+            )\n+\n+\n+@LOSSES.register_module()\n+class SelfLearningMFD(nn.Module):\n+    def __init__(\n+        self,\n+        bev_shape=[128, 128],\n+        pc_range=[-51.2, -51.2, -5, 51.2, 51.2, 3],\n+        voxel_size=[0.1, 0.1, 0.1],\n+        score_threshold=0.7,\n+        add_stu_decode_bboxes=False,\n+        loss_weight=1e-2,\n+        bbox_coder=None,\n+        student_channels=256,\n+        teacher_channels=512,\n+    ):\n+        super().__init__()\n+        self.bev_shape = bev_shape\n+        self.pc_range = pc_range\n+        self.voxel_size = voxel_size\n+        self.shape_resize_times = bbox_coder.out_size_factor if bbox_coder else 8\n+        self.score_threshold = score_threshold\n+        self.add_stu_decode_bboxes = add_stu_decode_bboxes\n+        self.loss_weight = loss_weight\n+\n+        self.align = nn.Sequential(\n+            nn.Conv2d(student_channels, teacher_channels, kernel_size=1, padding=0),\n+            # nn.BatchNorm2d(teacher_channels),\n+        )\n+\n+        if bbox_coder is not None:\n+            self.bbox_coder = build_bbox_coder(bbox_coder)\n+\n+    def pred2bboxes(self, preds_dict):\n+        \"\"\"SelfLearningMFD-pred2bboxes forward\n+\n+        Args:\n+            pred_bboxes (list):\n+            [task1[[task_head_reg,task_head_heatmap,...,]],\n+            task2...,\n+            taskn]\n+        \"\"\"\n+\n+        for task_id, pred_data in enumerate(preds_dict):\n+            bath_tmp = []\n+            batch_size = pred_data[0][\"heatmap\"].shape[0]\n+            batch_heatmap = pred_data[0][\"heatmap\"].sigmoid()\n+            batch_reg = pred_data[0][\"reg\"]\n+            batch_hei = pred_data[0][\"height\"]\n+\n+            # denormalization\n+            batch_dim = torch.exp(pred_data[0][\"dim\"])\n+\n+            batch_rot_sin = pred_data[0][\"rot\"][:, 0].unsqueeze(1)\n+            batch_rot_cos = pred_data[0][\"rot\"][:, 1].unsqueeze(1)\n+\n+            if \"vel\" in pred_data[0].keys():\n+                batch_vel = pred_data[0][\"vel\"]\n+\n+            bath_tmp.append(batch_heatmap)\n+            bath_tmp.append(batch_rot_sin)\n+            bath_tmp.append(batch_rot_cos)\n+            bath_tmp.append(batch_hei)\n+            bath_tmp.append(batch_dim)\n+            bath_tmp.append(batch_vel)\n+            bath_tmp.append(batch_reg)\n+\n+            bboxes_decode = self.bbox_coder.decode(*bath_tmp)\n+\n+        return bboxes_decode\n+\n+    def aug_boxes(self, gt_boxes, rot_mat):\n+        rot_lim = (-22.5, 22.5)\n+        scale_lim = (0.95, 1.05)\n+        rotate_angle = np.random.uniform(*rot_lim)\n+        scale_ratio = np.random.uniform(*scale_lim)\n+        gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n+        gt_boxes[:, 3:6] *= scale_ratio\n+        gt_boxes[:, 6] += rotate_angle\n+        gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n+        gt_boxes[:, 6] = -gt_boxes[:, 6]\n+        gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(-1)\n+        gt_boxes[:, :3] = gt_boxes[:, :3]\n+\n+        return gt_boxes\n+\n+    @force_fp32(\n+        apply_to=(\n+            \"student_bev_feat\",\n+            \"teacher_bev_feat\",\n+        )\n+    )\n+    def forward(\n+        self,\n+        student_bev_feat,\n+        teacher_bev_feat,\n+        gt_bboxes_list=None,\n+        masks_bboxes=None,\n+        bda_mat=None,\n+    ):\n+        \"\"\"SelfLearningMFD forward.\n+\n+        Args:\n+            student_bev_feats (torch.tensor): Calculate student feats\n+            teacher_bev_feats (torch.tensor): Calculate teacher feats\n+            masks_bboxes (list): Self-learning mask detection\n+            gt_bboxes_list (list): [LiDARInstance3DBoxes(gravity_center+tensor(num_objs,9))]\n+\n+        Returns:\n+            dict: _description_\n+        \"\"\"\n+\n+        bs = student_bev_feat.size(0)\n+\n+        if student_bev_feat.size(1) != teacher_bev_feat.size(1):\n+            student_bev_feat = self.align(student_bev_feat)\n+\n+        if masks_bboxes is not None and self.add_stu_decode_bboxes:\n+            student_pred_bboxes_list = self.pred2bboxes(\n+                masks_bboxes\n+            )  # list of task : each shape of [(num_bboxes,9)]\n+\n+            if bda_mat is not None:\n+                student_pred_bboxes_list = self.aug_boxes(student_pred_bboxes_list)\n+\n+        bev_feat_shape = torch.tensor(self.bev_shape)\n+        voxel_size = torch.tensor(self.voxel_size)\n+        feature_map_size = bev_feat_shape[:2]\n+\n+        if gt_bboxes_list is not None:\n+            device = student_bev_feat.device\n+\n+            gt_bboxes_list = [\n+                torch.cat(\n+                    (gt_bboxes.gravity_center, gt_bboxes.tensor[:, 3:]), dim=1\n+                ).to(device)\n+                for gt_bboxes in gt_bboxes_list\n+            ]\n+\n+            if self.add_stu_decode_bboxes:\n+                for idx, data in enumerate(student_pred_bboxes_list):\n+\n+                    bboxes_dim = data[\"bboxes\"].size(1)\n+\n+                    scroes_mask = data[\"scores\"] > self.score_threshold\n+\n+                    new_select_by_mask = data[\"bboxes\"][scroes_mask, :]\n+\n+                    gt_bboxes_list[idx] = torch.cat(\n+                        [gt_bboxes_list[idx], new_select_by_mask], dim=0\n+                    )\n+\n+            fg_map = student_bev_feat.new_zeros(\n+                (len(gt_bboxes_list), feature_map_size[1], feature_map_size[0])\n+            )\n+\n+            for idx in range(len(gt_bboxes_list)):\n+                num_objs = gt_bboxes_list[idx].shape[0]\n+\n+                for k in range(num_objs):\n+                    width = gt_bboxes_list[idx][k][3]\n+                    length = gt_bboxes_list[idx][k][4]\n+                    width = width / voxel_size[0] / self.shape_resize_times\n+                    length = length / voxel_size[1] / self.shape_resize_times\n+\n+                    if width > 0 and length > 0:\n+                        radius = gaussian_radius((length, width), min_overlap=0.1)\n+                        radius = max(1, int(radius))\n+\n+                        # be really careful for the coordinate system of\n+                        # your box annotation.\n+                        x, y, z = (\n+                            gt_bboxes_list[idx][k][0],\n+                            gt_bboxes_list[idx][k][1],\n+                            gt_bboxes_list[idx][k][2],\n+                        )\n+\n+                        coor_x = (\n+                            (x - self.pc_range[0])\n+                            / voxel_size[0]\n+                            / self.shape_resize_times\n+                        )\n+                        coor_y = (\n+                            (y - self.pc_range[1])\n+                            / voxel_size[1]\n+                            / self.shape_resize_times\n+                        )\n+\n+                        center = torch.tensor(\n+                            [coor_x, coor_y], dtype=torch.float32, device=device\n+                        )\n+                        center_int = center.to(torch.int32)\n+\n+                        # throw out not in range objects to avoid out of array\n+                        # area when creating the heatmap\n+                        if not (\n+                            0 <= center_int[0] < feature_map_size[0]\n+                            and 0 <= center_int[1] < feature_map_size[1]\n+                        ):\n+                            continue\n+\n+                        draw_heatmap_gaussian(fg_map[idx], center_int, radius)\n+\n+        if fg_map is None:\n+            fg_map = student_bev_feat.new_ones(\n+                (student_bev_feat.shape[0], feature_map_size[1], feature_map_size[0])\n+            )\n+\n+        if bs > 1:\n+            fg_map = fg_map.unsqueeze(1)\n+\n+        fit_loss = F.mse_loss(student_bev_feat, teacher_bev_feat, reduction=\"none\")\n+        fit_loss = torch.sum(fit_loss * fg_map) / torch.sum(fg_map)\n+\n+        return fit_loss * self.loss_weight, fg_map\n+\n+    # def forward(\n+    #     self, student_bev_feats, teacher_bev_feats, masks_bboxes, gt_bboxes_list\n+    # ):\n+    #     \"\"\"SelfLearningMFD forward.\n+\n+    #     Args:\n+    #         student_bev_feats (torch.tensor): _description_\n+    #         teacher_bev_feats (torch.tensor): _description_\n+    #         masks_bboxes (): _description_\n+    #         gt_bboxes_list (_type_): _description_\n+    #     \"\"\"\n+    #     assert isinstance(masks_bboxes, list)\n+    #     ret_dict = multi_apply(\n+    #         self._forward,\n+    #         student_bev_feats,\n+    #         teacher_bev_feats,\n+    #         masks_bboxes,\n+    #         gt_bboxes_list,\n+    #     )\n"
                },
                {
                    "date": 1716022000449,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1008,1015 +1008,4 @@\n     #         teacher_bev_feats,\n     #         masks_bboxes,\n     #         gt_bboxes_list,\n     #     )\n-import torch.nn as nn\n-import torch.nn.functional as F\n-import torch\n-from ..builder import LOSSES, build_loss\n-from ..utils.gaussian_utils import calculate_box_mask_gaussian\n-from torch import distributed as dist\n-from mmdet.core import multi_apply, build_bbox_coder\n-from mmdet3d.core import draw_heatmap_gaussian, gaussian_radius\n-from mmdet3d.core.bbox.coders.centerpoint_bbox_coders import (\n-    CenterPointBBoxCoder as bboxcoder,\n-)\n-from mmdet.models.losses import QualityFocalLoss\n-from mmcv.runner import force_fp32\n-import torchsort\n-import torchvision\n-import numpy as np\n-\n-\n-def get_world_size() -> int:\n-    if not dist.is_available():\n-        return 1\n-    if not dist.is_initialized():\n-        return 1\n-\n-    return dist.get_world_size()\n-\n-\n-def reduce_sum(tensor):\n-    world_size = get_world_size()\n-    if world_size < 2:\n-        return tensor\n-    tensor = tensor.clone()\n-    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n-    return tensor\n-\n-\n-def reduce_mean(tensor):\n-    return reduce_sum(tensor) / float(get_world_size())\n-\n-\n-def _sigmoid(x):\n-    # y = torch.clamp(x.sigmoid(), min=1e-3, max=1 - 1e-3)\n-    y = x.sigmoid()\n-    return y\n-\n-\n-def off_diagonal(x):\n-    # return a flattened view of the off-diagonal elements of a square matrix\n-    n, m = x.shape\n-    assert n == m\n-    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n-\n-\n-def response_list_wrap(response_lists: list):\n-    \"\"\"response_list_wrap\n-\n-    Args:\n-        response_lists (list): Pack list content\n-\n-    Returns:\n-        list: Return the new list of packaged integration\n-    \"\"\"\n-    assert len(response_lists) == 1\n-\n-    tmp_resp = []\n-    for response in response_lists:\n-        tmp_resp.append(response[0])\n-\n-    return tmp_resp\n-\n-\n-@LOSSES.register_module()\n-class QualityFocalLoss_(nn.Module):\n-    \"\"\"\n-    input[B,M,C] not sigmoid\n-    target[B,M,C], sigmoid\n-    \"\"\"\n-\n-    def __init__(self, beta=2.0):\n-\n-        super(QualityFocalLoss_, self).__init__()\n-        self.beta = beta\n-\n-    def forward(\n-        self,\n-        input: torch.Tensor,\n-        target: torch.Tensor,\n-        pos_normalizer=torch.tensor(1.0),\n-    ):\n-\n-        pred_sigmoid = torch.sigmoid(input)\n-        scale_factor = pred_sigmoid - target\n-\n-        # pred_sigmoid = torch.sigmoid(input)\n-        # scale_factor = input - target\n-        loss = F.binary_cross_entropy_with_logits(input, target, reduction=\"none\") * (\n-            scale_factor.abs().pow(self.beta)\n-        )\n-        loss /= torch.clamp(pos_normalizer, min=1.0)\n-        return loss\n-\n-\n-@LOSSES.register_module()\n-class SimpleL1(nn.Module):\n-    def __init__(self, criterion=\"L1\", student_ch=256, teacher_ch=512):\n-        super().__init__()\n-        self.criterion = criterion\n-        if criterion == \"L1\":\n-            self.criterion_loss = nn.L1Loss(reduce=False)\n-        elif criterion == \"SmoothL1\":\n-            self.criterion_loss = nn.SmoothL1Loss(reduce=False)\n-        elif criterion == \"MSE\":\n-            self.criterion_loss = nn.MSELoss(reduction=\"none\")\n-\n-        if student_ch != teacher_ch:\n-            self.align = nn.Conv2d(student_ch, teacher_ch, kernel_size=1)\n-\n-    def forward(self, feats1, feats2, *args, **kwargs):\n-        if self.criterion == \"MSE\":\n-            feats1 = self.align(feats1) if getattr(self, \"align\", None) else feats1\n-            losses = self.criterion_loss(feats1, feats2).mean()\n-            return losses\n-        else:\n-            return self.criterion_loss(feats1, feats2)\n-\n-\n-@LOSSES.register_module()\n-class Relevance_Distillation(nn.Module):\n-    def __init__(self, bs=2, bn_dim=512, lambd=0.0051):\n-        super().__init__()\n-        self.bs = bs\n-        self.bn_dim = bn_dim\n-        self.lambd = lambd\n-\n-        # normalization layer for the representations z1 and z2\n-        self.bn = nn.BatchNorm1d(self.bn_dim, affine=False)\n-\n-        self.align = nn.Conv2d(256, 512, kernel_size=1)\n-\n-    def forward(self, student_bev, teacher_bev, *args, **kwargs):\n-        student_bev = self.align(student_bev)\n-\n-        student_bev = student_bev.flatten(2)\n-        teacher_bev = teacher_bev.flatten(2)\n-\n-        # empirical cross-correlation matrix\n-        c = self.bn(student_bev).flatten(1).T @ self.bn(teacher_bev).flatten(1)\n-\n-        # sum the cross-correlation matrix between all gpus\n-        c.div_(self.bs)\n-        if self.bs == 1:\n-            pass\n-        else:\n-            torch.distributed.all_reduce(c)\n-\n-        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n-        off_diag = off_diagonal(c).pow_(2).sum()\n-        loss = on_diag + self.lambd * off_diag\n-        return loss\n-\n-\n-@LOSSES.register_module()\n-class FeatureLoss(nn.Module):\n-    \"\"\"PyTorch version of `Masked Generative Distillation`\n-\n-    Args:\n-        student_channels(int): Number of channels in the student's feature map.\n-        teacher_channels(int): Number of channels in the teacher's feature map.\n-        name (str): the loss name of the layer\n-        alpha_mgd (float, optional): Weight of dis_loss. Defaults to 0.00002\n-        lambda_mgd (float, optional): masked ratio. Defaults to 0.65\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        student_channels,\n-        teacher_channels,\n-        name,\n-        alpha_mgd=0.00002,\n-        lambda_mgd=0.65,\n-    ):\n-        super(FeatureLoss, self).__init__()\n-        self.alpha_mgd = alpha_mgd\n-        self.lambda_mgd = lambda_mgd\n-        self.name = name\n-\n-        if student_channels != teacher_channels:\n-            self.align = nn.Conv2d(\n-                student_channels, teacher_channels, kernel_size=1, stride=1, padding=0\n-            )\n-        else:\n-            self.align = None\n-\n-        self.generation = nn.Sequential(\n-            nn.Conv2d(teacher_channels, teacher_channels, kernel_size=3, padding=1),\n-            nn.ReLU(inplace=True),\n-            nn.Conv2d(teacher_channels, teacher_channels, kernel_size=3, padding=1),\n-        )\n-\n-    def forward(self, preds_S, preds_T, **kwargs):\n-        \"\"\"Forward function.\n-        Args:\n-            preds_S(Tensor): Bs*C*H*W, student's feature map\n-            preds_T(Tensor): Bs*C*H*W, teacher's feature map\n-        \"\"\"\n-        assert preds_S.shape[-2:] == preds_T.shape[-2:]\n-\n-        if self.align is not None:\n-            preds_S = self.align(preds_S)\n-\n-        loss = self.get_dis_loss(preds_S, preds_T) * self.alpha_mgd\n-\n-        return loss\n-\n-    def get_dis_loss(self, preds_S, preds_T):\n-        loss_mse = nn.MSELoss(reduction=\"sum\")\n-        N, C, H, W = preds_T.shape\n-\n-        device = preds_S.device\n-        mat = torch.rand((N, 1, H, W)).to(device)\n-        mat = torch.where(mat > 1 - self.lambda_mgd, 0, 1).to(device)\n-\n-        masked_fea = torch.mul(preds_S, mat)\n-        new_fea = self.generation(masked_fea)\n-\n-        dis_loss = loss_mse(new_fea, preds_T) / N\n-\n-        return dis_loss\n-\n-\n-@LOSSES.register_module()\n-class FeatureLoss_InnerClip(nn.Module):\n-    def __init__(\n-        self,\n-        x_sample_num=24,\n-        y_sample_num=24,\n-        inter_keypoint_weight=1,\n-        inter_channel_weight=10,\n-        enlarge_width=1.6,\n-        embed_channels=[256, 512],\n-        inner_feats_distill=None,\n-    ):\n-        super().__init__()\n-        self.x_sample_num = x_sample_num\n-        self.y_sample_num = y_sample_num\n-        self.inter_keypoint_weight = inter_keypoint_weight\n-        self.inter_channel_weight = inter_channel_weight\n-        self.enlarge_width = enlarge_width\n-\n-        self.img_view_transformer = None\n-\n-        self.embed_channels = embed_channels\n-\n-        self.imgbev_embed = nn.Sequential(\n-            nn.Conv2d(\n-                embed_channels[0],\n-                embed_channels[1],\n-                kernel_size=1,\n-                stride=1,\n-                padding=0,\n-                bias=False,\n-            ),\n-            nn.BatchNorm2d(embed_channels[1]),\n-        )\n-\n-        self.inner_feats_loss = (\n-            build_loss(inner_feats_distill) if inner_feats_distill is not None else None\n-        )\n-\n-    def get_gt_sample_grid(self, corner_points2d):\n-        dH_x, dH_y = corner_points2d[0] - corner_points2d[1]\n-        dW_x, dW_y = corner_points2d[0] - corner_points2d[2]\n-        raw_grid_x = (\n-            torch.linspace(\n-                corner_points2d[0][0], corner_points2d[1][0], self.x_sample_num\n-            )\n-            .view(1, -1)\n-            .repeat(self.y_sample_num, 1)\n-        )\n-        raw_grid_y = (\n-            torch.linspace(\n-                corner_points2d[0][1], corner_points2d[2][1], self.y_sample_num\n-            )\n-            .view(-1, 1)\n-            .repeat(1, self.x_sample_num)\n-        )\n-        raw_grid = torch.cat((raw_grid_x.unsqueeze(2), raw_grid_y.unsqueeze(2)), dim=2)\n-        raw_grid_x_offset = (\n-            torch.linspace(0, -dW_x, self.x_sample_num)\n-            .view(-1, 1)\n-            .repeat(1, self.y_sample_num)\n-        )\n-        raw_grid_y_offset = (\n-            torch.linspace(0, -dH_y, self.y_sample_num)\n-            .view(1, -1)\n-            .repeat(self.x_sample_num, 1)\n-        )\n-        raw_grid_offset = torch.cat(\n-            (raw_grid_x_offset.unsqueeze(2), raw_grid_y_offset.unsqueeze(2)), dim=2\n-        )\n-        grid = raw_grid + raw_grid_offset  # X_sample,Y_sample,2\n-        grid[:, :, 0] = torch.clip(\n-            (\n-                (\n-                    grid[:, :, 0]\n-                    - (\n-                        self.img_view_transformer[\"bx\"][0].to(grid.device)\n-                        - self.img_view_transformer[\"dx\"][0].to(grid.device) / 2.0\n-                    )\n-                )\n-                / self.img_view_transformer[\"dx\"][0].to(grid.device)\n-                / (self.img_view_transformer[\"nx\"][0].to(grid.device) - 1)\n-            )\n-            * 2.0\n-            - 1.0,\n-            min=-1.0,\n-            max=1.0,\n-        )\n-        grid[:, :, 1] = torch.clip(\n-            (\n-                (\n-                    grid[:, :, 1]\n-                    - (\n-                        self.img_view_transformer[\"bx\"][1].to(grid.device)\n-                        - self.img_view_transformer[\"dx\"][1].to(grid.device) / 2.0\n-                    )\n-                )\n-                / self.img_view_transformer[\"dx\"][1].to(grid.device)\n-                / (self.img_view_transformer[\"nx\"][1].to(grid.device) - 1)\n-            )\n-            * 2.0\n-            - 1.0,\n-            min=-1.0,\n-            max=1.0,\n-        )\n-\n-        return grid.unsqueeze(0)\n-\n-    def get_inner_feat(self, gt_bboxes_3d, img_feats, pts_feats):\n-        \"\"\"Use grid to sample features of key points\"\"\"\n-        device = img_feats.device\n-        dtype = img_feats[0].dtype\n-\n-        img_feats_sampled_list = []\n-        pts_feats_sampled_list = []\n-\n-        for sample_ind in torch.arange(len(gt_bboxes_3d)):\n-            img_feat = img_feats[sample_ind].unsqueeze(0)  # 1,C,H,W\n-            pts_feat = pts_feats[sample_ind].unsqueeze(0)  # 1,C,H,W\n-\n-            bbox_num, corner_num, point_num = gt_bboxes_3d[sample_ind].corners.shape\n-\n-            for bbox_ind in torch.arange(bbox_num):\n-                if self.enlarge_width > 0:\n-                    gt_sample_grid = self.get_gt_sample_grid(\n-                        gt_bboxes_3d[sample_ind]\n-                        .enlarged_box(self.enlarge_width)\n-                        .corners[bbox_ind][[0, 2, 4, 6], :-1]\n-                    ).to(device)\n-                else:\n-                    gt_sample_grid = self.get_gt_sample_grid(\n-                        gt_bboxes_3d[sample_ind].corners[bbox_ind][[0, 2, 4, 6], :-1]\n-                    ).to(\n-                        device\n-                    )  # 1,sample_y,sample_x,2\n-\n-                img_feats_sampled_list.append(\n-                    F.grid_sample(\n-                        img_feat,\n-                        grid=gt_sample_grid,\n-                        align_corners=False,\n-                        mode=\"bilinear\",\n-                    )\n-                )  # 'bilinear')) #all_bbox_num,C,y_sample,x_sample\n-                pts_feats_sampled_list.append(\n-                    F.grid_sample(\n-                        pts_feat,\n-                        grid=gt_sample_grid,\n-                        align_corners=False,\n-                        mode=\"bilinear\",\n-                    )\n-                )  # 'bilinear')) #all_bbox_num,C,y_sample,x_sample\n-\n-        return torch.cat(img_feats_sampled_list, dim=0), torch.cat(\n-            pts_feats_sampled_list, dim=0\n-        )\n-\n-    @force_fp32(apply_to=(\"img_feats_kd\", \"pts_feats_kd\"))\n-    def get_inter_channel_loss(self, img_feats_kd, pts_feats_kd):\n-        \"\"\"Calculate the inter-channel similarities, guide the student keypoint features to mimic the channel-wise relationships of the teacher`s\"\"\"\n-\n-        C_img = img_feats_kd.shape[1]\n-        C_pts = pts_feats_kd.shape[1]\n-        N = self.x_sample_num * self.y_sample_num\n-\n-        img_feats_kd = img_feats_kd.view(-1, C_img, N).matmul(\n-            img_feats_kd.view(-1, C_img, N).permute(0, 2, 1)\n-        )  # -1,N,N\n-        pts_feats_kd = pts_feats_kd.view(-1, C_pts, N).matmul(\n-            pts_feats_kd.view(-1, C_pts, N).permute(0, 2, 1)\n-        )\n-\n-        img_feats_kd = F.normalize(img_feats_kd, dim=2)\n-        pts_feats_kd = F.normalize(pts_feats_kd, dim=2)\n-\n-        loss_inter_channel = F.mse_loss(img_feats_kd, pts_feats_kd, reduction=\"none\")\n-        loss_inter_channel = loss_inter_channel.sum(-1)\n-        loss_inter_channel = loss_inter_channel.mean()\n-        loss_inter_channel = self.inter_channel_weight * loss_inter_channel\n-        return loss_inter_channel\n-\n-    def forward(self, student_feats, teacher_feats, gt_bboxes_list, **kwargs):\n-\n-        self.img_view_transformer = kwargs.get(\"ivt_cfg\")\n-\n-        if student_feats.size(1) != teacher_feats.size(1):\n-            student_feats = self.imgbev_embed(student_feats)\n-\n-        if self.inter_keypoint_weight > 0 or self.inter_channel_weight > 0:\n-            img_feats_kd, pts_feats_kd = self.get_inner_feat(\n-                gt_bboxes_list, student_feats, teacher_feats\n-            )\n-\n-        if self.inner_feats_loss:\n-            return self.inner_feats_loss(img_feats_kd, pts_feats_kd)\n-\n-        # if self.inter_keypoint_weight > 0:\n-        #     loss_inter_keypoint = self.get_inter_keypoint_loss(\n-        #         img_feats_kd, pts_feats_kd\n-        #     )\n-        #     losses.update({\"loss_inter_keypoint_img_bev\": loss_inter_keypoint})\n-\n-        if self.inter_channel_weight > 0:\n-            loss_inter_channel = self.get_inter_channel_loss(img_feats_kd, pts_feats_kd)\n-\n-        return loss_inter_channel\n-\n-\n-@LOSSES.register_module()\n-class FeatureLoss_Affinity(nn.Module):\n-    def __init__(self):\n-        super().__init__()\n-\n-    def forward(self, student_feats, teacher_feats, *args, **kwargs):\n-        student_feats = [student_feats]\n-        teacher_feats = [teacher_feats]\n-\n-        feature_ditill_loss = 0.0\n-\n-        resize_shape = student_feats[-1].shape[-2:]\n-        if isinstance(student_feats, list):\n-            for i in range(len(student_feats)):\n-                feature_target = teacher_feats[i].detach()\n-                feature_pred = student_feats[i]\n-\n-                B, C, H, W = student_feats[-1].shape\n-\n-                if student_feats[-1].size(-1) != teacher_feats[-1].size(-1):\n-                    feature_pred_down = F.interpolate(\n-                        feature_pred, size=resize_shape, mode=\"bilinear\"\n-                    )\n-                    feature_target_down = F.interpolate(\n-                        feature_target, size=resize_shape, mode=\"bilinear\"\n-                    )\n-                else:\n-                    feature_pred_down = feature_pred\n-                    feature_target_down = feature_target\n-\n-                feature_target_down = feature_target_down.reshape(B, C, -1)\n-                depth_affinity = torch.bmm(\n-                    feature_target_down.permute(0, 2, 1), feature_target_down\n-                )\n-\n-                feature_pred_down = feature_pred_down.reshape(B, C, -1)\n-                rgb_affinity = torch.bmm(\n-                    feature_pred_down.permute(0, 2, 1), feature_pred_down\n-                )\n-\n-                feature_ditill_loss = (\n-                    feature_ditill_loss\n-                    + F.l1_loss(rgb_affinity, depth_affinity, reduction=\"mean\") / B\n-                )\n-\n-        else:\n-            raise NotImplementedError\n-\n-        return feature_ditill_loss\n-\n-\n-@LOSSES.register_module()\n-class FeatureLoss_Coefficient(nn.Module):\n-    def __init__(self, **kwargs):\n-        super().__init__()\n-\n-    def corrcoef(self, target, pred):\n-        pred_n = pred - pred.mean()\n-        target_n = target - target.mean()\n-        pred_n = pred_n / pred_n.norm()\n-        target_n = target_n / target_n.norm()\n-        return (pred_n * target_n).sum()\n-\n-    def forward(\n-        self,\n-        pred,\n-        target,\n-        regularization=\"l2\",\n-        regularization_strength=1.0,\n-    ):\n-        pred = [pred.clone()]\n-        target = [target.clone()]\n-        spearman_loss = 0.0\n-\n-        resize_shape = pred[-1].shape[-2:]  # save training time\n-\n-        if isinstance(pred, list):\n-            for i in range(len(pred)):\n-                feature_target = target[i]\n-                feature_pred = pred[i]\n-                B, C, H, W = feature_pred.shape\n-\n-                feature_pred_down = F.interpolate(\n-                    feature_pred, size=resize_shape, mode=\"bilinear\"\n-                )\n-                feature_target_down = F.interpolate(\n-                    feature_target, size=resize_shape, mode=\"bilinear\"\n-                )\n-\n-                # if feature_pred.size(-1) != feature_target.size(-1):\n-\n-                #     feature_pred_down = F.interpolate(\n-                #         feature_pred, size=resize_shape, mode=\"bilinear\"\n-                #     )\n-                #     feature_target_down = F.interpolate(\n-                #         feature_target, size=resize_shape, mode=\"bilinear\"\n-                #     )\n-                # else:\n-                #     feature_pred_down = feature_pred\n-                #     feature_target_down = feature_target\n-\n-                feature_pred_down = feature_pred_down.reshape(B, -1)\n-                feature_target_down = feature_target_down.reshape(B, -1)\n-\n-                feature_pred_down = torchsort.soft_rank(\n-                    feature_pred_down,\n-                    regularization=regularization,\n-                    regularization_strength=regularization_strength,\n-                )\n-                spearman_loss += 1 - self.corrcoef(\n-                    feature_target_down, feature_pred_down / feature_pred_down.shape[-1]\n-                )\n-\n-        return spearman_loss\n-\n-\n-@LOSSES.register_module()\n-class Radar_MSDistilll(nn.Module):\n-    def __init__(self, num_layers=2, each_layer_loss_cfg=[]):\n-        super().__init__()\n-        self.num_layers = num_layers\n-        assert num_layers == len(each_layer_loss_cfg)\n-        for idx, cfg_dict in enumerate(each_layer_loss_cfg):\n-            setattr(self, f\"layer_loss_{idx}\", build_loss(cfg_dict))\n-\n-    def forward(self, radar_ms_feats, pts_ms_feats):\n-        assert isinstance(radar_ms_feats, list)\n-        losses = 0.0\n-\n-        for idx in range(self.num_layers):\n-            losses += getattr(self, f\"layer_loss_{idx}\")(\n-                radar_ms_feats[idx], pts_ms_feats[idx]\n-            )\n-\n-        return losses / self.num_layers\n-\n-\n-@LOSSES.register_module()\n-class InfoMax(nn.Module):\n-    def __init__(self, **kwargs):\n-        super().__init__()\n-\n-    def forward(self, x1, x2):\n-        import pdb\n-\n-        pdb.set_trace()\n-        x1 = x1 / (torch.norm(x1, p=2, dim=1, keepdim=True) + 1e-10)\n-        x2 = x2 / (torch.norm(x2, p=2, dim=1, keepdim=True) + 1e-10)\n-        bs = x1.size(0)\n-        s = torch.matmul(x1, x2.permute(1, 0))\n-        mask_joint = torch.eye(bs).cuda()\n-        mask_marginal = 1 - mask_joint\n-\n-        Ej = (s * mask_joint).mean()\n-        Em = torch.exp(s * mask_marginal).mean()\n-        # decoupled comtrastive learning?!!!!\n-        # infomax_loss = - (Ej - torch.log(Em)) * self.alpha\n-        infomax_loss = -(Ej - torch.log(Em))  # / Em\n-        return infomax_loss\n-\n-\n-@LOSSES.register_module()\n-class HeatMapAug(nn.Module):\n-    def __init__(self):\n-        super().__init__()\n-\n-    @force_fp32(apply_to=(\"stu_pred\", \"tea_pred\"))\n-    def forward(self, stu_pred, tea_pred, fg_map):\n-\n-        num_task = len(stu_pred)\n-        kl_loss = 0\n-        for task_id in range(num_task):\n-            student_pred = stu_pred[task_id][0][\"heatmap\"].sigmoid()\n-            teacher_pred = tea_pred[task_id][0][\"heatmap\"].sigmoid()\n-            fg_map = fg_map.unsqueeze(1)\n-            task_kl_loss = F.binary_cross_entropy(student_pred, teacher_pred.detach())\n-            task_kl_loss = torch.sum(task_kl_loss * fg_map) / torch.sum(fg_map)\n-            kl_loss += task_kl_loss\n-\n-        return kl_loss\n-\n-\n-@LOSSES.register_module()\n-class Dc_ResultDistill(nn.Module):\n-    def __init__(\n-        self,\n-        pc_range=[],\n-        voxel_size=[],\n-        out_size_scale=8,\n-        ret_sum=False,\n-        loss_weight_reg=10,\n-        loss_weight_cls=10,\n-        max_cls=True,\n-    ):\n-        super().__init__()\n-\n-        self.pc_range = pc_range\n-        self.voxel_size = voxel_size\n-        self.out_size_scale = out_size_scale\n-        self.ret_sum = ret_sum\n-        self.loss_weight_reg = loss_weight_reg\n-        self.loss_weight_cls = loss_weight_cls\n-        self.max_cls = max_cls\n-\n-    def forward(self, resp_lidar, resp_fuse, gt_boxes):\n-        \"\"\"Dc_ResultDistill forward.\n-\n-        Args:\n-            resp_lidar (_type_):\n-            resp_fuse (_type_):\n-            gt_boxes (_type_):\n-\n-        Returns:\n-            _type_: _description_\n-        \"\"\"\n-\n-        tmp_resp_lidar = []\n-        tmp_resp_fuse = []\n-        for res_lidar, res_fuse in zip(resp_lidar, resp_fuse):\n-            tmp_resp_lidar.append(res_lidar[0])\n-            tmp_resp_fuse.append(res_fuse[0])\n-\n-        tmp_gt_boxes = []\n-        for bs_idx in range(len(gt_boxes)):\n-\n-            gt_bboxes_3d = torch.cat(\n-                (gt_boxes[bs_idx].gravity_center, gt_boxes[bs_idx].tensor[:, 3:]), dim=1\n-            )\n-\n-            tmp_gt_boxes.append(gt_bboxes_3d)\n-\n-        cls_lidar = []\n-        reg_lidar = []\n-        cls_fuse = []\n-        reg_fuse = []\n-\n-        # criterion = nn.L1Loss(reduce=False)\n-        # criterion_cls = QualityFocalLoss_()\n-\n-        criterion = nn.SmoothL1Loss(reduce=False)\n-        criterion_cls = nn.L1Loss(reduce=False)\n-\n-        for task_id, task_out in enumerate(tmp_resp_lidar):\n-            cls_lidar.append(task_out[\"heatmap\"])\n-            cls_fuse.append(_sigmoid(tmp_resp_fuse[task_id][\"heatmap\"] / 2))\n-            reg_lidar.append(\n-                torch.cat(\n-                    [\n-                        task_out[\"reg\"],\n-                        task_out[\"height\"],\n-                        task_out[\"dim\"],\n-                        task_out[\"rot\"],\n-                        task_out[\"vel\"],\n-                        # task_out[\"iou\"],\n-                    ],\n-                    dim=1,\n-                )\n-            )\n-            reg_fuse.append(\n-                torch.cat(\n-                    [\n-                        tmp_resp_fuse[task_id][\"reg\"],\n-                        tmp_resp_fuse[task_id][\"height\"],\n-                        tmp_resp_fuse[task_id][\"dim\"],\n-                        tmp_resp_fuse[task_id][\"rot\"],\n-                        tmp_resp_fuse[task_id][\"vel\"],\n-                        # resp_fuse[task_id][\"iou\"],\n-                    ],\n-                    dim=1,\n-                )\n-            )\n-        cls_lidar = torch.cat(cls_lidar, dim=1)\n-        reg_lidar = torch.cat(reg_lidar, dim=1)\n-        cls_fuse = torch.cat(cls_fuse, dim=1)\n-        reg_fuse = torch.cat(reg_fuse, dim=1)\n-\n-        if self.max_cls:\n-            cls_lidar_max, _ = torch.max(cls_lidar, dim=1)\n-            cls_fuse_max, _ = torch.max(cls_fuse, dim=1)\n-        else:\n-            _, _, ht_h, ht_w = cls_fuse.shape\n-            cls_lidar_max = cls_lidar.flatten(2).permute(0, 2, 1)\n-            cls_fuse_max = cls_fuse.flatten(2).permute(0, 2, 1)\n-\n-        gaussian_mask = calculate_box_mask_gaussian(\n-            reg_lidar.shape,\n-            tmp_gt_boxes,\n-            self.pc_range,\n-            self.voxel_size,\n-            self.out_size_scale,\n-        )\n-\n-        # # diff_reg = criterion(reg_lidar, reg_fuse)\n-        # # diff_cls = criterion(cls_lidar_max, cls_fuse_max)\n-        # diff_reg = criterion_reg(reg_lidar, reg_fuse)\n-\n-        # cls_lidar_max = torch.sigmoid(cls_lidar_max)\n-        # diff_cls = criterion_cls(\n-        #     cls_fuse_max, cls_lidar_max\n-        # )  # Compared with directly using the L1 loss constraint, replace it with bce focal loss and change the position?\n-\n-        # weight = gaussian_mask.sum()\n-        # weight = reduce_mean(weight)\n-\n-        # # diff_cls = criterion_cls(cls_lidar_max, cls_fuse_max)\n-        # diff_reg = torch.mean(diff_reg, dim=1)\n-        # diff_reg = diff_reg * gaussian_mask\n-        # loss_reg_distill = torch.sum(diff_reg) / (weight + 1e-4)\n-\n-        # if not self.max_cls:\n-        #     diff_cls = diff_cls\n-        #     loss_cls_distill = diff_cls.sum() * 2\n-        # else:\n-        #     diff_cls = diff_cls * gaussian_mask\n-        #     loss_cls_distill = torch.sum(diff_cls) / (weight + 1e-4)\n-\n-        diff_reg = criterion(reg_lidar, reg_fuse)\n-\n-        diff_cls = criterion_cls(cls_lidar_max, cls_fuse_max)\n-        diff_reg = torch.mean(diff_reg, dim=1)\n-        diff_reg = diff_reg * gaussian_mask\n-        diff_cls = diff_cls * gaussian_mask\n-        weight = gaussian_mask.sum()\n-        weight = reduce_mean(weight)\n-        loss_reg_distill = torch.sum(diff_reg) / (weight + 1e-4)\n-        loss_cls_distill = torch.sum(diff_cls) / (weight + 1e-4)\n-\n-        if self.ret_sum:\n-\n-            loss_det_distill = self.loss_weight * (loss_reg_distill + loss_cls_distill)\n-\n-            return loss_det_distill\n-        else:\n-\n-            return (\n-                self.loss_weight_reg * loss_reg_distill,\n-                self.loss_weight_cls * loss_cls_distill,\n-            )\n-\n-\n-@LOSSES.register_module()\n-class SelfLearningMFD(nn.Module):\n-    def __init__(\n-        self,\n-        bev_shape=[128, 128],\n-        pc_range=[-51.2, -51.2, -5, 51.2, 51.2, 3],\n-        voxel_size=[0.1, 0.1, 0.1],\n-        score_threshold=0.7,\n-        add_stu_decode_bboxes=False,\n-        loss_weight=1e-2,\n-        bbox_coder=None,\n-        student_channels=256,\n-        teacher_channels=512,\n-    ):\n-        super().__init__()\n-        self.bev_shape = bev_shape\n-        self.pc_range = pc_range\n-        self.voxel_size = voxel_size\n-        self.shape_resize_times = bbox_coder.out_size_factor if bbox_coder else 8\n-        self.score_threshold = score_threshold\n-        self.add_stu_decode_bboxes = add_stu_decode_bboxes\n-        self.loss_weight = loss_weight\n-\n-        self.align = nn.Sequential(\n-            nn.Conv2d(student_channels, teacher_channels, kernel_size=1, padding=0),\n-            # nn.BatchNorm2d(teacher_channels),\n-        )\n-\n-        if bbox_coder is not None:\n-            self.bbox_coder = build_bbox_coder(bbox_coder)\n-\n-    def pred2bboxes(self, preds_dict):\n-        \"\"\"SelfLearningMFD-pred2bboxes forward\n-\n-        Args:\n-            pred_bboxes (list):\n-            [task1[[task_head_reg,task_head_heatmap,...,]],\n-            task2...,\n-            taskn]\n-        \"\"\"\n-\n-        for task_id, pred_data in enumerate(preds_dict):\n-            bath_tmp = []\n-            batch_size = pred_data[0][\"heatmap\"].shape[0]\n-            batch_heatmap = pred_data[0][\"heatmap\"].sigmoid()\n-            batch_reg = pred_data[0][\"reg\"]\n-            batch_hei = pred_data[0][\"height\"]\n-\n-            # denormalization\n-            batch_dim = torch.exp(pred_data[0][\"dim\"])\n-\n-            batch_rot_sin = pred_data[0][\"rot\"][:, 0].unsqueeze(1)\n-            batch_rot_cos = pred_data[0][\"rot\"][:, 1].unsqueeze(1)\n-\n-            if \"vel\" in pred_data[0].keys():\n-                batch_vel = pred_data[0][\"vel\"]\n-\n-            bath_tmp.append(batch_heatmap)\n-            bath_tmp.append(batch_rot_sin)\n-            bath_tmp.append(batch_rot_cos)\n-            bath_tmp.append(batch_hei)\n-            bath_tmp.append(batch_dim)\n-            bath_tmp.append(batch_vel)\n-            bath_tmp.append(batch_reg)\n-\n-            bboxes_decode = self.bbox_coder.decode(*bath_tmp)\n-\n-        return bboxes_decode\n-\n-    def aug_boxes(self, gt_boxes, rot_mat):\n-        rot_lim = (-22.5, 22.5)\n-        scale_lim = (0.95, 1.05)\n-        rotate_angle = np.random.uniform(*rot_lim)\n-        scale_ratio = np.random.uniform(*scale_lim)\n-        gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n-        gt_boxes[:, 3:6] *= scale_ratio\n-        gt_boxes[:, 6] += rotate_angle\n-        gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n-        gt_boxes[:, 6] = -gt_boxes[:, 6]\n-        gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(-1)\n-        gt_boxes[:, :3] = gt_boxes[:, :3]\n-\n-        return gt_boxes\n-\n-    @force_fp32(\n-        apply_to=(\n-            \"student_bev_feat\",\n-            \"teacher_bev_feat\",\n-        )\n-    )\n-    def forward(\n-        self,\n-        student_bev_feat,\n-        teacher_bev_feat,\n-        gt_bboxes_list=None,\n-        masks_bboxes=None,\n-        bda_mat=None,\n-    ):\n-        \"\"\"SelfLearningMFD forward.\n-\n-        Args:\n-            student_bev_feats (torch.tensor): Calculate student feats\n-            teacher_bev_feats (torch.tensor): Calculate teacher feats\n-            masks_bboxes (list): Self-learning mask detection\n-            gt_bboxes_list (list): [LiDARInstance3DBoxes(gravity_center+tensor(num_objs,9))]\n-\n-        Returns:\n-            dict: _description_\n-        \"\"\"\n-\n-        bs = student_bev_feat.size(0)\n-\n-        if student_bev_feat.size(1) != teacher_bev_feat.size(1):\n-            student_bev_feat = self.align(student_bev_feat)\n-\n-        if masks_bboxes is not None and self.add_stu_decode_bboxes:\n-            student_pred_bboxes_list = self.pred2bboxes(\n-                masks_bboxes\n-            )  # list of task : each shape of [(num_bboxes,9)]\n-\n-            if bda_mat is not None:\n-                student_pred_bboxes_list = self.aug_boxes(student_pred_bboxes_list)\n-\n-        bev_feat_shape = torch.tensor(self.bev_shape)\n-        voxel_size = torch.tensor(self.voxel_size)\n-        feature_map_size = bev_feat_shape[:2]\n-\n-        if gt_bboxes_list is not None:\n-            device = student_bev_feat.device\n-\n-            gt_bboxes_list = [\n-                torch.cat(\n-                    (gt_bboxes.gravity_center, gt_bboxes.tensor[:, 3:]), dim=1\n-                ).to(device)\n-                for gt_bboxes in gt_bboxes_list\n-            ]\n-\n-            if self.add_stu_decode_bboxes:\n-                for idx, data in enumerate(student_pred_bboxes_list):\n-\n-                    bboxes_dim = data[\"bboxes\"].size(1)\n-\n-                    scroes_mask = data[\"scores\"] > self.score_threshold\n-\n-                    new_select_by_mask = data[\"bboxes\"][scroes_mask, :]\n-\n-                    gt_bboxes_list[idx] = torch.cat(\n-                        [gt_bboxes_list[idx], new_select_by_mask], dim=0\n-                    )\n-\n-            fg_map = student_bev_feat.new_zeros(\n-                (len(gt_bboxes_list), feature_map_size[1], feature_map_size[0])\n-            )\n-\n-            for idx in range(len(gt_bboxes_list)):\n-                num_objs = gt_bboxes_list[idx].shape[0]\n-\n-                for k in range(num_objs):\n-                    width = gt_bboxes_list[idx][k][3]\n-                    length = gt_bboxes_list[idx][k][4]\n-                    width = width / voxel_size[0] / self.shape_resize_times\n-                    length = length / voxel_size[1] / self.shape_resize_times\n-\n-                    if width > 0 and length > 0:\n-                        radius = gaussian_radius((length, width), min_overlap=0.1)\n-                        radius = max(1, int(radius))\n-\n-                        # be really careful for the coordinate system of\n-                        # your box annotation.\n-                        x, y, z = (\n-                            gt_bboxes_list[idx][k][0],\n-                            gt_bboxes_list[idx][k][1],\n-                            gt_bboxes_list[idx][k][2],\n-                        )\n-\n-                        coor_x = (\n-                            (x - self.pc_range[0])\n-                            / voxel_size[0]\n-                            / self.shape_resize_times\n-                        )\n-                        coor_y = (\n-                            (y - self.pc_range[1])\n-                            / voxel_size[1]\n-                            / self.shape_resize_times\n-                        )\n-\n-                        center = torch.tensor(\n-                            [coor_x, coor_y], dtype=torch.float32, device=device\n-                        )\n-                        center_int = center.to(torch.int32)\n-\n-                        # throw out not in range objects to avoid out of array\n-                        # area when creating the heatmap\n-                        if not (\n-                            0 <= center_int[0] < feature_map_size[0]\n-                            and 0 <= center_int[1] < feature_map_size[1]\n-                        ):\n-                            continue\n-\n-                        draw_heatmap_gaussian(fg_map[idx], center_int, radius)\n-\n-        if fg_map is None:\n-            fg_map = student_bev_feat.new_ones(\n-                (student_bev_feat.shape[0], feature_map_size[1], feature_map_size[0])\n-            )\n-\n-        if bs > 1:\n-            fg_map = fg_map.unsqueeze(1)\n-\n-        fit_loss = F.mse_loss(student_bev_feat, teacher_bev_feat, reduction=\"none\")\n-        fit_loss = torch.sum(fit_loss * fg_map) / torch.sum(fg_map)\n-\n-        return fit_loss * self.loss_weight, fg_map\n-\n-    # def forward(\n-    #     self, student_bev_feats, teacher_bev_feats, masks_bboxes, gt_bboxes_list\n-    # ):\n-    #     \"\"\"SelfLearningMFD forward.\n-\n-    #     Args:\n-    #         student_bev_feats (torch.tensor): _description_\n-    #         teacher_bev_feats (torch.tensor): _description_\n-    #         masks_bboxes (): _description_\n-    #         gt_bboxes_list (_type_): _description_\n-    #     \"\"\"\n-    #     assert isinstance(masks_bboxes, list)\n-    #     ret_dict = multi_apply(\n-    #         self._forward,\n-    #         student_bev_feats,\n-    #         teacher_bev_feats,\n-    #         masks_bboxes,\n-    #         gt_bboxes_list,\n-    #     )\n"
                },
                {
                    "date": 1716031348524,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,1011 @@\n+import torch.nn as nn\n+import torch.nn.functional as F\n+import torch\n+from ..builder import LOSSES, build_loss\n+from ..utils.gaussian_utils import calculate_box_mask_gaussian\n+from torch import distributed as dist\n+from mmdet.core import multi_apply, build_bbox_coder\n+from mmdet3d.core import draw_heatmap_gaussian, gaussian_radius\n+from mmdet3d.core.bbox.coders.centerpoint_bbox_coders import (\n+    CenterPointBBoxCoder as bboxcoder,\n+)\n+from mmdet.models.losses import QualityFocalLoss\n+from mmcv.runner import force_fp32\n+import torchsort\n+import torchvision\n+import numpy as np\n+\n+\n+def get_world_size() -> int:\n+    if not dist.is_available():\n+        return 1\n+    if not dist.is_initialized():\n+        return 1\n+\n+    return dist.get_world_size()\n+\n+\n+def reduce_sum(tensor):\n+    world_size = get_world_size()\n+    if world_size < 2:\n+        return tensor\n+    tensor = tensor.clone()\n+    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n+    return tensor\n+\n+\n+def reduce_mean(tensor):\n+    return reduce_sum(tensor) / float(get_world_size())\n+\n+\n+def _sigmoid(x):\n+    # y = torch.clamp(x.sigmoid(), min=1e-3, max=1 - 1e-3)\n+    y = x.sigmoid()\n+    return y\n+\n+\n+def off_diagonal(x):\n+    # return a flattened view of the off-diagonal elements of a square matrix\n+    n, m = x.shape\n+    assert n == m\n+    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n+\n+\n+def response_list_wrap(response_lists: list):\n+    \"\"\"response_list_wrap\n+\n+    Args:\n+        response_lists (list): Pack list content\n+\n+    Returns:\n+        list: Return the new list of packaged integration\n+    \"\"\"\n+    assert len(response_lists) == 1\n+\n+    tmp_resp = []\n+    for response in response_lists:\n+        tmp_resp.append(response[0])\n+\n+    return tmp_resp\n+\n+\n+@LOSSES.register_module()\n+class QualityFocalLoss_(nn.Module):\n+    \"\"\"\n+    input[B,M,C] not sigmoid\n+    target[B,M,C], sigmoid\n+    \"\"\"\n+\n+    def __init__(self, beta=2.0):\n+\n+        super(QualityFocalLoss_, self).__init__()\n+        self.beta = beta\n+\n+    def forward(\n+        self,\n+        input: torch.Tensor,\n+        target: torch.Tensor,\n+        pos_normalizer=torch.tensor(1.0),\n+    ):\n+\n+        pred_sigmoid = torch.sigmoid(input)\n+        scale_factor = pred_sigmoid - target\n+\n+        # pred_sigmoid = torch.sigmoid(input)\n+        # scale_factor = input - target\n+        loss = F.binary_cross_entropy_with_logits(input, target, reduction=\"none\") * (\n+            scale_factor.abs().pow(self.beta)\n+        )\n+        loss /= torch.clamp(pos_normalizer, min=1.0)\n+        return loss\n+\n+\n+@LOSSES.register_module()\n+class SimpleL1(nn.Module):\n+    def __init__(self, criterion=\"L1\", student_ch=256, teacher_ch=512):\n+        super().__init__()\n+        self.criterion = criterion\n+        if criterion == \"L1\":\n+            self.criterion_loss = nn.L1Loss(reduce=False)\n+        elif criterion == \"SmoothL1\":\n+            self.criterion_loss = nn.SmoothL1Loss(reduce=False)\n+        elif criterion == \"MSE\":\n+            self.criterion_loss = nn.MSELoss(reduction=\"none\")\n+\n+        if student_ch != teacher_ch:\n+            self.align = nn.Conv2d(student_ch, teacher_ch, kernel_size=1)\n+\n+    def forward(self, feats1, feats2, *args, **kwargs):\n+        if self.criterion == \"MSE\":\n+            feats1 = self.align(feats1) if getattr(self, \"align\", None) else feats1\n+            losses = self.criterion_loss(feats1, feats2).mean()\n+            return losses\n+        else:\n+            return self.criterion_loss(feats1, feats2)\n+\n+\n+@LOSSES.register_module()\n+class Relevance_Distillation(nn.Module):\n+    def __init__(self, bs=2, bn_dim=512, lambd=0.0051):\n+        super().__init__()\n+        self.bs = bs\n+        self.bn_dim = bn_dim\n+        self.lambd = lambd\n+\n+        # normalization layer for the representations z1 and z2\n+        self.bn = nn.BatchNorm1d(self.bn_dim, affine=False)\n+\n+        self.align = nn.Conv2d(256, 512, kernel_size=1)\n+\n+    def forward(self, student_bev, teacher_bev, *args, **kwargs):\n+        student_bev = self.align(student_bev)\n+\n+        student_bev = student_bev.flatten(2)\n+        teacher_bev = teacher_bev.flatten(2)\n+\n+        # empirical cross-correlation matrix\n+        c = self.bn(student_bev).flatten(1).T @ self.bn(teacher_bev).flatten(1)\n+\n+        # sum the cross-correlation matrix between all gpus\n+        c.div_(self.bs)\n+        if self.bs == 1:\n+            pass\n+        else:\n+            torch.distributed.all_reduce(c)\n+\n+        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n+        off_diag = off_diagonal(c).pow_(2).sum()\n+        loss = on_diag + self.lambd * off_diag\n+        return loss\n+\n+\n+@LOSSES.register_module()\n+class FeatureLoss(nn.Module):\n+    \"\"\"PyTorch version of `Masked Generative Distillation`\n+\n+    Args:\n+        student_channels(int): Number of channels in the student's feature map.\n+        teacher_channels(int): Number of channels in the teacher's feature map.\n+        name (str): the loss name of the layer\n+        alpha_mgd (float, optional): Weight of dis_loss. Defaults to 0.00002\n+        lambda_mgd (float, optional): masked ratio. Defaults to 0.65\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        student_channels,\n+        teacher_channels,\n+        name,\n+        alpha_mgd=0.00002,\n+        lambda_mgd=0.65,\n+    ):\n+        super(FeatureLoss, self).__init__()\n+        self.alpha_mgd = alpha_mgd\n+        self.lambda_mgd = lambda_mgd\n+        self.name = name\n+\n+        if student_channels != teacher_channels:\n+            self.align = nn.Conv2d(\n+                student_channels, teacher_channels, kernel_size=1, stride=1, padding=0\n+            )\n+        else:\n+            self.align = None\n+\n+        self.generation = nn.Sequential(\n+            nn.Conv2d(teacher_channels, teacher_channels, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.Conv2d(teacher_channels, teacher_channels, kernel_size=3, padding=1),\n+        )\n+\n+    def forward(self, preds_S, preds_T, **kwargs):\n+        \"\"\"Forward function.\n+        Args:\n+            preds_S(Tensor): Bs*C*H*W, student's feature map\n+            preds_T(Tensor): Bs*C*H*W, teacher's feature map\n+        \"\"\"\n+        assert preds_S.shape[-2:] == preds_T.shape[-2:]\n+\n+        if self.align is not None:\n+            preds_S = self.align(preds_S)\n+\n+        loss = self.get_dis_loss(preds_S, preds_T) * self.alpha_mgd\n+\n+        return loss\n+\n+    def get_dis_loss(self, preds_S, preds_T):\n+        loss_mse = nn.MSELoss(reduction=\"sum\")\n+        N, C, H, W = preds_T.shape\n+\n+        device = preds_S.device\n+        mat = torch.rand((N, 1, H, W)).to(device)\n+        mat = torch.where(mat > 1 - self.lambda_mgd, 0, 1).to(device)\n+\n+        masked_fea = torch.mul(preds_S, mat)\n+        new_fea = self.generation(masked_fea)\n+\n+        dis_loss = loss_mse(new_fea, preds_T) / N\n+\n+        return dis_loss\n+\n+\n+@LOSSES.register_module()\n+class FeatureLoss_InnerClip(nn.Module):\n+    def __init__(\n+        self,\n+        x_sample_num=24,\n+        y_sample_num=24,\n+        inter_keypoint_weight=1,\n+        inter_channel_weight=10,\n+        enlarge_width=1.6,\n+        embed_channels=[256, 512],\n+        inner_feats_distill=None,\n+    ):\n+        super().__init__()\n+        self.x_sample_num = x_sample_num\n+        self.y_sample_num = y_sample_num\n+        self.inter_keypoint_weight = inter_keypoint_weight\n+        self.inter_channel_weight = inter_channel_weight\n+        self.enlarge_width = enlarge_width\n+\n+        self.img_view_transformer = None\n+\n+        self.embed_channels = embed_channels\n+\n+        self.imgbev_embed = nn.Sequential(\n+            nn.Conv2d(\n+                embed_channels[0],\n+                embed_channels[1],\n+                kernel_size=1,\n+                stride=1,\n+                padding=0,\n+                bias=False,\n+            ),\n+            nn.BatchNorm2d(embed_channels[1]),\n+        )\n+\n+        self.inner_feats_loss = (\n+            build_loss(inner_feats_distill) if inner_feats_distill is not None else None\n+        )\n+\n+    def get_gt_sample_grid(self, corner_points2d):\n+        dH_x, dH_y = corner_points2d[0] - corner_points2d[1]\n+        dW_x, dW_y = corner_points2d[0] - corner_points2d[2]\n+        raw_grid_x = (\n+            torch.linspace(\n+                corner_points2d[0][0], corner_points2d[1][0], self.x_sample_num\n+            )\n+            .view(1, -1)\n+            .repeat(self.y_sample_num, 1)\n+        )\n+        raw_grid_y = (\n+            torch.linspace(\n+                corner_points2d[0][1], corner_points2d[2][1], self.y_sample_num\n+            )\n+            .view(-1, 1)\n+            .repeat(1, self.x_sample_num)\n+        )\n+        raw_grid = torch.cat((raw_grid_x.unsqueeze(2), raw_grid_y.unsqueeze(2)), dim=2)\n+        raw_grid_x_offset = (\n+            torch.linspace(0, -dW_x, self.x_sample_num)\n+            .view(-1, 1)\n+            .repeat(1, self.y_sample_num)\n+        )\n+        raw_grid_y_offset = (\n+            torch.linspace(0, -dH_y, self.y_sample_num)\n+            .view(1, -1)\n+            .repeat(self.x_sample_num, 1)\n+        )\n+        raw_grid_offset = torch.cat(\n+            (raw_grid_x_offset.unsqueeze(2), raw_grid_y_offset.unsqueeze(2)), dim=2\n+        )\n+        grid = raw_grid + raw_grid_offset  # X_sample,Y_sample,2\n+        grid[:, :, 0] = torch.clip(\n+            (\n+                (\n+                    grid[:, :, 0]\n+                    - (\n+                        self.img_view_transformer[\"bx\"][0].to(grid.device)\n+                        - self.img_view_transformer[\"dx\"][0].to(grid.device) / 2.0\n+                    )\n+                )\n+                / self.img_view_transformer[\"dx\"][0].to(grid.device)\n+                / (self.img_view_transformer[\"nx\"][0].to(grid.device) - 1)\n+            )\n+            * 2.0\n+            - 1.0,\n+            min=-1.0,\n+            max=1.0,\n+        )\n+        grid[:, :, 1] = torch.clip(\n+            (\n+                (\n+                    grid[:, :, 1]\n+                    - (\n+                        self.img_view_transformer[\"bx\"][1].to(grid.device)\n+                        - self.img_view_transformer[\"dx\"][1].to(grid.device) / 2.0\n+                    )\n+                )\n+                / self.img_view_transformer[\"dx\"][1].to(grid.device)\n+                / (self.img_view_transformer[\"nx\"][1].to(grid.device) - 1)\n+            )\n+            * 2.0\n+            - 1.0,\n+            min=-1.0,\n+            max=1.0,\n+        )\n+\n+        return grid.unsqueeze(0)\n+\n+    def get_inner_feat(self, gt_bboxes_3d, img_feats, pts_feats):\n+        \"\"\"Use grid to sample features of key points\"\"\"\n+        device = img_feats.device\n+        dtype = img_feats[0].dtype\n+\n+        img_feats_sampled_list = []\n+        pts_feats_sampled_list = []\n+\n+        for sample_ind in torch.arange(len(gt_bboxes_3d)):\n+            img_feat = img_feats[sample_ind].unsqueeze(0)  # 1,C,H,W\n+            pts_feat = pts_feats[sample_ind].unsqueeze(0)  # 1,C,H,W\n+\n+            bbox_num, corner_num, point_num = gt_bboxes_3d[sample_ind].corners.shape\n+\n+            for bbox_ind in torch.arange(bbox_num):\n+                if self.enlarge_width > 0:\n+                    gt_sample_grid = self.get_gt_sample_grid(\n+                        gt_bboxes_3d[sample_ind]\n+                        .enlarged_box(self.enlarge_width)\n+                        .corners[bbox_ind][[0, 2, 4, 6], :-1]\n+                    ).to(device)\n+                else:\n+                    gt_sample_grid = self.get_gt_sample_grid(\n+                        gt_bboxes_3d[sample_ind].corners[bbox_ind][[0, 2, 4, 6], :-1]\n+                    ).to(\n+                        device\n+                    )  # 1,sample_y,sample_x,2\n+\n+                img_feats_sampled_list.append(\n+                    F.grid_sample(\n+                        img_feat,\n+                        grid=gt_sample_grid,\n+                        align_corners=False,\n+                        mode=\"bilinear\",\n+                    )\n+                )  # 'bilinear')) #all_bbox_num,C,y_sample,x_sample\n+                pts_feats_sampled_list.append(\n+                    F.grid_sample(\n+                        pts_feat,\n+                        grid=gt_sample_grid,\n+                        align_corners=False,\n+                        mode=\"bilinear\",\n+                    )\n+                )  # 'bilinear')) #all_bbox_num,C,y_sample,x_sample\n+\n+        return torch.cat(img_feats_sampled_list, dim=0), torch.cat(\n+            pts_feats_sampled_list, dim=0\n+        )\n+\n+    @force_fp32(apply_to=(\"img_feats_kd\", \"pts_feats_kd\"))\n+    def get_inter_channel_loss(self, img_feats_kd, pts_feats_kd):\n+        \"\"\"Calculate the inter-channel similarities, guide the student keypoint features to mimic the channel-wise relationships of the teacher`s\"\"\"\n+\n+        C_img = img_feats_kd.shape[1]\n+        C_pts = pts_feats_kd.shape[1]\n+        N = self.x_sample_num * self.y_sample_num\n+\n+        img_feats_kd = img_feats_kd.view(-1, C_img, N).matmul(\n+            img_feats_kd.view(-1, C_img, N).permute(0, 2, 1)\n+        )  # -1,N,N\n+        pts_feats_kd = pts_feats_kd.view(-1, C_pts, N).matmul(\n+            pts_feats_kd.view(-1, C_pts, N).permute(0, 2, 1)\n+        )\n+\n+        img_feats_kd = F.normalize(img_feats_kd, dim=2)\n+        pts_feats_kd = F.normalize(pts_feats_kd, dim=2)\n+\n+        loss_inter_channel = F.mse_loss(img_feats_kd, pts_feats_kd, reduction=\"none\")\n+        loss_inter_channel = loss_inter_channel.sum(-1)\n+        loss_inter_channel = loss_inter_channel.mean()\n+        loss_inter_channel = self.inter_channel_weight * loss_inter_channel\n+        return loss_inter_channel\n+\n+    def forward(self, student_feats, teacher_feats, gt_bboxes_list, **kwargs):\n+\n+        self.img_view_transformer = kwargs.get(\"ivt_cfg\")\n+\n+        if student_feats.size(1) != teacher_feats.size(1):\n+            student_feats = self.imgbev_embed(student_feats)\n+\n+        if self.inter_keypoint_weight > 0 or self.inter_channel_weight > 0:\n+            img_feats_kd, pts_feats_kd = self.get_inner_feat(\n+                gt_bboxes_list, student_feats, teacher_feats\n+            )\n+\n+        if self.inner_feats_loss:\n+            return self.inner_feats_loss(img_feats_kd, pts_feats_kd)\n+\n+        # if self.inter_keypoint_weight > 0:\n+        #     loss_inter_keypoint = self.get_inter_keypoint_loss(\n+        #         img_feats_kd, pts_feats_kd\n+        #     )\n+        #     losses.update({\"loss_inter_keypoint_img_bev\": loss_inter_keypoint})\n+\n+        if self.inter_channel_weight > 0:\n+            loss_inter_channel = self.get_inter_channel_loss(img_feats_kd, pts_feats_kd)\n+\n+        return loss_inter_channel\n+\n+\n+@LOSSES.register_module()\n+class FeatureLoss_Affinity(nn.Module):\n+    def __init__(self):\n+        super().__init__()\n+\n+    def forward(self, student_feats, teacher_feats, *args, **kwargs):\n+        student_feats = [student_feats]\n+        teacher_feats = [teacher_feats]\n+\n+        feature_ditill_loss = 0.0\n+\n+        resize_shape = student_feats[-1].shape[-2:]\n+        if isinstance(student_feats, list):\n+            for i in range(len(student_feats)):\n+                feature_target = teacher_feats[i].detach()\n+                feature_pred = student_feats[i]\n+\n+                B, C, H, W = student_feats[-1].shape\n+\n+                if student_feats[-1].size(-1) != teacher_feats[-1].size(-1):\n+                    feature_pred_down = F.interpolate(\n+                        feature_pred, size=resize_shape, mode=\"bilinear\"\n+                    )\n+                    feature_target_down = F.interpolate(\n+                        feature_target, size=resize_shape, mode=\"bilinear\"\n+                    )\n+                else:\n+                    feature_pred_down = feature_pred\n+                    feature_target_down = feature_target\n+\n+                feature_target_down = feature_target_down.reshape(B, C, -1)\n+                depth_affinity = torch.bmm(\n+                    feature_target_down.permute(0, 2, 1), feature_target_down\n+                )\n+\n+                feature_pred_down = feature_pred_down.reshape(B, C, -1)\n+                rgb_affinity = torch.bmm(\n+                    feature_pred_down.permute(0, 2, 1), feature_pred_down\n+                )\n+\n+                feature_ditill_loss = (\n+                    feature_ditill_loss\n+                    + F.l1_loss(rgb_affinity, depth_affinity, reduction=\"mean\") / B\n+                )\n+\n+        else:\n+            raise NotImplementedError\n+\n+        return feature_ditill_loss\n+\n+\n+@LOSSES.register_module()\n+class FeatureLoss_Coefficient(nn.Module):\n+    def __init__(self, **kwargs):\n+        super().__init__()\n+\n+    def corrcoef(self, target, pred):\n+        pred_n = pred - pred.mean()\n+        target_n = target - target.mean()\n+        pred_n = pred_n / pred_n.norm()\n+        target_n = target_n / target_n.norm()\n+        return (pred_n * target_n).sum()\n+\n+    def forward(\n+        self,\n+        pred,\n+        target,\n+        regularization=\"l2\",\n+        regularization_strength=1.0,\n+    ):\n+        pred = [pred.clone()]\n+        target = [target.clone()]\n+        spearman_loss = 0.0\n+\n+        resize_shape = pred[-1].shape[-2:]  # save training time\n+\n+        if isinstance(pred, list):\n+            for i in range(len(pred)):\n+                feature_target = target[i]\n+                feature_pred = pred[i]\n+                B, C, H, W = feature_pred.shape\n+\n+                feature_pred_down = F.interpolate(\n+                    feature_pred, size=resize_shape, mode=\"bilinear\"\n+                )\n+                feature_target_down = F.interpolate(\n+                    feature_target, size=resize_shape, mode=\"bilinear\"\n+                )\n+\n+                # if feature_pred.size(-1) != feature_target.size(-1):\n+\n+                #     feature_pred_down = F.interpolate(\n+                #         feature_pred, size=resize_shape, mode=\"bilinear\"\n+                #     )\n+                #     feature_target_down = F.interpolate(\n+                #         feature_target, size=resize_shape, mode=\"bilinear\"\n+                #     )\n+                # else:\n+                #     feature_pred_down = feature_pred\n+                #     feature_target_down = feature_target\n+\n+                feature_pred_down = feature_pred_down.reshape(B, -1)\n+                feature_target_down = feature_target_down.reshape(B, -1)\n+\n+                feature_pred_down = torchsort.soft_rank(\n+                    feature_pred_down,\n+                    regularization=regularization,\n+                    regularization_strength=regularization_strength,\n+                )\n+                spearman_loss += 1 - self.corrcoef(\n+                    feature_target_down, feature_pred_down / feature_pred_down.shape[-1]\n+                )\n+\n+        return spearman_loss\n+\n+\n+@LOSSES.register_module()\n+class Radar_MSDistilll(nn.Module):\n+    def __init__(self, num_layers=2, each_layer_loss_cfg=[]):\n+        super().__init__()\n+        self.num_layers = num_layers\n+        assert num_layers == len(each_layer_loss_cfg)\n+        for idx, cfg_dict in enumerate(each_layer_loss_cfg):\n+            setattr(self, f\"layer_loss_{idx}\", build_loss(cfg_dict))\n+\n+    def forward(self, radar_ms_feats, pts_ms_feats):\n+        assert isinstance(radar_ms_feats, list)\n+        losses = 0.0\n+\n+        for idx in range(self.num_layers):\n+            losses += getattr(self, f\"layer_loss_{idx}\")(\n+                radar_ms_feats[idx], pts_ms_feats[idx]\n+            )\n+\n+        return losses / self.num_layers\n+\n+\n+@LOSSES.register_module()\n+class InfoMax(nn.Module):\n+    def __init__(self, **kwargs):\n+        super().__init__()\n+\n+    def forward(self, x1, x2):\n+        import pdb\n+\n+        pdb.set_trace()\n+        x1 = x1 / (torch.norm(x1, p=2, dim=1, keepdim=True) + 1e-10)\n+        x2 = x2 / (torch.norm(x2, p=2, dim=1, keepdim=True) + 1e-10)\n+        bs = x1.size(0)\n+        s = torch.matmul(x1, x2.permute(1, 0))\n+        mask_joint = torch.eye(bs).cuda()\n+        mask_marginal = 1 - mask_joint\n+\n+        Ej = (s * mask_joint).mean()\n+        Em = torch.exp(s * mask_marginal).mean()\n+        # decoupled comtrastive learning?!!!!\n+        # infomax_loss = - (Ej - torch.log(Em)) * self.alpha\n+        infomax_loss = -(Ej - torch.log(Em))  # / Em\n+        return infomax_loss\n+\n+\n+@LOSSES.register_module()\n+class HeatMapAug(nn.Module):\n+    def __init__(self):\n+        super().__init__()\n+\n+    @force_fp32(apply_to=(\"stu_pred\", \"tea_pred\"))\n+    def forward(self, stu_pred, tea_pred, fg_map):\n+\n+        num_task = len(stu_pred)\n+        kl_loss = 0\n+        for task_id in range(num_task):\n+            student_pred = stu_pred[task_id][0][\"heatmap\"].sigmoid()\n+            teacher_pred = tea_pred[task_id][0][\"heatmap\"].sigmoid()\n+            fg_map = fg_map.unsqueeze(1)\n+            task_kl_loss = F.binary_cross_entropy(student_pred, teacher_pred.detach())\n+            task_kl_loss = torch.sum(task_kl_loss * fg_map) / torch.sum(fg_map)\n+            kl_loss += task_kl_loss\n+\n+        return kl_loss\n+\n+\n+@LOSSES.register_module()\n+class Dc_ResultDistill(nn.Module):\n+    def __init__(\n+        self,\n+        pc_range=[],\n+        voxel_size=[],\n+        out_size_scale=8,\n+        ret_sum=False,\n+        loss_weight_reg=10,\n+        loss_weight_cls=10,\n+        max_cls=True,\n+    ):\n+        super().__init__()\n+\n+        self.pc_range = pc_range\n+        self.voxel_size = voxel_size\n+        self.out_size_scale = out_size_scale\n+        self.ret_sum = ret_sum\n+        self.loss_weight_reg = loss_weight_reg\n+        self.loss_weight_cls = loss_weight_cls\n+        self.max_cls = max_cls\n+\n+    def forward(self, resp_lidar, resp_fuse, gt_boxes):\n+        \"\"\"Dc_ResultDistill forward.\n+\n+        Args:\n+            resp_lidar (_type_):\n+            resp_fuse (_type_):\n+            gt_boxes (_type_):\n+\n+        Returns:\n+            _type_: _description_\n+        \"\"\"\n+\n+        tmp_resp_lidar = []\n+        tmp_resp_fuse = []\n+        for res_lidar, res_fuse in zip(resp_lidar, resp_fuse):\n+            tmp_resp_lidar.append(res_lidar[0])\n+            tmp_resp_fuse.append(res_fuse[0])\n+\n+        tmp_gt_boxes = []\n+        for bs_idx in range(len(gt_boxes)):\n+\n+            gt_bboxes_3d = torch.cat(\n+                (gt_boxes[bs_idx].gravity_center, gt_boxes[bs_idx].tensor[:, 3:]), dim=1\n+            )\n+\n+            tmp_gt_boxes.append(gt_bboxes_3d)\n+\n+        cls_lidar = []\n+        reg_lidar = []\n+        cls_fuse = []\n+        reg_fuse = []\n+\n+        # criterion = nn.L1Loss(reduce=False)\n+        # criterion_cls = QualityFocalLoss_()\n+\n+        criterion = nn.SmoothL1Loss(reduce=False)\n+        criterion_cls = nn.L1Loss(reduce=False)\n+\n+        for task_id, task_out in enumerate(tmp_resp_lidar):\n+            cls_lidar.append(task_out[\"heatmap\"])\n+            cls_fuse.append(_sigmoid(tmp_resp_fuse[task_id][\"heatmap\"] / 2))\n+            reg_lidar.append(\n+                torch.cat(\n+                    [\n+                        task_out[\"reg\"],\n+                        task_out[\"height\"],\n+                        task_out[\"dim\"],\n+                        task_out[\"rot\"],\n+                        task_out[\"vel\"],\n+                        # task_out[\"iou\"],\n+                    ],\n+                    dim=1,\n+                )\n+            )\n+            reg_fuse.append(\n+                torch.cat(\n+                    [\n+                        tmp_resp_fuse[task_id][\"reg\"],\n+                        tmp_resp_fuse[task_id][\"height\"],\n+                        tmp_resp_fuse[task_id][\"dim\"],\n+                        tmp_resp_fuse[task_id][\"rot\"],\n+                        tmp_resp_fuse[task_id][\"vel\"],\n+                        # resp_fuse[task_id][\"iou\"],\n+                    ],\n+                    dim=1,\n+                )\n+            )\n+        cls_lidar = torch.cat(cls_lidar, dim=1)\n+        reg_lidar = torch.cat(reg_lidar, dim=1)\n+        cls_fuse = torch.cat(cls_fuse, dim=1)\n+        reg_fuse = torch.cat(reg_fuse, dim=1)\n+\n+        if self.max_cls:\n+            cls_lidar_max, _ = torch.max(cls_lidar, dim=1)\n+            cls_fuse_max, _ = torch.max(cls_fuse, dim=1)\n+        else:\n+            _, _, ht_h, ht_w = cls_fuse.shape\n+            cls_lidar_max = cls_lidar.flatten(2).permute(0, 2, 1)\n+            cls_fuse_max = cls_fuse.flatten(2).permute(0, 2, 1)\n+\n+        gaussian_mask = calculate_box_mask_gaussian(\n+            reg_lidar.shape,\n+            tmp_gt_boxes,\n+            self.pc_range,\n+            self.voxel_size,\n+            self.out_size_scale,\n+        )\n+\n+        # # diff_reg = criterion(reg_lidar, reg_fuse)\n+        # # diff_cls = criterion(cls_lidar_max, cls_fuse_max)\n+        # diff_reg = criterion_reg(reg_lidar, reg_fuse)\n+\n+        # cls_lidar_max = torch.sigmoid(cls_lidar_max)\n+        # diff_cls = criterion_cls(\n+        #     cls_fuse_max, cls_lidar_max\n+        # )  # Compared with directly using the L1 loss constraint, replace it with bce focal loss and change the position?\n+\n+        # weight = gaussian_mask.sum()\n+        # weight = reduce_mean(weight)\n+\n+        # # diff_cls = criterion_cls(cls_lidar_max, cls_fuse_max)\n+        # diff_reg = torch.mean(diff_reg, dim=1)\n+        # diff_reg = diff_reg * gaussian_mask\n+        # loss_reg_distill = torch.sum(diff_reg) / (weight + 1e-4)\n+\n+        # if not self.max_cls:\n+        #     diff_cls = diff_cls\n+        #     loss_cls_distill = diff_cls.sum() * 2\n+        # else:\n+        #     diff_cls = diff_cls * gaussian_mask\n+        #     loss_cls_distill = torch.sum(diff_cls) / (weight + 1e-4)\n+\n+        diff_reg = criterion(reg_lidar, reg_fuse)\n+\n+        diff_cls = criterion_cls(cls_lidar_max, cls_fuse_max)\n+        diff_reg = torch.mean(diff_reg, dim=1)\n+        diff_reg = diff_reg * gaussian_mask\n+        diff_cls = diff_cls * gaussian_mask\n+        weight = gaussian_mask.sum()\n+        weight = reduce_mean(weight)\n+        loss_reg_distill = torch.sum(diff_reg) / (weight + 1e-4)\n+        loss_cls_distill = torch.sum(diff_cls) / (weight + 1e-4)\n+\n+        if self.ret_sum:\n+\n+            loss_det_distill = self.loss_weight * (loss_reg_distill + loss_cls_distill)\n+\n+            return loss_det_distill\n+        else:\n+\n+            return (\n+                self.loss_weight_reg * loss_reg_distill,\n+                self.loss_weight_cls * loss_cls_distill,\n+            )\n+\n+\n+@LOSSES.register_module()\n+class SelfLearningMFD(nn.Module):\n+    def __init__(\n+        self,\n+        bev_shape=[128, 128],\n+        pc_range=[-51.2, -51.2, -5, 51.2, 51.2, 3],\n+        voxel_size=[0.1, 0.1, 0.1],\n+        score_threshold=0.7,\n+        add_stu_decode_bboxes=False,\n+        loss_weight=1e-2,\n+        bbox_coder=None,\n+        student_channels=256,\n+        teacher_channels=512,\n+    ):\n+        super().__init__()\n+        self.bev_shape = bev_shape\n+        self.pc_range = pc_range\n+        self.voxel_size = voxel_size\n+        self.shape_resize_times = bbox_coder.out_size_factor if bbox_coder else 8\n+        self.score_threshold = score_threshold\n+        self.add_stu_decode_bboxes = add_stu_decode_bboxes\n+        self.loss_weight = loss_weight\n+\n+        self.align = nn.Sequential(\n+            nn.Conv2d(student_channels, teacher_channels, kernel_size=1, padding=0),\n+            # nn.BatchNorm2d(teacher_channels),\n+        )\n+\n+        if bbox_coder is not None:\n+            self.bbox_coder = build_bbox_coder(bbox_coder)\n+\n+    def pred2bboxes(self, preds_dict):\n+        \"\"\"SelfLearningMFD-pred2bboxes forward\n+\n+        Args:\n+            pred_bboxes (list):\n+            [task1[[task_head_reg,task_head_heatmap,...,]],\n+            task2...,\n+            taskn]\n+        \"\"\"\n+\n+        for task_id, pred_data in enumerate(preds_dict):\n+            bath_tmp = []\n+            batch_size = pred_data[0][\"heatmap\"].shape[0]\n+            batch_heatmap = pred_data[0][\"heatmap\"].sigmoid()\n+            batch_reg = pred_data[0][\"reg\"]\n+            batch_hei = pred_data[0][\"height\"]\n+\n+            # denormalization\n+            batch_dim = torch.exp(pred_data[0][\"dim\"])\n+\n+            batch_rot_sin = pred_data[0][\"rot\"][:, 0].unsqueeze(1)\n+            batch_rot_cos = pred_data[0][\"rot\"][:, 1].unsqueeze(1)\n+\n+            if \"vel\" in pred_data[0].keys():\n+                batch_vel = pred_data[0][\"vel\"]\n+\n+            bath_tmp.append(batch_heatmap)\n+            bath_tmp.append(batch_rot_sin)\n+            bath_tmp.append(batch_rot_cos)\n+            bath_tmp.append(batch_hei)\n+            bath_tmp.append(batch_dim)\n+            bath_tmp.append(batch_vel)\n+            bath_tmp.append(batch_reg)\n+\n+            bboxes_decode = self.bbox_coder.decode(*bath_tmp)\n+\n+        return bboxes_decode\n+\n+    def aug_boxes(self, gt_boxes, rot_mat):\n+        rot_lim = (-22.5, 22.5)\n+        scale_lim = (0.95, 1.05)\n+        rotate_angle = np.random.uniform(*rot_lim)\n+        scale_ratio = np.random.uniform(*scale_lim)\n+        gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n+        gt_boxes[:, 3:6] *= scale_ratio\n+        gt_boxes[:, 6] += rotate_angle\n+        gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n+        gt_boxes[:, 6] = -gt_boxes[:, 6]\n+        gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(-1)\n+        gt_boxes[:, :3] = gt_boxes[:, :3]\n+\n+        return gt_boxes\n+\n+    @force_fp32(\n+        apply_to=(\n+            \"student_bev_feat\",\n+            \"teacher_bev_feat\",\n+        )\n+    )\n+    def forward(\n+        self,\n+        student_bev_feat,\n+        teacher_bev_feat,\n+        gt_bboxes_list=None,\n+        masks_bboxes=None,\n+        bda_mat=None,\n+    ):\n+        \"\"\"SelfLearningMFD forward.\n+\n+        Args:\n+            student_bev_feats (torch.tensor): Calculate student feats\n+            teacher_bev_feats (torch.tensor): Calculate teacher feats\n+            masks_bboxes (list): Self-learning mask detection\n+            gt_bboxes_list (list): [LiDARInstance3DBoxes(gravity_center+tensor(num_objs,9))]\n+\n+        Returns:\n+            dict: _description_\n+        \"\"\"\n+\n+        bs = student_bev_feat.size(0)\n+\n+        if student_bev_feat.size(1) != teacher_bev_feat.size(1):\n+            student_bev_feat = self.align(student_bev_feat)\n+\n+        if masks_bboxes is not None and self.add_stu_decode_bboxes:\n+            student_pred_bboxes_list = self.pred2bboxes(\n+                masks_bboxes\n+            )  # list of task : each shape of [(num_bboxes,9)]\n+\n+            if bda_mat is not None:\n+                student_pred_bboxes_list = self.aug_boxes(student_pred_bboxes_list)\n+\n+        bev_feat_shape = torch.tensor(self.bev_shape)\n+        voxel_size = torch.tensor(self.voxel_size)\n+        feature_map_size = bev_feat_shape[:2]\n+\n+        if gt_bboxes_list is not None:\n+            device = student_bev_feat.device\n+\n+            gt_bboxes_list = [\n+                torch.cat(\n+                    (gt_bboxes.gravity_center, gt_bboxes.tensor[:, 3:]), dim=1\n+                ).to(device)\n+                for gt_bboxes in gt_bboxes_list\n+            ]\n+\n+            if self.add_stu_decode_bboxes:\n+                for idx, data in enumerate(student_pred_bboxes_list):\n+\n+                    bboxes_dim = data[\"bboxes\"].size(1)\n+\n+                    scroes_mask = data[\"scores\"] > self.score_threshold\n+\n+                    new_select_by_mask = data[\"bboxes\"][scroes_mask, :]\n+\n+                    gt_bboxes_list[idx] = torch.cat(\n+                        [gt_bboxes_list[idx], new_select_by_mask], dim=0\n+                    )\n+\n+            fg_map = student_bev_feat.new_zeros(\n+                (len(gt_bboxes_list), feature_map_size[1], feature_map_size[0])\n+            )\n+\n+            for idx in range(len(gt_bboxes_list)):\n+                num_objs = gt_bboxes_list[idx].shape[0]\n+\n+                for k in range(num_objs):\n+                    width = gt_bboxes_list[idx][k][3]\n+                    length = gt_bboxes_list[idx][k][4]\n+                    width = width / voxel_size[0] / self.shape_resize_times\n+                    length = length / voxel_size[1] / self.shape_resize_times\n+\n+                    if width > 0 and length > 0:\n+                        radius = gaussian_radius((length, width), min_overlap=0.1)\n+                        radius = max(1, int(radius))\n+\n+                        # be really careful for the coordinate system of\n+                        # your box annotation.\n+                        x, y, z = (\n+                            gt_bboxes_list[idx][k][0],\n+                            gt_bboxes_list[idx][k][1],\n+                            gt_bboxes_list[idx][k][2],\n+                        )\n+\n+                        coor_x = (\n+                            (x - self.pc_range[0])\n+                            / voxel_size[0]\n+                            / self.shape_resize_times\n+                        )\n+                        coor_y = (\n+                            (y - self.pc_range[1])\n+                            / voxel_size[1]\n+                            / self.shape_resize_times\n+                        )\n+\n+                        center = torch.tensor(\n+                            [coor_x, coor_y], dtype=torch.float32, device=device\n+                        )\n+                        center_int = center.to(torch.int32)\n+\n+                        # throw out not in range objects to avoid out of array\n+                        # area when creating the heatmap\n+                        if not (\n+                            0 <= center_int[0] < feature_map_size[0]\n+                            and 0 <= center_int[1] < feature_map_size[1]\n+                        ):\n+                            continue\n+\n+                        draw_heatmap_gaussian(fg_map[idx], center_int, radius)\n+\n+        if fg_map is None:\n+            fg_map = student_bev_feat.new_ones(\n+                (student_bev_feat.shape[0], feature_map_size[1], feature_map_size[0])\n+            )\n+\n+        if bs > 1:\n+            fg_map = fg_map.unsqueeze(1)\n+\n+        fit_loss = F.mse_loss(student_bev_feat, teacher_bev_feat, reduction=\"none\")\n+        fit_loss = torch.sum(fit_loss * fg_map) / torch.sum(fg_map)\n+\n+        return fit_loss * self.loss_weight, fg_map\n+\n+    # def forward(\n+    #     self, student_bev_feats, teacher_bev_feats, masks_bboxes, gt_bboxes_list\n+    # ):\n+    #     \"\"\"SelfLearningMFD forward.\n+\n+    #     Args:\n+    #         student_bev_feats (torch.tensor): _description_\n+    #         teacher_bev_feats (torch.tensor): _description_\n+    #         masks_bboxes (): _description_\n+    #         gt_bboxes_list (_type_): _description_\n+    #     \"\"\"\n+    #     assert isinstance(masks_bboxes, list)\n+    #     ret_dict = multi_apply(\n+    #         self._forward,\n+    #         student_bev_feats,\n+    #         teacher_bev_feats,\n+    #         masks_bboxes,\n+    #         gt_bboxes_list,\n+    #     )\n"
                },
                {
                    "date": 1716031371264,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1008,1015 +1008,4 @@\n     #         teacher_bev_feats,\n     #         masks_bboxes,\n     #         gt_bboxes_list,\n     #     )\n-import torch.nn as nn\n-import torch.nn.functional as F\n-import torch\n-from ..builder import LOSSES, build_loss\n-from ..utils.gaussian_utils import calculate_box_mask_gaussian\n-from torch import distributed as dist\n-from mmdet.core import multi_apply, build_bbox_coder\n-from mmdet3d.core import draw_heatmap_gaussian, gaussian_radius\n-from mmdet3d.core.bbox.coders.centerpoint_bbox_coders import (\n-    CenterPointBBoxCoder as bboxcoder,\n-)\n-from mmdet.models.losses import QualityFocalLoss\n-from mmcv.runner import force_fp32\n-import torchsort\n-import torchvision\n-import numpy as np\n-\n-\n-def get_world_size() -> int:\n-    if not dist.is_available():\n-        return 1\n-    if not dist.is_initialized():\n-        return 1\n-\n-    return dist.get_world_size()\n-\n-\n-def reduce_sum(tensor):\n-    world_size = get_world_size()\n-    if world_size < 2:\n-        return tensor\n-    tensor = tensor.clone()\n-    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n-    return tensor\n-\n-\n-def reduce_mean(tensor):\n-    return reduce_sum(tensor) / float(get_world_size())\n-\n-\n-def _sigmoid(x):\n-    # y = torch.clamp(x.sigmoid(), min=1e-3, max=1 - 1e-3)\n-    y = x.sigmoid()\n-    return y\n-\n-\n-def off_diagonal(x):\n-    # return a flattened view of the off-diagonal elements of a square matrix\n-    n, m = x.shape\n-    assert n == m\n-    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n-\n-\n-def response_list_wrap(response_lists: list):\n-    \"\"\"response_list_wrap\n-\n-    Args:\n-        response_lists (list): Pack list content\n-\n-    Returns:\n-        list: Return the new list of packaged integration\n-    \"\"\"\n-    assert len(response_lists) == 1\n-\n-    tmp_resp = []\n-    for response in response_lists:\n-        tmp_resp.append(response[0])\n-\n-    return tmp_resp\n-\n-\n-@LOSSES.register_module()\n-class QualityFocalLoss_(nn.Module):\n-    \"\"\"\n-    input[B,M,C] not sigmoid\n-    target[B,M,C], sigmoid\n-    \"\"\"\n-\n-    def __init__(self, beta=2.0):\n-\n-        super(QualityFocalLoss_, self).__init__()\n-        self.beta = beta\n-\n-    def forward(\n-        self,\n-        input: torch.Tensor,\n-        target: torch.Tensor,\n-        pos_normalizer=torch.tensor(1.0),\n-    ):\n-\n-        pred_sigmoid = torch.sigmoid(input)\n-        scale_factor = pred_sigmoid - target\n-\n-        # pred_sigmoid = torch.sigmoid(input)\n-        # scale_factor = input - target\n-        loss = F.binary_cross_entropy_with_logits(input, target, reduction=\"none\") * (\n-            scale_factor.abs().pow(self.beta)\n-        )\n-        loss /= torch.clamp(pos_normalizer, min=1.0)\n-        return loss\n-\n-\n-@LOSSES.register_module()\n-class SimpleL1(nn.Module):\n-    def __init__(self, criterion=\"L1\", student_ch=256, teacher_ch=512):\n-        super().__init__()\n-        self.criterion = criterion\n-        if criterion == \"L1\":\n-            self.criterion_loss = nn.L1Loss(reduce=False)\n-        elif criterion == \"SmoothL1\":\n-            self.criterion_loss = nn.SmoothL1Loss(reduce=False)\n-        elif criterion == \"MSE\":\n-            self.criterion_loss = nn.MSELoss(reduction=\"none\")\n-\n-        if student_ch != teacher_ch:\n-            self.align = nn.Conv2d(student_ch, teacher_ch, kernel_size=1)\n-\n-    def forward(self, feats1, feats2, *args, **kwargs):\n-        if self.criterion == \"MSE\":\n-            feats1 = self.align(feats1) if getattr(self, \"align\", None) else feats1\n-            losses = self.criterion_loss(feats1, feats2).mean()\n-            return losses\n-        else:\n-            return self.criterion_loss(feats1, feats2)\n-\n-\n-@LOSSES.register_module()\n-class Relevance_Distillation(nn.Module):\n-    def __init__(self, bs=2, bn_dim=512, lambd=0.0051):\n-        super().__init__()\n-        self.bs = bs\n-        self.bn_dim = bn_dim\n-        self.lambd = lambd\n-\n-        # normalization layer for the representations z1 and z2\n-        self.bn = nn.BatchNorm1d(self.bn_dim, affine=False)\n-\n-        self.align = nn.Conv2d(256, 512, kernel_size=1)\n-\n-    def forward(self, student_bev, teacher_bev, *args, **kwargs):\n-        student_bev = self.align(student_bev)\n-\n-        student_bev = student_bev.flatten(2)\n-        teacher_bev = teacher_bev.flatten(2)\n-\n-        # empirical cross-correlation matrix\n-        c = self.bn(student_bev).flatten(1).T @ self.bn(teacher_bev).flatten(1)\n-\n-        # sum the cross-correlation matrix between all gpus\n-        c.div_(self.bs)\n-        if self.bs == 1:\n-            pass\n-        else:\n-            torch.distributed.all_reduce(c)\n-\n-        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n-        off_diag = off_diagonal(c).pow_(2).sum()\n-        loss = on_diag + self.lambd * off_diag\n-        return loss\n-\n-\n-@LOSSES.register_module()\n-class FeatureLoss(nn.Module):\n-    \"\"\"PyTorch version of `Masked Generative Distillation`\n-\n-    Args:\n-        student_channels(int): Number of channels in the student's feature map.\n-        teacher_channels(int): Number of channels in the teacher's feature map.\n-        name (str): the loss name of the layer\n-        alpha_mgd (float, optional): Weight of dis_loss. Defaults to 0.00002\n-        lambda_mgd (float, optional): masked ratio. Defaults to 0.65\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        student_channels,\n-        teacher_channels,\n-        name,\n-        alpha_mgd=0.00002,\n-        lambda_mgd=0.65,\n-    ):\n-        super(FeatureLoss, self).__init__()\n-        self.alpha_mgd = alpha_mgd\n-        self.lambda_mgd = lambda_mgd\n-        self.name = name\n-\n-        if student_channels != teacher_channels:\n-            self.align = nn.Conv2d(\n-                student_channels, teacher_channels, kernel_size=1, stride=1, padding=0\n-            )\n-        else:\n-            self.align = None\n-\n-        self.generation = nn.Sequential(\n-            nn.Conv2d(teacher_channels, teacher_channels, kernel_size=3, padding=1),\n-            nn.ReLU(inplace=True),\n-            nn.Conv2d(teacher_channels, teacher_channels, kernel_size=3, padding=1),\n-        )\n-\n-    def forward(self, preds_S, preds_T, **kwargs):\n-        \"\"\"Forward function.\n-        Args:\n-            preds_S(Tensor): Bs*C*H*W, student's feature map\n-            preds_T(Tensor): Bs*C*H*W, teacher's feature map\n-        \"\"\"\n-        assert preds_S.shape[-2:] == preds_T.shape[-2:]\n-\n-        if self.align is not None:\n-            preds_S = self.align(preds_S)\n-\n-        loss = self.get_dis_loss(preds_S, preds_T) * self.alpha_mgd\n-\n-        return loss\n-\n-    def get_dis_loss(self, preds_S, preds_T):\n-        loss_mse = nn.MSELoss(reduction=\"sum\")\n-        N, C, H, W = preds_T.shape\n-\n-        device = preds_S.device\n-        mat = torch.rand((N, 1, H, W)).to(device)\n-        mat = torch.where(mat > 1 - self.lambda_mgd, 0, 1).to(device)\n-\n-        masked_fea = torch.mul(preds_S, mat)\n-        new_fea = self.generation(masked_fea)\n-\n-        dis_loss = loss_mse(new_fea, preds_T) / N\n-\n-        return dis_loss\n-\n-\n-@LOSSES.register_module()\n-class FeatureLoss_InnerClip(nn.Module):\n-    def __init__(\n-        self,\n-        x_sample_num=24,\n-        y_sample_num=24,\n-        inter_keypoint_weight=1,\n-        inter_channel_weight=10,\n-        enlarge_width=1.6,\n-        embed_channels=[256, 512],\n-        inner_feats_distill=None,\n-    ):\n-        super().__init__()\n-        self.x_sample_num = x_sample_num\n-        self.y_sample_num = y_sample_num\n-        self.inter_keypoint_weight = inter_keypoint_weight\n-        self.inter_channel_weight = inter_channel_weight\n-        self.enlarge_width = enlarge_width\n-\n-        self.img_view_transformer = None\n-\n-        self.embed_channels = embed_channels\n-\n-        self.imgbev_embed = nn.Sequential(\n-            nn.Conv2d(\n-                embed_channels[0],\n-                embed_channels[1],\n-                kernel_size=1,\n-                stride=1,\n-                padding=0,\n-                bias=False,\n-            ),\n-            nn.BatchNorm2d(embed_channels[1]),\n-        )\n-\n-        self.inner_feats_loss = (\n-            build_loss(inner_feats_distill) if inner_feats_distill is not None else None\n-        )\n-\n-    def get_gt_sample_grid(self, corner_points2d):\n-        dH_x, dH_y = corner_points2d[0] - corner_points2d[1]\n-        dW_x, dW_y = corner_points2d[0] - corner_points2d[2]\n-        raw_grid_x = (\n-            torch.linspace(\n-                corner_points2d[0][0], corner_points2d[1][0], self.x_sample_num\n-            )\n-            .view(1, -1)\n-            .repeat(self.y_sample_num, 1)\n-        )\n-        raw_grid_y = (\n-            torch.linspace(\n-                corner_points2d[0][1], corner_points2d[2][1], self.y_sample_num\n-            )\n-            .view(-1, 1)\n-            .repeat(1, self.x_sample_num)\n-        )\n-        raw_grid = torch.cat((raw_grid_x.unsqueeze(2), raw_grid_y.unsqueeze(2)), dim=2)\n-        raw_grid_x_offset = (\n-            torch.linspace(0, -dW_x, self.x_sample_num)\n-            .view(-1, 1)\n-            .repeat(1, self.y_sample_num)\n-        )\n-        raw_grid_y_offset = (\n-            torch.linspace(0, -dH_y, self.y_sample_num)\n-            .view(1, -1)\n-            .repeat(self.x_sample_num, 1)\n-        )\n-        raw_grid_offset = torch.cat(\n-            (raw_grid_x_offset.unsqueeze(2), raw_grid_y_offset.unsqueeze(2)), dim=2\n-        )\n-        grid = raw_grid + raw_grid_offset  # X_sample,Y_sample,2\n-        grid[:, :, 0] = torch.clip(\n-            (\n-                (\n-                    grid[:, :, 0]\n-                    - (\n-                        self.img_view_transformer[\"bx\"][0].to(grid.device)\n-                        - self.img_view_transformer[\"dx\"][0].to(grid.device) / 2.0\n-                    )\n-                )\n-                / self.img_view_transformer[\"dx\"][0].to(grid.device)\n-                / (self.img_view_transformer[\"nx\"][0].to(grid.device) - 1)\n-            )\n-            * 2.0\n-            - 1.0,\n-            min=-1.0,\n-            max=1.0,\n-        )\n-        grid[:, :, 1] = torch.clip(\n-            (\n-                (\n-                    grid[:, :, 1]\n-                    - (\n-                        self.img_view_transformer[\"bx\"][1].to(grid.device)\n-                        - self.img_view_transformer[\"dx\"][1].to(grid.device) / 2.0\n-                    )\n-                )\n-                / self.img_view_transformer[\"dx\"][1].to(grid.device)\n-                / (self.img_view_transformer[\"nx\"][1].to(grid.device) - 1)\n-            )\n-            * 2.0\n-            - 1.0,\n-            min=-1.0,\n-            max=1.0,\n-        )\n-\n-        return grid.unsqueeze(0)\n-\n-    def get_inner_feat(self, gt_bboxes_3d, img_feats, pts_feats):\n-        \"\"\"Use grid to sample features of key points\"\"\"\n-        device = img_feats.device\n-        dtype = img_feats[0].dtype\n-\n-        img_feats_sampled_list = []\n-        pts_feats_sampled_list = []\n-\n-        for sample_ind in torch.arange(len(gt_bboxes_3d)):\n-            img_feat = img_feats[sample_ind].unsqueeze(0)  # 1,C,H,W\n-            pts_feat = pts_feats[sample_ind].unsqueeze(0)  # 1,C,H,W\n-\n-            bbox_num, corner_num, point_num = gt_bboxes_3d[sample_ind].corners.shape\n-\n-            for bbox_ind in torch.arange(bbox_num):\n-                if self.enlarge_width > 0:\n-                    gt_sample_grid = self.get_gt_sample_grid(\n-                        gt_bboxes_3d[sample_ind]\n-                        .enlarged_box(self.enlarge_width)\n-                        .corners[bbox_ind][[0, 2, 4, 6], :-1]\n-                    ).to(device)\n-                else:\n-                    gt_sample_grid = self.get_gt_sample_grid(\n-                        gt_bboxes_3d[sample_ind].corners[bbox_ind][[0, 2, 4, 6], :-1]\n-                    ).to(\n-                        device\n-                    )  # 1,sample_y,sample_x,2\n-\n-                img_feats_sampled_list.append(\n-                    F.grid_sample(\n-                        img_feat,\n-                        grid=gt_sample_grid,\n-                        align_corners=False,\n-                        mode=\"bilinear\",\n-                    )\n-                )  # 'bilinear')) #all_bbox_num,C,y_sample,x_sample\n-                pts_feats_sampled_list.append(\n-                    F.grid_sample(\n-                        pts_feat,\n-                        grid=gt_sample_grid,\n-                        align_corners=False,\n-                        mode=\"bilinear\",\n-                    )\n-                )  # 'bilinear')) #all_bbox_num,C,y_sample,x_sample\n-\n-        return torch.cat(img_feats_sampled_list, dim=0), torch.cat(\n-            pts_feats_sampled_list, dim=0\n-        )\n-\n-    @force_fp32(apply_to=(\"img_feats_kd\", \"pts_feats_kd\"))\n-    def get_inter_channel_loss(self, img_feats_kd, pts_feats_kd):\n-        \"\"\"Calculate the inter-channel similarities, guide the student keypoint features to mimic the channel-wise relationships of the teacher`s\"\"\"\n-\n-        C_img = img_feats_kd.shape[1]\n-        C_pts = pts_feats_kd.shape[1]\n-        N = self.x_sample_num * self.y_sample_num\n-\n-        img_feats_kd = img_feats_kd.view(-1, C_img, N).matmul(\n-            img_feats_kd.view(-1, C_img, N).permute(0, 2, 1)\n-        )  # -1,N,N\n-        pts_feats_kd = pts_feats_kd.view(-1, C_pts, N).matmul(\n-            pts_feats_kd.view(-1, C_pts, N).permute(0, 2, 1)\n-        )\n-\n-        img_feats_kd = F.normalize(img_feats_kd, dim=2)\n-        pts_feats_kd = F.normalize(pts_feats_kd, dim=2)\n-\n-        loss_inter_channel = F.mse_loss(img_feats_kd, pts_feats_kd, reduction=\"none\")\n-        loss_inter_channel = loss_inter_channel.sum(-1)\n-        loss_inter_channel = loss_inter_channel.mean()\n-        loss_inter_channel = self.inter_channel_weight * loss_inter_channel\n-        return loss_inter_channel\n-\n-    def forward(self, student_feats, teacher_feats, gt_bboxes_list, **kwargs):\n-\n-        self.img_view_transformer = kwargs.get(\"ivt_cfg\")\n-\n-        if student_feats.size(1) != teacher_feats.size(1):\n-            student_feats = self.imgbev_embed(student_feats)\n-\n-        if self.inter_keypoint_weight > 0 or self.inter_channel_weight > 0:\n-            img_feats_kd, pts_feats_kd = self.get_inner_feat(\n-                gt_bboxes_list, student_feats, teacher_feats\n-            )\n-\n-        if self.inner_feats_loss:\n-            return self.inner_feats_loss(img_feats_kd, pts_feats_kd)\n-\n-        # if self.inter_keypoint_weight > 0:\n-        #     loss_inter_keypoint = self.get_inter_keypoint_loss(\n-        #         img_feats_kd, pts_feats_kd\n-        #     )\n-        #     losses.update({\"loss_inter_keypoint_img_bev\": loss_inter_keypoint})\n-\n-        if self.inter_channel_weight > 0:\n-            loss_inter_channel = self.get_inter_channel_loss(img_feats_kd, pts_feats_kd)\n-\n-        return loss_inter_channel\n-\n-\n-@LOSSES.register_module()\n-class FeatureLoss_Affinity(nn.Module):\n-    def __init__(self):\n-        super().__init__()\n-\n-    def forward(self, student_feats, teacher_feats, *args, **kwargs):\n-        student_feats = [student_feats]\n-        teacher_feats = [teacher_feats]\n-\n-        feature_ditill_loss = 0.0\n-\n-        resize_shape = student_feats[-1].shape[-2:]\n-        if isinstance(student_feats, list):\n-            for i in range(len(student_feats)):\n-                feature_target = teacher_feats[i].detach()\n-                feature_pred = student_feats[i]\n-\n-                B, C, H, W = student_feats[-1].shape\n-\n-                if student_feats[-1].size(-1) != teacher_feats[-1].size(-1):\n-                    feature_pred_down = F.interpolate(\n-                        feature_pred, size=resize_shape, mode=\"bilinear\"\n-                    )\n-                    feature_target_down = F.interpolate(\n-                        feature_target, size=resize_shape, mode=\"bilinear\"\n-                    )\n-                else:\n-                    feature_pred_down = feature_pred\n-                    feature_target_down = feature_target\n-\n-                feature_target_down = feature_target_down.reshape(B, C, -1)\n-                depth_affinity = torch.bmm(\n-                    feature_target_down.permute(0, 2, 1), feature_target_down\n-                )\n-\n-                feature_pred_down = feature_pred_down.reshape(B, C, -1)\n-                rgb_affinity = torch.bmm(\n-                    feature_pred_down.permute(0, 2, 1), feature_pred_down\n-                )\n-\n-                feature_ditill_loss = (\n-                    feature_ditill_loss\n-                    + F.l1_loss(rgb_affinity, depth_affinity, reduction=\"mean\") / B\n-                )\n-\n-        else:\n-            raise NotImplementedError\n-\n-        return feature_ditill_loss\n-\n-\n-@LOSSES.register_module()\n-class FeatureLoss_Coefficient(nn.Module):\n-    def __init__(self, **kwargs):\n-        super().__init__()\n-\n-    def corrcoef(self, target, pred):\n-        pred_n = pred - pred.mean()\n-        target_n = target - target.mean()\n-        pred_n = pred_n / pred_n.norm()\n-        target_n = target_n / target_n.norm()\n-        return (pred_n * target_n).sum()\n-\n-    def forward(\n-        self,\n-        pred,\n-        target,\n-        regularization=\"l2\",\n-        regularization_strength=1.0,\n-    ):\n-        pred = [pred.clone()]\n-        target = [target.clone()]\n-        spearman_loss = 0.0\n-\n-        resize_shape = pred[-1].shape[-2:]  # save training time\n-\n-        if isinstance(pred, list):\n-            for i in range(len(pred)):\n-                feature_target = target[i]\n-                feature_pred = pred[i]\n-                B, C, H, W = feature_pred.shape\n-\n-                feature_pred_down = F.interpolate(\n-                    feature_pred, size=resize_shape, mode=\"bilinear\"\n-                )\n-                feature_target_down = F.interpolate(\n-                    feature_target, size=resize_shape, mode=\"bilinear\"\n-                )\n-\n-                # if feature_pred.size(-1) != feature_target.size(-1):\n-\n-                #     feature_pred_down = F.interpolate(\n-                #         feature_pred, size=resize_shape, mode=\"bilinear\"\n-                #     )\n-                #     feature_target_down = F.interpolate(\n-                #         feature_target, size=resize_shape, mode=\"bilinear\"\n-                #     )\n-                # else:\n-                #     feature_pred_down = feature_pred\n-                #     feature_target_down = feature_target\n-\n-                feature_pred_down = feature_pred_down.reshape(B, -1)\n-                feature_target_down = feature_target_down.reshape(B, -1)\n-\n-                feature_pred_down = torchsort.soft_rank(\n-                    feature_pred_down,\n-                    regularization=regularization,\n-                    regularization_strength=regularization_strength,\n-                )\n-                spearman_loss += 1 - self.corrcoef(\n-                    feature_target_down, feature_pred_down / feature_pred_down.shape[-1]\n-                )\n-\n-        return spearman_loss\n-\n-\n-@LOSSES.register_module()\n-class Radar_MSDistilll(nn.Module):\n-    def __init__(self, num_layers=2, each_layer_loss_cfg=[]):\n-        super().__init__()\n-        self.num_layers = num_layers\n-        assert num_layers == len(each_layer_loss_cfg)\n-        for idx, cfg_dict in enumerate(each_layer_loss_cfg):\n-            setattr(self, f\"layer_loss_{idx}\", build_loss(cfg_dict))\n-\n-    def forward(self, radar_ms_feats, pts_ms_feats):\n-        assert isinstance(radar_ms_feats, list)\n-        losses = 0.0\n-\n-        for idx in range(self.num_layers):\n-            losses += getattr(self, f\"layer_loss_{idx}\")(\n-                radar_ms_feats[idx], pts_ms_feats[idx]\n-            )\n-\n-        return losses / self.num_layers\n-\n-\n-@LOSSES.register_module()\n-class InfoMax(nn.Module):\n-    def __init__(self, **kwargs):\n-        super().__init__()\n-\n-    def forward(self, x1, x2):\n-        import pdb\n-\n-        pdb.set_trace()\n-        x1 = x1 / (torch.norm(x1, p=2, dim=1, keepdim=True) + 1e-10)\n-        x2 = x2 / (torch.norm(x2, p=2, dim=1, keepdim=True) + 1e-10)\n-        bs = x1.size(0)\n-        s = torch.matmul(x1, x2.permute(1, 0))\n-        mask_joint = torch.eye(bs).cuda()\n-        mask_marginal = 1 - mask_joint\n-\n-        Ej = (s * mask_joint).mean()\n-        Em = torch.exp(s * mask_marginal).mean()\n-        # decoupled comtrastive learning?!!!!\n-        # infomax_loss = - (Ej - torch.log(Em)) * self.alpha\n-        infomax_loss = -(Ej - torch.log(Em))  # / Em\n-        return infomax_loss\n-\n-\n-@LOSSES.register_module()\n-class HeatMapAug(nn.Module):\n-    def __init__(self):\n-        super().__init__()\n-\n-    @force_fp32(apply_to=(\"stu_pred\", \"tea_pred\"))\n-    def forward(self, stu_pred, tea_pred, fg_map):\n-\n-        num_task = len(stu_pred)\n-        kl_loss = 0\n-        for task_id in range(num_task):\n-            student_pred = stu_pred[task_id][0][\"heatmap\"].sigmoid()\n-            teacher_pred = tea_pred[task_id][0][\"heatmap\"].sigmoid()\n-            fg_map = fg_map.unsqueeze(1)\n-            task_kl_loss = F.binary_cross_entropy(student_pred, teacher_pred.detach())\n-            task_kl_loss = torch.sum(task_kl_loss * fg_map) / torch.sum(fg_map)\n-            kl_loss += task_kl_loss\n-\n-        return kl_loss\n-\n-\n-@LOSSES.register_module()\n-class Dc_ResultDistill(nn.Module):\n-    def __init__(\n-        self,\n-        pc_range=[],\n-        voxel_size=[],\n-        out_size_scale=8,\n-        ret_sum=False,\n-        loss_weight_reg=10,\n-        loss_weight_cls=10,\n-        max_cls=True,\n-    ):\n-        super().__init__()\n-\n-        self.pc_range = pc_range\n-        self.voxel_size = voxel_size\n-        self.out_size_scale = out_size_scale\n-        self.ret_sum = ret_sum\n-        self.loss_weight_reg = loss_weight_reg\n-        self.loss_weight_cls = loss_weight_cls\n-        self.max_cls = max_cls\n-\n-    def forward(self, resp_lidar, resp_fuse, gt_boxes):\n-        \"\"\"Dc_ResultDistill forward.\n-\n-        Args:\n-            resp_lidar (_type_):\n-            resp_fuse (_type_):\n-            gt_boxes (_type_):\n-\n-        Returns:\n-            _type_: _description_\n-        \"\"\"\n-\n-        tmp_resp_lidar = []\n-        tmp_resp_fuse = []\n-        for res_lidar, res_fuse in zip(resp_lidar, resp_fuse):\n-            tmp_resp_lidar.append(res_lidar[0])\n-            tmp_resp_fuse.append(res_fuse[0])\n-\n-        tmp_gt_boxes = []\n-        for bs_idx in range(len(gt_boxes)):\n-\n-            gt_bboxes_3d = torch.cat(\n-                (gt_boxes[bs_idx].gravity_center, gt_boxes[bs_idx].tensor[:, 3:]), dim=1\n-            )\n-\n-            tmp_gt_boxes.append(gt_bboxes_3d)\n-\n-        cls_lidar = []\n-        reg_lidar = []\n-        cls_fuse = []\n-        reg_fuse = []\n-\n-        # criterion = nn.L1Loss(reduce=False)\n-        # criterion_cls = QualityFocalLoss_()\n-\n-        criterion = nn.SmoothL1Loss(reduce=False)\n-        criterion_cls = nn.L1Loss(reduce=False)\n-\n-        for task_id, task_out in enumerate(tmp_resp_lidar):\n-            cls_lidar.append(task_out[\"heatmap\"])\n-            cls_fuse.append(_sigmoid(tmp_resp_fuse[task_id][\"heatmap\"] / 2))\n-            reg_lidar.append(\n-                torch.cat(\n-                    [\n-                        task_out[\"reg\"],\n-                        task_out[\"height\"],\n-                        task_out[\"dim\"],\n-                        task_out[\"rot\"],\n-                        task_out[\"vel\"],\n-                        # task_out[\"iou\"],\n-                    ],\n-                    dim=1,\n-                )\n-            )\n-            reg_fuse.append(\n-                torch.cat(\n-                    [\n-                        tmp_resp_fuse[task_id][\"reg\"],\n-                        tmp_resp_fuse[task_id][\"height\"],\n-                        tmp_resp_fuse[task_id][\"dim\"],\n-                        tmp_resp_fuse[task_id][\"rot\"],\n-                        tmp_resp_fuse[task_id][\"vel\"],\n-                        # resp_fuse[task_id][\"iou\"],\n-                    ],\n-                    dim=1,\n-                )\n-            )\n-        cls_lidar = torch.cat(cls_lidar, dim=1)\n-        reg_lidar = torch.cat(reg_lidar, dim=1)\n-        cls_fuse = torch.cat(cls_fuse, dim=1)\n-        reg_fuse = torch.cat(reg_fuse, dim=1)\n-\n-        if self.max_cls:\n-            cls_lidar_max, _ = torch.max(cls_lidar, dim=1)\n-            cls_fuse_max, _ = torch.max(cls_fuse, dim=1)\n-        else:\n-            _, _, ht_h, ht_w = cls_fuse.shape\n-            cls_lidar_max = cls_lidar.flatten(2).permute(0, 2, 1)\n-            cls_fuse_max = cls_fuse.flatten(2).permute(0, 2, 1)\n-\n-        gaussian_mask = calculate_box_mask_gaussian(\n-            reg_lidar.shape,\n-            tmp_gt_boxes,\n-            self.pc_range,\n-            self.voxel_size,\n-            self.out_size_scale,\n-        )\n-\n-        # # diff_reg = criterion(reg_lidar, reg_fuse)\n-        # # diff_cls = criterion(cls_lidar_max, cls_fuse_max)\n-        # diff_reg = criterion_reg(reg_lidar, reg_fuse)\n-\n-        # cls_lidar_max = torch.sigmoid(cls_lidar_max)\n-        # diff_cls = criterion_cls(\n-        #     cls_fuse_max, cls_lidar_max\n-        # )  # Compared with directly using the L1 loss constraint, replace it with bce focal loss and change the position?\n-\n-        # weight = gaussian_mask.sum()\n-        # weight = reduce_mean(weight)\n-\n-        # # diff_cls = criterion_cls(cls_lidar_max, cls_fuse_max)\n-        # diff_reg = torch.mean(diff_reg, dim=1)\n-        # diff_reg = diff_reg * gaussian_mask\n-        # loss_reg_distill = torch.sum(diff_reg) / (weight + 1e-4)\n-\n-        # if not self.max_cls:\n-        #     diff_cls = diff_cls\n-        #     loss_cls_distill = diff_cls.sum() * 2\n-        # else:\n-        #     diff_cls = diff_cls * gaussian_mask\n-        #     loss_cls_distill = torch.sum(diff_cls) / (weight + 1e-4)\n-\n-        diff_reg = criterion(reg_lidar, reg_fuse)\n-\n-        diff_cls = criterion_cls(cls_lidar_max, cls_fuse_max)\n-        diff_reg = torch.mean(diff_reg, dim=1)\n-        diff_reg = diff_reg * gaussian_mask\n-        diff_cls = diff_cls * gaussian_mask\n-        weight = gaussian_mask.sum()\n-        weight = reduce_mean(weight)\n-        loss_reg_distill = torch.sum(diff_reg) / (weight + 1e-4)\n-        loss_cls_distill = torch.sum(diff_cls) / (weight + 1e-4)\n-\n-        if self.ret_sum:\n-\n-            loss_det_distill = self.loss_weight * (loss_reg_distill + loss_cls_distill)\n-\n-            return loss_det_distill\n-        else:\n-\n-            return (\n-                self.loss_weight_reg * loss_reg_distill,\n-                self.loss_weight_cls * loss_cls_distill,\n-            )\n-\n-\n-@LOSSES.register_module()\n-class SelfLearningMFD(nn.Module):\n-    def __init__(\n-        self,\n-        bev_shape=[128, 128],\n-        pc_range=[-51.2, -51.2, -5, 51.2, 51.2, 3],\n-        voxel_size=[0.1, 0.1, 0.1],\n-        score_threshold=0.7,\n-        add_stu_decode_bboxes=False,\n-        loss_weight=1e-2,\n-        bbox_coder=None,\n-        student_channels=256,\n-        teacher_channels=512,\n-    ):\n-        super().__init__()\n-        self.bev_shape = bev_shape\n-        self.pc_range = pc_range\n-        self.voxel_size = voxel_size\n-        self.shape_resize_times = bbox_coder.out_size_factor if bbox_coder else 8\n-        self.score_threshold = score_threshold\n-        self.add_stu_decode_bboxes = add_stu_decode_bboxes\n-        self.loss_weight = loss_weight\n-\n-        self.align = nn.Sequential(\n-            nn.Conv2d(student_channels, teacher_channels, kernel_size=1, padding=0),\n-            # nn.BatchNorm2d(teacher_channels),\n-        )\n-\n-        if bbox_coder is not None:\n-            self.bbox_coder = build_bbox_coder(bbox_coder)\n-\n-    def pred2bboxes(self, preds_dict):\n-        \"\"\"SelfLearningMFD-pred2bboxes forward\n-\n-        Args:\n-            pred_bboxes (list):\n-            [task1[[task_head_reg,task_head_heatmap,...,]],\n-            task2...,\n-            taskn]\n-        \"\"\"\n-\n-        for task_id, pred_data in enumerate(preds_dict):\n-            bath_tmp = []\n-            batch_size = pred_data[0][\"heatmap\"].shape[0]\n-            batch_heatmap = pred_data[0][\"heatmap\"].sigmoid()\n-            batch_reg = pred_data[0][\"reg\"]\n-            batch_hei = pred_data[0][\"height\"]\n-\n-            # denormalization\n-            batch_dim = torch.exp(pred_data[0][\"dim\"])\n-\n-            batch_rot_sin = pred_data[0][\"rot\"][:, 0].unsqueeze(1)\n-            batch_rot_cos = pred_data[0][\"rot\"][:, 1].unsqueeze(1)\n-\n-            if \"vel\" in pred_data[0].keys():\n-                batch_vel = pred_data[0][\"vel\"]\n-\n-            bath_tmp.append(batch_heatmap)\n-            bath_tmp.append(batch_rot_sin)\n-            bath_tmp.append(batch_rot_cos)\n-            bath_tmp.append(batch_hei)\n-            bath_tmp.append(batch_dim)\n-            bath_tmp.append(batch_vel)\n-            bath_tmp.append(batch_reg)\n-\n-            bboxes_decode = self.bbox_coder.decode(*bath_tmp)\n-\n-        return bboxes_decode\n-\n-    def aug_boxes(self, gt_boxes, rot_mat):\n-        rot_lim = (-22.5, 22.5)\n-        scale_lim = (0.95, 1.05)\n-        rotate_angle = np.random.uniform(*rot_lim)\n-        scale_ratio = np.random.uniform(*scale_lim)\n-        gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n-        gt_boxes[:, 3:6] *= scale_ratio\n-        gt_boxes[:, 6] += rotate_angle\n-        gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n-        gt_boxes[:, 6] = -gt_boxes[:, 6]\n-        gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(-1)\n-        gt_boxes[:, :3] = gt_boxes[:, :3]\n-\n-        return gt_boxes\n-\n-    @force_fp32(\n-        apply_to=(\n-            \"student_bev_feat\",\n-            \"teacher_bev_feat\",\n-        )\n-    )\n-    def forward(\n-        self,\n-        student_bev_feat,\n-        teacher_bev_feat,\n-        gt_bboxes_list=None,\n-        masks_bboxes=None,\n-        bda_mat=None,\n-    ):\n-        \"\"\"SelfLearningMFD forward.\n-\n-        Args:\n-            student_bev_feats (torch.tensor): Calculate student feats\n-            teacher_bev_feats (torch.tensor): Calculate teacher feats\n-            masks_bboxes (list): Self-learning mask detection\n-            gt_bboxes_list (list): [LiDARInstance3DBoxes(gravity_center+tensor(num_objs,9))]\n-\n-        Returns:\n-            dict: _description_\n-        \"\"\"\n-\n-        bs = student_bev_feat.size(0)\n-\n-        if student_bev_feat.size(1) != teacher_bev_feat.size(1):\n-            student_bev_feat = self.align(student_bev_feat)\n-\n-        if masks_bboxes is not None and self.add_stu_decode_bboxes:\n-            student_pred_bboxes_list = self.pred2bboxes(\n-                masks_bboxes\n-            )  # list of task : each shape of [(num_bboxes,9)]\n-\n-            if bda_mat is not None:\n-                student_pred_bboxes_list = self.aug_boxes(student_pred_bboxes_list)\n-\n-        bev_feat_shape = torch.tensor(self.bev_shape)\n-        voxel_size = torch.tensor(self.voxel_size)\n-        feature_map_size = bev_feat_shape[:2]\n-\n-        if gt_bboxes_list is not None:\n-            device = student_bev_feat.device\n-\n-            gt_bboxes_list = [\n-                torch.cat(\n-                    (gt_bboxes.gravity_center, gt_bboxes.tensor[:, 3:]), dim=1\n-                ).to(device)\n-                for gt_bboxes in gt_bboxes_list\n-            ]\n-\n-            if self.add_stu_decode_bboxes:\n-                for idx, data in enumerate(student_pred_bboxes_list):\n-\n-                    bboxes_dim = data[\"bboxes\"].size(1)\n-\n-                    scroes_mask = data[\"scores\"] > self.score_threshold\n-\n-                    new_select_by_mask = data[\"bboxes\"][scroes_mask, :]\n-\n-                    gt_bboxes_list[idx] = torch.cat(\n-                        [gt_bboxes_list[idx], new_select_by_mask], dim=0\n-                    )\n-\n-            fg_map = student_bev_feat.new_zeros(\n-                (len(gt_bboxes_list), feature_map_size[1], feature_map_size[0])\n-            )\n-\n-            for idx in range(len(gt_bboxes_list)):\n-                num_objs = gt_bboxes_list[idx].shape[0]\n-\n-                for k in range(num_objs):\n-                    width = gt_bboxes_list[idx][k][3]\n-                    length = gt_bboxes_list[idx][k][4]\n-                    width = width / voxel_size[0] / self.shape_resize_times\n-                    length = length / voxel_size[1] / self.shape_resize_times\n-\n-                    if width > 0 and length > 0:\n-                        radius = gaussian_radius((length, width), min_overlap=0.1)\n-                        radius = max(1, int(radius))\n-\n-                        # be really careful for the coordinate system of\n-                        # your box annotation.\n-                        x, y, z = (\n-                            gt_bboxes_list[idx][k][0],\n-                            gt_bboxes_list[idx][k][1],\n-                            gt_bboxes_list[idx][k][2],\n-                        )\n-\n-                        coor_x = (\n-                            (x - self.pc_range[0])\n-                            / voxel_size[0]\n-                            / self.shape_resize_times\n-                        )\n-                        coor_y = (\n-                            (y - self.pc_range[1])\n-                            / voxel_size[1]\n-                            / self.shape_resize_times\n-                        )\n-\n-                        center = torch.tensor(\n-                            [coor_x, coor_y], dtype=torch.float32, device=device\n-                        )\n-                        center_int = center.to(torch.int32)\n-\n-                        # throw out not in range objects to avoid out of array\n-                        # area when creating the heatmap\n-                        if not (\n-                            0 <= center_int[0] < feature_map_size[0]\n-                            and 0 <= center_int[1] < feature_map_size[1]\n-                        ):\n-                            continue\n-\n-                        draw_heatmap_gaussian(fg_map[idx], center_int, radius)\n-\n-        if fg_map is None:\n-            fg_map = student_bev_feat.new_ones(\n-                (student_bev_feat.shape[0], feature_map_size[1], feature_map_size[0])\n-            )\n-\n-        if bs > 1:\n-            fg_map = fg_map.unsqueeze(1)\n-\n-        fit_loss = F.mse_loss(student_bev_feat, teacher_bev_feat, reduction=\"none\")\n-        fit_loss = torch.sum(fit_loss * fg_map) / torch.sum(fg_map)\n-\n-        return fit_loss * self.loss_weight, fg_map\n-\n-    # def forward(\n-    #     self, student_bev_feats, teacher_bev_feats, masks_bboxes, gt_bboxes_list\n-    # ):\n-    #     \"\"\"SelfLearningMFD forward.\n-\n-    #     Args:\n-    #         student_bev_feats (torch.tensor): _description_\n-    #         teacher_bev_feats (torch.tensor): _description_\n-    #         masks_bboxes (): _description_\n-    #         gt_bboxes_list (_type_): _description_\n-    #     \"\"\"\n-    #     assert isinstance(masks_bboxes, list)\n-    #     ret_dict = multi_apply(\n-    #         self._forward,\n-    #         student_bev_feats,\n-    #         teacher_bev_feats,\n-    #         masks_bboxes,\n-    #         gt_bboxes_list,\n-    #     )\n"
                },
                {
                    "date": 1716031484857,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -895,10 +895,10 @@\n             student_pred_bboxes_list = self.pred2bboxes(\n                 masks_bboxes\n             )  # list of task : each shape of [(num_bboxes,9)]\n \n-            if bda_mat is not None:\n-                student_pred_bboxes_list = self.aug_boxes(student_pred_bboxes_list)\n+            # if bda_mat is not None:\n+            #     student_pred_bboxes_list = self.aug_boxes(student_pred_bboxes_list)\n \n         bev_feat_shape = torch.tensor(self.bev_shape)\n         voxel_size = torch.tensor(self.voxel_size)\n         feature_map_size = bev_feat_shape[:2]\n"
                },
                {
                    "date": 1716031496046,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -560,9 +560,9 @@\n         assert num_layers == len(each_layer_loss_cfg)\n         for idx, cfg_dict in enumerate(each_layer_loss_cfg):\n             setattr(self, f\"layer_loss_{idx}\", build_loss(cfg_dict))\n \n-    def forward(self, radar_ms_feats, pts_ms_feats):\n+    def forward(self, radar_ms_feats, pts_ms_feats,gt_bboxes_3d):\n         assert isinstance(radar_ms_feats, list)\n         losses = 0.0\n \n         for idx in range(self.num_layers):\n"
                },
                {
                    "date": 1716031503984,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -560,15 +560,15 @@\n         assert num_layers == len(each_layer_loss_cfg)\n         for idx, cfg_dict in enumerate(each_layer_loss_cfg):\n             setattr(self, f\"layer_loss_{idx}\", build_loss(cfg_dict))\n \n-    def forward(self, radar_ms_feats, pts_ms_feats,gt_bboxes_3d):\n+    def forward(self, radar_ms_feats, pts_ms_feats, gt_bboxes_3d):\n         assert isinstance(radar_ms_feats, list)\n         losses = 0.0\n \n         for idx in range(self.num_layers):\n             losses += getattr(self, f\"layer_loss_{idx}\")(\n-                radar_ms_feats[idx], pts_ms_feats[idx]\n+                radar_ms_feats[idx], pts_ms_feats[idx], gt_bboxes_3d\n             )\n \n         return losses / self.num_layers\n \n"
                },
                {
                    "date": 1716031629379,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -560,9 +560,9 @@\n         assert num_layers == len(each_layer_loss_cfg)\n         for idx, cfg_dict in enumerate(each_layer_loss_cfg):\n             setattr(self, f\"layer_loss_{idx}\", build_loss(cfg_dict))\n \n-    def forward(self, radar_ms_feats, pts_ms_feats, gt_bboxes_3d):\n+    def forward(self, radar_ms_feats, pts_ms_feats, gt_bboxes_3d=None):\n         assert isinstance(radar_ms_feats, list)\n         losses = 0.0\n \n         for idx in range(self.num_layers):\n"
                },
                {
                    "date": 1716032146253,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -559,8 +559,17 @@\n         self.num_layers = num_layers\n         assert num_layers == len(each_layer_loss_cfg)\n         for idx, cfg_dict in enumerate(each_layer_loss_cfg):\n             setattr(self, f\"layer_loss_{idx}\", build_loss(cfg_dict))\n+            \n+    def unify_feat_size(self, student_feat, teacher_feat):\n+        bs, s_c, s_w, s_h = student_feat.shape\n+        bs, t_c, t_w, t_h = teacher_feat.shape\n+        if s_w == t_w:\n+            return student_feat, teacher_feat\n+        else:\n+            teacher_feat = F.interpolate(teacher_feat, (s_w, s_h), mode=\"bilinear\")\n+            return student_feat, teacher_feat\n \n     def forward(self, radar_ms_feats, pts_ms_feats, gt_bboxes_3d=None):\n         assert isinstance(radar_ms_feats, list)\n         losses = 0.0\n"
                },
                {
                    "date": 1716032237838,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -574,8 +574,9 @@\n         assert isinstance(radar_ms_feats, list)\n         losses = 0.0\n \n         for idx in range(self.num_layers):\n+            tmp_radar_feats,tmp_pts_feats=self.unify_feat_size(radar_ms_feats[idx],pts_ms_feats[idx])\n             losses += getattr(self, f\"layer_loss_{idx}\")(\n                 radar_ms_feats[idx], pts_ms_feats[idx], gt_bboxes_3d\n             )\n \n"
                },
                {
                    "date": 1716032244485,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -559,9 +559,9 @@\n         self.num_layers = num_layers\n         assert num_layers == len(each_layer_loss_cfg)\n         for idx, cfg_dict in enumerate(each_layer_loss_cfg):\n             setattr(self, f\"layer_loss_{idx}\", build_loss(cfg_dict))\n-            \n+\n     def unify_feat_size(self, student_feat, teacher_feat):\n         bs, s_c, s_w, s_h = student_feat.shape\n         bs, t_c, t_w, t_h = teacher_feat.shape\n         if s_w == t_w:\n@@ -574,11 +574,13 @@\n         assert isinstance(radar_ms_feats, list)\n         losses = 0.0\n \n         for idx in range(self.num_layers):\n-            tmp_radar_feats,tmp_pts_feats=self.unify_feat_size(radar_ms_feats[idx],pts_ms_feats[idx])\n+            tmp_radar_feats, tmp_pts_feats = self.unify_feat_size(\n+                radar_ms_feats[idx], pts_ms_feats[idx]\n+            )\n             losses += getattr(self, f\"layer_loss_{idx}\")(\n-                radar_ms_feats[idx], pts_ms_feats[idx], gt_bboxes_3d\n+                tmp_radar_feats,tmp_pts_feats, gt_bboxes_3d\n             )\n \n         return losses / self.num_layers\n \n"
                },
                {
                    "date": 1716032256687,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -578,9 +578,9 @@\n             tmp_radar_feats, tmp_pts_feats = self.unify_feat_size(\n                 radar_ms_feats[idx], pts_ms_feats[idx]\n             )\n             losses += getattr(self, f\"layer_loss_{idx}\")(\n-                tmp_radar_feats,tmp_pts_feats, gt_bboxes_3d\n+                tmp_radar_feats, tmp_pts_feats, gt_bboxes_3d\n             )\n \n         return losses / self.num_layers\n \n"
                },
                {
                    "date": 1716032484239,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -577,9 +577,9 @@\n         for idx in range(self.num_layers):\n             tmp_radar_feats, tmp_pts_feats = self.unify_feat_size(\n                 radar_ms_feats[idx], pts_ms_feats[idx]\n             )\n-            losses += getattr(self, f\"layer_loss_{idx}\")(\n+            _losses = getattr(self, f\"layer_loss_{idx}\")(\n                 tmp_radar_feats, tmp_pts_feats, gt_bboxes_3d\n             )\n \n         return losses / self.num_layers\n"
                },
                {
                    "date": 1716032504483,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -581,8 +581,10 @@\n             _losses = getattr(self, f\"layer_loss_{idx}\")(\n                 tmp_radar_feats, tmp_pts_feats, gt_bboxes_3d\n             )\n \n+            losses += _losses\n+\n         return losses / self.num_layers\n \n \n @LOSSES.register_module()\n"
                },
                {
                    "date": 1716032537665,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -577,14 +577,14 @@\n         for idx in range(self.num_layers):\n             tmp_radar_feats, tmp_pts_feats = self.unify_feat_size(\n                 radar_ms_feats[idx], pts_ms_feats[idx]\n             )\n-            _losses = getattr(self, f\"layer_loss_{idx}\")(\n-                tmp_radar_feats, tmp_pts_feats, gt_bboxes_3d\n-            )\n+            \n+            if gt_bboxes_3d is not None:\n+                losses += getattr(self, f\"layer_loss_{idx}\")(\n+                    tmp_radar_feats, tmp_pts_feats, gt_bboxes_3d\n+                )[0]\n \n-            losses += _losses\n-\n         return losses / self.num_layers\n \n \n @LOSSES.register_module()\n"
                },
                {
                    "date": 1716032548662,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -577,13 +577,17 @@\n         for idx in range(self.num_layers):\n             tmp_radar_feats, tmp_pts_feats = self.unify_feat_size(\n                 radar_ms_feats[idx], pts_ms_feats[idx]\n             )\n-            \n+\n             if gt_bboxes_3d is not None:\n                 losses += getattr(self, f\"layer_loss_{idx}\")(\n                     tmp_radar_feats, tmp_pts_feats, gt_bboxes_3d\n                 )[0]\n+            else:\n+                losses += getattr(self, f\"layer_loss_{idx}\")(\n+                    tmp_radar_feats, tmp_pts_feats, gt_bboxes_3d\n+                )\n \n         return losses / self.num_layers\n \n \n"
                },
                {
                    "date": 1716032603668,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,1029 @@\n+import torch.nn as nn\n+import torch.nn.functional as F\n+import torch\n+from ..builder import LOSSES, build_loss\n+from ..utils.gaussian_utils import calculate_box_mask_gaussian\n+from torch import distributed as dist\n+from mmdet.core import multi_apply, build_bbox_coder\n+from mmdet3d.core import draw_heatmap_gaussian, gaussian_radius\n+from mmdet3d.core.bbox.coders.centerpoint_bbox_coders import (\n+    CenterPointBBoxCoder as bboxcoder,\n+)\n+from mmdet.models.losses import QualityFocalLoss\n+from mmcv.runner import force_fp32\n+import torchsort\n+import torchvision\n+import numpy as np\n+\n+\n+def get_world_size() -> int:\n+    if not dist.is_available():\n+        return 1\n+    if not dist.is_initialized():\n+        return 1\n+\n+    return dist.get_world_size()\n+\n+\n+def reduce_sum(tensor):\n+    world_size = get_world_size()\n+    if world_size < 2:\n+        return tensor\n+    tensor = tensor.clone()\n+    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n+    return tensor\n+\n+\n+def reduce_mean(tensor):\n+    return reduce_sum(tensor) / float(get_world_size())\n+\n+\n+def _sigmoid(x):\n+    # y = torch.clamp(x.sigmoid(), min=1e-3, max=1 - 1e-3)\n+    y = x.sigmoid()\n+    return y\n+\n+\n+def off_diagonal(x):\n+    # return a flattened view of the off-diagonal elements of a square matrix\n+    n, m = x.shape\n+    assert n == m\n+    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n+\n+\n+def response_list_wrap(response_lists: list):\n+    \"\"\"response_list_wrap\n+\n+    Args:\n+        response_lists (list): Pack list content\n+\n+    Returns:\n+        list: Return the new list of packaged integration\n+    \"\"\"\n+    assert len(response_lists) == 1\n+\n+    tmp_resp = []\n+    for response in response_lists:\n+        tmp_resp.append(response[0])\n+\n+    return tmp_resp\n+\n+\n+@LOSSES.register_module()\n+class QualityFocalLoss_(nn.Module):\n+    \"\"\"\n+    input[B,M,C] not sigmoid\n+    target[B,M,C], sigmoid\n+    \"\"\"\n+\n+    def __init__(self, beta=2.0):\n+\n+        super(QualityFocalLoss_, self).__init__()\n+        self.beta = beta\n+\n+    def forward(\n+        self,\n+        input: torch.Tensor,\n+        target: torch.Tensor,\n+        pos_normalizer=torch.tensor(1.0),\n+    ):\n+\n+        pred_sigmoid = torch.sigmoid(input)\n+        scale_factor = pred_sigmoid - target\n+\n+        # pred_sigmoid = torch.sigmoid(input)\n+        # scale_factor = input - target\n+        loss = F.binary_cross_entropy_with_logits(input, target, reduction=\"none\") * (\n+            scale_factor.abs().pow(self.beta)\n+        )\n+        loss /= torch.clamp(pos_normalizer, min=1.0)\n+        return loss\n+\n+\n+@LOSSES.register_module()\n+class SimpleL1(nn.Module):\n+    def __init__(self, criterion=\"L1\", student_ch=256, teacher_ch=512):\n+        super().__init__()\n+        self.criterion = criterion\n+        if criterion == \"L1\":\n+            self.criterion_loss = nn.L1Loss(reduce=False)\n+        elif criterion == \"SmoothL1\":\n+            self.criterion_loss = nn.SmoothL1Loss(reduce=False)\n+        elif criterion == \"MSE\":\n+            self.criterion_loss = nn.MSELoss(reduction=\"none\")\n+\n+        if student_ch != teacher_ch:\n+            self.align = nn.Conv2d(student_ch, teacher_ch, kernel_size=1)\n+\n+    def forward(self, feats1, feats2, *args, **kwargs):\n+        if self.criterion == \"MSE\":\n+            feats1 = self.align(feats1) if getattr(self, \"align\", None) else feats1\n+            losses = self.criterion_loss(feats1, feats2).mean()\n+            return losses\n+        else:\n+            return self.criterion_loss(feats1, feats2)\n+\n+\n+@LOSSES.register_module()\n+class Relevance_Distillation(nn.Module):\n+    def __init__(self, bs=2, bn_dim=512, lambd=0.0051):\n+        super().__init__()\n+        self.bs = bs\n+        self.bn_dim = bn_dim\n+        self.lambd = lambd\n+\n+        # normalization layer for the representations z1 and z2\n+        self.bn = nn.BatchNorm1d(self.bn_dim, affine=False)\n+\n+        self.align = nn.Conv2d(256, 512, kernel_size=1)\n+\n+    def forward(self, student_bev, teacher_bev, *args, **kwargs):\n+        student_bev = self.align(student_bev)\n+\n+        student_bev = student_bev.flatten(2)\n+        teacher_bev = teacher_bev.flatten(2)\n+\n+        # empirical cross-correlation matrix\n+        c = self.bn(student_bev).flatten(1).T @ self.bn(teacher_bev).flatten(1)\n+\n+        # sum the cross-correlation matrix between all gpus\n+        c.div_(self.bs)\n+        if self.bs == 1:\n+            pass\n+        else:\n+            torch.distributed.all_reduce(c)\n+\n+        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n+        off_diag = off_diagonal(c).pow_(2).sum()\n+        loss = on_diag + self.lambd * off_diag\n+        return loss\n+\n+\n+@LOSSES.register_module()\n+class FeatureLoss(nn.Module):\n+    \"\"\"PyTorch version of `Masked Generative Distillation`\n+\n+    Args:\n+        student_channels(int): Number of channels in the student's feature map.\n+        teacher_channels(int): Number of channels in the teacher's feature map.\n+        name (str): the loss name of the layer\n+        alpha_mgd (float, optional): Weight of dis_loss. Defaults to 0.00002\n+        lambda_mgd (float, optional): masked ratio. Defaults to 0.65\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        student_channels,\n+        teacher_channels,\n+        name,\n+        alpha_mgd=0.00002,\n+        lambda_mgd=0.65,\n+    ):\n+        super(FeatureLoss, self).__init__()\n+        self.alpha_mgd = alpha_mgd\n+        self.lambda_mgd = lambda_mgd\n+        self.name = name\n+\n+        if student_channels != teacher_channels:\n+            self.align = nn.Conv2d(\n+                student_channels, teacher_channels, kernel_size=1, stride=1, padding=0\n+            )\n+        else:\n+            self.align = None\n+\n+        self.generation = nn.Sequential(\n+            nn.Conv2d(teacher_channels, teacher_channels, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.Conv2d(teacher_channels, teacher_channels, kernel_size=3, padding=1),\n+        )\n+\n+    def forward(self, preds_S, preds_T, **kwargs):\n+        \"\"\"Forward function.\n+        Args:\n+            preds_S(Tensor): Bs*C*H*W, student's feature map\n+            preds_T(Tensor): Bs*C*H*W, teacher's feature map\n+        \"\"\"\n+        assert preds_S.shape[-2:] == preds_T.shape[-2:]\n+\n+        if self.align is not None:\n+            preds_S = self.align(preds_S)\n+\n+        loss = self.get_dis_loss(preds_S, preds_T) * self.alpha_mgd\n+\n+        return loss\n+\n+    def get_dis_loss(self, preds_S, preds_T):\n+        loss_mse = nn.MSELoss(reduction=\"sum\")\n+        N, C, H, W = preds_T.shape\n+\n+        device = preds_S.device\n+        mat = torch.rand((N, 1, H, W)).to(device)\n+        mat = torch.where(mat > 1 - self.lambda_mgd, 0, 1).to(device)\n+\n+        masked_fea = torch.mul(preds_S, mat)\n+        new_fea = self.generation(masked_fea)\n+\n+        dis_loss = loss_mse(new_fea, preds_T) / N\n+\n+        return dis_loss\n+\n+\n+@LOSSES.register_module()\n+class FeatureLoss_InnerClip(nn.Module):\n+    def __init__(\n+        self,\n+        x_sample_num=24,\n+        y_sample_num=24,\n+        inter_keypoint_weight=1,\n+        inter_channel_weight=10,\n+        enlarge_width=1.6,\n+        embed_channels=[256, 512],\n+        inner_feats_distill=None,\n+    ):\n+        super().__init__()\n+        self.x_sample_num = x_sample_num\n+        self.y_sample_num = y_sample_num\n+        self.inter_keypoint_weight = inter_keypoint_weight\n+        self.inter_channel_weight = inter_channel_weight\n+        self.enlarge_width = enlarge_width\n+\n+        self.img_view_transformer = None\n+\n+        self.embed_channels = embed_channels\n+\n+        self.imgbev_embed = nn.Sequential(\n+            nn.Conv2d(\n+                embed_channels[0],\n+                embed_channels[1],\n+                kernel_size=1,\n+                stride=1,\n+                padding=0,\n+                bias=False,\n+            ),\n+            nn.BatchNorm2d(embed_channels[1]),\n+        )\n+\n+        self.inner_feats_loss = (\n+            build_loss(inner_feats_distill) if inner_feats_distill is not None else None\n+        )\n+\n+    def get_gt_sample_grid(self, corner_points2d):\n+        dH_x, dH_y = corner_points2d[0] - corner_points2d[1]\n+        dW_x, dW_y = corner_points2d[0] - corner_points2d[2]\n+        raw_grid_x = (\n+            torch.linspace(\n+                corner_points2d[0][0], corner_points2d[1][0], self.x_sample_num\n+            )\n+            .view(1, -1)\n+            .repeat(self.y_sample_num, 1)\n+        )\n+        raw_grid_y = (\n+            torch.linspace(\n+                corner_points2d[0][1], corner_points2d[2][1], self.y_sample_num\n+            )\n+            .view(-1, 1)\n+            .repeat(1, self.x_sample_num)\n+        )\n+        raw_grid = torch.cat((raw_grid_x.unsqueeze(2), raw_grid_y.unsqueeze(2)), dim=2)\n+        raw_grid_x_offset = (\n+            torch.linspace(0, -dW_x, self.x_sample_num)\n+            .view(-1, 1)\n+            .repeat(1, self.y_sample_num)\n+        )\n+        raw_grid_y_offset = (\n+            torch.linspace(0, -dH_y, self.y_sample_num)\n+            .view(1, -1)\n+            .repeat(self.x_sample_num, 1)\n+        )\n+        raw_grid_offset = torch.cat(\n+            (raw_grid_x_offset.unsqueeze(2), raw_grid_y_offset.unsqueeze(2)), dim=2\n+        )\n+        grid = raw_grid + raw_grid_offset  # X_sample,Y_sample,2\n+        grid[:, :, 0] = torch.clip(\n+            (\n+                (\n+                    grid[:, :, 0]\n+                    - (\n+                        self.img_view_transformer[\"bx\"][0].to(grid.device)\n+                        - self.img_view_transformer[\"dx\"][0].to(grid.device) / 2.0\n+                    )\n+                )\n+                / self.img_view_transformer[\"dx\"][0].to(grid.device)\n+                / (self.img_view_transformer[\"nx\"][0].to(grid.device) - 1)\n+            )\n+            * 2.0\n+            - 1.0,\n+            min=-1.0,\n+            max=1.0,\n+        )\n+        grid[:, :, 1] = torch.clip(\n+            (\n+                (\n+                    grid[:, :, 1]\n+                    - (\n+                        self.img_view_transformer[\"bx\"][1].to(grid.device)\n+                        - self.img_view_transformer[\"dx\"][1].to(grid.device) / 2.0\n+                    )\n+                )\n+                / self.img_view_transformer[\"dx\"][1].to(grid.device)\n+                / (self.img_view_transformer[\"nx\"][1].to(grid.device) - 1)\n+            )\n+            * 2.0\n+            - 1.0,\n+            min=-1.0,\n+            max=1.0,\n+        )\n+\n+        return grid.unsqueeze(0)\n+\n+    def get_inner_feat(self, gt_bboxes_3d, img_feats, pts_feats):\n+        \"\"\"Use grid to sample features of key points\"\"\"\n+        device = img_feats.device\n+        dtype = img_feats[0].dtype\n+\n+        img_feats_sampled_list = []\n+        pts_feats_sampled_list = []\n+\n+        for sample_ind in torch.arange(len(gt_bboxes_3d)):\n+            img_feat = img_feats[sample_ind].unsqueeze(0)  # 1,C,H,W\n+            pts_feat = pts_feats[sample_ind].unsqueeze(0)  # 1,C,H,W\n+\n+            bbox_num, corner_num, point_num = gt_bboxes_3d[sample_ind].corners.shape\n+\n+            for bbox_ind in torch.arange(bbox_num):\n+                if self.enlarge_width > 0:\n+                    gt_sample_grid = self.get_gt_sample_grid(\n+                        gt_bboxes_3d[sample_ind]\n+                        .enlarged_box(self.enlarge_width)\n+                        .corners[bbox_ind][[0, 2, 4, 6], :-1]\n+                    ).to(device)\n+                else:\n+                    gt_sample_grid = self.get_gt_sample_grid(\n+                        gt_bboxes_3d[sample_ind].corners[bbox_ind][[0, 2, 4, 6], :-1]\n+                    ).to(\n+                        device\n+                    )  # 1,sample_y,sample_x,2\n+\n+                img_feats_sampled_list.append(\n+                    F.grid_sample(\n+                        img_feat,\n+                        grid=gt_sample_grid,\n+                        align_corners=False,\n+                        mode=\"bilinear\",\n+                    )\n+                )  # 'bilinear')) #all_bbox_num,C,y_sample,x_sample\n+                pts_feats_sampled_list.append(\n+                    F.grid_sample(\n+                        pts_feat,\n+                        grid=gt_sample_grid,\n+                        align_corners=False,\n+                        mode=\"bilinear\",\n+                    )\n+                )  # 'bilinear')) #all_bbox_num,C,y_sample,x_sample\n+\n+        return torch.cat(img_feats_sampled_list, dim=0), torch.cat(\n+            pts_feats_sampled_list, dim=0\n+        )\n+\n+    @force_fp32(apply_to=(\"img_feats_kd\", \"pts_feats_kd\"))\n+    def get_inter_channel_loss(self, img_feats_kd, pts_feats_kd):\n+        \"\"\"Calculate the inter-channel similarities, guide the student keypoint features to mimic the channel-wise relationships of the teacher`s\"\"\"\n+\n+        C_img = img_feats_kd.shape[1]\n+        C_pts = pts_feats_kd.shape[1]\n+        N = self.x_sample_num * self.y_sample_num\n+\n+        img_feats_kd = img_feats_kd.view(-1, C_img, N).matmul(\n+            img_feats_kd.view(-1, C_img, N).permute(0, 2, 1)\n+        )  # -1,N,N\n+        pts_feats_kd = pts_feats_kd.view(-1, C_pts, N).matmul(\n+            pts_feats_kd.view(-1, C_pts, N).permute(0, 2, 1)\n+        )\n+\n+        img_feats_kd = F.normalize(img_feats_kd, dim=2)\n+        pts_feats_kd = F.normalize(pts_feats_kd, dim=2)\n+\n+        loss_inter_channel = F.mse_loss(img_feats_kd, pts_feats_kd, reduction=\"none\")\n+        loss_inter_channel = loss_inter_channel.sum(-1)\n+        loss_inter_channel = loss_inter_channel.mean()\n+        loss_inter_channel = self.inter_channel_weight * loss_inter_channel\n+        return loss_inter_channel\n+\n+    def forward(self, student_feats, teacher_feats, gt_bboxes_list, **kwargs):\n+\n+        self.img_view_transformer = kwargs.get(\"ivt_cfg\")\n+\n+        if student_feats.size(1) != teacher_feats.size(1):\n+            student_feats = self.imgbev_embed(student_feats)\n+\n+        if self.inter_keypoint_weight > 0 or self.inter_channel_weight > 0:\n+            img_feats_kd, pts_feats_kd = self.get_inner_feat(\n+                gt_bboxes_list, student_feats, teacher_feats\n+            )\n+\n+        if self.inner_feats_loss:\n+            return self.inner_feats_loss(img_feats_kd, pts_feats_kd)\n+\n+        # if self.inter_keypoint_weight > 0:\n+        #     loss_inter_keypoint = self.get_inter_keypoint_loss(\n+        #         img_feats_kd, pts_feats_kd\n+        #     )\n+        #     losses.update({\"loss_inter_keypoint_img_bev\": loss_inter_keypoint})\n+\n+        if self.inter_channel_weight > 0:\n+            loss_inter_channel = self.get_inter_channel_loss(img_feats_kd, pts_feats_kd)\n+\n+        return loss_inter_channel\n+\n+\n+@LOSSES.register_module()\n+class FeatureLoss_Affinity(nn.Module):\n+    def __init__(self):\n+        super().__init__()\n+\n+    def forward(self, student_feats, teacher_feats, *args, **kwargs):\n+        student_feats = [student_feats]\n+        teacher_feats = [teacher_feats]\n+\n+        feature_ditill_loss = 0.0\n+\n+        resize_shape = student_feats[-1].shape[-2:]\n+        if isinstance(student_feats, list):\n+            for i in range(len(student_feats)):\n+                feature_target = teacher_feats[i].detach()\n+                feature_pred = student_feats[i]\n+\n+                B, C, H, W = student_feats[-1].shape\n+\n+                if student_feats[-1].size(-1) != teacher_feats[-1].size(-1):\n+                    feature_pred_down = F.interpolate(\n+                        feature_pred, size=resize_shape, mode=\"bilinear\"\n+                    )\n+                    feature_target_down = F.interpolate(\n+                        feature_target, size=resize_shape, mode=\"bilinear\"\n+                    )\n+                else:\n+                    feature_pred_down = feature_pred\n+                    feature_target_down = feature_target\n+\n+                feature_target_down = feature_target_down.reshape(B, C, -1)\n+                depth_affinity = torch.bmm(\n+                    feature_target_down.permute(0, 2, 1), feature_target_down\n+                )\n+\n+                feature_pred_down = feature_pred_down.reshape(B, C, -1)\n+                rgb_affinity = torch.bmm(\n+                    feature_pred_down.permute(0, 2, 1), feature_pred_down\n+                )\n+\n+                feature_ditill_loss = (\n+                    feature_ditill_loss\n+                    + F.l1_loss(rgb_affinity, depth_affinity, reduction=\"mean\") / B\n+                )\n+\n+        else:\n+            raise NotImplementedError\n+\n+        return feature_ditill_loss\n+\n+\n+@LOSSES.register_module()\n+class FeatureLoss_Coefficient(nn.Module):\n+    def __init__(self, **kwargs):\n+        super().__init__()\n+\n+    def corrcoef(self, target, pred):\n+        pred_n = pred - pred.mean()\n+        target_n = target - target.mean()\n+        pred_n = pred_n / pred_n.norm()\n+        target_n = target_n / target_n.norm()\n+        return (pred_n * target_n).sum()\n+\n+    def forward(\n+        self,\n+        pred,\n+        target,\n+        regularization=\"l2\",\n+        regularization_strength=1.0,\n+    ):\n+        pred = [pred.clone()]\n+        target = [target.clone()]\n+        spearman_loss = 0.0\n+\n+        resize_shape = pred[-1].shape[-2:]  # save training time\n+\n+        if isinstance(pred, list):\n+            for i in range(len(pred)):\n+                feature_target = target[i]\n+                feature_pred = pred[i]\n+                B, C, H, W = feature_pred.shape\n+\n+                feature_pred_down = F.interpolate(\n+                    feature_pred, size=resize_shape, mode=\"bilinear\"\n+                )\n+                feature_target_down = F.interpolate(\n+                    feature_target, size=resize_shape, mode=\"bilinear\"\n+                )\n+\n+                # if feature_pred.size(-1) != feature_target.size(-1):\n+\n+                #     feature_pred_down = F.interpolate(\n+                #         feature_pred, size=resize_shape, mode=\"bilinear\"\n+                #     )\n+                #     feature_target_down = F.interpolate(\n+                #         feature_target, size=resize_shape, mode=\"bilinear\"\n+                #     )\n+                # else:\n+                #     feature_pred_down = feature_pred\n+                #     feature_target_down = feature_target\n+\n+                feature_pred_down = feature_pred_down.reshape(B, -1)\n+                feature_target_down = feature_target_down.reshape(B, -1)\n+\n+                feature_pred_down = torchsort.soft_rank(\n+                    feature_pred_down,\n+                    regularization=regularization,\n+                    regularization_strength=regularization_strength,\n+                )\n+                spearman_loss += 1 - self.corrcoef(\n+                    feature_target_down, feature_pred_down / feature_pred_down.shape[-1]\n+                )\n+\n+        return spearman_loss\n+\n+\n+@LOSSES.register_module()\n+class Radar_MSDistilll(nn.Module):\n+    def __init__(self, num_layers=2, each_layer_loss_cfg=[]):\n+        super().__init__()\n+        self.num_layers = num_layers\n+        assert num_layers == len(each_layer_loss_cfg)\n+        for idx, cfg_dict in enumerate(each_layer_loss_cfg):\n+            setattr(self, f\"layer_loss_{idx}\", build_loss(cfg_dict))\n+\n+    def unify_feat_size(self, student_feat, teacher_feat):\n+        bs, s_c, s_w, s_h = student_feat.shape\n+        bs, t_c, t_w, t_h = teacher_feat.shape\n+        if s_w == t_w:\n+            return student_feat, teacher_feat\n+        else:\n+            teacher_feat = F.interpolate(teacher_feat, (s_w, s_h), mode=\"bilinear\")\n+            return student_feat, teacher_feat\n+\n+    def forward(self, radar_ms_feats, pts_ms_feats, gt_bboxes_3d=None):\n+        assert isinstance(radar_ms_feats, list)\n+        losses = 0.0\n+\n+        for idx in range(self.num_layers):\n+            tmp_radar_feats, tmp_pts_feats = self.unify_feat_size(\n+                radar_ms_feats[idx], pts_ms_feats[idx]\n+            )\n+\n+            if gt_bboxes_3d is not None:\n+                losses += getattr(self, f\"layer_loss_{idx}\")(\n+                    tmp_radar_feats, tmp_pts_feats, gt_bboxes_3d\n+                )[0]\n+            else:\n+                losses += getattr(self, f\"layer_loss_{idx}\")(\n+                    tmp_radar_feats, tmp_pts_feats, gt_bboxes_3d\n+                )\n+\n+        return losses / self.num_layers\n+\n+\n+@LOSSES.register_module()\n+class InfoMax(nn.Module):\n+    def __init__(self, **kwargs):\n+        super().__init__()\n+\n+    def forward(self, x1, x2):\n+        import pdb\n+\n+        pdb.set_trace()\n+        x1 = x1 / (torch.norm(x1, p=2, dim=1, keepdim=True) + 1e-10)\n+        x2 = x2 / (torch.norm(x2, p=2, dim=1, keepdim=True) + 1e-10)\n+        bs = x1.size(0)\n+        s = torch.matmul(x1, x2.permute(1, 0))\n+        mask_joint = torch.eye(bs).cuda()\n+        mask_marginal = 1 - mask_joint\n+\n+        Ej = (s * mask_joint).mean()\n+        Em = torch.exp(s * mask_marginal).mean()\n+        # decoupled comtrastive learning?!!!!\n+        # infomax_loss = - (Ej - torch.log(Em)) * self.alpha\n+        infomax_loss = -(Ej - torch.log(Em))  # / Em\n+        return infomax_loss\n+\n+\n+@LOSSES.register_module()\n+class HeatMapAug(nn.Module):\n+    def __init__(self):\n+        super().__init__()\n+\n+    @force_fp32(apply_to=(\"stu_pred\", \"tea_pred\"))\n+    def forward(self, stu_pred, tea_pred, fg_map):\n+\n+        num_task = len(stu_pred)\n+        kl_loss = 0\n+        for task_id in range(num_task):\n+            student_pred = stu_pred[task_id][0][\"heatmap\"].sigmoid()\n+            teacher_pred = tea_pred[task_id][0][\"heatmap\"].sigmoid()\n+            fg_map = fg_map.unsqueeze(1)\n+            task_kl_loss = F.binary_cross_entropy(student_pred, teacher_pred.detach())\n+            task_kl_loss = torch.sum(task_kl_loss * fg_map) / torch.sum(fg_map)\n+            kl_loss += task_kl_loss\n+\n+        return kl_loss\n+\n+\n+@LOSSES.register_module()\n+class Dc_ResultDistill(nn.Module):\n+    def __init__(\n+        self,\n+        pc_range=[],\n+        voxel_size=[],\n+        out_size_scale=8,\n+        ret_sum=False,\n+        loss_weight_reg=10,\n+        loss_weight_cls=10,\n+        max_cls=True,\n+    ):\n+        super().__init__()\n+\n+        self.pc_range = pc_range\n+        self.voxel_size = voxel_size\n+        self.out_size_scale = out_size_scale\n+        self.ret_sum = ret_sum\n+        self.loss_weight_reg = loss_weight_reg\n+        self.loss_weight_cls = loss_weight_cls\n+        self.max_cls = max_cls\n+\n+    def forward(self, resp_lidar, resp_fuse, gt_boxes):\n+        \"\"\"Dc_ResultDistill forward.\n+\n+        Args:\n+            resp_lidar (_type_):\n+            resp_fuse (_type_):\n+            gt_boxes (_type_):\n+\n+        Returns:\n+            _type_: _description_\n+        \"\"\"\n+\n+        tmp_resp_lidar = []\n+        tmp_resp_fuse = []\n+        for res_lidar, res_fuse in zip(resp_lidar, resp_fuse):\n+            tmp_resp_lidar.append(res_lidar[0])\n+            tmp_resp_fuse.append(res_fuse[0])\n+\n+        tmp_gt_boxes = []\n+        for bs_idx in range(len(gt_boxes)):\n+\n+            gt_bboxes_3d = torch.cat(\n+                (gt_boxes[bs_idx].gravity_center, gt_boxes[bs_idx].tensor[:, 3:]), dim=1\n+            )\n+\n+            tmp_gt_boxes.append(gt_bboxes_3d)\n+\n+        cls_lidar = []\n+        reg_lidar = []\n+        cls_fuse = []\n+        reg_fuse = []\n+\n+        # criterion = nn.L1Loss(reduce=False)\n+        # criterion_cls = QualityFocalLoss_()\n+\n+        criterion = nn.SmoothL1Loss(reduce=False)\n+        criterion_cls = nn.L1Loss(reduce=False)\n+\n+        for task_id, task_out in enumerate(tmp_resp_lidar):\n+            cls_lidar.append(task_out[\"heatmap\"])\n+            cls_fuse.append(_sigmoid(tmp_resp_fuse[task_id][\"heatmap\"] / 2))\n+            reg_lidar.append(\n+                torch.cat(\n+                    [\n+                        task_out[\"reg\"],\n+                        task_out[\"height\"],\n+                        task_out[\"dim\"],\n+                        task_out[\"rot\"],\n+                        task_out[\"vel\"],\n+                        # task_out[\"iou\"],\n+                    ],\n+                    dim=1,\n+                )\n+            )\n+            reg_fuse.append(\n+                torch.cat(\n+                    [\n+                        tmp_resp_fuse[task_id][\"reg\"],\n+                        tmp_resp_fuse[task_id][\"height\"],\n+                        tmp_resp_fuse[task_id][\"dim\"],\n+                        tmp_resp_fuse[task_id][\"rot\"],\n+                        tmp_resp_fuse[task_id][\"vel\"],\n+                        # resp_fuse[task_id][\"iou\"],\n+                    ],\n+                    dim=1,\n+                )\n+            )\n+        cls_lidar = torch.cat(cls_lidar, dim=1)\n+        reg_lidar = torch.cat(reg_lidar, dim=1)\n+        cls_fuse = torch.cat(cls_fuse, dim=1)\n+        reg_fuse = torch.cat(reg_fuse, dim=1)\n+\n+        if self.max_cls:\n+            cls_lidar_max, _ = torch.max(cls_lidar, dim=1)\n+            cls_fuse_max, _ = torch.max(cls_fuse, dim=1)\n+        else:\n+            _, _, ht_h, ht_w = cls_fuse.shape\n+            cls_lidar_max = cls_lidar.flatten(2).permute(0, 2, 1)\n+            cls_fuse_max = cls_fuse.flatten(2).permute(0, 2, 1)\n+\n+        gaussian_mask = calculate_box_mask_gaussian(\n+            reg_lidar.shape,\n+            tmp_gt_boxes,\n+            self.pc_range,\n+            self.voxel_size,\n+            self.out_size_scale,\n+        )\n+\n+        # # diff_reg = criterion(reg_lidar, reg_fuse)\n+        # # diff_cls = criterion(cls_lidar_max, cls_fuse_max)\n+        # diff_reg = criterion_reg(reg_lidar, reg_fuse)\n+\n+        # cls_lidar_max = torch.sigmoid(cls_lidar_max)\n+        # diff_cls = criterion_cls(\n+        #     cls_fuse_max, cls_lidar_max\n+        # )  # Compared with directly using the L1 loss constraint, replace it with bce focal loss and change the position?\n+\n+        # weight = gaussian_mask.sum()\n+        # weight = reduce_mean(weight)\n+\n+        # # diff_cls = criterion_cls(cls_lidar_max, cls_fuse_max)\n+        # diff_reg = torch.mean(diff_reg, dim=1)\n+        # diff_reg = diff_reg * gaussian_mask\n+        # loss_reg_distill = torch.sum(diff_reg) / (weight + 1e-4)\n+\n+        # if not self.max_cls:\n+        #     diff_cls = diff_cls\n+        #     loss_cls_distill = diff_cls.sum() * 2\n+        # else:\n+        #     diff_cls = diff_cls * gaussian_mask\n+        #     loss_cls_distill = torch.sum(diff_cls) / (weight + 1e-4)\n+\n+        diff_reg = criterion(reg_lidar, reg_fuse)\n+\n+        diff_cls = criterion_cls(cls_lidar_max, cls_fuse_max)\n+        diff_reg = torch.mean(diff_reg, dim=1)\n+        diff_reg = diff_reg * gaussian_mask\n+        diff_cls = diff_cls * gaussian_mask\n+        weight = gaussian_mask.sum()\n+        weight = reduce_mean(weight)\n+        loss_reg_distill = torch.sum(diff_reg) / (weight + 1e-4)\n+        loss_cls_distill = torch.sum(diff_cls) / (weight + 1e-4)\n+\n+        if self.ret_sum:\n+\n+            loss_det_distill = self.loss_weight * (loss_reg_distill + loss_cls_distill)\n+\n+            return loss_det_distill\n+        else:\n+\n+            return (\n+                self.loss_weight_reg * loss_reg_distill,\n+                self.loss_weight_cls * loss_cls_distill,\n+            )\n+\n+\n+@LOSSES.register_module()\n+class SelfLearningMFD(nn.Module):\n+    def __init__(\n+        self,\n+        bev_shape=[128, 128],\n+        pc_range=[-51.2, -51.2, -5, 51.2, 51.2, 3],\n+        voxel_size=[0.1, 0.1, 0.1],\n+        score_threshold=0.7,\n+        add_stu_decode_bboxes=False,\n+        loss_weight=1e-2,\n+        bbox_coder=None,\n+        student_channels=256,\n+        teacher_channels=512,\n+    ):\n+        super().__init__()\n+        self.bev_shape = bev_shape\n+        self.pc_range = pc_range\n+        self.voxel_size = voxel_size\n+        self.shape_resize_times = bbox_coder.out_size_factor if bbox_coder else 8\n+        self.score_threshold = score_threshold\n+        self.add_stu_decode_bboxes = add_stu_decode_bboxes\n+        self.loss_weight = loss_weight\n+\n+        self.align = nn.Sequential(\n+            nn.Conv2d(student_channels, teacher_channels, kernel_size=1, padding=0),\n+            # nn.BatchNorm2d(teacher_channels),\n+        )\n+\n+        if bbox_coder is not None:\n+            self.bbox_coder = build_bbox_coder(bbox_coder)\n+\n+    def pred2bboxes(self, preds_dict):\n+        \"\"\"SelfLearningMFD-pred2bboxes forward\n+\n+        Args:\n+            pred_bboxes (list):\n+            [task1[[task_head_reg,task_head_heatmap,...,]],\n+            task2...,\n+            taskn]\n+        \"\"\"\n+\n+        for task_id, pred_data in enumerate(preds_dict):\n+            bath_tmp = []\n+            batch_size = pred_data[0][\"heatmap\"].shape[0]\n+            batch_heatmap = pred_data[0][\"heatmap\"].sigmoid()\n+            batch_reg = pred_data[0][\"reg\"]\n+            batch_hei = pred_data[0][\"height\"]\n+\n+            # denormalization\n+            batch_dim = torch.exp(pred_data[0][\"dim\"])\n+\n+            batch_rot_sin = pred_data[0][\"rot\"][:, 0].unsqueeze(1)\n+            batch_rot_cos = pred_data[0][\"rot\"][:, 1].unsqueeze(1)\n+\n+            if \"vel\" in pred_data[0].keys():\n+                batch_vel = pred_data[0][\"vel\"]\n+\n+            bath_tmp.append(batch_heatmap)\n+            bath_tmp.append(batch_rot_sin)\n+            bath_tmp.append(batch_rot_cos)\n+            bath_tmp.append(batch_hei)\n+            bath_tmp.append(batch_dim)\n+            bath_tmp.append(batch_vel)\n+            bath_tmp.append(batch_reg)\n+\n+            bboxes_decode = self.bbox_coder.decode(*bath_tmp)\n+\n+        return bboxes_decode\n+\n+    def aug_boxes(self, gt_boxes, rot_mat):\n+        rot_lim = (-22.5, 22.5)\n+        scale_lim = (0.95, 1.05)\n+        rotate_angle = np.random.uniform(*rot_lim)\n+        scale_ratio = np.random.uniform(*scale_lim)\n+        gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n+        gt_boxes[:, 3:6] *= scale_ratio\n+        gt_boxes[:, 6] += rotate_angle\n+        gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n+        gt_boxes[:, 6] = -gt_boxes[:, 6]\n+        gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(-1)\n+        gt_boxes[:, :3] = gt_boxes[:, :3]\n+\n+        return gt_boxes\n+\n+    @force_fp32(\n+        apply_to=(\n+            \"student_bev_feat\",\n+            \"teacher_bev_feat\",\n+        )\n+    )\n+    def forward(\n+        self,\n+        student_bev_feat,\n+        teacher_bev_feat,\n+        gt_bboxes_list=None,\n+        masks_bboxes=None,\n+        bda_mat=None,\n+    ):\n+        \"\"\"SelfLearningMFD forward.\n+\n+        Args:\n+            student_bev_feats (torch.tensor): Calculate student feats\n+            teacher_bev_feats (torch.tensor): Calculate teacher feats\n+            masks_bboxes (list): Self-learning mask detection\n+            gt_bboxes_list (list): [LiDARInstance3DBoxes(gravity_center+tensor(num_objs,9))]\n+\n+        Returns:\n+            dict: _description_\n+        \"\"\"\n+\n+        bs = student_bev_feat.size(0)\n+\n+        if student_bev_feat.size(1) != teacher_bev_feat.size(1):\n+            student_bev_feat = self.align(student_bev_feat)\n+\n+        if masks_bboxes is not None and self.add_stu_decode_bboxes:\n+            student_pred_bboxes_list = self.pred2bboxes(\n+                masks_bboxes\n+            )  # list of task : each shape of [(num_bboxes,9)]\n+\n+            # if bda_mat is not None:\n+            #     student_pred_bboxes_list = self.aug_boxes(student_pred_bboxes_list)\n+\n+        bev_feat_shape = torch.tensor(self.bev_shape)\n+        voxel_size = torch.tensor(self.voxel_size)\n+        feature_map_size = bev_feat_shape[:2]\n+\n+        if gt_bboxes_list is not None:\n+            device = student_bev_feat.device\n+\n+            gt_bboxes_list = [\n+                torch.cat(\n+                    (gt_bboxes.gravity_center, gt_bboxes.tensor[:, 3:]), dim=1\n+                ).to(device)\n+                for gt_bboxes in gt_bboxes_list\n+            ]\n+\n+            if self.add_stu_decode_bboxes:\n+                for idx, data in enumerate(student_pred_bboxes_list):\n+\n+                    bboxes_dim = data[\"bboxes\"].size(1)\n+\n+                    scroes_mask = data[\"scores\"] > self.score_threshold\n+\n+                    new_select_by_mask = data[\"bboxes\"][scroes_mask, :]\n+\n+                    gt_bboxes_list[idx] = torch.cat(\n+                        [gt_bboxes_list[idx], new_select_by_mask], dim=0\n+                    )\n+\n+            fg_map = student_bev_feat.new_zeros(\n+                (len(gt_bboxes_list), feature_map_size[1], feature_map_size[0])\n+            )\n+\n+            for idx in range(len(gt_bboxes_list)):\n+                num_objs = gt_bboxes_list[idx].shape[0]\n+\n+                for k in range(num_objs):\n+                    width = gt_bboxes_list[idx][k][3]\n+                    length = gt_bboxes_list[idx][k][4]\n+                    width = width / voxel_size[0] / self.shape_resize_times\n+                    length = length / voxel_size[1] / self.shape_resize_times\n+\n+                    if width > 0 and length > 0:\n+                        radius = gaussian_radius((length, width), min_overlap=0.1)\n+                        radius = max(1, int(radius))\n+\n+                        # be really careful for the coordinate system of\n+                        # your box annotation.\n+                        x, y, z = (\n+                            gt_bboxes_list[idx][k][0],\n+                            gt_bboxes_list[idx][k][1],\n+                            gt_bboxes_list[idx][k][2],\n+                        )\n+\n+                        coor_x = (\n+                            (x - self.pc_range[0])\n+                            / voxel_size[0]\n+                            / self.shape_resize_times\n+                        )\n+                        coor_y = (\n+                            (y - self.pc_range[1])\n+                            / voxel_size[1]\n+                            / self.shape_resize_times\n+                        )\n+\n+                        center = torch.tensor(\n+                            [coor_x, coor_y], dtype=torch.float32, device=device\n+                        )\n+                        center_int = center.to(torch.int32)\n+\n+                        # throw out not in range objects to avoid out of array\n+                        # area when creating the heatmap\n+                        if not (\n+                            0 <= center_int[0] < feature_map_size[0]\n+                            and 0 <= center_int[1] < feature_map_size[1]\n+                        ):\n+                            continue\n+\n+                        draw_heatmap_gaussian(fg_map[idx], center_int, radius)\n+\n+        if fg_map is None:\n+            fg_map = student_bev_feat.new_ones(\n+                (student_bev_feat.shape[0], feature_map_size[1], feature_map_size[0])\n+            )\n+\n+        if bs > 1:\n+            fg_map = fg_map.unsqueeze(1)\n+\n+        fit_loss = F.mse_loss(student_bev_feat, teacher_bev_feat, reduction=\"none\")\n+        fit_loss = torch.sum(fit_loss * fg_map) / torch.sum(fg_map)\n+\n+        return fit_loss * self.loss_weight, fg_map\n+\n+    # def forward(\n+    #     self, student_bev_feats, teacher_bev_feats, masks_bboxes, gt_bboxes_list\n+    # ):\n+    #     \"\"\"SelfLearningMFD forward.\n+\n+    #     Args:\n+    #         student_bev_feats (torch.tensor): _description_\n+    #         teacher_bev_feats (torch.tensor): _description_\n+    #         masks_bboxes (): _description_\n+    #         gt_bboxes_list (_type_): _description_\n+    #     \"\"\"\n+    #     assert isinstance(masks_bboxes, list)\n+    #     ret_dict = multi_apply(\n+    #         self._forward,\n+    #         student_bev_feats,\n+    #         teacher_bev_feats,\n+    #         masks_bboxes,\n+    #         gt_bboxes_list,\n+    #     )\n"
                },
                {
                    "date": 1716032637142,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1026,1033 +1026,4 @@\n     #         teacher_bev_feats,\n     #         masks_bboxes,\n     #         gt_bboxes_list,\n     #     )\n-import torch.nn as nn\n-import torch.nn.functional as F\n-import torch\n-from ..builder import LOSSES, build_loss\n-from ..utils.gaussian_utils import calculate_box_mask_gaussian\n-from torch import distributed as dist\n-from mmdet.core import multi_apply, build_bbox_coder\n-from mmdet3d.core import draw_heatmap_gaussian, gaussian_radius\n-from mmdet3d.core.bbox.coders.centerpoint_bbox_coders import (\n-    CenterPointBBoxCoder as bboxcoder,\n-)\n-from mmdet.models.losses import QualityFocalLoss\n-from mmcv.runner import force_fp32\n-import torchsort\n-import torchvision\n-import numpy as np\n-\n-\n-def get_world_size() -> int:\n-    if not dist.is_available():\n-        return 1\n-    if not dist.is_initialized():\n-        return 1\n-\n-    return dist.get_world_size()\n-\n-\n-def reduce_sum(tensor):\n-    world_size = get_world_size()\n-    if world_size < 2:\n-        return tensor\n-    tensor = tensor.clone()\n-    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n-    return tensor\n-\n-\n-def reduce_mean(tensor):\n-    return reduce_sum(tensor) / float(get_world_size())\n-\n-\n-def _sigmoid(x):\n-    # y = torch.clamp(x.sigmoid(), min=1e-3, max=1 - 1e-3)\n-    y = x.sigmoid()\n-    return y\n-\n-\n-def off_diagonal(x):\n-    # return a flattened view of the off-diagonal elements of a square matrix\n-    n, m = x.shape\n-    assert n == m\n-    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n-\n-\n-def response_list_wrap(response_lists: list):\n-    \"\"\"response_list_wrap\n-\n-    Args:\n-        response_lists (list): Pack list content\n-\n-    Returns:\n-        list: Return the new list of packaged integration\n-    \"\"\"\n-    assert len(response_lists) == 1\n-\n-    tmp_resp = []\n-    for response in response_lists:\n-        tmp_resp.append(response[0])\n-\n-    return tmp_resp\n-\n-\n-@LOSSES.register_module()\n-class QualityFocalLoss_(nn.Module):\n-    \"\"\"\n-    input[B,M,C] not sigmoid\n-    target[B,M,C], sigmoid\n-    \"\"\"\n-\n-    def __init__(self, beta=2.0):\n-\n-        super(QualityFocalLoss_, self).__init__()\n-        self.beta = beta\n-\n-    def forward(\n-        self,\n-        input: torch.Tensor,\n-        target: torch.Tensor,\n-        pos_normalizer=torch.tensor(1.0),\n-    ):\n-\n-        pred_sigmoid = torch.sigmoid(input)\n-        scale_factor = pred_sigmoid - target\n-\n-        # pred_sigmoid = torch.sigmoid(input)\n-        # scale_factor = input - target\n-        loss = F.binary_cross_entropy_with_logits(input, target, reduction=\"none\") * (\n-            scale_factor.abs().pow(self.beta)\n-        )\n-        loss /= torch.clamp(pos_normalizer, min=1.0)\n-        return loss\n-\n-\n-@LOSSES.register_module()\n-class SimpleL1(nn.Module):\n-    def __init__(self, criterion=\"L1\", student_ch=256, teacher_ch=512):\n-        super().__init__()\n-        self.criterion = criterion\n-        if criterion == \"L1\":\n-            self.criterion_loss = nn.L1Loss(reduce=False)\n-        elif criterion == \"SmoothL1\":\n-            self.criterion_loss = nn.SmoothL1Loss(reduce=False)\n-        elif criterion == \"MSE\":\n-            self.criterion_loss = nn.MSELoss(reduction=\"none\")\n-\n-        if student_ch != teacher_ch:\n-            self.align = nn.Conv2d(student_ch, teacher_ch, kernel_size=1)\n-\n-    def forward(self, feats1, feats2, *args, **kwargs):\n-        if self.criterion == \"MSE\":\n-            feats1 = self.align(feats1) if getattr(self, \"align\", None) else feats1\n-            losses = self.criterion_loss(feats1, feats2).mean()\n-            return losses\n-        else:\n-            return self.criterion_loss(feats1, feats2)\n-\n-\n-@LOSSES.register_module()\n-class Relevance_Distillation(nn.Module):\n-    def __init__(self, bs=2, bn_dim=512, lambd=0.0051):\n-        super().__init__()\n-        self.bs = bs\n-        self.bn_dim = bn_dim\n-        self.lambd = lambd\n-\n-        # normalization layer for the representations z1 and z2\n-        self.bn = nn.BatchNorm1d(self.bn_dim, affine=False)\n-\n-        self.align = nn.Conv2d(256, 512, kernel_size=1)\n-\n-    def forward(self, student_bev, teacher_bev, *args, **kwargs):\n-        student_bev = self.align(student_bev)\n-\n-        student_bev = student_bev.flatten(2)\n-        teacher_bev = teacher_bev.flatten(2)\n-\n-        # empirical cross-correlation matrix\n-        c = self.bn(student_bev).flatten(1).T @ self.bn(teacher_bev).flatten(1)\n-\n-        # sum the cross-correlation matrix between all gpus\n-        c.div_(self.bs)\n-        if self.bs == 1:\n-            pass\n-        else:\n-            torch.distributed.all_reduce(c)\n-\n-        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n-        off_diag = off_diagonal(c).pow_(2).sum()\n-        loss = on_diag + self.lambd * off_diag\n-        return loss\n-\n-\n-@LOSSES.register_module()\n-class FeatureLoss(nn.Module):\n-    \"\"\"PyTorch version of `Masked Generative Distillation`\n-\n-    Args:\n-        student_channels(int): Number of channels in the student's feature map.\n-        teacher_channels(int): Number of channels in the teacher's feature map.\n-        name (str): the loss name of the layer\n-        alpha_mgd (float, optional): Weight of dis_loss. Defaults to 0.00002\n-        lambda_mgd (float, optional): masked ratio. Defaults to 0.65\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        student_channels,\n-        teacher_channels,\n-        name,\n-        alpha_mgd=0.00002,\n-        lambda_mgd=0.65,\n-    ):\n-        super(FeatureLoss, self).__init__()\n-        self.alpha_mgd = alpha_mgd\n-        self.lambda_mgd = lambda_mgd\n-        self.name = name\n-\n-        if student_channels != teacher_channels:\n-            self.align = nn.Conv2d(\n-                student_channels, teacher_channels, kernel_size=1, stride=1, padding=0\n-            )\n-        else:\n-            self.align = None\n-\n-        self.generation = nn.Sequential(\n-            nn.Conv2d(teacher_channels, teacher_channels, kernel_size=3, padding=1),\n-            nn.ReLU(inplace=True),\n-            nn.Conv2d(teacher_channels, teacher_channels, kernel_size=3, padding=1),\n-        )\n-\n-    def forward(self, preds_S, preds_T, **kwargs):\n-        \"\"\"Forward function.\n-        Args:\n-            preds_S(Tensor): Bs*C*H*W, student's feature map\n-            preds_T(Tensor): Bs*C*H*W, teacher's feature map\n-        \"\"\"\n-        assert preds_S.shape[-2:] == preds_T.shape[-2:]\n-\n-        if self.align is not None:\n-            preds_S = self.align(preds_S)\n-\n-        loss = self.get_dis_loss(preds_S, preds_T) * self.alpha_mgd\n-\n-        return loss\n-\n-    def get_dis_loss(self, preds_S, preds_T):\n-        loss_mse = nn.MSELoss(reduction=\"sum\")\n-        N, C, H, W = preds_T.shape\n-\n-        device = preds_S.device\n-        mat = torch.rand((N, 1, H, W)).to(device)\n-        mat = torch.where(mat > 1 - self.lambda_mgd, 0, 1).to(device)\n-\n-        masked_fea = torch.mul(preds_S, mat)\n-        new_fea = self.generation(masked_fea)\n-\n-        dis_loss = loss_mse(new_fea, preds_T) / N\n-\n-        return dis_loss\n-\n-\n-@LOSSES.register_module()\n-class FeatureLoss_InnerClip(nn.Module):\n-    def __init__(\n-        self,\n-        x_sample_num=24,\n-        y_sample_num=24,\n-        inter_keypoint_weight=1,\n-        inter_channel_weight=10,\n-        enlarge_width=1.6,\n-        embed_channels=[256, 512],\n-        inner_feats_distill=None,\n-    ):\n-        super().__init__()\n-        self.x_sample_num = x_sample_num\n-        self.y_sample_num = y_sample_num\n-        self.inter_keypoint_weight = inter_keypoint_weight\n-        self.inter_channel_weight = inter_channel_weight\n-        self.enlarge_width = enlarge_width\n-\n-        self.img_view_transformer = None\n-\n-        self.embed_channels = embed_channels\n-\n-        self.imgbev_embed = nn.Sequential(\n-            nn.Conv2d(\n-                embed_channels[0],\n-                embed_channels[1],\n-                kernel_size=1,\n-                stride=1,\n-                padding=0,\n-                bias=False,\n-            ),\n-            nn.BatchNorm2d(embed_channels[1]),\n-        )\n-\n-        self.inner_feats_loss = (\n-            build_loss(inner_feats_distill) if inner_feats_distill is not None else None\n-        )\n-\n-    def get_gt_sample_grid(self, corner_points2d):\n-        dH_x, dH_y = corner_points2d[0] - corner_points2d[1]\n-        dW_x, dW_y = corner_points2d[0] - corner_points2d[2]\n-        raw_grid_x = (\n-            torch.linspace(\n-                corner_points2d[0][0], corner_points2d[1][0], self.x_sample_num\n-            )\n-            .view(1, -1)\n-            .repeat(self.y_sample_num, 1)\n-        )\n-        raw_grid_y = (\n-            torch.linspace(\n-                corner_points2d[0][1], corner_points2d[2][1], self.y_sample_num\n-            )\n-            .view(-1, 1)\n-            .repeat(1, self.x_sample_num)\n-        )\n-        raw_grid = torch.cat((raw_grid_x.unsqueeze(2), raw_grid_y.unsqueeze(2)), dim=2)\n-        raw_grid_x_offset = (\n-            torch.linspace(0, -dW_x, self.x_sample_num)\n-            .view(-1, 1)\n-            .repeat(1, self.y_sample_num)\n-        )\n-        raw_grid_y_offset = (\n-            torch.linspace(0, -dH_y, self.y_sample_num)\n-            .view(1, -1)\n-            .repeat(self.x_sample_num, 1)\n-        )\n-        raw_grid_offset = torch.cat(\n-            (raw_grid_x_offset.unsqueeze(2), raw_grid_y_offset.unsqueeze(2)), dim=2\n-        )\n-        grid = raw_grid + raw_grid_offset  # X_sample,Y_sample,2\n-        grid[:, :, 0] = torch.clip(\n-            (\n-                (\n-                    grid[:, :, 0]\n-                    - (\n-                        self.img_view_transformer[\"bx\"][0].to(grid.device)\n-                        - self.img_view_transformer[\"dx\"][0].to(grid.device) / 2.0\n-                    )\n-                )\n-                / self.img_view_transformer[\"dx\"][0].to(grid.device)\n-                / (self.img_view_transformer[\"nx\"][0].to(grid.device) - 1)\n-            )\n-            * 2.0\n-            - 1.0,\n-            min=-1.0,\n-            max=1.0,\n-        )\n-        grid[:, :, 1] = torch.clip(\n-            (\n-                (\n-                    grid[:, :, 1]\n-                    - (\n-                        self.img_view_transformer[\"bx\"][1].to(grid.device)\n-                        - self.img_view_transformer[\"dx\"][1].to(grid.device) / 2.0\n-                    )\n-                )\n-                / self.img_view_transformer[\"dx\"][1].to(grid.device)\n-                / (self.img_view_transformer[\"nx\"][1].to(grid.device) - 1)\n-            )\n-            * 2.0\n-            - 1.0,\n-            min=-1.0,\n-            max=1.0,\n-        )\n-\n-        return grid.unsqueeze(0)\n-\n-    def get_inner_feat(self, gt_bboxes_3d, img_feats, pts_feats):\n-        \"\"\"Use grid to sample features of key points\"\"\"\n-        device = img_feats.device\n-        dtype = img_feats[0].dtype\n-\n-        img_feats_sampled_list = []\n-        pts_feats_sampled_list = []\n-\n-        for sample_ind in torch.arange(len(gt_bboxes_3d)):\n-            img_feat = img_feats[sample_ind].unsqueeze(0)  # 1,C,H,W\n-            pts_feat = pts_feats[sample_ind].unsqueeze(0)  # 1,C,H,W\n-\n-            bbox_num, corner_num, point_num = gt_bboxes_3d[sample_ind].corners.shape\n-\n-            for bbox_ind in torch.arange(bbox_num):\n-                if self.enlarge_width > 0:\n-                    gt_sample_grid = self.get_gt_sample_grid(\n-                        gt_bboxes_3d[sample_ind]\n-                        .enlarged_box(self.enlarge_width)\n-                        .corners[bbox_ind][[0, 2, 4, 6], :-1]\n-                    ).to(device)\n-                else:\n-                    gt_sample_grid = self.get_gt_sample_grid(\n-                        gt_bboxes_3d[sample_ind].corners[bbox_ind][[0, 2, 4, 6], :-1]\n-                    ).to(\n-                        device\n-                    )  # 1,sample_y,sample_x,2\n-\n-                img_feats_sampled_list.append(\n-                    F.grid_sample(\n-                        img_feat,\n-                        grid=gt_sample_grid,\n-                        align_corners=False,\n-                        mode=\"bilinear\",\n-                    )\n-                )  # 'bilinear')) #all_bbox_num,C,y_sample,x_sample\n-                pts_feats_sampled_list.append(\n-                    F.grid_sample(\n-                        pts_feat,\n-                        grid=gt_sample_grid,\n-                        align_corners=False,\n-                        mode=\"bilinear\",\n-                    )\n-                )  # 'bilinear')) #all_bbox_num,C,y_sample,x_sample\n-\n-        return torch.cat(img_feats_sampled_list, dim=0), torch.cat(\n-            pts_feats_sampled_list, dim=0\n-        )\n-\n-    @force_fp32(apply_to=(\"img_feats_kd\", \"pts_feats_kd\"))\n-    def get_inter_channel_loss(self, img_feats_kd, pts_feats_kd):\n-        \"\"\"Calculate the inter-channel similarities, guide the student keypoint features to mimic the channel-wise relationships of the teacher`s\"\"\"\n-\n-        C_img = img_feats_kd.shape[1]\n-        C_pts = pts_feats_kd.shape[1]\n-        N = self.x_sample_num * self.y_sample_num\n-\n-        img_feats_kd = img_feats_kd.view(-1, C_img, N).matmul(\n-            img_feats_kd.view(-1, C_img, N).permute(0, 2, 1)\n-        )  # -1,N,N\n-        pts_feats_kd = pts_feats_kd.view(-1, C_pts, N).matmul(\n-            pts_feats_kd.view(-1, C_pts, N).permute(0, 2, 1)\n-        )\n-\n-        img_feats_kd = F.normalize(img_feats_kd, dim=2)\n-        pts_feats_kd = F.normalize(pts_feats_kd, dim=2)\n-\n-        loss_inter_channel = F.mse_loss(img_feats_kd, pts_feats_kd, reduction=\"none\")\n-        loss_inter_channel = loss_inter_channel.sum(-1)\n-        loss_inter_channel = loss_inter_channel.mean()\n-        loss_inter_channel = self.inter_channel_weight * loss_inter_channel\n-        return loss_inter_channel\n-\n-    def forward(self, student_feats, teacher_feats, gt_bboxes_list, **kwargs):\n-\n-        self.img_view_transformer = kwargs.get(\"ivt_cfg\")\n-\n-        if student_feats.size(1) != teacher_feats.size(1):\n-            student_feats = self.imgbev_embed(student_feats)\n-\n-        if self.inter_keypoint_weight > 0 or self.inter_channel_weight > 0:\n-            img_feats_kd, pts_feats_kd = self.get_inner_feat(\n-                gt_bboxes_list, student_feats, teacher_feats\n-            )\n-\n-        if self.inner_feats_loss:\n-            return self.inner_feats_loss(img_feats_kd, pts_feats_kd)\n-\n-        # if self.inter_keypoint_weight > 0:\n-        #     loss_inter_keypoint = self.get_inter_keypoint_loss(\n-        #         img_feats_kd, pts_feats_kd\n-        #     )\n-        #     losses.update({\"loss_inter_keypoint_img_bev\": loss_inter_keypoint})\n-\n-        if self.inter_channel_weight > 0:\n-            loss_inter_channel = self.get_inter_channel_loss(img_feats_kd, pts_feats_kd)\n-\n-        return loss_inter_channel\n-\n-\n-@LOSSES.register_module()\n-class FeatureLoss_Affinity(nn.Module):\n-    def __init__(self):\n-        super().__init__()\n-\n-    def forward(self, student_feats, teacher_feats, *args, **kwargs):\n-        student_feats = [student_feats]\n-        teacher_feats = [teacher_feats]\n-\n-        feature_ditill_loss = 0.0\n-\n-        resize_shape = student_feats[-1].shape[-2:]\n-        if isinstance(student_feats, list):\n-            for i in range(len(student_feats)):\n-                feature_target = teacher_feats[i].detach()\n-                feature_pred = student_feats[i]\n-\n-                B, C, H, W = student_feats[-1].shape\n-\n-                if student_feats[-1].size(-1) != teacher_feats[-1].size(-1):\n-                    feature_pred_down = F.interpolate(\n-                        feature_pred, size=resize_shape, mode=\"bilinear\"\n-                    )\n-                    feature_target_down = F.interpolate(\n-                        feature_target, size=resize_shape, mode=\"bilinear\"\n-                    )\n-                else:\n-                    feature_pred_down = feature_pred\n-                    feature_target_down = feature_target\n-\n-                feature_target_down = feature_target_down.reshape(B, C, -1)\n-                depth_affinity = torch.bmm(\n-                    feature_target_down.permute(0, 2, 1), feature_target_down\n-                )\n-\n-                feature_pred_down = feature_pred_down.reshape(B, C, -1)\n-                rgb_affinity = torch.bmm(\n-                    feature_pred_down.permute(0, 2, 1), feature_pred_down\n-                )\n-\n-                feature_ditill_loss = (\n-                    feature_ditill_loss\n-                    + F.l1_loss(rgb_affinity, depth_affinity, reduction=\"mean\") / B\n-                )\n-\n-        else:\n-            raise NotImplementedError\n-\n-        return feature_ditill_loss\n-\n-\n-@LOSSES.register_module()\n-class FeatureLoss_Coefficient(nn.Module):\n-    def __init__(self, **kwargs):\n-        super().__init__()\n-\n-    def corrcoef(self, target, pred):\n-        pred_n = pred - pred.mean()\n-        target_n = target - target.mean()\n-        pred_n = pred_n / pred_n.norm()\n-        target_n = target_n / target_n.norm()\n-        return (pred_n * target_n).sum()\n-\n-    def forward(\n-        self,\n-        pred,\n-        target,\n-        regularization=\"l2\",\n-        regularization_strength=1.0,\n-    ):\n-        pred = [pred.clone()]\n-        target = [target.clone()]\n-        spearman_loss = 0.0\n-\n-        resize_shape = pred[-1].shape[-2:]  # save training time\n-\n-        if isinstance(pred, list):\n-            for i in range(len(pred)):\n-                feature_target = target[i]\n-                feature_pred = pred[i]\n-                B, C, H, W = feature_pred.shape\n-\n-                feature_pred_down = F.interpolate(\n-                    feature_pred, size=resize_shape, mode=\"bilinear\"\n-                )\n-                feature_target_down = F.interpolate(\n-                    feature_target, size=resize_shape, mode=\"bilinear\"\n-                )\n-\n-                # if feature_pred.size(-1) != feature_target.size(-1):\n-\n-                #     feature_pred_down = F.interpolate(\n-                #         feature_pred, size=resize_shape, mode=\"bilinear\"\n-                #     )\n-                #     feature_target_down = F.interpolate(\n-                #         feature_target, size=resize_shape, mode=\"bilinear\"\n-                #     )\n-                # else:\n-                #     feature_pred_down = feature_pred\n-                #     feature_target_down = feature_target\n-\n-                feature_pred_down = feature_pred_down.reshape(B, -1)\n-                feature_target_down = feature_target_down.reshape(B, -1)\n-\n-                feature_pred_down = torchsort.soft_rank(\n-                    feature_pred_down,\n-                    regularization=regularization,\n-                    regularization_strength=regularization_strength,\n-                )\n-                spearman_loss += 1 - self.corrcoef(\n-                    feature_target_down, feature_pred_down / feature_pred_down.shape[-1]\n-                )\n-\n-        return spearman_loss\n-\n-\n-@LOSSES.register_module()\n-class Radar_MSDistilll(nn.Module):\n-    def __init__(self, num_layers=2, each_layer_loss_cfg=[]):\n-        super().__init__()\n-        self.num_layers = num_layers\n-        assert num_layers == len(each_layer_loss_cfg)\n-        for idx, cfg_dict in enumerate(each_layer_loss_cfg):\n-            setattr(self, f\"layer_loss_{idx}\", build_loss(cfg_dict))\n-\n-    def unify_feat_size(self, student_feat, teacher_feat):\n-        bs, s_c, s_w, s_h = student_feat.shape\n-        bs, t_c, t_w, t_h = teacher_feat.shape\n-        if s_w == t_w:\n-            return student_feat, teacher_feat\n-        else:\n-            teacher_feat = F.interpolate(teacher_feat, (s_w, s_h), mode=\"bilinear\")\n-            return student_feat, teacher_feat\n-\n-    def forward(self, radar_ms_feats, pts_ms_feats, gt_bboxes_3d=None):\n-        assert isinstance(radar_ms_feats, list)\n-        losses = 0.0\n-\n-        for idx in range(self.num_layers):\n-            tmp_radar_feats, tmp_pts_feats = self.unify_feat_size(\n-                radar_ms_feats[idx], pts_ms_feats[idx]\n-            )\n-\n-            if gt_bboxes_3d is not None:\n-                losses += getattr(self, f\"layer_loss_{idx}\")(\n-                    tmp_radar_feats, tmp_pts_feats, gt_bboxes_3d\n-                )[0]\n-            else:\n-                losses += getattr(self, f\"layer_loss_{idx}\")(\n-                    tmp_radar_feats, tmp_pts_feats, gt_bboxes_3d\n-                )\n-\n-        return losses / self.num_layers\n-\n-\n-@LOSSES.register_module()\n-class InfoMax(nn.Module):\n-    def __init__(self, **kwargs):\n-        super().__init__()\n-\n-    def forward(self, x1, x2):\n-        import pdb\n-\n-        pdb.set_trace()\n-        x1 = x1 / (torch.norm(x1, p=2, dim=1, keepdim=True) + 1e-10)\n-        x2 = x2 / (torch.norm(x2, p=2, dim=1, keepdim=True) + 1e-10)\n-        bs = x1.size(0)\n-        s = torch.matmul(x1, x2.permute(1, 0))\n-        mask_joint = torch.eye(bs).cuda()\n-        mask_marginal = 1 - mask_joint\n-\n-        Ej = (s * mask_joint).mean()\n-        Em = torch.exp(s * mask_marginal).mean()\n-        # decoupled comtrastive learning?!!!!\n-        # infomax_loss = - (Ej - torch.log(Em)) * self.alpha\n-        infomax_loss = -(Ej - torch.log(Em))  # / Em\n-        return infomax_loss\n-\n-\n-@LOSSES.register_module()\n-class HeatMapAug(nn.Module):\n-    def __init__(self):\n-        super().__init__()\n-\n-    @force_fp32(apply_to=(\"stu_pred\", \"tea_pred\"))\n-    def forward(self, stu_pred, tea_pred, fg_map):\n-\n-        num_task = len(stu_pred)\n-        kl_loss = 0\n-        for task_id in range(num_task):\n-            student_pred = stu_pred[task_id][0][\"heatmap\"].sigmoid()\n-            teacher_pred = tea_pred[task_id][0][\"heatmap\"].sigmoid()\n-            fg_map = fg_map.unsqueeze(1)\n-            task_kl_loss = F.binary_cross_entropy(student_pred, teacher_pred.detach())\n-            task_kl_loss = torch.sum(task_kl_loss * fg_map) / torch.sum(fg_map)\n-            kl_loss += task_kl_loss\n-\n-        return kl_loss\n-\n-\n-@LOSSES.register_module()\n-class Dc_ResultDistill(nn.Module):\n-    def __init__(\n-        self,\n-        pc_range=[],\n-        voxel_size=[],\n-        out_size_scale=8,\n-        ret_sum=False,\n-        loss_weight_reg=10,\n-        loss_weight_cls=10,\n-        max_cls=True,\n-    ):\n-        super().__init__()\n-\n-        self.pc_range = pc_range\n-        self.voxel_size = voxel_size\n-        self.out_size_scale = out_size_scale\n-        self.ret_sum = ret_sum\n-        self.loss_weight_reg = loss_weight_reg\n-        self.loss_weight_cls = loss_weight_cls\n-        self.max_cls = max_cls\n-\n-    def forward(self, resp_lidar, resp_fuse, gt_boxes):\n-        \"\"\"Dc_ResultDistill forward.\n-\n-        Args:\n-            resp_lidar (_type_):\n-            resp_fuse (_type_):\n-            gt_boxes (_type_):\n-\n-        Returns:\n-            _type_: _description_\n-        \"\"\"\n-\n-        tmp_resp_lidar = []\n-        tmp_resp_fuse = []\n-        for res_lidar, res_fuse in zip(resp_lidar, resp_fuse):\n-            tmp_resp_lidar.append(res_lidar[0])\n-            tmp_resp_fuse.append(res_fuse[0])\n-\n-        tmp_gt_boxes = []\n-        for bs_idx in range(len(gt_boxes)):\n-\n-            gt_bboxes_3d = torch.cat(\n-                (gt_boxes[bs_idx].gravity_center, gt_boxes[bs_idx].tensor[:, 3:]), dim=1\n-            )\n-\n-            tmp_gt_boxes.append(gt_bboxes_3d)\n-\n-        cls_lidar = []\n-        reg_lidar = []\n-        cls_fuse = []\n-        reg_fuse = []\n-\n-        # criterion = nn.L1Loss(reduce=False)\n-        # criterion_cls = QualityFocalLoss_()\n-\n-        criterion = nn.SmoothL1Loss(reduce=False)\n-        criterion_cls = nn.L1Loss(reduce=False)\n-\n-        for task_id, task_out in enumerate(tmp_resp_lidar):\n-            cls_lidar.append(task_out[\"heatmap\"])\n-            cls_fuse.append(_sigmoid(tmp_resp_fuse[task_id][\"heatmap\"] / 2))\n-            reg_lidar.append(\n-                torch.cat(\n-                    [\n-                        task_out[\"reg\"],\n-                        task_out[\"height\"],\n-                        task_out[\"dim\"],\n-                        task_out[\"rot\"],\n-                        task_out[\"vel\"],\n-                        # task_out[\"iou\"],\n-                    ],\n-                    dim=1,\n-                )\n-            )\n-            reg_fuse.append(\n-                torch.cat(\n-                    [\n-                        tmp_resp_fuse[task_id][\"reg\"],\n-                        tmp_resp_fuse[task_id][\"height\"],\n-                        tmp_resp_fuse[task_id][\"dim\"],\n-                        tmp_resp_fuse[task_id][\"rot\"],\n-                        tmp_resp_fuse[task_id][\"vel\"],\n-                        # resp_fuse[task_id][\"iou\"],\n-                    ],\n-                    dim=1,\n-                )\n-            )\n-        cls_lidar = torch.cat(cls_lidar, dim=1)\n-        reg_lidar = torch.cat(reg_lidar, dim=1)\n-        cls_fuse = torch.cat(cls_fuse, dim=1)\n-        reg_fuse = torch.cat(reg_fuse, dim=1)\n-\n-        if self.max_cls:\n-            cls_lidar_max, _ = torch.max(cls_lidar, dim=1)\n-            cls_fuse_max, _ = torch.max(cls_fuse, dim=1)\n-        else:\n-            _, _, ht_h, ht_w = cls_fuse.shape\n-            cls_lidar_max = cls_lidar.flatten(2).permute(0, 2, 1)\n-            cls_fuse_max = cls_fuse.flatten(2).permute(0, 2, 1)\n-\n-        gaussian_mask = calculate_box_mask_gaussian(\n-            reg_lidar.shape,\n-            tmp_gt_boxes,\n-            self.pc_range,\n-            self.voxel_size,\n-            self.out_size_scale,\n-        )\n-\n-        # # diff_reg = criterion(reg_lidar, reg_fuse)\n-        # # diff_cls = criterion(cls_lidar_max, cls_fuse_max)\n-        # diff_reg = criterion_reg(reg_lidar, reg_fuse)\n-\n-        # cls_lidar_max = torch.sigmoid(cls_lidar_max)\n-        # diff_cls = criterion_cls(\n-        #     cls_fuse_max, cls_lidar_max\n-        # )  # Compared with directly using the L1 loss constraint, replace it with bce focal loss and change the position?\n-\n-        # weight = gaussian_mask.sum()\n-        # weight = reduce_mean(weight)\n-\n-        # # diff_cls = criterion_cls(cls_lidar_max, cls_fuse_max)\n-        # diff_reg = torch.mean(diff_reg, dim=1)\n-        # diff_reg = diff_reg * gaussian_mask\n-        # loss_reg_distill = torch.sum(diff_reg) / (weight + 1e-4)\n-\n-        # if not self.max_cls:\n-        #     diff_cls = diff_cls\n-        #     loss_cls_distill = diff_cls.sum() * 2\n-        # else:\n-        #     diff_cls = diff_cls * gaussian_mask\n-        #     loss_cls_distill = torch.sum(diff_cls) / (weight + 1e-4)\n-\n-        diff_reg = criterion(reg_lidar, reg_fuse)\n-\n-        diff_cls = criterion_cls(cls_lidar_max, cls_fuse_max)\n-        diff_reg = torch.mean(diff_reg, dim=1)\n-        diff_reg = diff_reg * gaussian_mask\n-        diff_cls = diff_cls * gaussian_mask\n-        weight = gaussian_mask.sum()\n-        weight = reduce_mean(weight)\n-        loss_reg_distill = torch.sum(diff_reg) / (weight + 1e-4)\n-        loss_cls_distill = torch.sum(diff_cls) / (weight + 1e-4)\n-\n-        if self.ret_sum:\n-\n-            loss_det_distill = self.loss_weight * (loss_reg_distill + loss_cls_distill)\n-\n-            return loss_det_distill\n-        else:\n-\n-            return (\n-                self.loss_weight_reg * loss_reg_distill,\n-                self.loss_weight_cls * loss_cls_distill,\n-            )\n-\n-\n-@LOSSES.register_module()\n-class SelfLearningMFD(nn.Module):\n-    def __init__(\n-        self,\n-        bev_shape=[128, 128],\n-        pc_range=[-51.2, -51.2, -5, 51.2, 51.2, 3],\n-        voxel_size=[0.1, 0.1, 0.1],\n-        score_threshold=0.7,\n-        add_stu_decode_bboxes=False,\n-        loss_weight=1e-2,\n-        bbox_coder=None,\n-        student_channels=256,\n-        teacher_channels=512,\n-    ):\n-        super().__init__()\n-        self.bev_shape = bev_shape\n-        self.pc_range = pc_range\n-        self.voxel_size = voxel_size\n-        self.shape_resize_times = bbox_coder.out_size_factor if bbox_coder else 8\n-        self.score_threshold = score_threshold\n-        self.add_stu_decode_bboxes = add_stu_decode_bboxes\n-        self.loss_weight = loss_weight\n-\n-        self.align = nn.Sequential(\n-            nn.Conv2d(student_channels, teacher_channels, kernel_size=1, padding=0),\n-            # nn.BatchNorm2d(teacher_channels),\n-        )\n-\n-        if bbox_coder is not None:\n-            self.bbox_coder = build_bbox_coder(bbox_coder)\n-\n-    def pred2bboxes(self, preds_dict):\n-        \"\"\"SelfLearningMFD-pred2bboxes forward\n-\n-        Args:\n-            pred_bboxes (list):\n-            [task1[[task_head_reg,task_head_heatmap,...,]],\n-            task2...,\n-            taskn]\n-        \"\"\"\n-\n-        for task_id, pred_data in enumerate(preds_dict):\n-            bath_tmp = []\n-            batch_size = pred_data[0][\"heatmap\"].shape[0]\n-            batch_heatmap = pred_data[0][\"heatmap\"].sigmoid()\n-            batch_reg = pred_data[0][\"reg\"]\n-            batch_hei = pred_data[0][\"height\"]\n-\n-            # denormalization\n-            batch_dim = torch.exp(pred_data[0][\"dim\"])\n-\n-            batch_rot_sin = pred_data[0][\"rot\"][:, 0].unsqueeze(1)\n-            batch_rot_cos = pred_data[0][\"rot\"][:, 1].unsqueeze(1)\n-\n-            if \"vel\" in pred_data[0].keys():\n-                batch_vel = pred_data[0][\"vel\"]\n-\n-            bath_tmp.append(batch_heatmap)\n-            bath_tmp.append(batch_rot_sin)\n-            bath_tmp.append(batch_rot_cos)\n-            bath_tmp.append(batch_hei)\n-            bath_tmp.append(batch_dim)\n-            bath_tmp.append(batch_vel)\n-            bath_tmp.append(batch_reg)\n-\n-            bboxes_decode = self.bbox_coder.decode(*bath_tmp)\n-\n-        return bboxes_decode\n-\n-    def aug_boxes(self, gt_boxes, rot_mat):\n-        rot_lim = (-22.5, 22.5)\n-        scale_lim = (0.95, 1.05)\n-        rotate_angle = np.random.uniform(*rot_lim)\n-        scale_ratio = np.random.uniform(*scale_lim)\n-        gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n-        gt_boxes[:, 3:6] *= scale_ratio\n-        gt_boxes[:, 6] += rotate_angle\n-        gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n-        gt_boxes[:, 6] = -gt_boxes[:, 6]\n-        gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(-1)\n-        gt_boxes[:, :3] = gt_boxes[:, :3]\n-\n-        return gt_boxes\n-\n-    @force_fp32(\n-        apply_to=(\n-            \"student_bev_feat\",\n-            \"teacher_bev_feat\",\n-        )\n-    )\n-    def forward(\n-        self,\n-        student_bev_feat,\n-        teacher_bev_feat,\n-        gt_bboxes_list=None,\n-        masks_bboxes=None,\n-        bda_mat=None,\n-    ):\n-        \"\"\"SelfLearningMFD forward.\n-\n-        Args:\n-            student_bev_feats (torch.tensor): Calculate student feats\n-            teacher_bev_feats (torch.tensor): Calculate teacher feats\n-            masks_bboxes (list): Self-learning mask detection\n-            gt_bboxes_list (list): [LiDARInstance3DBoxes(gravity_center+tensor(num_objs,9))]\n-\n-        Returns:\n-            dict: _description_\n-        \"\"\"\n-\n-        bs = student_bev_feat.size(0)\n-\n-        if student_bev_feat.size(1) != teacher_bev_feat.size(1):\n-            student_bev_feat = self.align(student_bev_feat)\n-\n-        if masks_bboxes is not None and self.add_stu_decode_bboxes:\n-            student_pred_bboxes_list = self.pred2bboxes(\n-                masks_bboxes\n-            )  # list of task : each shape of [(num_bboxes,9)]\n-\n-            # if bda_mat is not None:\n-            #     student_pred_bboxes_list = self.aug_boxes(student_pred_bboxes_list)\n-\n-        bev_feat_shape = torch.tensor(self.bev_shape)\n-        voxel_size = torch.tensor(self.voxel_size)\n-        feature_map_size = bev_feat_shape[:2]\n-\n-        if gt_bboxes_list is not None:\n-            device = student_bev_feat.device\n-\n-            gt_bboxes_list = [\n-                torch.cat(\n-                    (gt_bboxes.gravity_center, gt_bboxes.tensor[:, 3:]), dim=1\n-                ).to(device)\n-                for gt_bboxes in gt_bboxes_list\n-            ]\n-\n-            if self.add_stu_decode_bboxes:\n-                for idx, data in enumerate(student_pred_bboxes_list):\n-\n-                    bboxes_dim = data[\"bboxes\"].size(1)\n-\n-                    scroes_mask = data[\"scores\"] > self.score_threshold\n-\n-                    new_select_by_mask = data[\"bboxes\"][scroes_mask, :]\n-\n-                    gt_bboxes_list[idx] = torch.cat(\n-                        [gt_bboxes_list[idx], new_select_by_mask], dim=0\n-                    )\n-\n-            fg_map = student_bev_feat.new_zeros(\n-                (len(gt_bboxes_list), feature_map_size[1], feature_map_size[0])\n-            )\n-\n-            for idx in range(len(gt_bboxes_list)):\n-                num_objs = gt_bboxes_list[idx].shape[0]\n-\n-                for k in range(num_objs):\n-                    width = gt_bboxes_list[idx][k][3]\n-                    length = gt_bboxes_list[idx][k][4]\n-                    width = width / voxel_size[0] / self.shape_resize_times\n-                    length = length / voxel_size[1] / self.shape_resize_times\n-\n-                    if width > 0 and length > 0:\n-                        radius = gaussian_radius((length, width), min_overlap=0.1)\n-                        radius = max(1, int(radius))\n-\n-                        # be really careful for the coordinate system of\n-                        # your box annotation.\n-                        x, y, z = (\n-                            gt_bboxes_list[idx][k][0],\n-                            gt_bboxes_list[idx][k][1],\n-                            gt_bboxes_list[idx][k][2],\n-                        )\n-\n-                        coor_x = (\n-                            (x - self.pc_range[0])\n-                            / voxel_size[0]\n-                            / self.shape_resize_times\n-                        )\n-                        coor_y = (\n-                            (y - self.pc_range[1])\n-                            / voxel_size[1]\n-                            / self.shape_resize_times\n-                        )\n-\n-                        center = torch.tensor(\n-                            [coor_x, coor_y], dtype=torch.float32, device=device\n-                        )\n-                        center_int = center.to(torch.int32)\n-\n-                        # throw out not in range objects to avoid out of array\n-                        # area when creating the heatmap\n-                        if not (\n-                            0 <= center_int[0] < feature_map_size[0]\n-                            and 0 <= center_int[1] < feature_map_size[1]\n-                        ):\n-                            continue\n-\n-                        draw_heatmap_gaussian(fg_map[idx], center_int, radius)\n-\n-        if fg_map is None:\n-            fg_map = student_bev_feat.new_ones(\n-                (student_bev_feat.shape[0], feature_map_size[1], feature_map_size[0])\n-            )\n-\n-        if bs > 1:\n-            fg_map = fg_map.unsqueeze(1)\n-\n-        fit_loss = F.mse_loss(student_bev_feat, teacher_bev_feat, reduction=\"none\")\n-        fit_loss = torch.sum(fit_loss * fg_map) / torch.sum(fg_map)\n-\n-        return fit_loss * self.loss_weight, fg_map\n-\n-    # def forward(\n-    #     self, student_bev_feats, teacher_bev_feats, masks_bboxes, gt_bboxes_list\n-    # ):\n-    #     \"\"\"SelfLearningMFD forward.\n-\n-    #     Args:\n-    #         student_bev_feats (torch.tensor): _description_\n-    #         teacher_bev_feats (torch.tensor): _description_\n-    #         masks_bboxes (): _description_\n-    #         gt_bboxes_list (_type_): _description_\n-    #     \"\"\"\n-    #     assert isinstance(masks_bboxes, list)\n-    #     ret_dict = multi_apply(\n-    #         self._forward,\n-    #         student_bev_feats,\n-    #         teacher_bev_feats,\n-    #         masks_bboxes,\n-    #         gt_bboxes_list,\n-    #     )\n"
                },
                {
                    "date": 1716032779764,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -804,8 +804,9 @@\n         score_threshold=0.7,\n         add_stu_decode_bboxes=False,\n         loss_weight=1e-2,\n         bbox_coder=None,\n+        shape_resize_times=8,\n         student_channels=256,\n         teacher_channels=512,\n     ):\n         super().__init__()\n"
                },
                {
                    "date": 1716032870723,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -804,9 +804,9 @@\n         score_threshold=0.7,\n         add_stu_decode_bboxes=False,\n         loss_weight=1e-2,\n         bbox_coder=None,\n-        shape_resize_times=8,\n+        shape_resize_times=None,\n         student_channels=256,\n         teacher_channels=512,\n     ):\n         super().__init__()\n"
                },
                {
                    "date": 1716032924130,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -812,9 +812,9 @@\n         super().__init__()\n         self.bev_shape = bev_shape\n         self.pc_range = pc_range\n         self.voxel_size = voxel_size\n-        self.shape_resize_times = bbox_coder.out_size_factor if bbox_coder else 8\n+        self.shape_resize_times = bbox_coder.out_size_factor if bbox_coder else shape_resize_times\n         self.score_threshold = score_threshold\n         self.add_stu_decode_bboxes = add_stu_decode_bboxes\n         self.loss_weight = loss_weight\n \n"
                },
                {
                    "date": 1716032930167,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -812,9 +812,11 @@\n         super().__init__()\n         self.bev_shape = bev_shape\n         self.pc_range = pc_range\n         self.voxel_size = voxel_size\n-        self.shape_resize_times = bbox_coder.out_size_factor if bbox_coder else shape_resize_times\n+        self.shape_resize_times = (\n+            bbox_coder.out_size_factor if bbox_coder else shape_resize_times\n+        )\n         self.score_threshold = score_threshold\n         self.add_stu_decode_bboxes = add_stu_decode_bboxes\n         self.loss_weight = loss_weight\n \n"
                },
                {
                    "date": 1716032963820,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -817,8 +817,9 @@\n         if bbox_coder:\n             self.shape_resize_times = bbox_coder.out_size_factor\n         elif shape_resize_times:\n             self.shape_resize_times=shape_resize_times\n+            \n         self.score_threshold = score_threshold\n         self.add_stu_decode_bboxes = add_stu_decode_bboxes\n         self.loss_weight = loss_weight\n \n"
                },
                {
                    "date": 1716602748408,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,1036 @@\n+import torch.nn as nn\n+import torch.nn.functional as F\n+import torch\n+from ..builder import LOSSES, build_loss\n+from ..utils.gaussian_utils import calculate_box_mask_gaussian\n+from torch import distributed as dist\n+from mmdet.core import multi_apply, build_bbox_coder\n+from mmdet3d.core import draw_heatmap_gaussian, gaussian_radius\n+from mmdet3d.core.bbox.coders.centerpoint_bbox_coders import (\n+    CenterPointBBoxCoder as bboxcoder,\n+)\n+from mmdet.models.losses import QualityFocalLoss\n+from mmcv.runner import force_fp32\n+import torchsort\n+import torchvision\n+import numpy as np\n+\n+\n+def get_world_size() -> int:\n+    if not dist.is_available():\n+        return 1\n+    if not dist.is_initialized():\n+        return 1\n+\n+    return dist.get_world_size()\n+\n+\n+def reduce_sum(tensor):\n+    world_size = get_world_size()\n+    if world_size < 2:\n+        return tensor\n+    tensor = tensor.clone()\n+    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n+    return tensor\n+\n+\n+def reduce_mean(tensor):\n+    return reduce_sum(tensor) / float(get_world_size())\n+\n+\n+def _sigmoid(x):\n+    # y = torch.clamp(x.sigmoid(), min=1e-3, max=1 - 1e-3)\n+    y = x.sigmoid()\n+    return y\n+\n+\n+def off_diagonal(x):\n+    # return a flattened view of the off-diagonal elements of a square matrix\n+    n, m = x.shape\n+    assert n == m\n+    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n+\n+\n+def response_list_wrap(response_lists: list):\n+    \"\"\"response_list_wrap\n+\n+    Args:\n+        response_lists (list): Pack list content\n+\n+    Returns:\n+        list: Return the new list of packaged integration\n+    \"\"\"\n+    assert len(response_lists) == 1\n+\n+    tmp_resp = []\n+    for response in response_lists:\n+        tmp_resp.append(response[0])\n+\n+    return tmp_resp\n+\n+\n+@LOSSES.register_module()\n+class QualityFocalLoss_(nn.Module):\n+    \"\"\"\n+    input[B,M,C] not sigmoid\n+    target[B,M,C], sigmoid\n+    \"\"\"\n+\n+    def __init__(self, beta=2.0):\n+\n+        super(QualityFocalLoss_, self).__init__()\n+        self.beta = beta\n+\n+    def forward(\n+        self,\n+        input: torch.Tensor,\n+        target: torch.Tensor,\n+        pos_normalizer=torch.tensor(1.0),\n+    ):\n+\n+        pred_sigmoid = torch.sigmoid(input)\n+        scale_factor = pred_sigmoid - target\n+\n+        # pred_sigmoid = torch.sigmoid(input)\n+        # scale_factor = input - target\n+        loss = F.binary_cross_entropy_with_logits(input, target, reduction=\"none\") * (\n+            scale_factor.abs().pow(self.beta)\n+        )\n+        loss /= torch.clamp(pos_normalizer, min=1.0)\n+        return loss\n+\n+\n+@LOSSES.register_module()\n+class SimpleL1(nn.Module):\n+    def __init__(self, criterion=\"L1\", student_ch=256, teacher_ch=512):\n+        super().__init__()\n+        self.criterion = criterion\n+        if criterion == \"L1\":\n+            self.criterion_loss = nn.L1Loss(reduce=False)\n+        elif criterion == \"SmoothL1\":\n+            self.criterion_loss = nn.SmoothL1Loss(reduce=False)\n+        elif criterion == \"MSE\":\n+            self.criterion_loss = nn.MSELoss(reduction=\"none\")\n+\n+        if student_ch != teacher_ch:\n+            self.align = nn.Conv2d(student_ch, teacher_ch, kernel_size=1)\n+\n+    def forward(self, feats1, feats2, *args, **kwargs):\n+        if self.criterion == \"MSE\":\n+            feats1 = self.align(feats1) if getattr(self, \"align\", None) else feats1\n+            losses = self.criterion_loss(feats1, feats2).mean()\n+            return losses\n+        else:\n+            return self.criterion_loss(feats1, feats2)\n+\n+\n+@LOSSES.register_module()\n+class Relevance_Distillation(nn.Module):\n+    def __init__(self, bs=2, bn_dim=512, lambd=0.0051):\n+        super().__init__()\n+        self.bs = bs\n+        self.bn_dim = bn_dim\n+        self.lambd = lambd\n+\n+        # normalization layer for the representations z1 and z2\n+        self.bn = nn.BatchNorm1d(self.bn_dim, affine=False)\n+\n+        self.align = nn.Conv2d(256, 512, kernel_size=1)\n+\n+    def forward(self, student_bev, teacher_bev, *args, **kwargs):\n+        student_bev = self.align(student_bev)\n+\n+        student_bev = student_bev.flatten(2)\n+        teacher_bev = teacher_bev.flatten(2)\n+\n+        # empirical cross-correlation matrix\n+        c = self.bn(student_bev).flatten(1).T @ self.bn(teacher_bev).flatten(1)\n+\n+        # sum the cross-correlation matrix between all gpus\n+        c.div_(self.bs)\n+        if self.bs == 1:\n+            pass\n+        else:\n+            torch.distributed.all_reduce(c)\n+\n+        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n+        off_diag = off_diagonal(c).pow_(2).sum()\n+        loss = on_diag + self.lambd * off_diag\n+        return loss\n+\n+\n+@LOSSES.register_module()\n+class FeatureLoss(nn.Module):\n+    \"\"\"PyTorch version of `Masked Generative Distillation`\n+\n+    Args:\n+        student_channels(int): Number of channels in the student's feature map.\n+        teacher_channels(int): Number of channels in the teacher's feature map.\n+        name (str): the loss name of the layer\n+        alpha_mgd (float, optional): Weight of dis_loss. Defaults to 0.00002\n+        lambda_mgd (float, optional): masked ratio. Defaults to 0.65\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        student_channels,\n+        teacher_channels,\n+        name,\n+        alpha_mgd=0.00002,\n+        lambda_mgd=0.65,\n+    ):\n+        super(FeatureLoss, self).__init__()\n+        self.alpha_mgd = alpha_mgd\n+        self.lambda_mgd = lambda_mgd\n+        self.name = name\n+\n+        if student_channels != teacher_channels:\n+            self.align = nn.Conv2d(\n+                student_channels, teacher_channels, kernel_size=1, stride=1, padding=0\n+            )\n+        else:\n+            self.align = None\n+\n+        self.generation = nn.Sequential(\n+            nn.Conv2d(teacher_channels, teacher_channels, kernel_size=3, padding=1),\n+            nn.ReLU(inplace=True),\n+            nn.Conv2d(teacher_channels, teacher_channels, kernel_size=3, padding=1),\n+        )\n+\n+    def forward(self, preds_S, preds_T, **kwargs):\n+        \"\"\"Forward function.\n+        Args:\n+            preds_S(Tensor): Bs*C*H*W, student's feature map\n+            preds_T(Tensor): Bs*C*H*W, teacher's feature map\n+        \"\"\"\n+        assert preds_S.shape[-2:] == preds_T.shape[-2:]\n+\n+        if self.align is not None:\n+            preds_S = self.align(preds_S)\n+\n+        loss = self.get_dis_loss(preds_S, preds_T) * self.alpha_mgd\n+\n+        return loss\n+\n+    def get_dis_loss(self, preds_S, preds_T):\n+        loss_mse = nn.MSELoss(reduction=\"sum\")\n+        N, C, H, W = preds_T.shape\n+\n+        device = preds_S.device\n+        mat = torch.rand((N, 1, H, W)).to(device)\n+        mat = torch.where(mat > 1 - self.lambda_mgd, 0, 1).to(device)\n+\n+        masked_fea = torch.mul(preds_S, mat)\n+        new_fea = self.generation(masked_fea)\n+\n+        dis_loss = loss_mse(new_fea, preds_T) / N\n+\n+        return dis_loss\n+\n+\n+@LOSSES.register_module()\n+class FeatureLoss_InnerClip(nn.Module):\n+    def __init__(\n+        self,\n+        x_sample_num=24,\n+        y_sample_num=24,\n+        inter_keypoint_weight=1,\n+        inter_channel_weight=10,\n+        enlarge_width=1.6,\n+        embed_channels=[256, 512],\n+        inner_feats_distill=None,\n+    ):\n+        super().__init__()\n+        self.x_sample_num = x_sample_num\n+        self.y_sample_num = y_sample_num\n+        self.inter_keypoint_weight = inter_keypoint_weight\n+        self.inter_channel_weight = inter_channel_weight\n+        self.enlarge_width = enlarge_width\n+\n+        self.img_view_transformer = None\n+\n+        self.embed_channels = embed_channels\n+\n+        self.imgbev_embed = nn.Sequential(\n+            nn.Conv2d(\n+                embed_channels[0],\n+                embed_channels[1],\n+                kernel_size=1,\n+                stride=1,\n+                padding=0,\n+                bias=False,\n+            ),\n+            nn.BatchNorm2d(embed_channels[1]),\n+        )\n+\n+        self.inner_feats_loss = (\n+            build_loss(inner_feats_distill) if inner_feats_distill is not None else None\n+        )\n+\n+    def get_gt_sample_grid(self, corner_points2d):\n+        dH_x, dH_y = corner_points2d[0] - corner_points2d[1]\n+        dW_x, dW_y = corner_points2d[0] - corner_points2d[2]\n+        raw_grid_x = (\n+            torch.linspace(\n+                corner_points2d[0][0], corner_points2d[1][0], self.x_sample_num\n+            )\n+            .view(1, -1)\n+            .repeat(self.y_sample_num, 1)\n+        )\n+        raw_grid_y = (\n+            torch.linspace(\n+                corner_points2d[0][1], corner_points2d[2][1], self.y_sample_num\n+            )\n+            .view(-1, 1)\n+            .repeat(1, self.x_sample_num)\n+        )\n+        raw_grid = torch.cat((raw_grid_x.unsqueeze(2), raw_grid_y.unsqueeze(2)), dim=2)\n+        raw_grid_x_offset = (\n+            torch.linspace(0, -dW_x, self.x_sample_num)\n+            .view(-1, 1)\n+            .repeat(1, self.y_sample_num)\n+        )\n+        raw_grid_y_offset = (\n+            torch.linspace(0, -dH_y, self.y_sample_num)\n+            .view(1, -1)\n+            .repeat(self.x_sample_num, 1)\n+        )\n+        raw_grid_offset = torch.cat(\n+            (raw_grid_x_offset.unsqueeze(2), raw_grid_y_offset.unsqueeze(2)), dim=2\n+        )\n+        grid = raw_grid + raw_grid_offset  # X_sample,Y_sample,2\n+        grid[:, :, 0] = torch.clip(\n+            (\n+                (\n+                    grid[:, :, 0]\n+                    - (\n+                        self.img_view_transformer[\"bx\"][0].to(grid.device)\n+                        - self.img_view_transformer[\"dx\"][0].to(grid.device) / 2.0\n+                    )\n+                )\n+                / self.img_view_transformer[\"dx\"][0].to(grid.device)\n+                / (self.img_view_transformer[\"nx\"][0].to(grid.device) - 1)\n+            )\n+            * 2.0\n+            - 1.0,\n+            min=-1.0,\n+            max=1.0,\n+        )\n+        grid[:, :, 1] = torch.clip(\n+            (\n+                (\n+                    grid[:, :, 1]\n+                    - (\n+                        self.img_view_transformer[\"bx\"][1].to(grid.device)\n+                        - self.img_view_transformer[\"dx\"][1].to(grid.device) / 2.0\n+                    )\n+                )\n+                / self.img_view_transformer[\"dx\"][1].to(grid.device)\n+                / (self.img_view_transformer[\"nx\"][1].to(grid.device) - 1)\n+            )\n+            * 2.0\n+            - 1.0,\n+            min=-1.0,\n+            max=1.0,\n+        )\n+\n+        return grid.unsqueeze(0)\n+\n+    def get_inner_feat(self, gt_bboxes_3d, img_feats, pts_feats):\n+        \"\"\"Use grid to sample features of key points\"\"\"\n+        device = img_feats.device\n+        dtype = img_feats[0].dtype\n+\n+        img_feats_sampled_list = []\n+        pts_feats_sampled_list = []\n+\n+        for sample_ind in torch.arange(len(gt_bboxes_3d)):\n+            img_feat = img_feats[sample_ind].unsqueeze(0)  # 1,C,H,W\n+            pts_feat = pts_feats[sample_ind].unsqueeze(0)  # 1,C,H,W\n+\n+            bbox_num, corner_num, point_num = gt_bboxes_3d[sample_ind].corners.shape\n+\n+            for bbox_ind in torch.arange(bbox_num):\n+                if self.enlarge_width > 0:\n+                    gt_sample_grid = self.get_gt_sample_grid(\n+                        gt_bboxes_3d[sample_ind]\n+                        .enlarged_box(self.enlarge_width)\n+                        .corners[bbox_ind][[0, 2, 4, 6], :-1]\n+                    ).to(device)\n+                else:\n+                    gt_sample_grid = self.get_gt_sample_grid(\n+                        gt_bboxes_3d[sample_ind].corners[bbox_ind][[0, 2, 4, 6], :-1]\n+                    ).to(\n+                        device\n+                    )  # 1,sample_y,sample_x,2\n+\n+                img_feats_sampled_list.append(\n+                    F.grid_sample(\n+                        img_feat,\n+                        grid=gt_sample_grid,\n+                        align_corners=False,\n+                        mode=\"bilinear\",\n+                    )\n+                )  # 'bilinear')) #all_bbox_num,C,y_sample,x_sample\n+                pts_feats_sampled_list.append(\n+                    F.grid_sample(\n+                        pts_feat,\n+                        grid=gt_sample_grid,\n+                        align_corners=False,\n+                        mode=\"bilinear\",\n+                    )\n+                )  # 'bilinear')) #all_bbox_num,C,y_sample,x_sample\n+\n+        return torch.cat(img_feats_sampled_list, dim=0), torch.cat(\n+            pts_feats_sampled_list, dim=0\n+        )\n+\n+    @force_fp32(apply_to=(\"img_feats_kd\", \"pts_feats_kd\"))\n+    def get_inter_channel_loss(self, img_feats_kd, pts_feats_kd):\n+        \"\"\"Calculate the inter-channel similarities, guide the student keypoint features to mimic the channel-wise relationships of the teacher`s\"\"\"\n+\n+        C_img = img_feats_kd.shape[1]\n+        C_pts = pts_feats_kd.shape[1]\n+        N = self.x_sample_num * self.y_sample_num\n+\n+        img_feats_kd = img_feats_kd.view(-1, C_img, N).matmul(\n+            img_feats_kd.view(-1, C_img, N).permute(0, 2, 1)\n+        )  # -1,N,N\n+        pts_feats_kd = pts_feats_kd.view(-1, C_pts, N).matmul(\n+            pts_feats_kd.view(-1, C_pts, N).permute(0, 2, 1)\n+        )\n+\n+        img_feats_kd = F.normalize(img_feats_kd, dim=2)\n+        pts_feats_kd = F.normalize(pts_feats_kd, dim=2)\n+\n+        loss_inter_channel = F.mse_loss(img_feats_kd, pts_feats_kd, reduction=\"none\")\n+        loss_inter_channel = loss_inter_channel.sum(-1)\n+        loss_inter_channel = loss_inter_channel.mean()\n+        loss_inter_channel = self.inter_channel_weight * loss_inter_channel\n+        return loss_inter_channel\n+\n+    def forward(self, student_feats, teacher_feats, gt_bboxes_list, **kwargs):\n+\n+        self.img_view_transformer = kwargs.get(\"ivt_cfg\")\n+\n+        if student_feats.size(1) != teacher_feats.size(1):\n+            student_feats = self.imgbev_embed(student_feats)\n+\n+        if self.inter_keypoint_weight > 0 or self.inter_channel_weight > 0:\n+            img_feats_kd, pts_feats_kd = self.get_inner_feat(\n+                gt_bboxes_list, student_feats, teacher_feats\n+            )\n+\n+        if self.inner_feats_loss:\n+            return self.inner_feats_loss(img_feats_kd, pts_feats_kd)\n+\n+        # if self.inter_keypoint_weight > 0:\n+        #     loss_inter_keypoint = self.get_inter_keypoint_loss(\n+        #         img_feats_kd, pts_feats_kd\n+        #     )\n+        #     losses.update({\"loss_inter_keypoint_img_bev\": loss_inter_keypoint})\n+\n+        if self.inter_channel_weight > 0:\n+            loss_inter_channel = self.get_inter_channel_loss(img_feats_kd, pts_feats_kd)\n+\n+        return loss_inter_channel\n+\n+\n+@LOSSES.register_module()\n+class FeatureLoss_Affinity(nn.Module):\n+    def __init__(self):\n+        super().__init__()\n+\n+    def forward(self, student_feats, teacher_feats, *args, **kwargs):\n+        student_feats = [student_feats]\n+        teacher_feats = [teacher_feats]\n+\n+        feature_ditill_loss = 0.0\n+\n+        resize_shape = student_feats[-1].shape[-2:]\n+        if isinstance(student_feats, list):\n+            for i in range(len(student_feats)):\n+                feature_target = teacher_feats[i].detach()\n+                feature_pred = student_feats[i]\n+\n+                B, C, H, W = student_feats[-1].shape\n+\n+                if student_feats[-1].size(-1) != teacher_feats[-1].size(-1):\n+                    feature_pred_down = F.interpolate(\n+                        feature_pred, size=resize_shape, mode=\"bilinear\"\n+                    )\n+                    feature_target_down = F.interpolate(\n+                        feature_target, size=resize_shape, mode=\"bilinear\"\n+                    )\n+                else:\n+                    feature_pred_down = feature_pred\n+                    feature_target_down = feature_target\n+\n+                feature_target_down = feature_target_down.reshape(B, C, -1)\n+                depth_affinity = torch.bmm(\n+                    feature_target_down.permute(0, 2, 1), feature_target_down\n+                )\n+\n+                feature_pred_down = feature_pred_down.reshape(B, C, -1)\n+                rgb_affinity = torch.bmm(\n+                    feature_pred_down.permute(0, 2, 1), feature_pred_down\n+                )\n+\n+                feature_ditill_loss = (\n+                    feature_ditill_loss\n+                    + F.l1_loss(rgb_affinity, depth_affinity, reduction=\"mean\") / B\n+                )\n+\n+        else:\n+            raise NotImplementedError\n+\n+        return feature_ditill_loss\n+\n+\n+@LOSSES.register_module()\n+class FeatureLoss_Coefficient(nn.Module):\n+    def __init__(self, **kwargs):\n+        super().__init__()\n+\n+    def corrcoef(self, target, pred):\n+        pred_n = pred - pred.mean()\n+        target_n = target - target.mean()\n+        pred_n = pred_n / pred_n.norm()\n+        target_n = target_n / target_n.norm()\n+        return (pred_n * target_n).sum()\n+\n+    def forward(\n+        self,\n+        pred,\n+        target,\n+        regularization=\"l2\",\n+        regularization_strength=1.0,\n+    ):\n+        pred = [pred.clone()]\n+        target = [target.clone()]\n+        spearman_loss = 0.0\n+\n+        resize_shape = pred[-1].shape[-2:]  # save training time\n+\n+        if isinstance(pred, list):\n+            for i in range(len(pred)):\n+                feature_target = target[i]\n+                feature_pred = pred[i]\n+                B, C, H, W = feature_pred.shape\n+\n+                feature_pred_down = F.interpolate(\n+                    feature_pred, size=resize_shape, mode=\"bilinear\"\n+                )\n+                feature_target_down = F.interpolate(\n+                    feature_target, size=resize_shape, mode=\"bilinear\"\n+                )\n+\n+                # if feature_pred.size(-1) != feature_target.size(-1):\n+\n+                #     feature_pred_down = F.interpolate(\n+                #         feature_pred, size=resize_shape, mode=\"bilinear\"\n+                #     )\n+                #     feature_target_down = F.interpolate(\n+                #         feature_target, size=resize_shape, mode=\"bilinear\"\n+                #     )\n+                # else:\n+                #     feature_pred_down = feature_pred\n+                #     feature_target_down = feature_target\n+\n+                feature_pred_down = feature_pred_down.reshape(B, -1)\n+                feature_target_down = feature_target_down.reshape(B, -1)\n+\n+                feature_pred_down = torchsort.soft_rank(\n+                    feature_pred_down,\n+                    regularization=regularization,\n+                    regularization_strength=regularization_strength,\n+                )\n+                spearman_loss += 1 - self.corrcoef(\n+                    feature_target_down, feature_pred_down / feature_pred_down.shape[-1]\n+                )\n+\n+        return spearman_loss\n+\n+\n+@LOSSES.register_module()\n+class Radar_MSDistilll(nn.Module):\n+    def __init__(self, num_layers=2, each_layer_loss_cfg=[]):\n+        super().__init__()\n+        self.num_layers = num_layers\n+        assert num_layers == len(each_layer_loss_cfg)\n+        for idx, cfg_dict in enumerate(each_layer_loss_cfg):\n+            setattr(self, f\"layer_loss_{idx}\", build_loss(cfg_dict))\n+\n+    def unify_feat_size(self, student_feat, teacher_feat):\n+        bs, s_c, s_w, s_h = student_feat.shape\n+        bs, t_c, t_w, t_h = teacher_feat.shape\n+        if s_w == t_w:\n+            return student_feat, teacher_feat\n+        else:\n+            teacher_feat = F.interpolate(teacher_feat, (s_w, s_h), mode=\"bilinear\")\n+            return student_feat, teacher_feat\n+\n+    def forward(self, radar_ms_feats, pts_ms_feats, gt_bboxes_3d=None):\n+        assert isinstance(radar_ms_feats, list)\n+        losses = 0.0\n+\n+        for idx in range(self.num_layers):\n+            tmp_radar_feats, tmp_pts_feats = self.unify_feat_size(\n+                radar_ms_feats[idx], pts_ms_feats[idx]\n+            )\n+\n+            if gt_bboxes_3d is not None:\n+                losses += getattr(self, f\"layer_loss_{idx}\")(\n+                    tmp_radar_feats, tmp_pts_feats, gt_bboxes_3d\n+                )[0]\n+            else:\n+                losses += getattr(self, f\"layer_loss_{idx}\")(\n+                    tmp_radar_feats, tmp_pts_feats, gt_bboxes_3d\n+                )\n+\n+        return losses / self.num_layers\n+\n+\n+@LOSSES.register_module()\n+class InfoMax(nn.Module):\n+    def __init__(self, **kwargs):\n+        super().__init__()\n+\n+    def forward(self, x1, x2):\n+        import pdb\n+\n+        pdb.set_trace()\n+        x1 = x1 / (torch.norm(x1, p=2, dim=1, keepdim=True) + 1e-10)\n+        x2 = x2 / (torch.norm(x2, p=2, dim=1, keepdim=True) + 1e-10)\n+        bs = x1.size(0)\n+        s = torch.matmul(x1, x2.permute(1, 0))\n+        mask_joint = torch.eye(bs).cuda()\n+        mask_marginal = 1 - mask_joint\n+\n+        Ej = (s * mask_joint).mean()\n+        Em = torch.exp(s * mask_marginal).mean()\n+        # decoupled comtrastive learning?!!!!\n+        # infomax_loss = - (Ej - torch.log(Em)) * self.alpha\n+        infomax_loss = -(Ej - torch.log(Em))  # / Em\n+        return infomax_loss\n+\n+\n+@LOSSES.register_module()\n+class HeatMapAug(nn.Module):\n+    def __init__(self):\n+        super().__init__()\n+\n+    @force_fp32(apply_to=(\"stu_pred\", \"tea_pred\"))\n+    def forward(self, stu_pred, tea_pred, fg_map):\n+\n+        num_task = len(stu_pred)\n+        kl_loss = 0\n+        for task_id in range(num_task):\n+            student_pred = stu_pred[task_id][0][\"heatmap\"].sigmoid()\n+            teacher_pred = tea_pred[task_id][0][\"heatmap\"].sigmoid()\n+            fg_map = fg_map.unsqueeze(1)\n+            task_kl_loss = F.binary_cross_entropy(student_pred, teacher_pred.detach())\n+            task_kl_loss = torch.sum(task_kl_loss * fg_map) / torch.sum(fg_map)\n+            kl_loss += task_kl_loss\n+\n+        return kl_loss\n+\n+\n+@LOSSES.register_module()\n+class Dc_ResultDistill(nn.Module):\n+    def __init__(\n+        self,\n+        pc_range=[],\n+        voxel_size=[],\n+        out_size_scale=8,\n+        ret_sum=False,\n+        loss_weight_reg=10,\n+        loss_weight_cls=10,\n+        max_cls=True,\n+    ):\n+        super().__init__()\n+\n+        self.pc_range = pc_range\n+        self.voxel_size = voxel_size\n+        self.out_size_scale = out_size_scale\n+        self.ret_sum = ret_sum\n+        self.loss_weight_reg = loss_weight_reg\n+        self.loss_weight_cls = loss_weight_cls\n+        self.max_cls = max_cls\n+\n+    def forward(self, resp_lidar, resp_fuse, gt_boxes):\n+        \"\"\"Dc_ResultDistill forward.\n+\n+        Args:\n+            resp_lidar (_type_):\n+            resp_fuse (_type_):\n+            gt_boxes (_type_):\n+\n+        Returns:\n+            _type_: _description_\n+        \"\"\"\n+\n+        tmp_resp_lidar = []\n+        tmp_resp_fuse = []\n+        for res_lidar, res_fuse in zip(resp_lidar, resp_fuse):\n+            tmp_resp_lidar.append(res_lidar[0])\n+            tmp_resp_fuse.append(res_fuse[0])\n+\n+        tmp_gt_boxes = []\n+        for bs_idx in range(len(gt_boxes)):\n+\n+            gt_bboxes_3d = torch.cat(\n+                (gt_boxes[bs_idx].gravity_center, gt_boxes[bs_idx].tensor[:, 3:]), dim=1\n+            )\n+\n+            tmp_gt_boxes.append(gt_bboxes_3d)\n+\n+        cls_lidar = []\n+        reg_lidar = []\n+        cls_fuse = []\n+        reg_fuse = []\n+\n+        # criterion = nn.L1Loss(reduce=False)\n+        # criterion_cls = QualityFocalLoss_()\n+\n+        criterion = nn.SmoothL1Loss(reduce=False)\n+        criterion_cls = nn.L1Loss(reduce=False)\n+\n+        for task_id, task_out in enumerate(tmp_resp_lidar):\n+            cls_lidar.append(task_out[\"heatmap\"])\n+            cls_fuse.append(_sigmoid(tmp_resp_fuse[task_id][\"heatmap\"] / 2))\n+            reg_lidar.append(\n+                torch.cat(\n+                    [\n+                        task_out[\"reg\"],\n+                        task_out[\"height\"],\n+                        task_out[\"dim\"],\n+                        task_out[\"rot\"],\n+                        task_out[\"vel\"],\n+                        # task_out[\"iou\"],\n+                    ],\n+                    dim=1,\n+                )\n+            )\n+            reg_fuse.append(\n+                torch.cat(\n+                    [\n+                        tmp_resp_fuse[task_id][\"reg\"],\n+                        tmp_resp_fuse[task_id][\"height\"],\n+                        tmp_resp_fuse[task_id][\"dim\"],\n+                        tmp_resp_fuse[task_id][\"rot\"],\n+                        tmp_resp_fuse[task_id][\"vel\"],\n+                        # resp_fuse[task_id][\"iou\"],\n+                    ],\n+                    dim=1,\n+                )\n+            )\n+        cls_lidar = torch.cat(cls_lidar, dim=1)\n+        reg_lidar = torch.cat(reg_lidar, dim=1)\n+        cls_fuse = torch.cat(cls_fuse, dim=1)\n+        reg_fuse = torch.cat(reg_fuse, dim=1)\n+\n+        if self.max_cls:\n+            cls_lidar_max, _ = torch.max(cls_lidar, dim=1)\n+            cls_fuse_max, _ = torch.max(cls_fuse, dim=1)\n+        else:\n+            _, _, ht_h, ht_w = cls_fuse.shape\n+            cls_lidar_max = cls_lidar.flatten(2).permute(0, 2, 1)\n+            cls_fuse_max = cls_fuse.flatten(2).permute(0, 2, 1)\n+\n+        gaussian_mask = calculate_box_mask_gaussian(\n+            reg_lidar.shape,\n+            tmp_gt_boxes,\n+            self.pc_range,\n+            self.voxel_size,\n+            self.out_size_scale,\n+        )\n+\n+        # # diff_reg = criterion(reg_lidar, reg_fuse)\n+        # # diff_cls = criterion(cls_lidar_max, cls_fuse_max)\n+        # diff_reg = criterion_reg(reg_lidar, reg_fuse)\n+\n+        # cls_lidar_max = torch.sigmoid(cls_lidar_max)\n+        # diff_cls = criterion_cls(\n+        #     cls_fuse_max, cls_lidar_max\n+        # )  # Compared with directly using the L1 loss constraint, replace it with bce focal loss and change the position?\n+\n+        # weight = gaussian_mask.sum()\n+        # weight = reduce_mean(weight)\n+\n+        # # diff_cls = criterion_cls(cls_lidar_max, cls_fuse_max)\n+        # diff_reg = torch.mean(diff_reg, dim=1)\n+        # diff_reg = diff_reg * gaussian_mask\n+        # loss_reg_distill = torch.sum(diff_reg) / (weight + 1e-4)\n+\n+        # if not self.max_cls:\n+        #     diff_cls = diff_cls\n+        #     loss_cls_distill = diff_cls.sum() * 2\n+        # else:\n+        #     diff_cls = diff_cls * gaussian_mask\n+        #     loss_cls_distill = torch.sum(diff_cls) / (weight + 1e-4)\n+\n+        diff_reg = criterion(reg_lidar, reg_fuse)\n+\n+        diff_cls = criterion_cls(cls_lidar_max, cls_fuse_max)\n+        diff_reg = torch.mean(diff_reg, dim=1)\n+        diff_reg = diff_reg * gaussian_mask\n+        diff_cls = diff_cls * gaussian_mask\n+        weight = gaussian_mask.sum()\n+        weight = reduce_mean(weight)\n+        loss_reg_distill = torch.sum(diff_reg) / (weight + 1e-4)\n+        loss_cls_distill = torch.sum(diff_cls) / (weight + 1e-4)\n+\n+        if self.ret_sum:\n+\n+            loss_det_distill = self.loss_weight * (loss_reg_distill + loss_cls_distill)\n+\n+            return loss_det_distill\n+        else:\n+\n+            return (\n+                self.loss_weight_reg * loss_reg_distill,\n+                self.loss_weight_cls * loss_cls_distill,\n+            )\n+\n+\n+@LOSSES.register_module()\n+class SelfLearningMFD(nn.Module):\n+    def __init__(\n+        self,\n+        bev_shape=[128, 128],\n+        pc_range=[-51.2, -51.2, -5, 51.2, 51.2, 3],\n+        voxel_size=[0.1, 0.1, 0.1],\n+        score_threshold=0.7,\n+        add_stu_decode_bboxes=False,\n+        loss_weight=1e-2,\n+        bbox_coder=None,\n+        shape_resize_times=None,\n+        student_channels=256,\n+        teacher_channels=512,\n+    ):\n+        super().__init__()\n+        self.bev_shape = bev_shape\n+        self.pc_range = pc_range\n+        self.voxel_size = voxel_size\n+        if bbox_coder:\n+            self.shape_resize_times = bbox_coder.out_size_factor\n+        elif shape_resize_times:\n+            self.shape_resize_times = shape_resize_times\n+        else:\n+            self.shape_resize_times = 8\n+        self.score_threshold = score_threshold\n+        self.add_stu_decode_bboxes = add_stu_decode_bboxes\n+        self.loss_weight = loss_weight\n+\n+        self.align = nn.Sequential(\n+            nn.Conv2d(student_channels, teacher_channels, kernel_size=1, padding=0),\n+            # nn.BatchNorm2d(teacher_channels),\n+        )\n+\n+        if bbox_coder is not None:\n+            self.bbox_coder = build_bbox_coder(bbox_coder)\n+\n+    def pred2bboxes(self, preds_dict):\n+        \"\"\"SelfLearningMFD-pred2bboxes forward\n+\n+        Args:\n+            pred_bboxes (list):\n+            [task1[[task_head_reg,task_head_heatmap,...,]],\n+            task2...,\n+            taskn]\n+        \"\"\"\n+\n+        for task_id, pred_data in enumerate(preds_dict):\n+            bath_tmp = []\n+            batch_size = pred_data[0][\"heatmap\"].shape[0]\n+            batch_heatmap = pred_data[0][\"heatmap\"].sigmoid()\n+            batch_reg = pred_data[0][\"reg\"]\n+            batch_hei = pred_data[0][\"height\"]\n+\n+            # denormalization\n+            batch_dim = torch.exp(pred_data[0][\"dim\"])\n+\n+            batch_rot_sin = pred_data[0][\"rot\"][:, 0].unsqueeze(1)\n+            batch_rot_cos = pred_data[0][\"rot\"][:, 1].unsqueeze(1)\n+\n+            if \"vel\" in pred_data[0].keys():\n+                batch_vel = pred_data[0][\"vel\"]\n+\n+            bath_tmp.append(batch_heatmap)\n+            bath_tmp.append(batch_rot_sin)\n+            bath_tmp.append(batch_rot_cos)\n+            bath_tmp.append(batch_hei)\n+            bath_tmp.append(batch_dim)\n+            bath_tmp.append(batch_vel)\n+            bath_tmp.append(batch_reg)\n+\n+            bboxes_decode = self.bbox_coder.decode(*bath_tmp)\n+\n+        return bboxes_decode\n+\n+    def aug_boxes(self, gt_boxes, rot_mat):\n+        rot_lim = (-22.5, 22.5)\n+        scale_lim = (0.95, 1.05)\n+        rotate_angle = np.random.uniform(*rot_lim)\n+        scale_ratio = np.random.uniform(*scale_lim)\n+        gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n+        gt_boxes[:, 3:6] *= scale_ratio\n+        gt_boxes[:, 6] += rotate_angle\n+        gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n+        gt_boxes[:, 6] = -gt_boxes[:, 6]\n+        gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(-1)\n+        gt_boxes[:, :3] = gt_boxes[:, :3]\n+\n+        return gt_boxes\n+\n+    @force_fp32(\n+        apply_to=(\n+            \"student_bev_feat\",\n+            \"teacher_bev_feat\",\n+        )\n+    )\n+    def forward(\n+        self,\n+        student_bev_feat,\n+        teacher_bev_feat,\n+        gt_bboxes_list=None,\n+        masks_bboxes=None,\n+        bda_mat=None,\n+        **kwargs,\n+    ):\n+        \"\"\"SelfLearningMFD forward.\n+\n+        Args:\n+            student_bev_feats (torch.tensor): Calculate student feats\n+            teacher_bev_feats (torch.tensor): Calculate teacher feats\n+            masks_bboxes (list): Self-learning mask detection\n+            gt_bboxes_list (list): [LiDARInstance3DBoxes(gravity_center+tensor(num_objs,9))]\n+\n+        Returns:\n+            dict: _description_\n+        \"\"\"\n+\n+        bs = student_bev_feat.size(0)\n+\n+        if student_bev_feat.size(1) != teacher_bev_feat.size(1):\n+            student_bev_feat = self.align(student_bev_feat)\n+\n+        if masks_bboxes is not None and self.add_stu_decode_bboxes:\n+            student_pred_bboxes_list = self.pred2bboxes(\n+                masks_bboxes\n+            )  # list of task : each shape of [(num_bboxes,9)]\n+\n+            # if bda_mat is not None:\n+            #     student_pred_bboxes_list = self.aug_boxes(student_pred_bboxes_list)\n+\n+        bev_feat_shape = torch.tensor(self.bev_shape)\n+        voxel_size = torch.tensor(self.voxel_size)\n+        feature_map_size = bev_feat_shape[:2]\n+\n+        if gt_bboxes_list is not None:\n+            device = student_bev_feat.device\n+\n+            gt_bboxes_list = [\n+                torch.cat(\n+                    (gt_bboxes.gravity_center, gt_bboxes.tensor[:, 3:]), dim=1\n+                ).to(device)\n+                for gt_bboxes in gt_bboxes_list\n+            ]\n+\n+            if self.add_stu_decode_bboxes:\n+                for idx, data in enumerate(student_pred_bboxes_list):\n+\n+                    bboxes_dim = data[\"bboxes\"].size(1)\n+\n+                    scroes_mask = data[\"scores\"] > self.score_threshold\n+\n+                    new_select_by_mask = data[\"bboxes\"][scroes_mask, :]\n+\n+                    gt_bboxes_list[idx] = torch.cat(\n+                        [gt_bboxes_list[idx], new_select_by_mask], dim=0\n+                    )\n+\n+            fg_map = student_bev_feat.new_zeros(\n+                (len(gt_bboxes_list), feature_map_size[1], feature_map_size[0])\n+            )\n+\n+            for idx in range(len(gt_bboxes_list)):\n+                num_objs = gt_bboxes_list[idx].shape[0]\n+\n+                for k in range(num_objs):\n+                    width = gt_bboxes_list[idx][k][3]\n+                    length = gt_bboxes_list[idx][k][4]\n+                    width = width / voxel_size[0] / self.shape_resize_times\n+                    length = length / voxel_size[1] / self.shape_resize_times\n+\n+                    if width > 0 and length > 0:\n+                        radius = gaussian_radius((length, width), min_overlap=0.1)\n+                        radius = max(1, int(radius))\n+\n+                        # be really careful for the coordinate system of\n+                        # your box annotation.\n+                        x, y, z = (\n+                            gt_bboxes_list[idx][k][0],\n+                            gt_bboxes_list[idx][k][1],\n+                            gt_bboxes_list[idx][k][2],\n+                        )\n+\n+                        coor_x = (\n+                            (x - self.pc_range[0])\n+                            / voxel_size[0]\n+                            / self.shape_resize_times\n+                        )\n+                        coor_y = (\n+                            (y - self.pc_range[1])\n+                            / voxel_size[1]\n+                            / self.shape_resize_times\n+                        )\n+\n+                        center = torch.tensor(\n+                            [coor_x, coor_y], dtype=torch.float32, device=device\n+                        )\n+                        center_int = center.to(torch.int32)\n+\n+                        # throw out not in range objects to avoid out of array\n+                        # area when creating the heatmap\n+                        if not (\n+                            0 <= center_int[0] < feature_map_size[0]\n+                            and 0 <= center_int[1] < feature_map_size[1]\n+                        ):\n+                            continue\n+\n+                        draw_heatmap_gaussian(fg_map[idx], center_int, radius)\n+\n+        if fg_map is None:\n+            fg_map = student_bev_feat.new_ones(\n+                (student_bev_feat.shape[0], feature_map_size[1], feature_map_size[0])\n+            )\n+\n+        if bs > 1:\n+            fg_map = fg_map.unsqueeze(1)\n+\n+        fit_loss = F.mse_loss(student_bev_feat, teacher_bev_feat, reduction=\"none\")\n+        fit_loss = torch.sum(fit_loss * fg_map) / torch.sum(fg_map)\n+\n+        return fit_loss * self.loss_weight, fg_map\n+\n+    # def forward(\n+    #     self, student_bev_feats, teacher_bev_feats, masks_bboxes, gt_bboxes_list\n+    # ):\n+    #     \"\"\"SelfLearningMFD forward.\n+\n+    #     Args:\n+    #         student_bev_feats (torch.tensor): _description_\n+    #         teacher_bev_feats (torch.tensor): _description_\n+    #         masks_bboxes (): _description_\n+    #         gt_bboxes_list (_type_): _description_\n+    #     \"\"\"\n+    #     assert isinstance(masks_bboxes, list)\n+    #     ret_dict = multi_apply(\n+    #         self._forward,\n+    #         student_bev_feats,\n+    #         teacher_bev_feats,\n+    #         masks_bboxes,\n+    #         gt_bboxes_list,\n+    #     )\n"
                }
            ],
            "date": 1716015764361,
            "name": "Commit-0",
            "content": "import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nfrom ..builder import LOSSES, build_loss\nfrom ..utils.gaussian_utils import calculate_box_mask_gaussian\nfrom torch import distributed as dist\nfrom mmdet.core import multi_apply, build_bbox_coder\nfrom mmdet3d.core import draw_heatmap_gaussian, gaussian_radius\nfrom mmdet3d.core.bbox.coders.centerpoint_bbox_coders import (\n    CenterPointBBoxCoder as bboxcoder,\n)\nfrom mmdet.models.losses import QualityFocalLoss\nfrom mmcv.runner import force_fp32\nimport torchsort\nimport torchvision\nimport numpy as np\n\n\ndef get_world_size() -> int:\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n\n    return dist.get_world_size()\n\n\ndef reduce_sum(tensor):\n    world_size = get_world_size()\n    if world_size < 2:\n        return tensor\n    tensor = tensor.clone()\n    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n    return tensor\n\n\ndef reduce_mean(tensor):\n    return reduce_sum(tensor) / float(get_world_size())\n\n\ndef _sigmoid(x):\n    # y = torch.clamp(x.sigmoid(), min=1e-3, max=1 - 1e-3)\n    y = x.sigmoid()\n    return y\n\n\ndef off_diagonal(x):\n    # return a flattened view of the off-diagonal elements of a square matrix\n    n, m = x.shape\n    assert n == m\n    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n\n\ndef response_list_wrap(response_lists: list):\n    \"\"\"response_list_wrap\n\n    Args:\n        response_lists (list): Pack list content\n\n    Returns:\n        list: Return the new list of packaged integration\n    \"\"\"\n    assert len(response_lists) == 1\n\n    tmp_resp = []\n    for response in response_lists:\n        tmp_resp.append(response[0])\n\n    return tmp_resp\n\n\n@LOSSES.register_module()\nclass QualityFocalLoss_(nn.Module):\n    \"\"\"\n    input[B,M,C] not sigmoid\n    target[B,M,C], sigmoid\n    \"\"\"\n\n    def __init__(self, beta=2.0):\n\n        super(QualityFocalLoss_, self).__init__()\n        self.beta = beta\n\n    def forward(\n        self,\n        input: torch.Tensor,\n        target: torch.Tensor,\n        pos_normalizer=torch.tensor(1.0),\n    ):\n\n        pred_sigmoid = torch.sigmoid(input)\n        scale_factor = pred_sigmoid - target\n\n        # pred_sigmoid = torch.sigmoid(input)\n        # scale_factor = input - target\n        loss = F.binary_cross_entropy_with_logits(input, target, reduction=\"none\") * (\n            scale_factor.abs().pow(self.beta)\n        )\n        loss /= torch.clamp(pos_normalizer, min=1.0)\n        return loss\n\n\n@LOSSES.register_module()\nclass SimpleL1(nn.Module):\n    def __init__(self, criterion=\"L1\", student_ch=256, teacher_ch=512):\n        super().__init__()\n        self.criterion = criterion\n        if criterion == \"L1\":\n            self.criterion_loss = nn.L1Loss(reduce=False)\n        elif criterion == \"SmoothL1\":\n            self.criterion_loss = nn.SmoothL1Loss(reduce=False)\n        elif criterion == \"MSE\":\n            self.criterion_loss = nn.MSELoss(reduction=\"none\")\n\n        if student_ch != teacher_ch:\n            self.align = nn.Conv2d(student_ch, teacher_ch, kernel_size=1)\n\n    def forward(self, feats1, feats2, *args, **kwargs):\n        if self.criterion == \"MSE\":\n            feats1 = self.align(feats1) if getattr(self, \"align\", None) else feats1\n            losses = self.criterion_loss(feats1, feats2).mean()\n            return losses\n        else:\n            return self.criterion_loss(feats1, feats2)\n\n\n@LOSSES.register_module()\nclass Relevance_Distillation(nn.Module):\n    def __init__(self, bs=2, bn_dim=512, lambd=0.0051):\n        super().__init__()\n        self.bs = bs\n        self.bn_dim = bn_dim\n        self.lambd = lambd\n\n        # normalization layer for the representations z1 and z2\n        self.bn = nn.BatchNorm1d(self.bn_dim, affine=False)\n\n        self.align = nn.Conv2d(256, 512, kernel_size=1)\n\n    def forward(self, student_bev, teacher_bev, *args, **kwargs):\n        student_bev = self.align(student_bev)\n\n        student_bev = student_bev.flatten(2)\n        teacher_bev = teacher_bev.flatten(2)\n\n        # empirical cross-correlation matrix\n        c = self.bn(student_bev).flatten(1).T @ self.bn(teacher_bev).flatten(1)\n\n        # sum the cross-correlation matrix between all gpus\n        c.div_(self.bs)\n        if self.bs == 1:\n            pass\n        else:\n            torch.distributed.all_reduce(c)\n\n        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n        off_diag = off_diagonal(c).pow_(2).sum()\n        loss = on_diag + self.lambd * off_diag\n        return loss\n\n\n@LOSSES.register_module()\nclass FeatureLoss(nn.Module):\n    \"\"\"PyTorch version of `Masked Generative Distillation`\n\n    Args:\n        student_channels(int): Number of channels in the student's feature map.\n        teacher_channels(int): Number of channels in the teacher's feature map.\n        name (str): the loss name of the layer\n        alpha_mgd (float, optional): Weight of dis_loss. Defaults to 0.00002\n        lambda_mgd (float, optional): masked ratio. Defaults to 0.65\n    \"\"\"\n\n    def __init__(\n        self,\n        student_channels,\n        teacher_channels,\n        name,\n        alpha_mgd=0.00002,\n        lambda_mgd=0.65,\n    ):\n        super(FeatureLoss, self).__init__()\n        self.alpha_mgd = alpha_mgd\n        self.lambda_mgd = lambda_mgd\n        self.name = name\n\n        if student_channels != teacher_channels:\n            self.align = nn.Conv2d(\n                student_channels, teacher_channels, kernel_size=1, stride=1, padding=0\n            )\n        else:\n            self.align = None\n\n        self.generation = nn.Sequential(\n            nn.Conv2d(teacher_channels, teacher_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(teacher_channels, teacher_channels, kernel_size=3, padding=1),\n        )\n\n    def forward(self, preds_S, preds_T, **kwargs):\n        \"\"\"Forward function.\n        Args:\n            preds_S(Tensor): Bs*C*H*W, student's feature map\n            preds_T(Tensor): Bs*C*H*W, teacher's feature map\n        \"\"\"\n        assert preds_S.shape[-2:] == preds_T.shape[-2:]\n\n        if self.align is not None:\n            preds_S = self.align(preds_S)\n\n        loss = self.get_dis_loss(preds_S, preds_T) * self.alpha_mgd\n\n        return loss\n\n    def get_dis_loss(self, preds_S, preds_T):\n        loss_mse = nn.MSELoss(reduction=\"sum\")\n        N, C, H, W = preds_T.shape\n\n        device = preds_S.device\n        mat = torch.rand((N, 1, H, W)).to(device)\n        mat = torch.where(mat > 1 - self.lambda_mgd, 0, 1).to(device)\n\n        masked_fea = torch.mul(preds_S, mat)\n        new_fea = self.generation(masked_fea)\n\n        dis_loss = loss_mse(new_fea, preds_T) / N\n\n        return dis_loss\n\n\n@LOSSES.register_module()\nclass FeatureLoss_InnerClip(nn.Module):\n    def __init__(\n        self,\n        x_sample_num=24,\n        y_sample_num=24,\n        inter_keypoint_weight=1,\n        inter_channel_weight=10,\n        enlarge_width=1.6,\n        embed_channels=[256, 512],\n        inner_feats_distill=None,\n    ):\n        super().__init__()\n        self.x_sample_num = x_sample_num\n        self.y_sample_num = y_sample_num\n        self.inter_keypoint_weight = inter_keypoint_weight\n        self.inter_channel_weight = inter_channel_weight\n        self.enlarge_width = enlarge_width\n\n        self.img_view_transformer = None\n\n        self.embed_channels = embed_channels\n\n        self.imgbev_embed = nn.Sequential(\n            nn.Conv2d(\n                embed_channels[0],\n                embed_channels[1],\n                kernel_size=1,\n                stride=1,\n                padding=0,\n                bias=False,\n            ),\n            nn.BatchNorm2d(embed_channels[1]),\n        )\n\n        self.inner_feats_loss = (\n            build_loss(inner_feats_distill) if inner_feats_distill is not None else None\n        )\n\n    def get_gt_sample_grid(self, corner_points2d):\n        dH_x, dH_y = corner_points2d[0] - corner_points2d[1]\n        dW_x, dW_y = corner_points2d[0] - corner_points2d[2]\n        raw_grid_x = (\n            torch.linspace(\n                corner_points2d[0][0], corner_points2d[1][0], self.x_sample_num\n            )\n            .view(1, -1)\n            .repeat(self.y_sample_num, 1)\n        )\n        raw_grid_y = (\n            torch.linspace(\n                corner_points2d[0][1], corner_points2d[2][1], self.y_sample_num\n            )\n            .view(-1, 1)\n            .repeat(1, self.x_sample_num)\n        )\n        raw_grid = torch.cat((raw_grid_x.unsqueeze(2), raw_grid_y.unsqueeze(2)), dim=2)\n        raw_grid_x_offset = (\n            torch.linspace(0, -dW_x, self.x_sample_num)\n            .view(-1, 1)\n            .repeat(1, self.y_sample_num)\n        )\n        raw_grid_y_offset = (\n            torch.linspace(0, -dH_y, self.y_sample_num)\n            .view(1, -1)\n            .repeat(self.x_sample_num, 1)\n        )\n        raw_grid_offset = torch.cat(\n            (raw_grid_x_offset.unsqueeze(2), raw_grid_y_offset.unsqueeze(2)), dim=2\n        )\n        grid = raw_grid + raw_grid_offset  # X_sample,Y_sample,2\n        grid[:, :, 0] = torch.clip(\n            (\n                (\n                    grid[:, :, 0]\n                    - (\n                        self.img_view_transformer[\"bx\"][0].to(grid.device)\n                        - self.img_view_transformer[\"dx\"][0].to(grid.device) / 2.0\n                    )\n                )\n                / self.img_view_transformer[\"dx\"][0].to(grid.device)\n                / (self.img_view_transformer[\"nx\"][0].to(grid.device) - 1)\n            )\n            * 2.0\n            - 1.0,\n            min=-1.0,\n            max=1.0,\n        )\n        grid[:, :, 1] = torch.clip(\n            (\n                (\n                    grid[:, :, 1]\n                    - (\n                        self.img_view_transformer[\"bx\"][1].to(grid.device)\n                        - self.img_view_transformer[\"dx\"][1].to(grid.device) / 2.0\n                    )\n                )\n                / self.img_view_transformer[\"dx\"][1].to(grid.device)\n                / (self.img_view_transformer[\"nx\"][1].to(grid.device) - 1)\n            )\n            * 2.0\n            - 1.0,\n            min=-1.0,\n            max=1.0,\n        )\n\n        return grid.unsqueeze(0)\n\n    def get_inner_feat(self, gt_bboxes_3d, img_feats, pts_feats):\n        \"\"\"Use grid to sample features of key points\"\"\"\n        device = img_feats.device\n        dtype = img_feats[0].dtype\n\n        img_feats_sampled_list = []\n        pts_feats_sampled_list = []\n\n        for sample_ind in torch.arange(len(gt_bboxes_3d)):\n            img_feat = img_feats[sample_ind].unsqueeze(0)  # 1,C,H,W\n            pts_feat = pts_feats[sample_ind].unsqueeze(0)  # 1,C,H,W\n\n            bbox_num, corner_num, point_num = gt_bboxes_3d[sample_ind].corners.shape\n\n            for bbox_ind in torch.arange(bbox_num):\n                if self.enlarge_width > 0:\n                    gt_sample_grid = self.get_gt_sample_grid(\n                        gt_bboxes_3d[sample_ind]\n                        .enlarged_box(self.enlarge_width)\n                        .corners[bbox_ind][[0, 2, 4, 6], :-1]\n                    ).to(device)\n                else:\n                    gt_sample_grid = self.get_gt_sample_grid(\n                        gt_bboxes_3d[sample_ind].corners[bbox_ind][[0, 2, 4, 6], :-1]\n                    ).to(\n                        device\n                    )  # 1,sample_y,sample_x,2\n\n                img_feats_sampled_list.append(\n                    F.grid_sample(\n                        img_feat,\n                        grid=gt_sample_grid,\n                        align_corners=False,\n                        mode=\"bilinear\",\n                    )\n                )  # 'bilinear')) #all_bbox_num,C,y_sample,x_sample\n                pts_feats_sampled_list.append(\n                    F.grid_sample(\n                        pts_feat,\n                        grid=gt_sample_grid,\n                        align_corners=False,\n                        mode=\"bilinear\",\n                    )\n                )  # 'bilinear')) #all_bbox_num,C,y_sample,x_sample\n\n        return torch.cat(img_feats_sampled_list, dim=0), torch.cat(\n            pts_feats_sampled_list, dim=0\n        )\n\n    @force_fp32(apply_to=(\"img_feats_kd\", \"pts_feats_kd\"))\n    def get_inter_channel_loss(self, img_feats_kd, pts_feats_kd):\n        \"\"\"Calculate the inter-channel similarities, guide the student keypoint features to mimic the channel-wise relationships of the teacher`s\"\"\"\n\n        C_img = img_feats_kd.shape[1]\n        C_pts = pts_feats_kd.shape[1]\n        N = self.x_sample_num * self.y_sample_num\n\n        img_feats_kd = img_feats_kd.view(-1, C_img, N).matmul(\n            img_feats_kd.view(-1, C_img, N).permute(0, 2, 1)\n        )  # -1,N,N\n        pts_feats_kd = pts_feats_kd.view(-1, C_pts, N).matmul(\n            pts_feats_kd.view(-1, C_pts, N).permute(0, 2, 1)\n        )\n\n        img_feats_kd = F.normalize(img_feats_kd, dim=2)\n        pts_feats_kd = F.normalize(pts_feats_kd, dim=2)\n\n        loss_inter_channel = F.mse_loss(img_feats_kd, pts_feats_kd, reduction=\"none\")\n        loss_inter_channel = loss_inter_channel.sum(-1)\n        loss_inter_channel = loss_inter_channel.mean()\n        loss_inter_channel = self.inter_channel_weight * loss_inter_channel\n        return loss_inter_channel\n\n    def forward(self, student_feats, teacher_feats, gt_bboxes_list, **kwargs):\n\n        self.img_view_transformer = kwargs.get(\"ivt_cfg\")\n\n        if student_feats.size(1) != teacher_feats.size(1):\n            student_feats = self.imgbev_embed(student_feats)\n\n        if self.inter_keypoint_weight > 0 or self.inter_channel_weight > 0:\n            img_feats_kd, pts_feats_kd = self.get_inner_feat(\n                gt_bboxes_list, student_feats, teacher_feats\n            )\n\n        if self.inner_feats_loss:\n            return self.inner_feats_loss(img_feats_kd, pts_feats_kd)\n\n        # if self.inter_keypoint_weight > 0:\n        #     loss_inter_keypoint = self.get_inter_keypoint_loss(\n        #         img_feats_kd, pts_feats_kd\n        #     )\n        #     losses.update({\"loss_inter_keypoint_img_bev\": loss_inter_keypoint})\n\n        if self.inter_channel_weight > 0:\n            loss_inter_channel = self.get_inter_channel_loss(img_feats_kd, pts_feats_kd)\n\n        return loss_inter_channel\n\n\n@LOSSES.register_module()\nclass FeatureLoss_Affinity(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, student_feats, teacher_feats, *args, **kwargs):\n        student_feats = [student_feats]\n        teacher_feats = [teacher_feats]\n\n        feature_ditill_loss = 0.0\n\n        resize_shape = student_feats[-1].shape[-2:]\n        if isinstance(student_feats, list):\n            for i in range(len(student_feats)):\n                feature_target = teacher_feats[i].detach()\n                feature_pred = student_feats[i]\n\n                B, C, H, W = student_feats[-1].shape\n\n                if student_feats[-1].size(-1) != teacher_feats[-1].size(-1):\n                    feature_pred_down = F.interpolate(\n                        feature_pred, size=resize_shape, mode=\"bilinear\"\n                    )\n                    feature_target_down = F.interpolate(\n                        feature_target, size=resize_shape, mode=\"bilinear\"\n                    )\n                else:\n                    feature_pred_down = feature_pred\n                    feature_target_down = feature_target\n\n                feature_target_down = feature_target_down.reshape(B, C, -1)\n                depth_affinity = torch.bmm(\n                    feature_target_down.permute(0, 2, 1), feature_target_down\n                )\n\n                feature_pred_down = feature_pred_down.reshape(B, C, -1)\n                rgb_affinity = torch.bmm(\n                    feature_pred_down.permute(0, 2, 1), feature_pred_down\n                )\n\n                feature_ditill_loss = (\n                    feature_ditill_loss\n                    + F.l1_loss(rgb_affinity, depth_affinity, reduction=\"mean\") / B\n                )\n\n        else:\n            raise NotImplementedError\n\n        return feature_ditill_loss\n\n\n@LOSSES.register_module()\nclass FeatureLoss_Coefficient(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n\n    def corrcoef(self, target, pred):\n        pred_n = pred - pred.mean()\n        target_n = target - target.mean()\n        pred_n = pred_n / pred_n.norm()\n        target_n = target_n / target_n.norm()\n        return (pred_n * target_n).sum()\n\n    def forward(\n        self,\n        pred,\n        target,\n        regularization=\"l2\",\n        regularization_strength=1.0,\n    ):\n        pred = [pred.clone()]\n        target = [target.clone()]\n        spearman_loss = 0.0\n\n        resize_shape = pred[-1].shape[-2:]  # save training time\n\n        if isinstance(pred, list):\n            for i in range(len(pred)):\n                feature_target = target[i]\n                feature_pred = pred[i]\n                B, C, H, W = feature_pred.shape\n\n                feature_pred_down = F.interpolate(\n                    feature_pred, size=resize_shape, mode=\"bilinear\"\n                )\n                feature_target_down = F.interpolate(\n                    feature_target, size=resize_shape, mode=\"bilinear\"\n                )\n\n                # if feature_pred.size(-1) != feature_target.size(-1):\n\n                #     feature_pred_down = F.interpolate(\n                #         feature_pred, size=resize_shape, mode=\"bilinear\"\n                #     )\n                #     feature_target_down = F.interpolate(\n                #         feature_target, size=resize_shape, mode=\"bilinear\"\n                #     )\n                # else:\n                #     feature_pred_down = feature_pred\n                #     feature_target_down = feature_target\n\n                feature_pred_down = feature_pred_down.reshape(B, -1)\n                feature_target_down = feature_target_down.reshape(B, -1)\n\n                feature_pred_down = torchsort.soft_rank(\n                    feature_pred_down,\n                    regularization=regularization,\n                    regularization_strength=regularization_strength,\n                )\n                spearman_loss += 1 - self.corrcoef(\n                    feature_target_down, feature_pred_down / feature_pred_down.shape[-1]\n                )\n\n        return spearman_loss\n\n\n@LOSSES.register_module()\nclass Radar_MSDistilll(nn.Module):\n    def __init__(self, num_layers=2, each_layer_loss_cfg=[]):\n        super().__init__()\n        self.num_layers = num_layers\n        assert num_layers == len(each_layer_loss_cfg)\n        for idx, cfg_dict in enumerate(each_layer_loss_cfg):\n            setattr(self, f\"layer_loss_{idx}\", build_loss(cfg_dict))\n\n    def forward(self, radar_ms_feats, pts_ms_feats):\n        assert isinstance(radar_ms_feats, list)\n        losses = 0.0\n\n        for idx in range(self.num_layers):\n            losses += getattr(self, f\"layer_loss_{idx}\")(\n                radar_ms_feats[idx], pts_ms_feats[idx]\n            )\n\n        return losses / self.num_layers\n\n\n@LOSSES.register_module()\nclass InfoMax(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n\n    def forward(self, x1, x2):\n        import pdb\n\n        pdb.set_trace()\n        x1 = x1 / (torch.norm(x1, p=2, dim=1, keepdim=True) + 1e-10)\n        x2 = x2 / (torch.norm(x2, p=2, dim=1, keepdim=True) + 1e-10)\n        bs = x1.size(0)\n        s = torch.matmul(x1, x2.permute(1, 0))\n        mask_joint = torch.eye(bs).cuda()\n        mask_marginal = 1 - mask_joint\n\n        Ej = (s * mask_joint).mean()\n        Em = torch.exp(s * mask_marginal).mean()\n        # decoupled comtrastive learning?!!!!\n        # infomax_loss = - (Ej - torch.log(Em)) * self.alpha\n        infomax_loss = -(Ej - torch.log(Em))  # / Em\n        return infomax_loss\n\n\n@LOSSES.register_module()\nclass HeatMapAug(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    @force_fp32(apply_to=(\"stu_pred\", \"tea_pred\"))\n    def forward(self, stu_pred, tea_pred, fg_map):\n\n        num_task = len(stu_pred)\n        kl_loss = 0\n        for task_id in range(num_task):\n            student_pred = stu_pred[task_id][0][\"heatmap\"].sigmoid()\n            teacher_pred = tea_pred[task_id][0][\"heatmap\"].sigmoid()\n            fg_map = fg_map.unsqueeze(1)\n            task_kl_loss = F.binary_cross_entropy(student_pred, teacher_pred.detach())\n            task_kl_loss = torch.sum(task_kl_loss * fg_map) / torch.sum(fg_map)\n            kl_loss += task_kl_loss\n\n        return kl_loss\n\n\n@LOSSES.register_module()\nclass Dc_ResultDistill(nn.Module):\n    def __init__(\n        self,\n        pc_range=[],\n        voxel_size=[],\n        out_size_scale=8,\n        ret_sum=False,\n        loss_weight_reg=10,\n        loss_weight_cls=10,\n        max_cls=True,\n    ):\n        super().__init__()\n\n        self.pc_range = pc_range\n        self.voxel_size = voxel_size\n        self.out_size_scale = out_size_scale\n        self.ret_sum = ret_sum\n        self.loss_weight_reg = loss_weight_reg\n        self.loss_weight_cls = loss_weight_cls\n        self.max_cls = max_cls\n\n    def forward(self, resp_lidar, resp_fuse, gt_boxes):\n        \"\"\"Dc_ResultDistill forward.\n\n        Args:\n            resp_lidar (_type_):\n            resp_fuse (_type_):\n            gt_boxes (_type_):\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n\n        tmp_resp_lidar = []\n        tmp_resp_fuse = []\n        for res_lidar, res_fuse in zip(resp_lidar, resp_fuse):\n            tmp_resp_lidar.append(res_lidar[0])\n            tmp_resp_fuse.append(res_fuse[0])\n\n        tmp_gt_boxes = []\n        for bs_idx in range(len(gt_boxes)):\n\n            gt_bboxes_3d = torch.cat(\n                (gt_boxes[bs_idx].gravity_center, gt_boxes[bs_idx].tensor[:, 3:]), dim=1\n            )\n\n            tmp_gt_boxes.append(gt_bboxes_3d)\n\n        cls_lidar = []\n        reg_lidar = []\n        cls_fuse = []\n        reg_fuse = []\n\n        # criterion = nn.L1Loss(reduce=False)\n        # criterion_cls = QualityFocalLoss_()\n\n        criterion = nn.SmoothL1Loss(reduce=False)\n        criterion_cls = nn.L1Loss(reduce=False)\n\n        for task_id, task_out in enumerate(tmp_resp_lidar):\n            cls_lidar.append(task_out[\"heatmap\"])\n            cls_fuse.append(_sigmoid(tmp_resp_fuse[task_id][\"heatmap\"] / 2))\n            reg_lidar.append(\n                torch.cat(\n                    [\n                        task_out[\"reg\"],\n                        task_out[\"height\"],\n                        task_out[\"dim\"],\n                        task_out[\"rot\"],\n                        task_out[\"vel\"],\n                        # task_out[\"iou\"],\n                    ],\n                    dim=1,\n                )\n            )\n            reg_fuse.append(\n                torch.cat(\n                    [\n                        tmp_resp_fuse[task_id][\"reg\"],\n                        tmp_resp_fuse[task_id][\"height\"],\n                        tmp_resp_fuse[task_id][\"dim\"],\n                        tmp_resp_fuse[task_id][\"rot\"],\n                        tmp_resp_fuse[task_id][\"vel\"],\n                        # resp_fuse[task_id][\"iou\"],\n                    ],\n                    dim=1,\n                )\n            )\n        cls_lidar = torch.cat(cls_lidar, dim=1)\n        reg_lidar = torch.cat(reg_lidar, dim=1)\n        cls_fuse = torch.cat(cls_fuse, dim=1)\n        reg_fuse = torch.cat(reg_fuse, dim=1)\n\n        if self.max_cls:\n            cls_lidar_max, _ = torch.max(cls_lidar, dim=1)\n            cls_fuse_max, _ = torch.max(cls_fuse, dim=1)\n        else:\n            _, _, ht_h, ht_w = cls_fuse.shape\n            cls_lidar_max = cls_lidar.flatten(2).permute(0, 2, 1)\n            cls_fuse_max = cls_fuse.flatten(2).permute(0, 2, 1)\n\n        gaussian_mask = calculate_box_mask_gaussian(\n            reg_lidar.shape,\n            tmp_gt_boxes,\n            self.pc_range,\n            self.voxel_size,\n            self.out_size_scale,\n        )\n\n        # # diff_reg = criterion(reg_lidar, reg_fuse)\n        # # diff_cls = criterion(cls_lidar_max, cls_fuse_max)\n        # diff_reg = criterion_reg(reg_lidar, reg_fuse)\n\n        # cls_lidar_max = torch.sigmoid(cls_lidar_max)\n        # diff_cls = criterion_cls(\n        #     cls_fuse_max, cls_lidar_max\n        # )  # Compared with directly using the L1 loss constraint, replace it with bce focal loss and change the position?\n\n        # weight = gaussian_mask.sum()\n        # weight = reduce_mean(weight)\n\n        # # diff_cls = criterion_cls(cls_lidar_max, cls_fuse_max)\n        # diff_reg = torch.mean(diff_reg, dim=1)\n        # diff_reg = diff_reg * gaussian_mask\n        # loss_reg_distill = torch.sum(diff_reg) / (weight + 1e-4)\n\n        # if not self.max_cls:\n        #     diff_cls = diff_cls\n        #     loss_cls_distill = diff_cls.sum() * 2\n        # else:\n        #     diff_cls = diff_cls * gaussian_mask\n        #     loss_cls_distill = torch.sum(diff_cls) / (weight + 1e-4)\n\n        diff_reg = criterion(reg_lidar, reg_fuse)\n\n        diff_cls = criterion_cls(cls_lidar_max, cls_fuse_max)\n        diff_reg = torch.mean(diff_reg, dim=1)\n        diff_reg = diff_reg * gaussian_mask\n        diff_cls = diff_cls * gaussian_mask\n        weight = gaussian_mask.sum()\n        weight = reduce_mean(weight)\n        loss_reg_distill = torch.sum(diff_reg) / (weight + 1e-4)\n        loss_cls_distill = torch.sum(diff_cls) / (weight + 1e-4)\n\n        if self.ret_sum:\n\n            loss_det_distill = self.loss_weight * (loss_reg_distill + loss_cls_distill)\n\n            return loss_det_distill\n        else:\n\n            return (\n                self.loss_weight_reg * loss_reg_distill,\n                self.loss_weight_cls * loss_cls_distill,\n            )\n\n\n@LOSSES.register_module()\nclass SelfLearningMFD(nn.Module):\n    def __init__(\n        self,\n        bev_shape=[128, 128],\n        pc_range=[-51.2, -51.2, -5, 51.2, 51.2, 3],\n        voxel_size=[0.1, 0.1, 0.1],\n        score_threshold=0.7,\n        add_stu_decode_bboxes=False,\n        loss_weight=1e-2,\n        bbox_coder=None,\n        student_channels=256,\n        teacher_channels=512,\n    ):\n        super().__init__()\n        self.bev_shape = bev_shape\n        self.pc_range = pc_range\n        self.voxel_size = voxel_size\n        self.shape_resize_times = bbox_coder.out_size_factor if bbox_coder else 8\n        self.score_threshold = score_threshold\n        self.add_stu_decode_bboxes = add_stu_decode_bboxes\n        self.loss_weight = loss_weight\n\n        self.align = nn.Sequential(\n            nn.Conv2d(student_channels, teacher_channels, kernel_size=1, padding=0),\n            # nn.BatchNorm2d(teacher_channels),\n        )\n\n        if bbox_coder is not None:\n            self.bbox_coder = build_bbox_coder(bbox_coder)\n\n    def pred2bboxes(self, preds_dict):\n        \"\"\"SelfLearningMFD-pred2bboxes forward\n\n        Args:\n            pred_bboxes (list):\n            [task1[[task_head_reg,task_head_heatmap,...,]],\n            task2...,\n            taskn]\n        \"\"\"\n\n        for task_id, pred_data in enumerate(preds_dict):\n            bath_tmp = []\n            batch_size = pred_data[0][\"heatmap\"].shape[0]\n            batch_heatmap = pred_data[0][\"heatmap\"].sigmoid()\n            batch_reg = pred_data[0][\"reg\"]\n            batch_hei = pred_data[0][\"height\"]\n\n            # denormalization\n            batch_dim = torch.exp(pred_data[0][\"dim\"])\n\n            batch_rot_sin = pred_data[0][\"rot\"][:, 0].unsqueeze(1)\n            batch_rot_cos = pred_data[0][\"rot\"][:, 1].unsqueeze(1)\n\n            if \"vel\" in pred_data[0].keys():\n                batch_vel = pred_data[0][\"vel\"]\n\n            bath_tmp.append(batch_heatmap)\n            bath_tmp.append(batch_rot_sin)\n            bath_tmp.append(batch_rot_cos)\n            bath_tmp.append(batch_hei)\n            bath_tmp.append(batch_dim)\n            bath_tmp.append(batch_vel)\n            bath_tmp.append(batch_reg)\n\n            bboxes_decode = self.bbox_coder.decode(*bath_tmp)\n\n        return bboxes_decode\n\n    def aug_boxes(self, gt_boxes, rot_mat):\n        rot_lim = (-22.5, 22.5)\n        scale_lim = (0.95, 1.05)\n        rotate_angle = np.random.uniform(*rot_lim)\n        scale_ratio = np.random.uniform(*scale_lim)\n        gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n        gt_boxes[:, 3:6] *= scale_ratio\n        gt_boxes[:, 6] += rotate_angle\n        gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n        gt_boxes[:, 6] = -gt_boxes[:, 6]\n        gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(-1)\n        gt_boxes[:, :3] = gt_boxes[:, :3]\n\n        return gt_boxes\n\n    @force_fp32(\n        apply_to=(\n            \"student_bev_feat\",\n            \"teacher_bev_feat\",\n        )\n    )\n    def forward(\n        self,\n        student_bev_feat,\n        teacher_bev_feat,\n        gt_bboxes_list=None,\n        masks_bboxes=None,\n        bda_mat=None,\n    ):\n        \"\"\"SelfLearningMFD forward.\n\n        Args:\n            student_bev_feats (torch.tensor): Calculate student feats\n            teacher_bev_feats (torch.tensor): Calculate teacher feats\n            masks_bboxes (list): Self-learning mask detection\n            gt_bboxes_list (list): [LiDARInstance3DBoxes(gravity_center+tensor(num_objs,9))]\n\n        Returns:\n            dict: _description_\n        \"\"\"\n\n        bs = student_bev_feat.size(0)\n\n        if student_bev_feat.size(1) != teacher_bev_feat.size(1):\n            student_bev_feat = self.align(student_bev_feat)\n\n        if masks_bboxes is not None and self.add_stu_decode_bboxes:\n            student_pred_bboxes_list = self.pred2bboxes(\n                masks_bboxes\n            )  # list of task : each shape of [(num_bboxes,9)]\n\n            if bda_mat is not None:\n                student_pred_bboxes_list = self.aug_boxes(student_pred_bboxes_list)\n\n        bev_feat_shape = torch.tensor(self.bev_shape)\n        voxel_size = torch.tensor(self.voxel_size)\n        feature_map_size = bev_feat_shape[:2]\n\n        if gt_bboxes_list is not None:\n            device = student_bev_feat.device\n\n            gt_bboxes_list = [\n                torch.cat(\n                    (gt_bboxes.gravity_center, gt_bboxes.tensor[:, 3:]), dim=1\n                ).to(device)\n                for gt_bboxes in gt_bboxes_list\n            ]\n\n            if self.add_stu_decode_bboxes:\n                for idx, data in enumerate(student_pred_bboxes_list):\n\n                    bboxes_dim = data[\"bboxes\"].size(1)\n\n                    scroes_mask = data[\"scores\"] > self.score_threshold\n\n                    new_select_by_mask = data[\"bboxes\"][scroes_mask, :]\n\n                    gt_bboxes_list[idx] = torch.cat(\n                        [gt_bboxes_list[idx], new_select_by_mask], dim=0\n                    )\n\n            fg_map = student_bev_feat.new_zeros(\n                (len(gt_bboxes_list), feature_map_size[1], feature_map_size[0])\n            )\n\n            for idx in range(len(gt_bboxes_list)):\n                num_objs = gt_bboxes_list[idx].shape[0]\n\n                for k in range(num_objs):\n                    width = gt_bboxes_list[idx][k][3]\n                    length = gt_bboxes_list[idx][k][4]\n                    width = width / voxel_size[0] / self.shape_resize_times\n                    length = length / voxel_size[1] / self.shape_resize_times\n\n                    if width > 0 and length > 0:\n                        radius = gaussian_radius((length, width), min_overlap=0.1)\n                        radius = max(1, int(radius))\n\n                        # be really careful for the coordinate system of\n                        # your box annotation.\n                        x, y, z = (\n                            gt_bboxes_list[idx][k][0],\n                            gt_bboxes_list[idx][k][1],\n                            gt_bboxes_list[idx][k][2],\n                        )\n\n                        coor_x = (\n                            (x - self.pc_range[0])\n                            / voxel_size[0]\n                            / self.shape_resize_times\n                        )\n                        coor_y = (\n                            (y - self.pc_range[1])\n                            / voxel_size[1]\n                            / self.shape_resize_times\n                        )\n\n                        center = torch.tensor(\n                            [coor_x, coor_y], dtype=torch.float32, device=device\n                        )\n                        center_int = center.to(torch.int32)\n\n                        # throw out not in range objects to avoid out of array\n                        # area when creating the heatmap\n                        if not (\n                            0 <= center_int[0] < feature_map_size[0]\n                            and 0 <= center_int[1] < feature_map_size[1]\n                        ):\n                            continue\n\n                        draw_heatmap_gaussian(fg_map[idx], center_int, radius)\n\n        if fg_map is None:\n            fg_map = student_bev_feat.new_ones(\n                (student_bev_feat.shape[0], feature_map_size[1], feature_map_size[0])\n            )\n\n        if bs > 1:\n            fg_map = fg_map.unsqueeze(1)\n\n        fit_loss = F.mse_loss(student_bev_feat, teacher_bev_feat, reduction=\"none\")\n        fit_loss = torch.sum(fit_loss * fg_map) / torch.sum(fg_map)\n\n        return fit_loss * self.loss_weight, fg_map\n\n    # def forward(\n    #     self, student_bev_feats, teacher_bev_feats, masks_bboxes, gt_bboxes_list\n    # ):\n    #     \"\"\"SelfLearningMFD forward.\n\n    #     Args:\n    #         student_bev_feats (torch.tensor): _description_\n    #         teacher_bev_feats (torch.tensor): _description_\n    #         masks_bboxes (): _description_\n    #         gt_bboxes_list (_type_): _description_\n    #     \"\"\"\n    #     assert isinstance(masks_bboxes, list)\n    #     ret_dict = multi_apply(\n    #         self._forward,\n    #         student_bev_feats,\n    #         teacher_bev_feats,\n    #         masks_bboxes,\n    #         gt_bboxes_list,\n    #     )\n"
        }
    ]
}