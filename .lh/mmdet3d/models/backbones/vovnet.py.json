{
    "sourceFile": "mmdet3d/models/backbones/vovnet.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 4,
            "patches": [
                {
                    "date": 1716025156052,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1716025231767,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,91 +1,83 @@\n-# ------------------------------------------------------------------------\n-# Copyright (c) 2022 megvii-model. All Rights Reserved.\n-# ------------------------------------------------------------------------\n-# Modified from DETR3D (https://github.com/WangYueFt/detr3d)\n-# Copyright (c) 2021 Wang, Yue\n-# ------------------------------------------------------------------------\n-# Copyright (c) Youngwan Lee (ETRI) All Rights Reserved.\n-# Copyright 2021 Toyota Research Institute.  All rights reserved.\n-# ------------------------------------------------------------------------\n+# adapted from DETR3D\n from collections import OrderedDict\n-from mmcv.runner import BaseModule\n+from mmcv.runner import BaseModule, force_fp32\n from mmdet.models.builder import BACKBONES\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n from torch.nn.modules.batchnorm import _BatchNorm\n-import warnings\n-import torch.utils.checkpoint as cp\n \n+__all__ = [\"VoVNet\"]\n+\n VoVNet19_slim_dw_eSE = {\n-    \"stem\": [64, 64, 64],\n-    \"stage_conv_ch\": [64, 80, 96, 112],\n-    \"stage_out_ch\": [112, 256, 384, 512],\n+    'stem': [64, 64, 64],\n+    'stage_conv_ch': [64, 80, 96, 112],\n+    'stage_out_ch': [112, 256, 384, 512],\n     \"layer_per_block\": 3,\n     \"block_per_stage\": [1, 1, 1, 1],\n     \"eSE\": True,\n-    \"dw\": True,\n+    \"dw\": True\n }\n \n VoVNet19_dw_eSE = {\n-    \"stem\": [64, 64, 64],\n+    'stem': [64, 64, 64],\n     \"stage_conv_ch\": [128, 160, 192, 224],\n     \"stage_out_ch\": [256, 512, 768, 1024],\n     \"layer_per_block\": 3,\n     \"block_per_stage\": [1, 1, 1, 1],\n     \"eSE\": True,\n-    \"dw\": True,\n+    \"dw\": True\n }\n \n VoVNet19_slim_eSE = {\n-    \"stem\": [64, 64, 128],\n-    \"stage_conv_ch\": [64, 80, 96, 112],\n-    \"stage_out_ch\": [112, 256, 384, 512],\n-    \"layer_per_block\": 3,\n-    \"block_per_stage\": [1, 1, 1, 1],\n-    \"eSE\": True,\n-    \"dw\": False,\n+    'stem': [64, 64, 128],\n+    'stage_conv_ch': [64, 80, 96, 112],\n+    'stage_out_ch': [112, 256, 384, 512],\n+    'layer_per_block': 3,\n+    'block_per_stage': [1, 1, 1, 1],\n+    'eSE': True,\n+    \"dw\": False\n }\n \n VoVNet19_eSE = {\n-    \"stem\": [64, 64, 128],\n+    'stem': [64, 64, 128],\n     \"stage_conv_ch\": [128, 160, 192, 224],\n     \"stage_out_ch\": [256, 512, 768, 1024],\n     \"layer_per_block\": 3,\n     \"block_per_stage\": [1, 1, 1, 1],\n     \"eSE\": True,\n-    \"dw\": False,\n+    \"dw\": False\n }\n \n VoVNet39_eSE = {\n-    \"stem\": [64, 64, 128],\n+    'stem': [64, 64, 128],\n     \"stage_conv_ch\": [128, 160, 192, 224],\n     \"stage_out_ch\": [256, 512, 768, 1024],\n     \"layer_per_block\": 5,\n     \"block_per_stage\": [1, 1, 2, 2],\n     \"eSE\": True,\n-    \"dw\": False,\n+    \"dw\": False\n }\n \n VoVNet57_eSE = {\n-    \"stem\": [64, 64, 128],\n+    'stem': [64, 64, 128],\n     \"stage_conv_ch\": [128, 160, 192, 224],\n     \"stage_out_ch\": [256, 512, 768, 1024],\n     \"layer_per_block\": 5,\n     \"block_per_stage\": [1, 1, 4, 3],\n     \"eSE\": True,\n-    \"dw\": False,\n+    \"dw\": False\n }\n \n VoVNet99_eSE = {\n-    \"stem\": [64, 64, 128],\n+    'stem': [64, 64, 128],\n     \"stage_conv_ch\": [128, 160, 192, 224],\n     \"stage_out_ch\": [256, 512, 768, 1024],\n     \"layer_per_block\": 5,\n     \"block_per_stage\": [1, 3, 9, 3],\n     \"eSE\": True,\n-    \"dw\": False,\n+    \"dw\": False\n }\n \n _STAGE_SPECS = {\n     \"V-19-slim-dw-eSE\": VoVNet19_slim_dw_eSE,\n@@ -97,52 +89,33 @@\n     \"V-99-eSE\": VoVNet99_eSE,\n }\n \n \n-def dw_conv3x3(\n-    in_channels, out_channels, module_name, postfix, stride=1, kernel_size=3, padding=1\n-):\n+def dw_conv3x3(in_channels, out_channels, module_name, postfix, stride=1, kernel_size=3, padding=1):\n     \"\"\"3x3 convolution with padding\"\"\"\n     return [\n         (\n-            \"{}_{}/dw_conv3x3\".format(module_name, postfix),\n+            '{}_{}/dw_conv3x3'.format(module_name, postfix),\n             nn.Conv2d(\n                 in_channels,\n                 out_channels,\n                 kernel_size=kernel_size,\n                 stride=stride,\n                 padding=padding,\n                 groups=out_channels,\n-                bias=False,\n-            ),\n+                bias=False\n+            )\n         ),\n         (\n-            \"{}_{}/pw_conv1x1\".format(module_name, postfix),\n-            nn.Conv2d(\n-                in_channels,\n-                out_channels,\n-                kernel_size=1,\n-                stride=1,\n-                padding=0,\n-                groups=1,\n-                bias=False,\n-            ),\n+            '{}_{}/pw_conv1x1'.format(module_name, postfix),\n+            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=1, bias=False)\n         ),\n-        (\"{}_{}/pw_norm\".format(module_name, postfix), nn.BatchNorm2d(out_channels)),\n-        (\"{}_{}/pw_relu\".format(module_name, postfix), nn.ReLU(inplace=True)),\n+        ('{}_{}/pw_norm'.format(module_name, postfix), nn.BatchNorm2d(out_channels)),\n+        ('{}_{}/pw_relu'.format(module_name, postfix), nn.ReLU(inplace=True)),\n     ]\n \n \n-def conv3x3(\n-    in_channels,\n-    out_channels,\n-    module_name,\n-    postfix,\n-    stride=1,\n-    groups=1,\n-    kernel_size=3,\n-    padding=1,\n-):\n+def conv3x3(in_channels, out_channels, module_name, postfix, stride=1, groups=1, kernel_size=3, padding=1):\n     \"\"\"3x3 convolution with padding\"\"\"\n     return [\n         (\n             f\"{module_name}_{postfix}/conv\",\n@@ -160,18 +133,9 @@\n         (f\"{module_name}_{postfix}/relu\", nn.ReLU(inplace=True)),\n     ]\n \n \n-def conv1x1(\n-    in_channels,\n-    out_channels,\n-    module_name,\n-    postfix,\n-    stride=1,\n-    groups=1,\n-    kernel_size=1,\n-    padding=0,\n-):\n+def conv1x1(in_channels, out_channels, module_name, postfix, stride=1, groups=1, kernel_size=1, padding=0):\n     \"\"\"1x1 convolution with padding\"\"\"\n     return [\n         (\n             f\"{module_name}_{postfix}/conv\",\n@@ -189,87 +153,67 @@\n         (f\"{module_name}_{postfix}/relu\", nn.ReLU(inplace=True)),\n     ]\n \n \n-class Hsigmoid(nn.Module):\n+class Hsigmoid(BaseModule):\n     def __init__(self, inplace=True):\n         super(Hsigmoid, self).__init__()\n         self.inplace = inplace\n+        self.fp16_enabled = False\n \n+    @force_fp32()\n     def forward(self, x):\n         return F.relu6(x + 3.0, inplace=self.inplace) / 6.0\n \n \n-class eSEModule(nn.Module):\n+class eSEModule(BaseModule):\n     def __init__(self, channel, reduction=4):\n         super(eSEModule, self).__init__()\n         self.avg_pool = nn.AdaptiveAvgPool2d(1)\n         self.fc = nn.Conv2d(channel, channel, kernel_size=1, padding=0)\n         self.hsigmoid = Hsigmoid()\n+        self.fp16_enabled = False\n \n+    @force_fp32()\n     def forward(self, x):\n         input = x\n         x = self.avg_pool(x)\n         x = self.fc(x)\n         x = self.hsigmoid(x)\n         return input * x\n \n \n-class _OSA_module(nn.Module):\n+class _OSA_module(BaseModule):\n     def __init__(\n-        self,\n-        in_ch,\n-        stage_ch,\n-        concat_ch,\n-        layer_per_block,\n-        module_name,\n-        SE=False,\n-        identity=False,\n-        depthwise=False,\n-        with_cp=True,\n+        self, in_ch, stage_ch, concat_ch, layer_per_block, module_name, SE=False, identity=False, depthwise=False\n     ):\n \n         super(_OSA_module, self).__init__()\n \n         self.identity = identity\n         self.depthwise = depthwise\n         self.isReduced = False\n-        self.use_checkpoint = with_cp\n         self.layers = nn.ModuleList()\n         in_channel = in_ch\n         if self.depthwise and in_channel != stage_ch:\n             self.isReduced = True\n             self.conv_reduction = nn.Sequential(\n-                OrderedDict(\n-                    conv1x1(\n-                        in_channel, stage_ch, \"{}_reduction\".format(module_name), \"0\"\n-                    )\n-                )\n+                OrderedDict(conv1x1(in_channel, stage_ch, \"{}_reduction\".format(module_name), \"0\"))\n             )\n         for i in range(layer_per_block):\n             if self.depthwise:\n-                self.layers.append(\n-                    nn.Sequential(\n-                        OrderedDict(dw_conv3x3(stage_ch, stage_ch, module_name, i))\n-                    )\n-                )\n+                self.layers.append(nn.Sequential(OrderedDict(dw_conv3x3(stage_ch, stage_ch, module_name, i))))\n             else:\n-                self.layers.append(\n-                    nn.Sequential(\n-                        OrderedDict(conv3x3(in_channel, stage_ch, module_name, i))\n-                    )\n-                )\n+                self.layers.append(nn.Sequential(OrderedDict(conv3x3(in_channel, stage_ch, module_name, i))))\n             in_channel = stage_ch\n \n         # feature aggregation\n         in_channel = in_ch + layer_per_block * stage_ch\n-        self.concat = nn.Sequential(\n-            OrderedDict(conv1x1(in_channel, concat_ch, module_name, \"concat\"))\n-        )\n+        self.concat = nn.Sequential(OrderedDict(conv1x1(in_channel, concat_ch, module_name, \"concat\")))\n \n         self.ese = eSEModule(concat_ch)\n \n-    def _forward(self, x):\n+    def forward(self, x):\n \n         identity_feat = x\n \n         output = []\n@@ -289,52 +233,24 @@\n             xt = xt + identity_feat\n \n         return xt\n \n-    def forward(self, x):\n \n-        if self.use_checkpoint and self.training:\n-            xt = cp.checkpoint(self._forward, x)\n-        else:\n-            xt = self._forward(x)\n-\n-        return xt\n-\n-\n class _OSA_stage(nn.Sequential):\n     def __init__(\n-        self,\n-        in_ch,\n-        stage_ch,\n-        concat_ch,\n-        block_per_stage,\n-        layer_per_block,\n-        stage_num,\n-        SE=False,\n-        depthwise=False,\n+        self, in_ch, stage_ch, concat_ch, block_per_stage, layer_per_block, stage_num, SE=False, depthwise=False\n     ):\n \n         super(_OSA_stage, self).__init__()\n \n         if not stage_num == 2:\n-            self.add_module(\n-                \"Pooling\", nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n-            )\n+            self.add_module(\"Pooling\", nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True))\n \n         if block_per_stage != 1:\n             SE = False\n         module_name = f\"OSA{stage_num}_1\"\n         self.add_module(\n-            module_name,\n-            _OSA_module(\n-                in_ch,\n-                stage_ch,\n-                concat_ch,\n-                layer_per_block,\n-                module_name,\n-                SE,\n-                depthwise=depthwise,\n-            ),\n+            module_name, _OSA_module(in_ch, stage_ch, concat_ch, layer_per_block, module_name, SE, depthwise=depthwise)\n         )\n         for i in range(block_per_stage - 1):\n             if i != block_per_stage - 2:  # last block\n                 SE = False\n@@ -348,44 +264,28 @@\n                     layer_per_block,\n                     module_name,\n                     SE,\n                     identity=True,\n-                    depthwise=depthwise,\n+                    depthwise=depthwise\n                 ),\n             )\n \n \n @BACKBONES.register_module()\n-class VoVNetCP(BaseModule):\n-    def __init__(\n-        self,\n-        spec_name,\n-        input_ch=3,\n-        out_features=None,\n-        frozen_stages=-1,\n-        norm_eval=True,\n-        pretrained=None,\n-        init_cfg=None,\n-    ):\n+class VoVNet(BaseModule):\n+    def __init__(self, spec_name, input_ch=3, out_features=None, \n+                 frozen_stages=-1, norm_eval=True, pretrained=None, init_cfg=None):\n         \"\"\"\n         Args:\n             input_ch(int) : the number of input channel\n             out_features (list[str]): name of the layers whose outputs should\n                 be returned in forward. Can be anything in \"stem\", \"stage2\" ...\n         \"\"\"\n-        super(VoVNetCP, self).__init__(init_cfg)\n+        super(VoVNet, self).__init__(init_cfg)\n         self.frozen_stages = frozen_stages\n         self.norm_eval = norm_eval\n \n-        self.pretrained_weight = pretrained\n-\n-        if isinstance(pretrained, str):\n-            warnings.warn(\n-                \"DeprecationWarning: pretrained is deprecated, \"\n-                'please use \"init_cfg\" instead'\n-            )\n-            self.init_cfg = dict(type=\"Pretrained\", checkpoint=pretrained)\n-\n+        #self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)\n         stage_specs = _STAGE_SPECS[spec_name]\n \n         stem_ch = stage_specs[\"stem\"]\n         config_stage_ch = stage_specs[\"stage_conv_ch\"]\n@@ -429,66 +329,26 @@\n             )\n \n             self._out_feature_channels[name] = config_concat_ch[i]\n             if not i == 0:\n-                self._out_feature_strides[name] = current_stirde = int(\n-                    current_stirde * 2\n-                )\n+                self._out_feature_strides[name] = current_stirde = int(current_stirde * 2)\n \n         # initialize weights\n         # self._initialize_weights()\n+        self.fp16_enabled = False\n \n     def _initialize_weights(self):\n         for m in self.modules():\n             if isinstance(m, nn.Conv2d):\n                 nn.init.kaiming_normal_(m.weight)\n \n-    # def forward(self, x):\n-    #     outputs = {}\n-    #     x = self.stem(x)\n-    #     if \"stem\" in self._out_features:\n-    #         outputs[\"stem\"] = x\n-    #     for name in self.stage_names:\n-    #         x = getattr(self, name)(x)\n-    #         if name in self._out_features:\n-    #             outputs[name] = x\n-\n-    #     return outputs\n-\n     def forward(self, x):\n-\n-        outputs = []\n+        outputs = {}\n         x = self.stem(x)\n-        \n         if \"stem\" in self._out_features:\n-            outputs.append(x)\n-\n+            outputs[\"stem\"] = x\n         for name in self.stage_names:\n             x = getattr(self, name)(x)\n             if name in self._out_features:\n-                outputs.append(x)\n+                outputs[name] = x\n \n         return outputs\n-\n-    def _freeze_stages(self):\n-        if self.frozen_stages >= 0:\n-            m = getattr(self, \"stem\")\n-            m.eval()\n-            for param in m.parameters():\n-                param.requires_grad = False\n-\n-        for i in range(1, self.frozen_stages + 1):\n-            m = getattr(self, f\"stage{i+1}\")\n-            m.eval()\n-            for param in m.parameters():\n-                param.requires_grad = False\n-\n-    def train(self, mode=True):\n-        \"\"\"Convert the model into training mode while keep normalization layer\n-        freezed.\"\"\"\n-        super(VoVNetCP, self).train(mode)\n-        self._freeze_stages()\n-        if mode and self.norm_eval:\n-            for m in self.modules():\n-                # trick: eval have effect on BatchNorm only\n-                if isinstance(m, _BatchNorm):\n-                    m.eval()\n"
                },
                {
                    "date": 1716025510998,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -349,6 +349,6 @@\n         for name in self.stage_names:\n             x = getattr(self, name)(x)\n             if name in self._out_features:\n                 outputs[name] = x\n-\n+        \n         return outputs\n"
                },
                {
                    "date": 1716025524392,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -349,6 +349,7 @@\n         for name in self.stage_names:\n             x = getattr(self, name)(x)\n             if name in self._out_features:\n                 outputs[name] = x\n-        \n+        import pdb\n+        pdb.set_trace()\n         return outputs\n"
                },
                {
                    "date": 1716025749153,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,75 +9,75 @@\n \n __all__ = [\"VoVNet\"]\n \n VoVNet19_slim_dw_eSE = {\n-    'stem': [64, 64, 64],\n-    'stage_conv_ch': [64, 80, 96, 112],\n-    'stage_out_ch': [112, 256, 384, 512],\n+    \"stem\": [64, 64, 64],\n+    \"stage_conv_ch\": [64, 80, 96, 112],\n+    \"stage_out_ch\": [112, 256, 384, 512],\n     \"layer_per_block\": 3,\n     \"block_per_stage\": [1, 1, 1, 1],\n     \"eSE\": True,\n-    \"dw\": True\n+    \"dw\": True,\n }\n \n VoVNet19_dw_eSE = {\n-    'stem': [64, 64, 64],\n+    \"stem\": [64, 64, 64],\n     \"stage_conv_ch\": [128, 160, 192, 224],\n     \"stage_out_ch\": [256, 512, 768, 1024],\n     \"layer_per_block\": 3,\n     \"block_per_stage\": [1, 1, 1, 1],\n     \"eSE\": True,\n-    \"dw\": True\n+    \"dw\": True,\n }\n \n VoVNet19_slim_eSE = {\n-    'stem': [64, 64, 128],\n-    'stage_conv_ch': [64, 80, 96, 112],\n-    'stage_out_ch': [112, 256, 384, 512],\n-    'layer_per_block': 3,\n-    'block_per_stage': [1, 1, 1, 1],\n-    'eSE': True,\n-    \"dw\": False\n+    \"stem\": [64, 64, 128],\n+    \"stage_conv_ch\": [64, 80, 96, 112],\n+    \"stage_out_ch\": [112, 256, 384, 512],\n+    \"layer_per_block\": 3,\n+    \"block_per_stage\": [1, 1, 1, 1],\n+    \"eSE\": True,\n+    \"dw\": False,\n }\n \n VoVNet19_eSE = {\n-    'stem': [64, 64, 128],\n+    \"stem\": [64, 64, 128],\n     \"stage_conv_ch\": [128, 160, 192, 224],\n     \"stage_out_ch\": [256, 512, 768, 1024],\n     \"layer_per_block\": 3,\n     \"block_per_stage\": [1, 1, 1, 1],\n     \"eSE\": True,\n-    \"dw\": False\n+    \"dw\": False,\n }\n \n VoVNet39_eSE = {\n-    'stem': [64, 64, 128],\n+    \"stem\": [64, 64, 128],\n     \"stage_conv_ch\": [128, 160, 192, 224],\n     \"stage_out_ch\": [256, 512, 768, 1024],\n     \"layer_per_block\": 5,\n     \"block_per_stage\": [1, 1, 2, 2],\n     \"eSE\": True,\n-    \"dw\": False\n+    \"dw\": False,\n }\n \n VoVNet57_eSE = {\n-    'stem': [64, 64, 128],\n+    \"stem\": [64, 64, 128],\n     \"stage_conv_ch\": [128, 160, 192, 224],\n     \"stage_out_ch\": [256, 512, 768, 1024],\n     \"layer_per_block\": 5,\n     \"block_per_stage\": [1, 1, 4, 3],\n     \"eSE\": True,\n-    \"dw\": False\n+    \"dw\": False,\n }\n \n VoVNet99_eSE = {\n-    'stem': [64, 64, 128],\n+    \"stem\": [64, 64, 128],\n     \"stage_conv_ch\": [128, 160, 192, 224],\n     \"stage_out_ch\": [256, 512, 768, 1024],\n     \"layer_per_block\": 5,\n     \"block_per_stage\": [1, 3, 9, 3],\n     \"eSE\": True,\n-    \"dw\": False\n+    \"dw\": False,\n }\n \n _STAGE_SPECS = {\n     \"V-19-slim-dw-eSE\": VoVNet19_slim_dw_eSE,\n@@ -89,33 +89,52 @@\n     \"V-99-eSE\": VoVNet99_eSE,\n }\n \n \n-def dw_conv3x3(in_channels, out_channels, module_name, postfix, stride=1, kernel_size=3, padding=1):\n+def dw_conv3x3(\n+    in_channels, out_channels, module_name, postfix, stride=1, kernel_size=3, padding=1\n+):\n     \"\"\"3x3 convolution with padding\"\"\"\n     return [\n         (\n-            '{}_{}/dw_conv3x3'.format(module_name, postfix),\n+            \"{}_{}/dw_conv3x3\".format(module_name, postfix),\n             nn.Conv2d(\n                 in_channels,\n                 out_channels,\n                 kernel_size=kernel_size,\n                 stride=stride,\n                 padding=padding,\n                 groups=out_channels,\n-                bias=False\n-            )\n+                bias=False,\n+            ),\n         ),\n         (\n-            '{}_{}/pw_conv1x1'.format(module_name, postfix),\n-            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, groups=1, bias=False)\n+            \"{}_{}/pw_conv1x1\".format(module_name, postfix),\n+            nn.Conv2d(\n+                in_channels,\n+                out_channels,\n+                kernel_size=1,\n+                stride=1,\n+                padding=0,\n+                groups=1,\n+                bias=False,\n+            ),\n         ),\n-        ('{}_{}/pw_norm'.format(module_name, postfix), nn.BatchNorm2d(out_channels)),\n-        ('{}_{}/pw_relu'.format(module_name, postfix), nn.ReLU(inplace=True)),\n+        (\"{}_{}/pw_norm\".format(module_name, postfix), nn.BatchNorm2d(out_channels)),\n+        (\"{}_{}/pw_relu\".format(module_name, postfix), nn.ReLU(inplace=True)),\n     ]\n \n \n-def conv3x3(in_channels, out_channels, module_name, postfix, stride=1, groups=1, kernel_size=3, padding=1):\n+def conv3x3(\n+    in_channels,\n+    out_channels,\n+    module_name,\n+    postfix,\n+    stride=1,\n+    groups=1,\n+    kernel_size=3,\n+    padding=1,\n+):\n     \"\"\"3x3 convolution with padding\"\"\"\n     return [\n         (\n             f\"{module_name}_{postfix}/conv\",\n@@ -133,9 +152,18 @@\n         (f\"{module_name}_{postfix}/relu\", nn.ReLU(inplace=True)),\n     ]\n \n \n-def conv1x1(in_channels, out_channels, module_name, postfix, stride=1, groups=1, kernel_size=1, padding=0):\n+def conv1x1(\n+    in_channels,\n+    out_channels,\n+    module_name,\n+    postfix,\n+    stride=1,\n+    groups=1,\n+    kernel_size=1,\n+    padding=0,\n+):\n     \"\"\"1x1 convolution with padding\"\"\"\n     return [\n         (\n             f\"{module_name}_{postfix}/conv\",\n@@ -183,9 +211,17 @@\n \n \n class _OSA_module(BaseModule):\n     def __init__(\n-        self, in_ch, stage_ch, concat_ch, layer_per_block, module_name, SE=False, identity=False, depthwise=False\n+        self,\n+        in_ch,\n+        stage_ch,\n+        concat_ch,\n+        layer_per_block,\n+        module_name,\n+        SE=False,\n+        identity=False,\n+        depthwise=False,\n     ):\n \n         super(_OSA_module, self).__init__()\n \n@@ -196,20 +232,34 @@\n         in_channel = in_ch\n         if self.depthwise and in_channel != stage_ch:\n             self.isReduced = True\n             self.conv_reduction = nn.Sequential(\n-                OrderedDict(conv1x1(in_channel, stage_ch, \"{}_reduction\".format(module_name), \"0\"))\n+                OrderedDict(\n+                    conv1x1(\n+                        in_channel, stage_ch, \"{}_reduction\".format(module_name), \"0\"\n+                    )\n+                )\n             )\n         for i in range(layer_per_block):\n             if self.depthwise:\n-                self.layers.append(nn.Sequential(OrderedDict(dw_conv3x3(stage_ch, stage_ch, module_name, i))))\n+                self.layers.append(\n+                    nn.Sequential(\n+                        OrderedDict(dw_conv3x3(stage_ch, stage_ch, module_name, i))\n+                    )\n+                )\n             else:\n-                self.layers.append(nn.Sequential(OrderedDict(conv3x3(in_channel, stage_ch, module_name, i))))\n+                self.layers.append(\n+                    nn.Sequential(\n+                        OrderedDict(conv3x3(in_channel, stage_ch, module_name, i))\n+                    )\n+                )\n             in_channel = stage_ch\n \n         # feature aggregation\n         in_channel = in_ch + layer_per_block * stage_ch\n-        self.concat = nn.Sequential(OrderedDict(conv1x1(in_channel, concat_ch, module_name, \"concat\")))\n+        self.concat = nn.Sequential(\n+            OrderedDict(conv1x1(in_channel, concat_ch, module_name, \"concat\"))\n+        )\n \n         self.ese = eSEModule(concat_ch)\n \n     def forward(self, x):\n@@ -236,21 +286,40 @@\n \n \n class _OSA_stage(nn.Sequential):\n     def __init__(\n-        self, in_ch, stage_ch, concat_ch, block_per_stage, layer_per_block, stage_num, SE=False, depthwise=False\n+        self,\n+        in_ch,\n+        stage_ch,\n+        concat_ch,\n+        block_per_stage,\n+        layer_per_block,\n+        stage_num,\n+        SE=False,\n+        depthwise=False,\n     ):\n \n         super(_OSA_stage, self).__init__()\n \n         if not stage_num == 2:\n-            self.add_module(\"Pooling\", nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True))\n+            self.add_module(\n+                \"Pooling\", nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n+            )\n \n         if block_per_stage != 1:\n             SE = False\n         module_name = f\"OSA{stage_num}_1\"\n         self.add_module(\n-            module_name, _OSA_module(in_ch, stage_ch, concat_ch, layer_per_block, module_name, SE, depthwise=depthwise)\n+            module_name,\n+            _OSA_module(\n+                in_ch,\n+                stage_ch,\n+                concat_ch,\n+                layer_per_block,\n+                module_name,\n+                SE,\n+                depthwise=depthwise,\n+            ),\n         )\n         for i in range(block_per_stage - 1):\n             if i != block_per_stage - 2:  # last block\n                 SE = False\n@@ -264,17 +333,25 @@\n                     layer_per_block,\n                     module_name,\n                     SE,\n                     identity=True,\n-                    depthwise=depthwise\n+                    depthwise=depthwise,\n                 ),\n             )\n \n \n @BACKBONES.register_module()\n class VoVNet(BaseModule):\n-    def __init__(self, spec_name, input_ch=3, out_features=None, \n-                 frozen_stages=-1, norm_eval=True, pretrained=None, init_cfg=None):\n+    def __init__(\n+        self,\n+        spec_name,\n+        input_ch=3,\n+        out_features=None,\n+        frozen_stages=-1,\n+        norm_eval=True,\n+        pretrained=None,\n+        init_cfg=None,\n+    ):\n         \"\"\"\n         Args:\n             input_ch(int) : the number of input channel\n             out_features (list[str]): name of the layers whose outputs should\n@@ -283,9 +360,9 @@\n         super(VoVNet, self).__init__(init_cfg)\n         self.frozen_stages = frozen_stages\n         self.norm_eval = norm_eval\n \n-        #self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)\n+        # self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)\n         stage_specs = _STAGE_SPECS[spec_name]\n \n         stem_ch = stage_specs[\"stem\"]\n         config_stage_ch = stage_specs[\"stage_conv_ch\"]\n@@ -329,9 +406,11 @@\n             )\n \n             self._out_feature_channels[name] = config_concat_ch[i]\n             if not i == 0:\n-                self._out_feature_strides[name] = current_stirde = int(current_stirde * 2)\n+                self._out_feature_strides[name] = current_stirde = int(\n+                    current_stirde * 2\n+                )\n \n         # initialize weights\n         # self._initialize_weights()\n         self.fp16_enabled = False\n@@ -350,10 +429,11 @@\n             x = getattr(self, name)(x)\n             if name in self._out_features:\n                 outputs[name] = x\n         import pdb\n+\n         pdb.set_trace()\n-        \n-        _outputs=[]\n-        for k,v in outputs.items():\n+\n+        _outputs = []\n+        for k, v in outputs.items():\n             _outputs.append(v)\n         return outputs\n"
                }
            ],
            "date": 1716025156052,
            "name": "Commit-0",
            "content": "# ------------------------------------------------------------------------\n# Copyright (c) 2022 megvii-model. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from DETR3D (https://github.com/WangYueFt/detr3d)\n# Copyright (c) 2021 Wang, Yue\n# ------------------------------------------------------------------------\n# Copyright (c) Youngwan Lee (ETRI) All Rights Reserved.\n# Copyright 2021 Toyota Research Institute.  All rights reserved.\n# ------------------------------------------------------------------------\nfrom collections import OrderedDict\nfrom mmcv.runner import BaseModule\nfrom mmdet.models.builder import BACKBONES\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.batchnorm import _BatchNorm\nimport warnings\nimport torch.utils.checkpoint as cp\n\nVoVNet19_slim_dw_eSE = {\n    \"stem\": [64, 64, 64],\n    \"stage_conv_ch\": [64, 80, 96, 112],\n    \"stage_out_ch\": [112, 256, 384, 512],\n    \"layer_per_block\": 3,\n    \"block_per_stage\": [1, 1, 1, 1],\n    \"eSE\": True,\n    \"dw\": True,\n}\n\nVoVNet19_dw_eSE = {\n    \"stem\": [64, 64, 64],\n    \"stage_conv_ch\": [128, 160, 192, 224],\n    \"stage_out_ch\": [256, 512, 768, 1024],\n    \"layer_per_block\": 3,\n    \"block_per_stage\": [1, 1, 1, 1],\n    \"eSE\": True,\n    \"dw\": True,\n}\n\nVoVNet19_slim_eSE = {\n    \"stem\": [64, 64, 128],\n    \"stage_conv_ch\": [64, 80, 96, 112],\n    \"stage_out_ch\": [112, 256, 384, 512],\n    \"layer_per_block\": 3,\n    \"block_per_stage\": [1, 1, 1, 1],\n    \"eSE\": True,\n    \"dw\": False,\n}\n\nVoVNet19_eSE = {\n    \"stem\": [64, 64, 128],\n    \"stage_conv_ch\": [128, 160, 192, 224],\n    \"stage_out_ch\": [256, 512, 768, 1024],\n    \"layer_per_block\": 3,\n    \"block_per_stage\": [1, 1, 1, 1],\n    \"eSE\": True,\n    \"dw\": False,\n}\n\nVoVNet39_eSE = {\n    \"stem\": [64, 64, 128],\n    \"stage_conv_ch\": [128, 160, 192, 224],\n    \"stage_out_ch\": [256, 512, 768, 1024],\n    \"layer_per_block\": 5,\n    \"block_per_stage\": [1, 1, 2, 2],\n    \"eSE\": True,\n    \"dw\": False,\n}\n\nVoVNet57_eSE = {\n    \"stem\": [64, 64, 128],\n    \"stage_conv_ch\": [128, 160, 192, 224],\n    \"stage_out_ch\": [256, 512, 768, 1024],\n    \"layer_per_block\": 5,\n    \"block_per_stage\": [1, 1, 4, 3],\n    \"eSE\": True,\n    \"dw\": False,\n}\n\nVoVNet99_eSE = {\n    \"stem\": [64, 64, 128],\n    \"stage_conv_ch\": [128, 160, 192, 224],\n    \"stage_out_ch\": [256, 512, 768, 1024],\n    \"layer_per_block\": 5,\n    \"block_per_stage\": [1, 3, 9, 3],\n    \"eSE\": True,\n    \"dw\": False,\n}\n\n_STAGE_SPECS = {\n    \"V-19-slim-dw-eSE\": VoVNet19_slim_dw_eSE,\n    \"V-19-dw-eSE\": VoVNet19_dw_eSE,\n    \"V-19-slim-eSE\": VoVNet19_slim_eSE,\n    \"V-19-eSE\": VoVNet19_eSE,\n    \"V-39-eSE\": VoVNet39_eSE,\n    \"V-57-eSE\": VoVNet57_eSE,\n    \"V-99-eSE\": VoVNet99_eSE,\n}\n\n\ndef dw_conv3x3(\n    in_channels, out_channels, module_name, postfix, stride=1, kernel_size=3, padding=1\n):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return [\n        (\n            \"{}_{}/dw_conv3x3\".format(module_name, postfix),\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n                groups=out_channels,\n                bias=False,\n            ),\n        ),\n        (\n            \"{}_{}/pw_conv1x1\".format(module_name, postfix),\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n                groups=1,\n                bias=False,\n            ),\n        ),\n        (\"{}_{}/pw_norm\".format(module_name, postfix), nn.BatchNorm2d(out_channels)),\n        (\"{}_{}/pw_relu\".format(module_name, postfix), nn.ReLU(inplace=True)),\n    ]\n\n\ndef conv3x3(\n    in_channels,\n    out_channels,\n    module_name,\n    postfix,\n    stride=1,\n    groups=1,\n    kernel_size=3,\n    padding=1,\n):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return [\n        (\n            f\"{module_name}_{postfix}/conv\",\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n                groups=groups,\n                bias=False,\n            ),\n        ),\n        (f\"{module_name}_{postfix}/norm\", nn.BatchNorm2d(out_channels)),\n        (f\"{module_name}_{postfix}/relu\", nn.ReLU(inplace=True)),\n    ]\n\n\ndef conv1x1(\n    in_channels,\n    out_channels,\n    module_name,\n    postfix,\n    stride=1,\n    groups=1,\n    kernel_size=1,\n    padding=0,\n):\n    \"\"\"1x1 convolution with padding\"\"\"\n    return [\n        (\n            f\"{module_name}_{postfix}/conv\",\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n                groups=groups,\n                bias=False,\n            ),\n        ),\n        (f\"{module_name}_{postfix}/norm\", nn.BatchNorm2d(out_channels)),\n        (f\"{module_name}_{postfix}/relu\", nn.ReLU(inplace=True)),\n    ]\n\n\nclass Hsigmoid(nn.Module):\n    def __init__(self, inplace=True):\n        super(Hsigmoid, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return F.relu6(x + 3.0, inplace=self.inplace) / 6.0\n\n\nclass eSEModule(nn.Module):\n    def __init__(self, channel, reduction=4):\n        super(eSEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Conv2d(channel, channel, kernel_size=1, padding=0)\n        self.hsigmoid = Hsigmoid()\n\n    def forward(self, x):\n        input = x\n        x = self.avg_pool(x)\n        x = self.fc(x)\n        x = self.hsigmoid(x)\n        return input * x\n\n\nclass _OSA_module(nn.Module):\n    def __init__(\n        self,\n        in_ch,\n        stage_ch,\n        concat_ch,\n        layer_per_block,\n        module_name,\n        SE=False,\n        identity=False,\n        depthwise=False,\n        with_cp=True,\n    ):\n\n        super(_OSA_module, self).__init__()\n\n        self.identity = identity\n        self.depthwise = depthwise\n        self.isReduced = False\n        self.use_checkpoint = with_cp\n        self.layers = nn.ModuleList()\n        in_channel = in_ch\n        if self.depthwise and in_channel != stage_ch:\n            self.isReduced = True\n            self.conv_reduction = nn.Sequential(\n                OrderedDict(\n                    conv1x1(\n                        in_channel, stage_ch, \"{}_reduction\".format(module_name), \"0\"\n                    )\n                )\n            )\n        for i in range(layer_per_block):\n            if self.depthwise:\n                self.layers.append(\n                    nn.Sequential(\n                        OrderedDict(dw_conv3x3(stage_ch, stage_ch, module_name, i))\n                    )\n                )\n            else:\n                self.layers.append(\n                    nn.Sequential(\n                        OrderedDict(conv3x3(in_channel, stage_ch, module_name, i))\n                    )\n                )\n            in_channel = stage_ch\n\n        # feature aggregation\n        in_channel = in_ch + layer_per_block * stage_ch\n        self.concat = nn.Sequential(\n            OrderedDict(conv1x1(in_channel, concat_ch, module_name, \"concat\"))\n        )\n\n        self.ese = eSEModule(concat_ch)\n\n    def _forward(self, x):\n\n        identity_feat = x\n\n        output = []\n        output.append(x)\n        if self.depthwise and self.isReduced:\n            x = self.conv_reduction(x)\n        for layer in self.layers:\n            x = layer(x)\n            output.append(x)\n\n        x = torch.cat(output, dim=1)\n        xt = self.concat(x)\n\n        xt = self.ese(xt)\n\n        if self.identity:\n            xt = xt + identity_feat\n\n        return xt\n\n    def forward(self, x):\n\n        if self.use_checkpoint and self.training:\n            xt = cp.checkpoint(self._forward, x)\n        else:\n            xt = self._forward(x)\n\n        return xt\n\n\nclass _OSA_stage(nn.Sequential):\n    def __init__(\n        self,\n        in_ch,\n        stage_ch,\n        concat_ch,\n        block_per_stage,\n        layer_per_block,\n        stage_num,\n        SE=False,\n        depthwise=False,\n    ):\n\n        super(_OSA_stage, self).__init__()\n\n        if not stage_num == 2:\n            self.add_module(\n                \"Pooling\", nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n            )\n\n        if block_per_stage != 1:\n            SE = False\n        module_name = f\"OSA{stage_num}_1\"\n        self.add_module(\n            module_name,\n            _OSA_module(\n                in_ch,\n                stage_ch,\n                concat_ch,\n                layer_per_block,\n                module_name,\n                SE,\n                depthwise=depthwise,\n            ),\n        )\n        for i in range(block_per_stage - 1):\n            if i != block_per_stage - 2:  # last block\n                SE = False\n            module_name = f\"OSA{stage_num}_{i + 2}\"\n            self.add_module(\n                module_name,\n                _OSA_module(\n                    concat_ch,\n                    stage_ch,\n                    concat_ch,\n                    layer_per_block,\n                    module_name,\n                    SE,\n                    identity=True,\n                    depthwise=depthwise,\n                ),\n            )\n\n\n@BACKBONES.register_module()\nclass VoVNetCP(BaseModule):\n    def __init__(\n        self,\n        spec_name,\n        input_ch=3,\n        out_features=None,\n        frozen_stages=-1,\n        norm_eval=True,\n        pretrained=None,\n        init_cfg=None,\n    ):\n        \"\"\"\n        Args:\n            input_ch(int) : the number of input channel\n            out_features (list[str]): name of the layers whose outputs should\n                be returned in forward. Can be anything in \"stem\", \"stage2\" ...\n        \"\"\"\n        super(VoVNetCP, self).__init__(init_cfg)\n        self.frozen_stages = frozen_stages\n        self.norm_eval = norm_eval\n\n        self.pretrained_weight = pretrained\n\n        if isinstance(pretrained, str):\n            warnings.warn(\n                \"DeprecationWarning: pretrained is deprecated, \"\n                'please use \"init_cfg\" instead'\n            )\n            self.init_cfg = dict(type=\"Pretrained\", checkpoint=pretrained)\n\n        stage_specs = _STAGE_SPECS[spec_name]\n\n        stem_ch = stage_specs[\"stem\"]\n        config_stage_ch = stage_specs[\"stage_conv_ch\"]\n        config_concat_ch = stage_specs[\"stage_out_ch\"]\n        block_per_stage = stage_specs[\"block_per_stage\"]\n        layer_per_block = stage_specs[\"layer_per_block\"]\n        SE = stage_specs[\"eSE\"]\n        depthwise = stage_specs[\"dw\"]\n\n        self._out_features = out_features\n\n        # Stem module\n        conv_type = dw_conv3x3 if depthwise else conv3x3\n        stem = conv3x3(input_ch, stem_ch[0], \"stem\", \"1\", 2)\n        stem += conv_type(stem_ch[0], stem_ch[1], \"stem\", \"2\", 1)\n        stem += conv_type(stem_ch[1], stem_ch[2], \"stem\", \"3\", 2)\n        self.add_module(\"stem\", nn.Sequential((OrderedDict(stem))))\n        current_stirde = 4\n        self._out_feature_strides = {\"stem\": current_stirde, \"stage2\": current_stirde}\n        self._out_feature_channels = {\"stem\": stem_ch[2]}\n\n        stem_out_ch = [stem_ch[2]]\n        in_ch_list = stem_out_ch + config_concat_ch[:-1]\n        # OSA stages\n        self.stage_names = []\n        for i in range(4):  # num_stages\n            name = \"stage%d\" % (i + 2)  # stage 2 ... stage 5\n            self.stage_names.append(name)\n            self.add_module(\n                name,\n                _OSA_stage(\n                    in_ch_list[i],\n                    config_stage_ch[i],\n                    config_concat_ch[i],\n                    block_per_stage[i],\n                    layer_per_block,\n                    i + 2,\n                    SE,\n                    depthwise,\n                ),\n            )\n\n            self._out_feature_channels[name] = config_concat_ch[i]\n            if not i == 0:\n                self._out_feature_strides[name] = current_stirde = int(\n                    current_stirde * 2\n                )\n\n        # initialize weights\n        # self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n\n    # def forward(self, x):\n    #     outputs = {}\n    #     x = self.stem(x)\n    #     if \"stem\" in self._out_features:\n    #         outputs[\"stem\"] = x\n    #     for name in self.stage_names:\n    #         x = getattr(self, name)(x)\n    #         if name in self._out_features:\n    #             outputs[name] = x\n\n    #     return outputs\n\n    def forward(self, x):\n\n        outputs = []\n        x = self.stem(x)\n        \n        if \"stem\" in self._out_features:\n            outputs.append(x)\n\n        for name in self.stage_names:\n            x = getattr(self, name)(x)\n            if name in self._out_features:\n                outputs.append(x)\n\n        return outputs\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            m = getattr(self, \"stem\")\n            m.eval()\n            for param in m.parameters():\n                param.requires_grad = False\n\n        for i in range(1, self.frozen_stages + 1):\n            m = getattr(self, f\"stage{i+1}\")\n            m.eval()\n            for param in m.parameters():\n                param.requires_grad = False\n\n    def train(self, mode=True):\n        \"\"\"Convert the model into training mode while keep normalization layer\n        freezed.\"\"\"\n        super(VoVNetCP, self).train(mode)\n        self._freeze_stages()\n        if mode and self.norm_eval:\n            for m in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(m, _BatchNorm):\n                    m.eval()\n"
        }
    ]
}