{
    "sourceFile": "mmdet3d/models/backbones/swin_bev.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 9,
            "patches": [
                {
                    "date": 1716024604781,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1716622714040,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,379 +1,28 @@\n # Copyright (c) OpenMMLab. All rights reserved.\n-from torch.nn.modules.utils import _pair as to_2tuple\n-\n import warnings\n from collections import OrderedDict\n from copy import deepcopy\n import numpy as np\n import random\n from scipy import interpolate\n-import math\n-from typing import Sequence\n \n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n import torch.utils.checkpoint as cp\n-from mmcv.cnn import (\n-    build_norm_layer,\n-    build_conv_layer,\n-    constant_init,\n-    trunc_normal_init,\n-)\n+from mmcv.cnn import build_norm_layer, constant_init, trunc_normal_init\n from mmcv.cnn.bricks.transformer import FFN, build_dropout\n from mmcv.cnn.utils.weight_init import trunc_normal_\n from mmcv.runner import BaseModule, ModuleList, _load_checkpoint\n from mmcv.utils import to_2tuple\n \n from mmdet.utils import get_root_logger\n from mmdet.models.builder import BACKBONES\n+from mmdet.models.utils.ckpt_convert import swin_converter\n+from mmdet.models.utils.transformer import PatchEmbed, PatchMerging\n \n-# from mmdet.models.utils.ckpt_convert import swin_converter\n-# from mmdet.models.utils.transformer import PatchEmbed, PatchMerging\n-# from .swin import PatchEmbed, PatchMerging\n \n-\n-class PatchMerging(BaseModule):\n-    \"\"\"Merge patch feature map.\n-\n-    This layer groups feature map by kernel_size, and applies norm and linear\n-    layers to the grouped feature map. Our implementation uses `nn.Unfold` to\n-    merge patch, which is about 25% faster than original implementation.\n-    Instead, we need to modify pretrained models for compatibility.\n-\n-    Args:\n-        in_channels (int): The num of input channels.\n-            to gets fully covered by filter and stride you specified..\n-            Default: True.\n-        out_channels (int): The num of output channels.\n-        kernel_size (int | tuple, optional): the kernel size in the unfold\n-            layer. Defaults to 2.\n-        stride (int | tuple, optional): the stride of the sliding blocks in the\n-            unfold layer. Default: None. (Would be set as `kernel_size`)\n-        padding (int | tuple | string ): The padding length of\n-            embedding conv. When it is a string, it means the mode\n-            of adaptive padding, support \"same\" and \"corner\" now.\n-            Default: \"corner\".\n-        dilation (int | tuple, optional): dilation parameter in the unfold\n-            layer. Default: 1.\n-        bias (bool, optional): Whether to add bias in linear layer or not.\n-            Defaults: False.\n-        norm_cfg (dict, optional): Config dict for normalization layer.\n-            Default: dict(type='LN').\n-        init_cfg (dict, optional): The extra config for initialization.\n-            Default: None.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        in_channels,\n-        out_channels,\n-        kernel_size=2,\n-        stride=None,\n-        padding=\"corner\",\n-        dilation=1,\n-        bias=False,\n-        norm_cfg=dict(type=\"LN\"),\n-        init_cfg=None,\n-    ):\n-        super().__init__(init_cfg=init_cfg)\n-        self.in_channels = in_channels\n-        self.out_channels = out_channels\n-        if stride:\n-            stride = stride\n-        else:\n-            stride = kernel_size\n-\n-        kernel_size = to_2tuple(kernel_size)\n-        stride = to_2tuple(stride)\n-        dilation = to_2tuple(dilation)\n-\n-        if isinstance(padding, str):\n-            self.adap_padding = AdaptivePadding(\n-                kernel_size=kernel_size,\n-                stride=stride,\n-                dilation=dilation,\n-                padding=padding,\n-            )\n-            # disable the padding of unfold\n-            padding = 0\n-        else:\n-            self.adap_padding = None\n-\n-        padding = to_2tuple(padding)\n-        self.sampler = nn.Unfold(\n-            kernel_size=kernel_size, dilation=dilation, padding=padding, stride=stride\n-        )\n-\n-        sample_dim = kernel_size[0] * kernel_size[1] * in_channels\n-\n-        if norm_cfg is not None:\n-            self.norm = build_norm_layer(norm_cfg, sample_dim)[1]\n-        else:\n-            self.norm = None\n-\n-        self.reduction = nn.Linear(sample_dim, out_channels, bias=bias)\n-\n-    def forward(self, x, input_size):\n-        \"\"\"\n-        Args:\n-            x (Tensor): Has shape (B, H*W, C_in).\n-            input_size (tuple[int]): The spatial shape of x, arrange as (H, W).\n-                Default: None.\n-\n-        Returns:\n-            tuple: Contains merged results and its spatial shape.\n-\n-                - x (Tensor): Has shape (B, Merged_H * Merged_W, C_out)\n-                - out_size (tuple[int]): Spatial shape of x, arrange as\n-                    (Merged_H, Merged_W).\n-        \"\"\"\n-        B, L, C = x.shape\n-        assert isinstance(input_size, Sequence), (\n-            f\"Expect \" f\"input_size is \" f\"`Sequence` \" f\"but get {input_size}\"\n-        )\n-\n-        H, W = input_size\n-        assert L == H * W, \"input feature has wrong size\"\n-\n-        x = x.view(B, H, W, C).permute([0, 3, 1, 2])  # B, C, H, W\n-        # Use nn.Unfold to merge patch. About 25% faster than original method,\n-        # but need to modify pretrained model for compatibility\n-\n-        if self.adap_padding:\n-            x = self.adap_padding(x)\n-            H, W = x.shape[-2:]\n-\n-        x = self.sampler(x)\n-        # if kernel_size=2 and stride=2, x should has shape (B, 4*C, H/2*W/2)\n-\n-        out_h = (\n-            H\n-            + 2 * self.sampler.padding[0]\n-            - self.sampler.dilation[0] * (self.sampler.kernel_size[0] - 1)\n-            - 1\n-        ) // self.sampler.stride[0] + 1\n-        out_w = (\n-            W\n-            + 2 * self.sampler.padding[1]\n-            - self.sampler.dilation[1] * (self.sampler.kernel_size[1] - 1)\n-            - 1\n-        ) // self.sampler.stride[1] + 1\n-\n-        output_size = (out_h, out_w)\n-        \n-        x = x.transpose(1, 2)  # B, H/2*W/2, 4*C\n-        x = self.norm(x) if self.norm else x\n-        x = self.reduction(x)\n-        return x, output_size\n-\n-\n-class AdaptivePadding(nn.Module):\n-    \"\"\"Applies padding to input (if needed) so that input can get fully covered\n-    by filter you specified. It support two modes \"same\" and \"corner\". The\n-    \"same\" mode is same with \"SAME\" padding mode in TensorFlow, pad zero around\n-    input. The \"corner\"  mode would pad zero to bottom right.\n-\n-    Args:\n-        kernel_size (int | tuple): Size of the kernel:\n-        stride (int | tuple): Stride of the filter. Default: 1:\n-        dilation (int | tuple): Spacing between kernel elements.\n-            Default: 1\n-        padding (str): Support \"same\" and \"corner\", \"corner\" mode\n-            would pad zero to bottom right, and \"same\" mode would\n-            pad zero around input. Default: \"corner\".\n-    Example:\n-        >>> kernel_size = 16\n-        >>> stride = 16\n-        >>> dilation = 1\n-        >>> input = torch.rand(1, 1, 15, 17)\n-        >>> adap_pad = AdaptivePadding(\n-        >>>     kernel_size=kernel_size,\n-        >>>     stride=stride,\n-        >>>     dilation=dilation,\n-        >>>     padding=\"corner\")\n-        >>> out = adap_pad(input)\n-        >>> assert (out.shape[2], out.shape[3]) == (16, 32)\n-        >>> input = torch.rand(1, 1, 16, 17)\n-        >>> out = adap_pad(input)\n-        >>> assert (out.shape[2], out.shape[3]) == (16, 32)\n-    \"\"\"\n-\n-    def __init__(self, kernel_size=1, stride=1, dilation=1, padding=\"corner\"):\n-        super(AdaptivePadding, self).__init__()\n-\n-        assert padding in (\"same\", \"corner\")\n-\n-        kernel_size = to_2tuple(kernel_size)\n-        stride = to_2tuple(stride)\n-        padding = to_2tuple(padding)\n-        dilation = to_2tuple(dilation)\n-\n-        self.padding = padding\n-        self.kernel_size = kernel_size\n-        self.stride = stride\n-        self.dilation = dilation\n-\n-    def get_pad_shape(self, input_shape):\n-        input_h, input_w = input_shape\n-        kernel_h, kernel_w = self.kernel_size\n-        stride_h, stride_w = self.stride\n-        output_h = math.ceil(input_h / stride_h)\n-        output_w = math.ceil(input_w / stride_w)\n-        pad_h = max(\n-            (output_h - 1) * stride_h + (kernel_h - 1) * self.dilation[0] + 1 - input_h,\n-            0,\n-        )\n-        pad_w = max(\n-            (output_w - 1) * stride_w + (kernel_w - 1) * self.dilation[1] + 1 - input_w,\n-            0,\n-        )\n-        return pad_h, pad_w\n-\n-    def forward(self, x):\n-        pad_h, pad_w = self.get_pad_shape(x.size()[-2:])\n-        if pad_h > 0 or pad_w > 0:\n-            if self.padding == \"corner\":\n-                x = F.pad(x, [0, pad_w, 0, pad_h])\n-            elif self.padding == \"same\":\n-                x = F.pad(\n-                    x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2]\n-                )\n-        return x\n-\n-\n-# Modified from Pytorch-Image-Models\n-class PatchEmbed(BaseModule):\n-    \"\"\"Image to Patch Embedding.\n-\n-    We use a conv layer to implement PatchEmbed.\n-\n-    Args:\n-        in_channels (int): The num of input channels. Default: 3\n-        embed_dims (int): The dimensions of embedding. Default: 768\n-        conv_type (str): The config dict for embedding\n-            conv layer type selection. Default: \"Conv2d.\n-        kernel_size (int): The kernel_size of embedding conv. Default: 16.\n-        stride (int): The slide stride of embedding conv.\n-            Default: None (Would be set as `kernel_size`).\n-        padding (int | tuple | string ): The padding length of\n-            embedding conv. When it is a string, it means the mode\n-            of adaptive padding, support \"same\" and \"corner\" now.\n-            Default: \"corner\".\n-        dilation (int): The dilation rate of embedding conv. Default: 1.\n-        bias (bool): Bias of embed conv. Default: True.\n-        norm_cfg (dict, optional): Config dict for normalization layer.\n-            Default: None.\n-        input_size (int | tuple | None): The size of input, which will be\n-            used to calculate the out size. Only work when `dynamic_size`\n-            is False. Default: None.\n-        init_cfg (`mmcv.ConfigDict`, optional): The Config for initialization.\n-            Default: None.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        in_channels=3,\n-        embed_dims=768,\n-        conv_type=\"Conv2d\",\n-        kernel_size=16,\n-        stride=16,\n-        padding=\"corner\",\n-        dilation=1,\n-        bias=True,\n-        norm_cfg=None,\n-        input_size=None,\n-        init_cfg=None,\n-    ):\n-        super(PatchEmbed, self).__init__(init_cfg=init_cfg)\n-\n-        self.embed_dims = embed_dims\n-        if stride is None:\n-            stride = kernel_size\n-\n-        kernel_size = to_2tuple(kernel_size)\n-        stride = to_2tuple(stride)\n-        dilation = to_2tuple(dilation)\n-\n-        if isinstance(padding, str):\n-            self.adap_padding = AdaptivePadding(\n-                kernel_size=kernel_size,\n-                stride=stride,\n-                dilation=dilation,\n-                padding=padding,\n-            )\n-            # disable the padding of conv\n-            padding = 0\n-        else:\n-            self.adap_padding = None\n-        padding = to_2tuple(padding)\n-\n-        self.projection = build_conv_layer(\n-            dict(type=conv_type),\n-            in_channels=in_channels,\n-            out_channels=embed_dims,\n-            kernel_size=kernel_size,\n-            stride=stride,\n-            padding=padding,\n-            dilation=dilation,\n-            bias=bias,\n-        )\n-\n-        if norm_cfg is not None:\n-            self.norm = build_norm_layer(norm_cfg, embed_dims)[1]\n-        else:\n-            self.norm = None\n-\n-        if input_size:\n-            input_size = to_2tuple(input_size)\n-            # `init_out_size` would be used outside to\n-            # calculate the num_patches\n-            # when `use_abs_pos_embed` outside\n-            self.init_input_size = input_size\n-            if self.adap_padding:\n-                pad_h, pad_w = self.adap_padding.get_pad_shape(input_size)\n-                input_h, input_w = input_size\n-                input_h = input_h + pad_h\n-                input_w = input_w + pad_w\n-                input_size = (input_h, input_w)\n-\n-            # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n-            h_out = (\n-                input_size[0] + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1\n-            ) // stride[0] + 1\n-            w_out = (\n-                input_size[1] + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1\n-            ) // stride[1] + 1\n-            self.init_out_size = (h_out, w_out)\n-        else:\n-            self.init_input_size = None\n-            self.init_out_size = None\n-\n-    def forward(self, x):\n-        \"\"\"\n-        Args:\n-            x (Tensor): Has shape (B, C, H, W). In most case, C is 3.\n-\n-        Returns:\n-            tuple: Contains merged results and its spatial shape.\n-\n-                - x (Tensor): Has shape (B, out_h * out_w, embed_dims)\n-                - out_size (tuple[int]): Spatial shape of x, arrange as\n-                    (out_h, out_w).\n-        \"\"\"\n-\n-        if self.adap_padding:\n-            x = self.adap_padding(x)\n-\n-        x = self.projection(x)\n-        out_size = (x.shape[2], x.shape[3])\n-        x = x.flatten(2).transpose(1, 2)\n-        if self.norm is not None:\n-            x = self.norm(x)\n-        return x, out_size\n-\n-\n class WindowMSA(BaseModule):\n     \"\"\"Window based multi-head self-attention (W-MSA) module with relative\n     position bias.\n \n@@ -1052,13 +701,12 @@\n             elif \"model\" in ckpt:\n                 _state_dict = ckpt[\"model\"]\n             else:\n                 _state_dict = ckpt\n+            if self.convert_weights:\n+                # supported loading weight from original repo,\n+                _state_dict = swin_converter(_state_dict)\n \n-            # if self.convert_weights:\n-            #     # supported loading weight from original repo,\n-            #     _state_dict = swin_converter(_state_dict)\n-\n             state_dict = OrderedDict()\n             for k, v in _state_dict.items():\n                 if \"relative_position_index\" in k:\n                     continue\n"
                },
                {
                    "date": 1716623096467,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -701,11 +701,12 @@\n             elif \"model\" in ckpt:\n                 _state_dict = ckpt[\"model\"]\n             else:\n                 _state_dict = ckpt\n+                \n             if self.convert_weights:\n-                # supported loading weight from original repo,\n-                _state_dict = swin_converter(_state_dict)\n+                # # supported loading weight from original repo,\n+                # _state_dict = swin_converter(_state_dict)\n \n             state_dict = OrderedDict()\n             for k, v in _state_dict.items():\n                 if \"relative_position_index\" in k:\n"
                },
                {
                    "date": 1716623147666,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,12 +17,221 @@\n from mmcv.utils import to_2tuple\n \n from mmdet.utils import get_root_logger\n from mmdet.models.builder import BACKBONES\n-from mmdet.models.utils.ckpt_convert import swin_converter\n+# from mmdet.models.utils.ckpt_convert import swin_converter\n from mmdet.models.utils.transformer import PatchEmbed, PatchMerging\n \n \n+class PatchMerging(BaseModule):\n+    \"\"\"Merge patch feature map.\n+\n+    This layer groups feature map by kernel_size, and applies norm and linear\n+    layers to the grouped feature map. Our implementation uses `nn.Unfold` to\n+    merge patch, which is about 25% faster than original implementation.\n+    Instead, we need to modify pretrained models for compatibility.\n+\n+    Args:\n+        in_channels (int): The num of input channels.\n+            to gets fully covered by filter and stride you specified..\n+            Default: True.\n+        out_channels (int): The num of output channels.\n+        kernel_size (int | tuple, optional): the kernel size in the unfold\n+            layer. Defaults to 2.\n+        stride (int | tuple, optional): the stride of the sliding blocks in the\n+            unfold layer. Default: None. (Would be set as `kernel_size`)\n+        padding (int | tuple | string ): The padding length of\n+            embedding conv. When it is a string, it means the mode\n+            of adaptive padding, support \"same\" and \"corner\" now.\n+            Default: \"corner\".\n+        dilation (int | tuple, optional): dilation parameter in the unfold\n+            layer. Default: 1.\n+        bias (bool, optional): Whether to add bias in linear layer or not.\n+            Defaults: False.\n+        norm_cfg (dict, optional): Config dict for normalization layer.\n+            Default: dict(type='LN').\n+        init_cfg (dict, optional): The extra config for initialization.\n+            Default: None.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        in_channels,\n+        out_channels,\n+        kernel_size=2,\n+        stride=None,\n+        padding=\"corner\",\n+        dilation=1,\n+        bias=False,\n+        norm_cfg=dict(type=\"LN\"),\n+        init_cfg=None,\n+    ):\n+        super().__init__(init_cfg=init_cfg)\n+        self.in_channels = in_channels\n+        self.out_channels = out_channels\n+        if stride:\n+            stride = stride\n+        else:\n+            stride = kernel_size\n+\n+        kernel_size = to_2tuple(kernel_size)\n+        stride = to_2tuple(stride)\n+        dilation = to_2tuple(dilation)\n+\n+        if isinstance(padding, str):\n+            self.adap_padding = AdaptivePadding(\n+                kernel_size=kernel_size,\n+                stride=stride,\n+                dilation=dilation,\n+                padding=padding,\n+            )\n+            # disable the padding of unfold\n+            padding = 0\n+        else:\n+            self.adap_padding = None\n+\n+        padding = to_2tuple(padding)\n+        self.sampler = nn.Unfold(\n+            kernel_size=kernel_size, dilation=dilation, padding=padding, stride=stride\n+        )\n+\n+        sample_dim = kernel_size[0] * kernel_size[1] * in_channels\n+\n+        if norm_cfg is not None:\n+            self.norm = build_norm_layer(norm_cfg, sample_dim)[1]\n+        else:\n+            self.norm = None\n+\n+        self.reduction = nn.Linear(sample_dim, out_channels, bias=bias)\n+\n+    def forward(self, x, input_size):\n+        \"\"\"\n+        Args:\n+            x (Tensor): Has shape (B, H*W, C_in).\n+            input_size (tuple[int]): The spatial shape of x, arrange as (H, W).\n+                Default: None.\n+\n+        Returns:\n+            tuple: Contains merged results and its spatial shape.\n+\n+                - x (Tensor): Has shape (B, Merged_H * Merged_W, C_out)\n+                - out_size (tuple[int]): Spatial shape of x, arrange as\n+                    (Merged_H, Merged_W).\n+        \"\"\"\n+        B, L, C = x.shape\n+        assert isinstance(input_size, Sequence), (\n+            f\"Expect \" f\"input_size is \" f\"`Sequence` \" f\"but get {input_size}\"\n+        )\n+\n+        H, W = input_size\n+        assert L == H * W, \"input feature has wrong size\"\n+\n+        x = x.view(B, H, W, C).permute([0, 3, 1, 2])  # B, C, H, W\n+        # Use nn.Unfold to merge patch. About 25% faster than original method,\n+        # but need to modify pretrained model for compatibility\n+\n+        if self.adap_padding:\n+            x = self.adap_padding(x)\n+            H, W = x.shape[-2:]\n+\n+        x = self.sampler(x)\n+        # if kernel_size=2 and stride=2, x should has shape (B, 4*C, H/2*W/2)\n+\n+        out_h = (\n+            H\n+            + 2 * self.sampler.padding[0]\n+            - self.sampler.dilation[0] * (self.sampler.kernel_size[0] - 1)\n+            - 1\n+        ) // self.sampler.stride[0] + 1\n+        out_w = (\n+            W\n+            + 2 * self.sampler.padding[1]\n+            - self.sampler.dilation[1] * (self.sampler.kernel_size[1] - 1)\n+            - 1\n+        ) // self.sampler.stride[1] + 1\n+\n+        output_size = (out_h, out_w)\n+        \n+        x = x.transpose(1, 2)  # B, H/2*W/2, 4*C\n+        x = self.norm(x) if self.norm else x\n+        x = self.reduction(x)\n+        return x, output_size\n+    \n+\n+class AdaptivePadding(nn.Module):\n+    \"\"\"Applies padding to input (if needed) so that input can get fully covered\n+    by filter you specified. It support two modes \"same\" and \"corner\". The\n+    \"same\" mode is same with \"SAME\" padding mode in TensorFlow, pad zero around\n+    input. The \"corner\"  mode would pad zero to bottom right.\n+\n+    Args:\n+        kernel_size (int | tuple): Size of the kernel:\n+        stride (int | tuple): Stride of the filter. Default: 1:\n+        dilation (int | tuple): Spacing between kernel elements.\n+            Default: 1\n+        padding (str): Support \"same\" and \"corner\", \"corner\" mode\n+            would pad zero to bottom right, and \"same\" mode would\n+            pad zero around input. Default: \"corner\".\n+    Example:\n+        >>> kernel_size = 16\n+        >>> stride = 16\n+        >>> dilation = 1\n+        >>> input = torch.rand(1, 1, 15, 17)\n+        >>> adap_pad = AdaptivePadding(\n+        >>>     kernel_size=kernel_size,\n+        >>>     stride=stride,\n+        >>>     dilation=dilation,\n+        >>>     padding=\"corner\")\n+        >>> out = adap_pad(input)\n+        >>> assert (out.shape[2], out.shape[3]) == (16, 32)\n+        >>> input = torch.rand(1, 1, 16, 17)\n+        >>> out = adap_pad(input)\n+        >>> assert (out.shape[2], out.shape[3]) == (16, 32)\n+    \"\"\"\n+\n+    def __init__(self, kernel_size=1, stride=1, dilation=1, padding=\"corner\"):\n+        super(AdaptivePadding, self).__init__()\n+\n+        assert padding in (\"same\", \"corner\")\n+\n+        kernel_size = to_2tuple(kernel_size)\n+        stride = to_2tuple(stride)\n+        padding = to_2tuple(padding)\n+        dilation = to_2tuple(dilation)\n+\n+        self.padding = padding\n+        self.kernel_size = kernel_size\n+        self.stride = stride\n+        self.dilation = dilation\n+\n+    def get_pad_shape(self, input_shape):\n+        input_h, input_w = input_shape\n+        kernel_h, kernel_w = self.kernel_size\n+        stride_h, stride_w = self.stride\n+        output_h = math.ceil(input_h / stride_h)\n+        output_w = math.ceil(input_w / stride_w)\n+        pad_h = max(\n+            (output_h - 1) * stride_h + (kernel_h - 1) * self.dilation[0] + 1 - input_h,\n+            0,\n+        )\n+        pad_w = max(\n+            (output_w - 1) * stride_w + (kernel_w - 1) * self.dilation[1] + 1 - input_w,\n+            0,\n+        )\n+        return pad_h, pad_w\n+\n+    def forward(self, x):\n+        pad_h, pad_w = self.get_pad_shape(x.size()[-2:])\n+        if pad_h > 0 or pad_w > 0:\n+            if self.padding == \"corner\":\n+                x = F.pad(x, [0, pad_w, 0, pad_h])\n+            elif self.padding == \"same\":\n+                x = F.pad(\n+                    x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2]\n+                )\n+        return x\n+\n+\n class WindowMSA(BaseModule):\n     \"\"\"Window based multi-head self-attention (W-MSA) module with relative\n     position bias.\n \n@@ -702,9 +911,9 @@\n                 _state_dict = ckpt[\"model\"]\n             else:\n                 _state_dict = ckpt\n                 \n-            if self.convert_weights:\n+            # if self.convert_weights:\n                 # # supported loading weight from original repo,\n                 # _state_dict = swin_converter(_state_dict)\n \n             state_dict = OrderedDict()\n"
                },
                {
                    "date": 1716623156448,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,8 +17,9 @@\n from mmcv.utils import to_2tuple\n \n from mmdet.utils import get_root_logger\n from mmdet.models.builder import BACKBONES\n+\n # from mmdet.models.utils.ckpt_convert import swin_converter\n from mmdet.models.utils.transformer import PatchEmbed, PatchMerging\n \n \n@@ -149,15 +150,15 @@\n             - 1\n         ) // self.sampler.stride[1] + 1\n \n         output_size = (out_h, out_w)\n-        \n+\n         x = x.transpose(1, 2)  # B, H/2*W/2, 4*C\n         x = self.norm(x) if self.norm else x\n         x = self.reduction(x)\n         return x, output_size\n-    \n \n+\n class AdaptivePadding(nn.Module):\n     \"\"\"Applies padding to input (if needed) so that input can get fully covered\n     by filter you specified. It support two modes \"same\" and \"corner\". The\n     \"same\" mode is same with \"SAME\" padding mode in TensorFlow, pad zero around\n@@ -230,8 +231,139 @@\n                 )\n         return x\n \n \n+# Modified from Pytorch-Image-Models\n+class PatchEmbed(BaseModule):\n+    \"\"\"Image to Patch Embedding.\n+\n+    We use a conv layer to implement PatchEmbed.\n+\n+    Args:\n+        in_channels (int): The num of input channels. Default: 3\n+        embed_dims (int): The dimensions of embedding. Default: 768\n+        conv_type (str): The config dict for embedding\n+            conv layer type selection. Default: \"Conv2d.\n+        kernel_size (int): The kernel_size of embedding conv. Default: 16.\n+        stride (int): The slide stride of embedding conv.\n+            Default: None (Would be set as `kernel_size`).\n+        padding (int | tuple | string ): The padding length of\n+            embedding conv. When it is a string, it means the mode\n+            of adaptive padding, support \"same\" and \"corner\" now.\n+            Default: \"corner\".\n+        dilation (int): The dilation rate of embedding conv. Default: 1.\n+        bias (bool): Bias of embed conv. Default: True.\n+        norm_cfg (dict, optional): Config dict for normalization layer.\n+            Default: None.\n+        input_size (int | tuple | None): The size of input, which will be\n+            used to calculate the out size. Only work when `dynamic_size`\n+            is False. Default: None.\n+        init_cfg (`mmcv.ConfigDict`, optional): The Config for initialization.\n+            Default: None.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        in_channels=3,\n+        embed_dims=768,\n+        conv_type=\"Conv2d\",\n+        kernel_size=16,\n+        stride=16,\n+        padding=\"corner\",\n+        dilation=1,\n+        bias=True,\n+        norm_cfg=None,\n+        input_size=None,\n+        init_cfg=None,\n+    ):\n+        super(PatchEmbed, self).__init__(init_cfg=init_cfg)\n+\n+        self.embed_dims = embed_dims\n+        if stride is None:\n+            stride = kernel_size\n+\n+        kernel_size = to_2tuple(kernel_size)\n+        stride = to_2tuple(stride)\n+        dilation = to_2tuple(dilation)\n+\n+        if isinstance(padding, str):\n+            self.adap_padding = AdaptivePadding(\n+                kernel_size=kernel_size,\n+                stride=stride,\n+                dilation=dilation,\n+                padding=padding,\n+            )\n+            # disable the padding of conv\n+            padding = 0\n+        else:\n+            self.adap_padding = None\n+        padding = to_2tuple(padding)\n+\n+        self.projection = build_conv_layer(\n+            dict(type=conv_type),\n+            in_channels=in_channels,\n+            out_channels=embed_dims,\n+            kernel_size=kernel_size,\n+            stride=stride,\n+            padding=padding,\n+            dilation=dilation,\n+            bias=bias,\n+        )\n+\n+        if norm_cfg is not None:\n+            self.norm = build_norm_layer(norm_cfg, embed_dims)[1]\n+        else:\n+            self.norm = None\n+\n+        if input_size:\n+            input_size = to_2tuple(input_size)\n+            # `init_out_size` would be used outside to\n+            # calculate the num_patches\n+            # when `use_abs_pos_embed` outside\n+            self.init_input_size = input_size\n+            if self.adap_padding:\n+                pad_h, pad_w = self.adap_padding.get_pad_shape(input_size)\n+                input_h, input_w = input_size\n+                input_h = input_h + pad_h\n+                input_w = input_w + pad_w\n+                input_size = (input_h, input_w)\n+\n+            # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n+            h_out = (\n+                input_size[0] + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1\n+            ) // stride[0] + 1\n+            w_out = (\n+                input_size[1] + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1\n+            ) // stride[1] + 1\n+            self.init_out_size = (h_out, w_out)\n+        else:\n+            self.init_input_size = None\n+            self.init_out_size = None\n+\n+    def forward(self, x):\n+        \"\"\"\n+        Args:\n+            x (Tensor): Has shape (B, C, H, W). In most case, C is 3.\n+\n+        Returns:\n+            tuple: Contains merged results and its spatial shape.\n+\n+                - x (Tensor): Has shape (B, out_h * out_w, embed_dims)\n+                - out_size (tuple[int]): Spatial shape of x, arrange as\n+                    (out_h, out_w).\n+        \"\"\"\n+\n+        if self.adap_padding:\n+            x = self.adap_padding(x)\n+\n+        x = self.projection(x)\n+        out_size = (x.shape[2], x.shape[3])\n+        x = x.flatten(2).transpose(1, 2)\n+        if self.norm is not None:\n+            x = self.norm(x)\n+        return x, out_size\n+\n+\n class WindowMSA(BaseModule):\n     \"\"\"Window based multi-head self-attention (W-MSA) module with relative\n     position bias.\n \n@@ -910,12 +1042,12 @@\n             elif \"model\" in ckpt:\n                 _state_dict = ckpt[\"model\"]\n             else:\n                 _state_dict = ckpt\n-                \n+\n             # if self.convert_weights:\n-                # # supported loading weight from original repo,\n-                # _state_dict = swin_converter(_state_dict)\n+            # # supported loading weight from original repo,\n+            # _state_dict = swin_converter(_state_dict)\n \n             state_dict = OrderedDict()\n             for k, v in _state_dict.items():\n                 if \"relative_position_index\" in k:\n"
                },
                {
                    "date": 1716623172346,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,9 +4,9 @@\n from copy import deepcopy\n import numpy as np\n import random\n from scipy import interpolate\n-\n+from typing import Sequence\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n import torch.utils.checkpoint as cp\n@@ -19,9 +19,9 @@\n from mmdet.utils import get_root_logger\n from mmdet.models.builder import BACKBONES\n \n # from mmdet.models.utils.ckpt_convert import swin_converter\n-from mmdet.models.utils.transformer import PatchEmbed, PatchMerging\n+# from mmdet.models.utils.transformer import PatchEmbed, PatchMerging\n \n \n class PatchMerging(BaseModule):\n     \"\"\"Merge patch feature map.\n"
                },
                {
                    "date": 1716623173640,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,8 +5,9 @@\n import numpy as np\n import random\n from scipy import interpolate\n from typing import Sequence\n+\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n import torch.utils.checkpoint as cp\n"
                },
                {
                    "date": 1716623184114,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,8 +4,10 @@\n from copy import deepcopy\n import numpy as np\n import random\n from scipy import interpolate\n+\n+import math\n from typing import Sequence\n \n import torch\n import torch.nn as nn\n"
                },
                {
                    "date": 1716623212719,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,8 +7,14 @@\n from scipy import interpolate\n \n import math\n from typing import Sequence\n+from mmcv.cnn import (\n+    build_norm_layer,\n+    build_conv_layer,\n+    constant_init,\n+    trunc_normal_init,\n+)\n \n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n"
                },
                {
                    "date": 1716623228444,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,20 +7,19 @@\n from scipy import interpolate\n \n import math\n from typing import Sequence\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+import torch.utils.checkpoint as cp\n from mmcv.cnn import (\n     build_norm_layer,\n-    build_conv_layer,\n     constant_init,\n     trunc_normal_init,\n+    build_conv_layer,\n )\n-\n-import torch\n-import torch.nn as nn\n-import torch.nn.functional as F\n-import torch.utils.checkpoint as cp\n-from mmcv.cnn import build_norm_layer, constant_init, trunc_normal_init\n from mmcv.cnn.bricks.transformer import FFN, build_dropout\n from mmcv.cnn.utils.weight_init import trunc_normal_\n from mmcv.runner import BaseModule, ModuleList, _load_checkpoint\n from mmcv.utils import to_2tuple\n"
                }
            ],
            "date": 1716024604781,
            "name": "Commit-0",
            "content": "# Copyright (c) OpenMMLab. All rights reserved.\nfrom torch.nn.modules.utils import _pair as to_2tuple\n\nimport warnings\nfrom collections import OrderedDict\nfrom copy import deepcopy\nimport numpy as np\nimport random\nfrom scipy import interpolate\nimport math\nfrom typing import Sequence\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as cp\nfrom mmcv.cnn import (\n    build_norm_layer,\n    build_conv_layer,\n    constant_init,\n    trunc_normal_init,\n)\nfrom mmcv.cnn.bricks.transformer import FFN, build_dropout\nfrom mmcv.cnn.utils.weight_init import trunc_normal_\nfrom mmcv.runner import BaseModule, ModuleList, _load_checkpoint\nfrom mmcv.utils import to_2tuple\n\nfrom mmdet.utils import get_root_logger\nfrom mmdet.models.builder import BACKBONES\n\n# from mmdet.models.utils.ckpt_convert import swin_converter\n# from mmdet.models.utils.transformer import PatchEmbed, PatchMerging\n# from .swin import PatchEmbed, PatchMerging\n\n\nclass PatchMerging(BaseModule):\n    \"\"\"Merge patch feature map.\n\n    This layer groups feature map by kernel_size, and applies norm and linear\n    layers to the grouped feature map. Our implementation uses `nn.Unfold` to\n    merge patch, which is about 25% faster than original implementation.\n    Instead, we need to modify pretrained models for compatibility.\n\n    Args:\n        in_channels (int): The num of input channels.\n            to gets fully covered by filter and stride you specified..\n            Default: True.\n        out_channels (int): The num of output channels.\n        kernel_size (int | tuple, optional): the kernel size in the unfold\n            layer. Defaults to 2.\n        stride (int | tuple, optional): the stride of the sliding blocks in the\n            unfold layer. Default: None. (Would be set as `kernel_size`)\n        padding (int | tuple | string ): The padding length of\n            embedding conv. When it is a string, it means the mode\n            of adaptive padding, support \"same\" and \"corner\" now.\n            Default: \"corner\".\n        dilation (int | tuple, optional): dilation parameter in the unfold\n            layer. Default: 1.\n        bias (bool, optional): Whether to add bias in linear layer or not.\n            Defaults: False.\n        norm_cfg (dict, optional): Config dict for normalization layer.\n            Default: dict(type='LN').\n        init_cfg (dict, optional): The extra config for initialization.\n            Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size=2,\n        stride=None,\n        padding=\"corner\",\n        dilation=1,\n        bias=False,\n        norm_cfg=dict(type=\"LN\"),\n        init_cfg=None,\n    ):\n        super().__init__(init_cfg=init_cfg)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        if stride:\n            stride = stride\n        else:\n            stride = kernel_size\n\n        kernel_size = to_2tuple(kernel_size)\n        stride = to_2tuple(stride)\n        dilation = to_2tuple(dilation)\n\n        if isinstance(padding, str):\n            self.adap_padding = AdaptivePadding(\n                kernel_size=kernel_size,\n                stride=stride,\n                dilation=dilation,\n                padding=padding,\n            )\n            # disable the padding of unfold\n            padding = 0\n        else:\n            self.adap_padding = None\n\n        padding = to_2tuple(padding)\n        self.sampler = nn.Unfold(\n            kernel_size=kernel_size, dilation=dilation, padding=padding, stride=stride\n        )\n\n        sample_dim = kernel_size[0] * kernel_size[1] * in_channels\n\n        if norm_cfg is not None:\n            self.norm = build_norm_layer(norm_cfg, sample_dim)[1]\n        else:\n            self.norm = None\n\n        self.reduction = nn.Linear(sample_dim, out_channels, bias=bias)\n\n    def forward(self, x, input_size):\n        \"\"\"\n        Args:\n            x (Tensor): Has shape (B, H*W, C_in).\n            input_size (tuple[int]): The spatial shape of x, arrange as (H, W).\n                Default: None.\n\n        Returns:\n            tuple: Contains merged results and its spatial shape.\n\n                - x (Tensor): Has shape (B, Merged_H * Merged_W, C_out)\n                - out_size (tuple[int]): Spatial shape of x, arrange as\n                    (Merged_H, Merged_W).\n        \"\"\"\n        B, L, C = x.shape\n        assert isinstance(input_size, Sequence), (\n            f\"Expect \" f\"input_size is \" f\"`Sequence` \" f\"but get {input_size}\"\n        )\n\n        H, W = input_size\n        assert L == H * W, \"input feature has wrong size\"\n\n        x = x.view(B, H, W, C).permute([0, 3, 1, 2])  # B, C, H, W\n        # Use nn.Unfold to merge patch. About 25% faster than original method,\n        # but need to modify pretrained model for compatibility\n\n        if self.adap_padding:\n            x = self.adap_padding(x)\n            H, W = x.shape[-2:]\n\n        x = self.sampler(x)\n        # if kernel_size=2 and stride=2, x should has shape (B, 4*C, H/2*W/2)\n\n        out_h = (\n            H\n            + 2 * self.sampler.padding[0]\n            - self.sampler.dilation[0] * (self.sampler.kernel_size[0] - 1)\n            - 1\n        ) // self.sampler.stride[0] + 1\n        out_w = (\n            W\n            + 2 * self.sampler.padding[1]\n            - self.sampler.dilation[1] * (self.sampler.kernel_size[1] - 1)\n            - 1\n        ) // self.sampler.stride[1] + 1\n\n        output_size = (out_h, out_w)\n        \n        x = x.transpose(1, 2)  # B, H/2*W/2, 4*C\n        x = self.norm(x) if self.norm else x\n        x = self.reduction(x)\n        return x, output_size\n\n\nclass AdaptivePadding(nn.Module):\n    \"\"\"Applies padding to input (if needed) so that input can get fully covered\n    by filter you specified. It support two modes \"same\" and \"corner\". The\n    \"same\" mode is same with \"SAME\" padding mode in TensorFlow, pad zero around\n    input. The \"corner\"  mode would pad zero to bottom right.\n\n    Args:\n        kernel_size (int | tuple): Size of the kernel:\n        stride (int | tuple): Stride of the filter. Default: 1:\n        dilation (int | tuple): Spacing between kernel elements.\n            Default: 1\n        padding (str): Support \"same\" and \"corner\", \"corner\" mode\n            would pad zero to bottom right, and \"same\" mode would\n            pad zero around input. Default: \"corner\".\n    Example:\n        >>> kernel_size = 16\n        >>> stride = 16\n        >>> dilation = 1\n        >>> input = torch.rand(1, 1, 15, 17)\n        >>> adap_pad = AdaptivePadding(\n        >>>     kernel_size=kernel_size,\n        >>>     stride=stride,\n        >>>     dilation=dilation,\n        >>>     padding=\"corner\")\n        >>> out = adap_pad(input)\n        >>> assert (out.shape[2], out.shape[3]) == (16, 32)\n        >>> input = torch.rand(1, 1, 16, 17)\n        >>> out = adap_pad(input)\n        >>> assert (out.shape[2], out.shape[3]) == (16, 32)\n    \"\"\"\n\n    def __init__(self, kernel_size=1, stride=1, dilation=1, padding=\"corner\"):\n        super(AdaptivePadding, self).__init__()\n\n        assert padding in (\"same\", \"corner\")\n\n        kernel_size = to_2tuple(kernel_size)\n        stride = to_2tuple(stride)\n        padding = to_2tuple(padding)\n        dilation = to_2tuple(dilation)\n\n        self.padding = padding\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.dilation = dilation\n\n    def get_pad_shape(self, input_shape):\n        input_h, input_w = input_shape\n        kernel_h, kernel_w = self.kernel_size\n        stride_h, stride_w = self.stride\n        output_h = math.ceil(input_h / stride_h)\n        output_w = math.ceil(input_w / stride_w)\n        pad_h = max(\n            (output_h - 1) * stride_h + (kernel_h - 1) * self.dilation[0] + 1 - input_h,\n            0,\n        )\n        pad_w = max(\n            (output_w - 1) * stride_w + (kernel_w - 1) * self.dilation[1] + 1 - input_w,\n            0,\n        )\n        return pad_h, pad_w\n\n    def forward(self, x):\n        pad_h, pad_w = self.get_pad_shape(x.size()[-2:])\n        if pad_h > 0 or pad_w > 0:\n            if self.padding == \"corner\":\n                x = F.pad(x, [0, pad_w, 0, pad_h])\n            elif self.padding == \"same\":\n                x = F.pad(\n                    x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2]\n                )\n        return x\n\n\n# Modified from Pytorch-Image-Models\nclass PatchEmbed(BaseModule):\n    \"\"\"Image to Patch Embedding.\n\n    We use a conv layer to implement PatchEmbed.\n\n    Args:\n        in_channels (int): The num of input channels. Default: 3\n        embed_dims (int): The dimensions of embedding. Default: 768\n        conv_type (str): The config dict for embedding\n            conv layer type selection. Default: \"Conv2d.\n        kernel_size (int): The kernel_size of embedding conv. Default: 16.\n        stride (int): The slide stride of embedding conv.\n            Default: None (Would be set as `kernel_size`).\n        padding (int | tuple | string ): The padding length of\n            embedding conv. When it is a string, it means the mode\n            of adaptive padding, support \"same\" and \"corner\" now.\n            Default: \"corner\".\n        dilation (int): The dilation rate of embedding conv. Default: 1.\n        bias (bool): Bias of embed conv. Default: True.\n        norm_cfg (dict, optional): Config dict for normalization layer.\n            Default: None.\n        input_size (int | tuple | None): The size of input, which will be\n            used to calculate the out size. Only work when `dynamic_size`\n            is False. Default: None.\n        init_cfg (`mmcv.ConfigDict`, optional): The Config for initialization.\n            Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels=3,\n        embed_dims=768,\n        conv_type=\"Conv2d\",\n        kernel_size=16,\n        stride=16,\n        padding=\"corner\",\n        dilation=1,\n        bias=True,\n        norm_cfg=None,\n        input_size=None,\n        init_cfg=None,\n    ):\n        super(PatchEmbed, self).__init__(init_cfg=init_cfg)\n\n        self.embed_dims = embed_dims\n        if stride is None:\n            stride = kernel_size\n\n        kernel_size = to_2tuple(kernel_size)\n        stride = to_2tuple(stride)\n        dilation = to_2tuple(dilation)\n\n        if isinstance(padding, str):\n            self.adap_padding = AdaptivePadding(\n                kernel_size=kernel_size,\n                stride=stride,\n                dilation=dilation,\n                padding=padding,\n            )\n            # disable the padding of conv\n            padding = 0\n        else:\n            self.adap_padding = None\n        padding = to_2tuple(padding)\n\n        self.projection = build_conv_layer(\n            dict(type=conv_type),\n            in_channels=in_channels,\n            out_channels=embed_dims,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            bias=bias,\n        )\n\n        if norm_cfg is not None:\n            self.norm = build_norm_layer(norm_cfg, embed_dims)[1]\n        else:\n            self.norm = None\n\n        if input_size:\n            input_size = to_2tuple(input_size)\n            # `init_out_size` would be used outside to\n            # calculate the num_patches\n            # when `use_abs_pos_embed` outside\n            self.init_input_size = input_size\n            if self.adap_padding:\n                pad_h, pad_w = self.adap_padding.get_pad_shape(input_size)\n                input_h, input_w = input_size\n                input_h = input_h + pad_h\n                input_w = input_w + pad_w\n                input_size = (input_h, input_w)\n\n            # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n            h_out = (\n                input_size[0] + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1\n            ) // stride[0] + 1\n            w_out = (\n                input_size[1] + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1\n            ) // stride[1] + 1\n            self.init_out_size = (h_out, w_out)\n        else:\n            self.init_input_size = None\n            self.init_out_size = None\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): Has shape (B, C, H, W). In most case, C is 3.\n\n        Returns:\n            tuple: Contains merged results and its spatial shape.\n\n                - x (Tensor): Has shape (B, out_h * out_w, embed_dims)\n                - out_size (tuple[int]): Spatial shape of x, arrange as\n                    (out_h, out_w).\n        \"\"\"\n\n        if self.adap_padding:\n            x = self.adap_padding(x)\n\n        x = self.projection(x)\n        out_size = (x.shape[2], x.shape[3])\n        x = x.flatten(2).transpose(1, 2)\n        if self.norm is not None:\n            x = self.norm(x)\n        return x, out_size\n\n\nclass WindowMSA(BaseModule):\n    \"\"\"Window based multi-head self-attention (W-MSA) module with relative\n    position bias.\n\n    Args:\n        embed_dims (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (tuple[int]): The height and width of the window.\n        qkv_bias (bool, optional):  If True, add a learnable bias to q, k, v.\n            Default: True.\n        qk_scale (float | None, optional): Override default qk scale of\n            head_dim ** -0.5 if set. Default: None.\n        attn_drop_rate (float, optional): Dropout ratio of attention weight.\n            Default: 0.0\n        proj_drop_rate (float, optional): Dropout ratio of output. Default: 0.\n        init_cfg (dict | None, optional): The Config for initialization.\n            Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dims,\n        num_heads,\n        window_size,\n        qkv_bias=True,\n        qk_scale=None,\n        attn_drop_rate=0.0,\n        proj_drop_rate=0.0,\n        init_cfg=None,\n        use_bias=True,\n    ):\n        super().__init__()\n        self.embed_dims = embed_dims\n        self.window_size = window_size  # Wh, Ww\n        self.num_heads = num_heads\n        head_embed_dims = embed_dims // num_heads\n        self.scale = qk_scale or head_embed_dims**-0.5\n        self.init_cfg = init_cfg\n        self.use_bias = use_bias\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n        )  # 2*Wh-1 * 2*Ww-1, nH\n\n        # About 2x faster than original impl\n        Wh, Ww = self.window_size\n        rel_index_coords = self.double_step_seq(2 * Ww - 1, Wh, 1, Ww)\n        rel_position_index = rel_index_coords + rel_index_coords.T\n        rel_position_index = rel_position_index.flip(1).contiguous()\n        self.register_buffer(\"relative_position_index\", rel_position_index)\n\n        self.qkv = nn.Linear(embed_dims, embed_dims * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop_rate)\n        self.proj = nn.Linear(embed_dims, embed_dims)\n        self.proj_drop = nn.Dropout(proj_drop_rate)\n\n        self.softmax = nn.Softmax(dim=-1)\n\n    def init_weights(self):\n        trunc_normal_(self.relative_position_bias_table, std=0.02)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n\n            x (tensor): input features with shape of (num_windows*B, N, C)\n            mask (tensor | None, Optional): mask with shape of (num_windows,\n                Wh*Ww, Wh*Ww), value should be between (-inf, 0].\n        \"\"\"\n        B, N, C = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        # make torchscript happy (cannot use tensor as tuple)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        q = q * self.scale\n        attn = q @ k.transpose(-2, -1)\n\n        if self.use_bias:\n            relative_position_bias = self.relative_position_bias_table[\n                self.relative_position_index.view(-1)\n            ].view(\n                self.window_size[0] * self.window_size[1],\n                self.window_size[0] * self.window_size[1],\n                -1,\n            )  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(\n                2, 0, 1\n            ).contiguous()  # nH, Wh*Ww, Wh*Ww\n            attn = attn + relative_position_bias.unsqueeze(0)\n\n        attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    @staticmethod\n    def double_step_seq(step1, len1, step2, len2):\n        seq1 = torch.arange(0, step1 * len1, step1)\n        seq2 = torch.arange(0, step2 * len2, step2)\n        return (seq1[:, None] + seq2[None, :]).reshape(1, -1)\n\n\nclass ShiftWindowMSA(BaseModule):\n    \"\"\"Shifted Window Multihead Self-Attention Module.\n\n    Args:\n        embed_dims (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): The height and width of the window.\n        shift_size (int, optional): The shift step of each window towards\n            right-bottom. If zero, act as regular window-msa. Defaults to 0.\n        qkv_bias (bool, optional): If True, add a learnable bias to q, k, v.\n            Default: True\n        qk_scale (float | None, optional): Override default qk scale of\n            head_dim ** -0.5 if set. Defaults: None.\n        attn_drop_rate (float, optional): Dropout ratio of attention weight.\n            Defaults: 0.\n        proj_drop_rate (float, optional): Dropout ratio of output.\n            Defaults: 0.\n        dropout_layer (dict, optional): The dropout_layer used before output.\n            Defaults: dict(type='DropPath', drop_prob=0.).\n        init_cfg (dict, optional): The extra config for initialization.\n            Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dims,\n        num_heads,\n        window_size,\n        shift_size=0,\n        qkv_bias=True,\n        qk_scale=None,\n        attn_drop_rate=0,\n        proj_drop_rate=0,\n        dropout_layer=dict(type=\"DropPath\", drop_prob=0.0),\n        init_cfg=None,\n        use_bias=True,\n    ):\n        super().__init__(init_cfg)\n\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.shift_size = 0\n        assert 0 <= self.shift_size < self.window_size\n\n        self.w_msa = WindowMSA(\n            embed_dims=embed_dims,\n            num_heads=num_heads,\n            window_size=to_2tuple(window_size),\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop_rate=attn_drop_rate,\n            proj_drop_rate=proj_drop_rate,\n            init_cfg=None,\n            use_bias=use_bias,\n        )\n\n        self.drop = build_dropout(dropout_layer)\n\n    def forward(self, query, hw_shape, mask=None):\n        B, L, C = query.shape\n        H, W = hw_shape\n        assert L == H * W, \"input feature has wrong size\"\n        query = query.view(B, H, W, C)\n\n        # pad feature maps to multiples of window size\n        pad_r = (self.window_size - W % self.window_size) % self.window_size\n        pad_b = (self.window_size - H % self.window_size) % self.window_size\n        query = F.pad(query, (0, 0, 0, pad_r, 0, pad_b))\n        H_pad, W_pad = query.shape[1], query.shape[2]\n\n        shifted_query = query\n        attn_mask = None\n\n        # nW*B, window_size, window_size, C\n        query_windows = self.window_partition(shifted_query)\n        # nW*B, window_size*window_size, C\n        query_windows = query_windows.view(-1, self.window_size**2, C)\n\n        # W-MSA/SW-MSA (nW*B, window_size*window_size, C)\n        attn_windows = self.w_msa(query_windows, mask=attn_mask)\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n\n        # B H' W' C\n        shifted_x = self.window_reverse(attn_windows, H_pad, W_pad)\n        x = shifted_x\n\n        if pad_r > 0 or pad_b:\n            x = x[:, :H, :W, :].contiguous()\n\n        x = x.view(B, H * W, C)\n\n        x = self.drop(x)\n        return x\n\n    def window_reverse(self, windows, H, W):\n        \"\"\"\n        Args:\n            windows: (num_windows*B, window_size, window_size, C)\n            H (int): Height of image\n            W (int): Width of image\n        Returns:\n            x: (B, H, W, C)\n        \"\"\"\n        window_size = self.window_size\n        B = int(windows.shape[0] / (H * W / window_size / window_size))\n        x = windows.view(\n            B, H // window_size, W // window_size, window_size, window_size, -1\n        )\n        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n        return x\n\n    def window_partition(self, x):\n        \"\"\"\n        Args:\n            x: (B, H, W, C)\n        Returns:\n            windows: (num_windows*B, window_size, window_size, C)\n        \"\"\"\n        B, H, W, C = x.shape\n        window_size = self.window_size\n        x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n        windows = windows.view(-1, window_size, window_size, C)\n        return windows\n\n\nclass SwinBlock(BaseModule):\n    \"\"\" \"\n    Args:\n        embed_dims (int): The feature dimension.\n        num_heads (int): Parallel attention heads.\n        feedforward_channels (int): The hidden dimension for FFNs.\n        window_size (int, optional): The local window scale. Default: 7.\n        shift (bool, optional): whether to shift window or not. Default False.\n        qkv_bias (bool, optional): enable bias for qkv if True. Default: True.\n        qk_scale (float | None, optional): Override default qk scale of\n            head_dim ** -0.5 if set. Default: None.\n        drop_rate (float, optional): Dropout rate. Default: 0.\n        attn_drop_rate (float, optional): Attention dropout rate. Default: 0.\n        drop_path_rate (float, optional): Stochastic depth rate. Default: 0.\n        act_cfg (dict, optional): The config dict of activation function.\n            Default: dict(type='GELU').\n        norm_cfg (dict, optional): The config dict of normalization.\n            Default: dict(type='LN').\n        with_cp (bool, optional): Use checkpoint or not. Using checkpoint\n            will save some memory while slowing down the training speed.\n            Default: False.\n        init_cfg (dict | list | None, optional): The init config.\n            Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dims,\n        num_heads,\n        feedforward_channels,\n        window_size=7,\n        shift=False,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        act_cfg=dict(type=\"GELU\"),\n        norm_cfg=dict(type=\"LN\"),\n        with_cp=False,\n        init_cfg=None,\n        use_bias=True,\n    ):\n        super(SwinBlock, self).__init__()\n\n        self.init_cfg = init_cfg\n        self.with_cp = with_cp\n\n        self.norm1 = build_norm_layer(norm_cfg, embed_dims)[1]\n        self.attn = ShiftWindowMSA(\n            embed_dims=embed_dims,\n            num_heads=num_heads,\n            window_size=window_size,\n            shift_size=window_size // 2 if shift else 0,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop_rate=attn_drop_rate,\n            proj_drop_rate=drop_rate,\n            dropout_layer=dict(type=\"DropPath\", drop_prob=drop_path_rate),\n            init_cfg=None,\n            use_bias=use_bias,\n        )\n\n        self.norm2 = build_norm_layer(norm_cfg, embed_dims)[1]\n        self.ffn = FFN(\n            embed_dims=embed_dims,\n            feedforward_channels=feedforward_channels,\n            num_fcs=2,\n            ffn_drop=drop_rate,\n            dropout_layer=dict(type=\"DropPath\", drop_prob=drop_path_rate),\n            act_cfg=act_cfg,\n            add_identity=True,\n            init_cfg=None,\n        )\n\n    def forward(self, x, hw_shape, mask=None):\n        def _inner_forward(x):\n            identity = x\n            x = self.norm1(x)\n            x = self.attn(x, hw_shape, mask=mask)\n\n            x = x + identity\n\n            identity = x\n            x = self.norm2(x)\n            x = self.ffn(x, identity=identity)\n\n            return x\n\n        if self.with_cp and x.requires_grad:\n            x = cp.checkpoint(_inner_forward, x)\n        else:\n            x = _inner_forward(x)\n\n        return x\n\n\nclass SwinBlockSequence(BaseModule):\n    \"\"\"Implements one stage in Swin Transformer.\n\n    Args:\n        embed_dims (int): The feature dimension.\n        num_heads (int): Parallel attention heads.\n        feedforward_channels (int): The hidden dimension for FFNs.\n        depth (int): The number of blocks in this stage.\n        window_size (int, optional): The local window scale. Default: 7.\n        qkv_bias (bool, optional): enable bias for qkv if True. Default: True.\n        qk_scale (float | None, optional): Override default qk scale of\n            head_dim ** -0.5 if set. Default: None.\n        drop_rate (float, optional): Dropout rate. Default: 0.\n        attn_drop_rate (float, optional): Attention dropout rate. Default: 0.\n        drop_path_rate (float | list[float], optional): Stochastic depth\n            rate. Default: 0.\n        downsample (BaseModule | None, optional): The downsample operation\n            module. Default: None.\n        act_cfg (dict, optional): The config dict of activation function.\n            Default: dict(type='GELU').\n        norm_cfg (dict, optional): The config dict of normalization.\n            Default: dict(type='LN').\n        with_cp (bool, optional): Use checkpoint or not. Using checkpoint\n            will save some memory while slowing down the training speed.\n            Default: False.\n        init_cfg (dict | list | None, optional): The init config.\n            Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dims,\n        num_heads,\n        feedforward_channels,\n        depth,\n        window_size=7,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        downsample=None,\n        act_cfg=dict(type=\"GELU\"),\n        norm_cfg=dict(type=\"LN\"),\n        with_cp=False,\n        init_cfg=None,\n    ):\n        super().__init__(init_cfg=init_cfg)\n\n        if isinstance(drop_path_rate, list):\n            drop_path_rates = drop_path_rate\n            assert len(drop_path_rates) == depth\n        else:\n            drop_path_rates = [deepcopy(drop_path_rate) for _ in range(depth)]\n\n        self.blocks = ModuleList()\n        for i in range(depth):\n            use_bias = True\n            this_window_size = window_size\n            block = SwinBlock(\n                embed_dims=embed_dims,\n                num_heads=num_heads,\n                feedforward_channels=feedforward_channels,\n                window_size=this_window_size,\n                shift=False if i % 2 == 0 else True,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop_rate=drop_rate,\n                attn_drop_rate=attn_drop_rate,\n                drop_path_rate=drop_path_rates[i],\n                act_cfg=act_cfg,\n                norm_cfg=norm_cfg,\n                with_cp=with_cp,\n                init_cfg=None,\n                use_bias=use_bias,\n            )\n            self.blocks.append(block)\n\n        self.downsample = downsample\n\n    def forward(self, x, hw_shape, mask=None):\n        for block in self.blocks:\n            x = block(x, hw_shape, mask=mask)\n\n        if self.downsample:\n            x_down, down_hw_shape = self.downsample(x, hw_shape)\n            return x_down, down_hw_shape, x, hw_shape\n        else:\n            return x, hw_shape, x, hw_shape\n\n\n@BACKBONES.register_module(force=True)\nclass SwinTransformerBEVFT(BaseModule):\n    \"\"\"Swin Transformer\n    A PyTorch implement of : `Swin Transformer:\n    Hierarchical Vision Transformer using Shifted Windows`  -\n        https://arxiv.org/abs/2103.14030\n\n    Inspiration from\n    https://github.com/microsoft/Swin-Transformer\n\n    Args:\n        pretrain_img_size (int | tuple[int]): The size of input image when\n            pretrain. Defaults: 224.\n        in_channels (int): The num of input channels.\n            Defaults: 3.\n        embed_dims (int): The feature dimension. Default: 96.\n        patch_size (int | tuple[int]): Patch size. Default: 4.\n        window_size (int): Window size. Default: 7.\n        mlp_ratio (int): Ratio of mlp hidden dim to embedding dim.\n            Default: 4.\n        depths (tuple[int]): Depths of each Swin Transformer stage.\n            Default: (2, 2, 6, 2).\n        num_heads (tuple[int]): Parallel attention heads of each Swin\n            Transformer stage. Default: (3, 6, 12, 24).\n        strides (tuple[int]): The patch merging or patch embedding stride of\n            each Swin Transformer stage. (In swin, we set kernel size equal to\n            stride.) Default: (4, 2, 2, 2).\n        out_indices (tuple[int]): Output from which stages.\n            Default: (0, 1, 2, 3).\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key,\n            value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of\n            head_dim ** -0.5 if set. Default: None.\n        patch_norm (bool): If add a norm layer for patch embed and patch\n            merging. Default: True.\n        drop_rate (float): Dropout rate. Defaults: 0.\n        attn_drop_rate (float): Attention dropout rate. Default: 0.\n        drop_path_rate (float): Stochastic depth rate. Defaults: 0.1.\n        use_abs_pos_embed (bool): If True, add absolute position embedding to\n            the patch embedding. Defaults: False.\n        act_cfg (dict): Config dict for activation layer.\n            Default: dict(type='LN').\n        norm_cfg (dict): Config dict for normalization layer at\n            output of backone. Defaults: dict(type='LN').\n        with_cp (bool, optional): Use checkpoint or not. Using checkpoint\n            will save some memory while slowing down the training speed.\n            Default: False.\n        pretrained (str, optional): model pretrained path. Default: None.\n        convert_weights (bool): The flag indicates whether the\n            pre-trained model is from the original repo. We may need\n            to convert some keys to make it compatible.\n            Default: False.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            Default: -1 (-1 means not freezing any parameters).\n        init_cfg (dict, optional): The Config for initialization.\n            Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrain_img_size=224,\n        in_channels=3,\n        embed_dims=128,\n        patch_size=4,\n        window_size=(16, 16, 16, 8),\n        mlp_ratio=4,\n        depths=(2, 2, 18, 2),\n        num_heads=(4, 8, 16, 32),\n        strides=(4, 2, 2, 2),\n        out_indices=(1, 2, 3),\n        qkv_bias=True,\n        qk_scale=None,\n        patch_norm=True,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        use_abs_pos_embed=True,\n        act_cfg=dict(type=\"GELU\"),\n        norm_cfg=dict(type=\"LN\"),\n        with_cp=False,\n        pretrained=None,\n        convert_weights=False,\n        frozen_stages=-1,\n        init_cfg=None,\n        return_stereo_feat=False,\n        output_missing_index_as_none=False,\n    ):\n        self.convert_weights = convert_weights\n        self.frozen_stages = frozen_stages\n        self.return_stereo_feat = return_stereo_feat\n        self.output_missing_index_as_none = output_missing_index_as_none\n        if isinstance(pretrain_img_size, int):\n            pretrain_img_size = to_2tuple(pretrain_img_size)\n        elif isinstance(pretrain_img_size, tuple):\n            if len(pretrain_img_size) == 1:\n                pretrain_img_size = to_2tuple(pretrain_img_size[0])\n            assert len(pretrain_img_size) == 2, (\n                f\"The size of image should have length 1 or 2, \"\n                f\"but got {len(pretrain_img_size)}\"\n            )\n\n        assert not (\n            init_cfg and pretrained\n        ), \"init_cfg and pretrained cannot be specified at the same time\"\n        if isinstance(pretrained, str):\n            warnings.warn(\n                \"DeprecationWarning: pretrained is deprecated, \"\n                'please use \"init_cfg\" instead'\n            )\n            self.init_cfg = dict(type=\"Pretrained\", checkpoint=pretrained)\n        elif pretrained is None:\n            self.init_cfg = init_cfg\n        else:\n            raise TypeError(\"pretrained must be a str or None\")\n\n        super(SwinTransformerBEVFT, self).__init__(init_cfg=init_cfg)\n\n        num_layers = len(depths)\n        self.out_indices = out_indices\n        self.use_abs_pos_embed = use_abs_pos_embed\n\n        assert strides[0] == patch_size, \"Use non-overlapping patch embed.\"\n\n        self.patch_embed = PatchEmbed(\n            in_channels=in_channels,\n            embed_dims=embed_dims,\n            conv_type=\"Conv2d\",\n            kernel_size=patch_size,\n            stride=strides[0],\n            norm_cfg=norm_cfg if patch_norm else None,\n            init_cfg=None,\n        )\n\n        if self.use_abs_pos_embed:\n            patch_row = pretrain_img_size[0] // patch_size\n            patch_col = pretrain_img_size[1] // patch_size\n            num_patches = patch_row * patch_col\n            self.absolute_pos_embed = nn.Parameter(\n                torch.zeros((1, embed_dims, patch_row, patch_col))\n            )\n\n        self.drop_after_pos = nn.Dropout(p=drop_rate)\n\n        # set stochastic depth decay rule\n        total_depth = sum(depths)\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, total_depth)]\n\n        self.stages = ModuleList()\n        in_channels = embed_dims\n        for i in range(num_layers):\n            if i < num_layers - 1:\n                downsample = PatchMerging(\n                    in_channels=in_channels,\n                    out_channels=2 * in_channels,\n                    stride=strides[i + 1],\n                    norm_cfg=norm_cfg if patch_norm else None,\n                    init_cfg=None,\n                )\n            else:\n                downsample = None\n\n            stage = SwinBlockSequence(\n                embed_dims=in_channels,\n                num_heads=num_heads[i],\n                feedforward_channels=mlp_ratio * in_channels,\n                depth=depths[i],\n                window_size=window_size[i],\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop_rate=drop_rate,\n                attn_drop_rate=attn_drop_rate,\n                drop_path_rate=dpr[sum(depths[:i]) : sum(depths[: i + 1])],\n                downsample=downsample,\n                act_cfg=act_cfg,\n                norm_cfg=norm_cfg,\n                with_cp=with_cp if isinstance(with_cp, bool) else with_cp > i,\n                init_cfg=None,\n            )\n            self.stages.append(stage)\n            if downsample:\n                in_channels = downsample.out_channels\n\n        self.num_features = [int(embed_dims * 2**i) for i in range(num_layers)]\n        # Add a norm layer for each output\n        for i in out_indices:\n            layer = build_norm_layer(norm_cfg, self.num_features[i])[1]\n            layer_name = f\"norm{i}\"\n            self.add_module(layer_name, layer)\n\n    def train(self, mode=True):\n        \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"\n        super(SwinTransformerBEVFT, self).train(mode)\n        # self._freeze_stages()\n\n    def _freeze_stages(self):\n        # as pretrain use cosine\n        # self.absolute_pos_embed.requires_grad = False\n        if self.frozen_stages >= 0:\n            self.patch_embed.eval()\n            for param in self.patch_embed.parameters():\n                param.requires_grad = False\n            # if self.use_abs_pos_embed:\n            # self.absolute_pos_embed.requires_grad = False\n            self.drop_after_pos.eval()\n\n        for i in range(1, self.frozen_stages + 1):\n            if (i - 1) in self.out_indices:\n                norm_layer = getattr(self, f\"norm{i-1}\")\n                norm_layer.eval()\n                for param in norm_layer.parameters():\n                    param.requires_grad = False\n\n            m = self.stages[i - 1]\n            m.eval()\n            for param in m.parameters():\n                param.requires_grad = False\n\n    def init_weights(self):\n        logger = get_root_logger()\n        if self.init_cfg is None:\n            logger.warn(\n                f\"No pre-trained weights for \"\n                f\"{self.__class__.__name__}, \"\n                f\"training start from scratch\"\n            )\n            # TODO cosine init\n            # if self.use_abs_pos_embed:\n            # trunc_normal_(self.absolute_pos_embed, std=0.02)\n            for m in self.modules():\n                if isinstance(m, nn.Linear):\n                    trunc_normal_init(m, std=0.02, bias=0.0)\n                elif isinstance(m, nn.LayerNorm):\n                    constant_init(m, 1.0)\n                if hasattr(m, \"init_weight\"):\n                    m.init_weight()\n        else:\n            for m in self.modules():\n                if hasattr(m, \"init_weight\"):\n                    m.init_weight()\n            assert \"checkpoint\" in self.init_cfg, (\n                f\"Only support \"\n                f\"specify `Pretrained` in \"\n                f\"`init_cfg` in \"\n                f\"{self.__class__.__name__} \"\n            )\n            ckpt = _load_checkpoint(\n                self.init_cfg[\"checkpoint\"], logger=logger, map_location=\"cpu\"\n            )\n            if \"state_dict\" in ckpt:\n                _state_dict = ckpt[\"state_dict\"]\n            elif \"model\" in ckpt:\n                _state_dict = ckpt[\"model\"]\n            else:\n                _state_dict = ckpt\n\n            # if self.convert_weights:\n            #     # supported loading weight from original repo,\n            #     _state_dict = swin_converter(_state_dict)\n\n            state_dict = OrderedDict()\n            for k, v in _state_dict.items():\n                if \"relative_position_index\" in k:\n                    continue\n                if k.startswith(\"encoders.\"):\n                    if not k.startswith(\"encoders.camera.backbone.\"):\n                        continue\n                    k = k.replace(\"encoders.camera.backbone.\", \"\")\n                if k.startswith(\"backbone.\"):\n                    k = k[9:]\n                state_dict[k] = v\n\n            # strip prefix of state_dict\n            if list(state_dict.keys())[0].startswith(\"module.\"):\n                state_dict = {k[7:]: v for k, v in state_dict.items()}\n\n            # reshape absolute position embedding\n            if state_dict.get(\"absolute_pos_embed\") is not None:\n                absolute_pos_embed = state_dict[\"absolute_pos_embed\"]\n                if len(absolute_pos_embed.size()) == 3:\n                    N1, L, C1 = absolute_pos_embed.size()\n                    N2, C2, H, W = self.absolute_pos_embed.size()\n                    if N1 != N2 or C1 != C2 or L != H * W:\n                        logger.warning(\"Error in loading absolute_pos_embed, pass\")\n                    else:\n                        state_dict[\"absolute_pos_embed\"] = (\n                            absolute_pos_embed.view(N2, H, W, C2)\n                            .permute(0, 3, 1, 2)\n                            .contiguous()\n                        )\n\n            # interpolate position bias table if needed\n            relative_position_bias_table_keys = [\n                k for k in state_dict.keys() if \"relative_position_bias_table\" in k\n            ]\n            for table_key in relative_position_bias_table_keys:\n                if not table_key in self.state_dict():\n                    print(f\"miss {table_key} in model\")\n                    continue\n                table_pretrained = state_dict[table_key]\n                table_current = self.state_dict()[table_key]\n                L1, nH1 = table_pretrained.size()\n                L2, nH2 = table_current.size()\n                if nH1 != nH2:\n                    logger.warning(f\"Error in loading {table_key}, pass\")\n                elif L1 != L2:\n                    S1 = int(L1**0.5)\n                    S2 = int(L2**0.5)\n\n                    def geometric_progression(a, r, n):\n                        return a * (1.0 - r**n) / (1.0 - r)\n\n                    left, right = 1.01, 1.5\n                    while right - left > 1e-6:\n                        q = (left + right) / 2.0\n                        gp = geometric_progression(1, q, S1 // 2)\n                        if gp > S2 // 2:\n                            right = q\n                        else:\n                            left = q\n                    dis = []\n                    cur = 1\n                    for i in range(S1 // 2):\n                        dis.append(cur)\n                        cur += q ** (i + 1)\n\n                    r_ids = [-_ for _ in reversed(dis)]\n\n                    x = r_ids + [0] + dis\n                    y = r_ids + [0] + dis\n\n                    t = S2 // 2.0\n                    dx = np.arange(-t, t + 0.1, 1.0)\n                    dy = np.arange(-t, t + 0.1, 1.0)\n\n                    # print(\"Original positions = %s\" % str(x))\n                    # print(\"Target positions = %s\" % str(dx))\n                    all_rel_pos_bias = []\n\n                    for i in range(nH2):\n                        z = table_pretrained[:, i].view(S1, S1).float().numpy()\n                        f = interpolate.interp2d(x, y, z, kind=\"cubic\")\n                        all_rel_pos_bias.append(\n                            torch.Tensor(f(dx, dy))\n                            .contiguous()\n                            .view(-1, 1)\n                            .to(table_pretrained.device)\n                        )\n\n                    rel_pos_bias = torch.cat(all_rel_pos_bias, dim=-1)\n                    state_dict[table_key] = rel_pos_bias\n\n            # load state_dict\n            msg = self.load_state_dict(state_dict, False)\n            logger.info(msg)\n\n    def forward(self, x):\n        x, hw_shape = self.patch_embed(x)\n\n        if self.use_abs_pos_embed:\n            absolute_pos_embed = F.interpolate(\n                self.absolute_pos_embed, size=hw_shape, mode=\"bicubic\"\n            )\n            x = x + absolute_pos_embed.flatten(2).transpose(1, 2)\n        x = self.drop_after_pos(x)\n\n        outs = []\n        all_hw_shapes = []\n        for i, stage in enumerate(self.stages):\n            x, hw_shape, out, out_hw_shape = stage(x, hw_shape)\n            if i == 0 and self.return_stereo_feat:\n                out = (\n                    out.view(-1, *out_hw_shape, self.num_features[i])\n                    .permute(0, 3, 1, 2)\n                    .contiguous()\n                )\n                outs.append(out)\n            if i in self.out_indices:\n                norm_layer = getattr(self, f\"norm{i}\")\n                out = norm_layer(out)\n                out = (\n                    out.view(-1, *out_hw_shape, self.num_features[i])\n                    .permute(0, 3, 1, 2)\n                    .contiguous()\n                )\n                outs.append(out)\n            elif self.output_missing_index_as_none:\n                outs.append(None)\n            all_hw_shapes.append(out_hw_shape)\n\n        return outs\n"
        }
    ]
}