{
    "sourceFile": "mmdet3d/ops/bev_pool_v2/bev_pool.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1720924276568,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1720924563622,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,19 +4,29 @@\n import torch\n \n from . import bev_pool_v2_ext\n \n-__all__ = ['bev_pool_v2', 'TRTBEVPoolv2']\n+__all__ = [\"bev_pool_v2\", \"TRTBEVPoolv2\"]\n \n \n class QuickCumsumCuda(torch.autograd.Function):\n     r\"\"\"BEVPoolv2 implementation for Lift-Splat-Shoot view transformation.\n \n     Please refer to the `paper <https://arxiv.org/abs/2211.17111>`_\n     \"\"\"\n+\n     @staticmethod\n-    def forward(ctx, depth, feat, ranks_depth, ranks_feat, ranks_bev,\n-                bev_feat_shape, interval_starts, interval_lengths):\n+    def forward(\n+        ctx,\n+        depth,\n+        feat,\n+        ranks_depth,\n+        ranks_feat,\n+        ranks_bev,\n+        bev_feat_shape,\n+        interval_starts,\n+        interval_lengths,\n+    ):\n         ranks_bev = ranks_bev.int()\n         depth = depth.contiguous().float()\n         feat = feat.contiguous().float()\n         ranks_depth = ranks_depth.contiguous().int()\n@@ -44,17 +54,18 @@\n     def backward(ctx, out_grad):\n         ranks_bev, depth, feat, ranks_feat, ranks_depth = ctx.saved_tensors\n \n         order = ranks_feat.argsort()\n-        ranks_feat, ranks_depth, ranks_bev = \\\n-            ranks_feat[order], ranks_depth[order], ranks_bev[order]\n-        kept = torch.ones(\n-            ranks_bev.shape[0], device=ranks_bev.device, dtype=torch.bool)\n+        ranks_feat, ranks_depth, ranks_bev = (\n+            ranks_feat[order],\n+            ranks_depth[order],\n+            ranks_bev[order],\n+        )\n+        kept = torch.ones(ranks_bev.shape[0], device=ranks_bev.device, dtype=torch.bool)\n         kept[1:] = ranks_feat[1:] != ranks_feat[:-1]\n         interval_starts_bp = torch.where(kept)[0].int()\n         interval_lengths_bp = torch.zeros_like(interval_starts_bp)\n-        interval_lengths_bp[:-1] = interval_starts_bp[\n-            1:] - interval_starts_bp[:-1]\n+        interval_lengths_bp[:-1] = interval_starts_bp[1:] - interval_starts_bp[:-1]\n         interval_lengths_bp[-1] = ranks_bev.shape[0] - interval_starts_bp[-1]\n \n         depth = depth.contiguous()\n         feat = feat.contiguous()\n@@ -78,66 +89,97 @@\n             ranks_bev,\n             interval_lengths_bp,\n             interval_starts_bp,\n         )\n-        return depth_grad, feat_grad, None, None, None, None, None, \\\n-            None, None, None\n+        return depth_grad, feat_grad, None, None, None, None, None, None, None, None\n \n \n-def bev_pool_v2(depth, feat, ranks_depth, ranks_feat, ranks_bev,\n-                bev_feat_shape, interval_starts, interval_lengths):\n-    x = QuickCumsumCuda.apply(depth, feat, ranks_depth, ranks_feat, ranks_bev,\n-                              bev_feat_shape, interval_starts,\n-                              interval_lengths)\n+def bev_pool_v2(\n+    depth,\n+    feat,\n+    ranks_depth,\n+    ranks_feat,\n+    ranks_bev,\n+    bev_feat_shape,\n+    interval_starts,\n+    interval_lengths,\n+):\n+    x = QuickCumsumCuda.apply(\n+        depth,\n+        feat,\n+        ranks_depth,\n+        ranks_feat,\n+        ranks_bev,\n+        bev_feat_shape,\n+        interval_starts,\n+        interval_lengths,\n+    )\n     x = x.permute(0, 4, 1, 2, 3).contiguous()\n     return x\n \n \n class TRTBEVPoolv2(torch.autograd.Function):\n \n     @staticmethod\n-    def symbolic(g,\n-                 depth,\n-                 feat,\n-                 ranks_depth,\n-                 ranks_feat,\n-                 ranks_bev,\n-                 interval_starts,\n-                 interval_lengths,\n-                 out_height=128,\n-                 out_width=128):\n+    def symbolic(\n+        g,\n+        depth,\n+        feat,\n+        ranks_depth,\n+        ranks_feat,\n+        ranks_bev,\n+        interval_starts,\n+        interval_lengths,\n+        out_height=128,\n+        out_width=128,\n+    ):\n         \"\"\"symbolic function for creating onnx op.\"\"\"\n         return g.op(\n-            'mmdeploy::bev_pool_v2',\n+            \"mmdeploy::bev_pool_v2\",\n             depth,\n             feat,\n             ranks_depth,\n             ranks_feat,\n             ranks_bev,\n             interval_starts,\n             interval_lengths,\n             out_height_i=out_height,\n-            out_width_i=out_width)\n+            out_width_i=out_width,\n+        )\n \n     @staticmethod\n-    def forward(g,\n-                depth,  # N,D,H,W\n-                feat,  # N,H,W,C\n-                ranks_depth,\n-                ranks_feat,\n-                ranks_bev,\n-                interval_starts,\n-                interval_lengths,\n-                out_height=128,\n-                out_width=128):\n+    def forward(\n+        g,\n+        depth,  # N,D,H,W\n+        feat,  # N,H,W,C\n+        ranks_depth,\n+        ranks_feat,\n+        ranks_bev,\n+        interval_starts,\n+        interval_lengths,\n+        out_height=128,\n+        out_width=128,\n+    ):\n         \"\"\"run forward.\"\"\"\n         feat = feat.unsqueeze(0)\n         depth = depth.unsqueeze(0)\n-        bev_feat_shape = (depth.shape[0], 1, out_height, out_width,\n-                          feat.shape[-1])  # (B, Z, Y, X, C)\n-        bev_feat = bev_pool_v2(depth, feat, ranks_depth, ranks_feat, ranks_bev,\n-                               bev_feat_shape, interval_starts,\n-                               interval_lengths)\n+        bev_feat_shape = (\n+            depth.shape[0],\n+            1,\n+            out_height,\n+            out_width,\n+            feat.shape[-1],\n+        )  # (B, Z, Y, X, C)\n+        bev_feat = bev_pool_v2(\n+            depth,\n+            feat,\n+            ranks_depth,\n+            ranks_feat,\n+            ranks_bev,\n+            bev_feat_shape,\n+            interval_starts,\n+            interval_lengths,\n+        )\n         bev_feat = bev_feat.squeeze(2)\n         bev_feat = bev_feat.permute(0, 2, 3, 1)\n         return bev_feat\n \n@@ -146,31 +188,38 @@\n     depth = np.array([0.3, 0.4, 0.2, 0.1, 0.7, 0.6, 0.8, 0.9])\n     depth = torch.from_numpy(depth).float().cuda()\n     depth = depth.view(1, 1, 2, 2, 2).requires_grad_()\n     feat = torch.ones(\n-        size=[1, 1, 2, 2, 2], dtype=torch.float,\n-        device='cuda').requires_grad_()\n+        size=[1, 1, 2, 2, 2], dtype=torch.float, device=\"cuda\"\n+    ).requires_grad_()\n     ranks_depth = torch.from_numpy(np.array([0, 4, 1, 6])).int().cuda()\n     ranks_feat = torch.from_numpy(np.array([0, 0, 1, 2])).int().cuda()\n     ranks_bev = torch.from_numpy(np.array([0, 0, 1, 1])).int().cuda()\n \n-    kept = torch.ones(\n-        ranks_bev.shape[0], device=ranks_bev.device, dtype=torch.bool)\n+    kept = torch.ones(ranks_bev.shape[0], device=ranks_bev.device, dtype=torch.bool)\n     kept[1:] = ranks_bev[1:] != ranks_bev[:-1]\n     interval_starts = torch.where(kept)[0].int()\n     if len(interval_starts) == 0:\n         return None, None, None, None, None\n     interval_lengths = torch.zeros_like(interval_starts)\n     interval_lengths[:-1] = interval_starts[1:] - interval_starts[:-1]\n     interval_lengths[-1] = ranks_bev.shape[0] - interval_starts[-1]\n-    bev_feat = bev_pool_v2(depth, feat, ranks_depth, ranks_feat, ranks_bev,\n-                           (1, 1, 2, 2, 2), interval_starts, interval_lengths)\n+    bev_feat = bev_pool_v2(\n+        depth,\n+        feat,\n+        ranks_depth,\n+        ranks_feat,\n+        ranks_bev,\n+        (1, 1, 2, 2, 2),\n+        interval_starts,\n+        interval_lengths,\n+    )\n     loss = torch.sum(bev_feat)\n     loss.backward()\n     assert loss == 4.4\n-    grad_depth = np.array([2., 2., 0., 0., 2., 0., 2., 0.])\n+    grad_depth = np.array([2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 0.0])\n     grad_depth = torch.from_numpy(grad_depth).float()\n     grad_depth = grad_depth.cuda().view(1, 1, 2, 2, 2)\n     assert depth.grad.allclose(grad_depth)\n-    grad_feat = np.array([1.0, 1.0, 0.4, 0.4, 0.8, 0.8, 0., 0.])\n+    grad_feat = np.array([1.0, 1.0, 0.4, 0.4, 0.8, 0.8, 0.0, 0.0])\n     grad_feat = torch.from_numpy(grad_feat).float().cuda().view(1, 1, 2, 2, 2)\n     assert feat.grad.allclose(grad_feat)\n"
                }
            ],
            "date": 1720924276568,
            "name": "Commit-0",
            "content": "# Copyright (c) Phigent Robotics. All rights reserved.\n\nimport numpy as np\nimport torch\n\nfrom . import bev_pool_v2_ext\n\n__all__ = ['bev_pool_v2', 'TRTBEVPoolv2']\n\n\nclass QuickCumsumCuda(torch.autograd.Function):\n    r\"\"\"BEVPoolv2 implementation for Lift-Splat-Shoot view transformation.\n\n    Please refer to the `paper <https://arxiv.org/abs/2211.17111>`_\n    \"\"\"\n    @staticmethod\n    def forward(ctx, depth, feat, ranks_depth, ranks_feat, ranks_bev,\n                bev_feat_shape, interval_starts, interval_lengths):\n        ranks_bev = ranks_bev.int()\n        depth = depth.contiguous().float()\n        feat = feat.contiguous().float()\n        ranks_depth = ranks_depth.contiguous().int()\n        ranks_feat = ranks_feat.contiguous().int()\n        interval_lengths = interval_lengths.contiguous().int()\n        interval_starts = interval_starts.contiguous().int()\n\n        out = feat.new_zeros(bev_feat_shape)\n\n        bev_pool_v2_ext.bev_pool_v2_forward(\n            depth,\n            feat,\n            out,\n            ranks_depth,\n            ranks_feat,\n            ranks_bev,\n            interval_lengths,\n            interval_starts,\n        )\n\n        ctx.save_for_backward(ranks_bev, depth, feat, ranks_feat, ranks_depth)\n        return out\n\n    @staticmethod\n    def backward(ctx, out_grad):\n        ranks_bev, depth, feat, ranks_feat, ranks_depth = ctx.saved_tensors\n\n        order = ranks_feat.argsort()\n        ranks_feat, ranks_depth, ranks_bev = \\\n            ranks_feat[order], ranks_depth[order], ranks_bev[order]\n        kept = torch.ones(\n            ranks_bev.shape[0], device=ranks_bev.device, dtype=torch.bool)\n        kept[1:] = ranks_feat[1:] != ranks_feat[:-1]\n        interval_starts_bp = torch.where(kept)[0].int()\n        interval_lengths_bp = torch.zeros_like(interval_starts_bp)\n        interval_lengths_bp[:-1] = interval_starts_bp[\n            1:] - interval_starts_bp[:-1]\n        interval_lengths_bp[-1] = ranks_bev.shape[0] - interval_starts_bp[-1]\n\n        depth = depth.contiguous()\n        feat = feat.contiguous()\n        ranks_depth = ranks_depth.contiguous()\n        ranks_feat = ranks_feat.contiguous()\n        ranks_bev = ranks_bev.contiguous()\n        interval_lengths_bp = interval_lengths_bp.contiguous()\n        interval_starts_bp = interval_starts_bp.contiguous()\n\n        depth_grad = depth.new_zeros(depth.shape)\n        feat_grad = feat.new_zeros(feat.shape)\n        out_grad = out_grad.contiguous()\n        bev_pool_v2_ext.bev_pool_v2_backward(\n            out_grad,\n            depth_grad,\n            feat_grad,\n            depth,\n            feat,\n            ranks_depth,\n            ranks_feat,\n            ranks_bev,\n            interval_lengths_bp,\n            interval_starts_bp,\n        )\n        return depth_grad, feat_grad, None, None, None, None, None, \\\n            None, None, None\n\n\ndef bev_pool_v2(depth, feat, ranks_depth, ranks_feat, ranks_bev,\n                bev_feat_shape, interval_starts, interval_lengths):\n    x = QuickCumsumCuda.apply(depth, feat, ranks_depth, ranks_feat, ranks_bev,\n                              bev_feat_shape, interval_starts,\n                              interval_lengths)\n    x = x.permute(0, 4, 1, 2, 3).contiguous()\n    return x\n\n\nclass TRTBEVPoolv2(torch.autograd.Function):\n\n    @staticmethod\n    def symbolic(g,\n                 depth,\n                 feat,\n                 ranks_depth,\n                 ranks_feat,\n                 ranks_bev,\n                 interval_starts,\n                 interval_lengths,\n                 out_height=128,\n                 out_width=128):\n        \"\"\"symbolic function for creating onnx op.\"\"\"\n        return g.op(\n            'mmdeploy::bev_pool_v2',\n            depth,\n            feat,\n            ranks_depth,\n            ranks_feat,\n            ranks_bev,\n            interval_starts,\n            interval_lengths,\n            out_height_i=out_height,\n            out_width_i=out_width)\n\n    @staticmethod\n    def forward(g,\n                depth,  # N,D,H,W\n                feat,  # N,H,W,C\n                ranks_depth,\n                ranks_feat,\n                ranks_bev,\n                interval_starts,\n                interval_lengths,\n                out_height=128,\n                out_width=128):\n        \"\"\"run forward.\"\"\"\n        feat = feat.unsqueeze(0)\n        depth = depth.unsqueeze(0)\n        bev_feat_shape = (depth.shape[0], 1, out_height, out_width,\n                          feat.shape[-1])  # (B, Z, Y, X, C)\n        bev_feat = bev_pool_v2(depth, feat, ranks_depth, ranks_feat, ranks_bev,\n                               bev_feat_shape, interval_starts,\n                               interval_lengths)\n        bev_feat = bev_feat.squeeze(2)\n        bev_feat = bev_feat.permute(0, 2, 3, 1)\n        return bev_feat\n\n\ndef test_bev_pool_v2():\n    depth = np.array([0.3, 0.4, 0.2, 0.1, 0.7, 0.6, 0.8, 0.9])\n    depth = torch.from_numpy(depth).float().cuda()\n    depth = depth.view(1, 1, 2, 2, 2).requires_grad_()\n    feat = torch.ones(\n        size=[1, 1, 2, 2, 2], dtype=torch.float,\n        device='cuda').requires_grad_()\n    ranks_depth = torch.from_numpy(np.array([0, 4, 1, 6])).int().cuda()\n    ranks_feat = torch.from_numpy(np.array([0, 0, 1, 2])).int().cuda()\n    ranks_bev = torch.from_numpy(np.array([0, 0, 1, 1])).int().cuda()\n\n    kept = torch.ones(\n        ranks_bev.shape[0], device=ranks_bev.device, dtype=torch.bool)\n    kept[1:] = ranks_bev[1:] != ranks_bev[:-1]\n    interval_starts = torch.where(kept)[0].int()\n    if len(interval_starts) == 0:\n        return None, None, None, None, None\n    interval_lengths = torch.zeros_like(interval_starts)\n    interval_lengths[:-1] = interval_starts[1:] - interval_starts[:-1]\n    interval_lengths[-1] = ranks_bev.shape[0] - interval_starts[-1]\n    bev_feat = bev_pool_v2(depth, feat, ranks_depth, ranks_feat, ranks_bev,\n                           (1, 1, 2, 2, 2), interval_starts, interval_lengths)\n    loss = torch.sum(bev_feat)\n    loss.backward()\n    assert loss == 4.4\n    grad_depth = np.array([2., 2., 0., 0., 2., 0., 2., 0.])\n    grad_depth = torch.from_numpy(grad_depth).float()\n    grad_depth = grad_depth.cuda().view(1, 1, 2, 2, 2)\n    assert depth.grad.allclose(grad_depth)\n    grad_feat = np.array([1.0, 1.0, 0.4, 0.4, 0.8, 0.8, 0., 0.])\n    grad_feat = torch.from_numpy(grad_feat).float().cuda().view(1, 1, 2, 2, 2)\n    assert feat.grad.allclose(grad_feat)\n"
        }
    ]
}