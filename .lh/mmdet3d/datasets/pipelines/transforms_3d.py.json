{
    "sourceFile": "mmdet3d/datasets/pipelines/transforms_3d.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1718523118599,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1718523118599,
            "name": "Commit-0",
            "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport random\nimport warnings\n\nimport cv2\nimport numpy as np\nfrom pyquaternion.quaternion import Quaternion\nfrom mmcv import is_tuple_of\nfrom mmcv.utils import build_from_cfg\n\nfrom mmdet3d.core import VoxelGenerator\nfrom mmdet3d.core.bbox import (CameraInstance3DBoxes, DepthInstance3DBoxes,\n                               LiDARInstance3DBoxes, box_np_ops)\nfrom mmdet3d.datasets.pipelines.compose import Compose\nfrom mmdet.datasets.pipelines import RandomCrop, RandomFlip, Rotate\nfrom ..builder import OBJECTSAMPLERS, PIPELINES\nfrom .data_augment_utils import noise_per_object_v3_\n\n\n@PIPELINES.register_module()\nclass RandomDropPointsColor(object):\n    r\"\"\"Randomly set the color of points to all zeros.\n\n    Once this transform is executed, all the points' color will be dropped.\n    Refer to `PAConv <https://github.com/CVMI-Lab/PAConv/blob/main/scene_seg/\n    util/transform.py#L223>`_ for more details.\n\n    Args:\n        drop_ratio (float, optional): The probability of dropping point colors.\n            Defaults to 0.2.\n    \"\"\"\n\n    def __init__(self, drop_ratio=0.2):\n        assert isinstance(drop_ratio, (int, float)) and 0 <= drop_ratio <= 1, \\\n            f'invalid drop_ratio value {drop_ratio}'\n        self.drop_ratio = drop_ratio\n\n    def __call__(self, input_dict):\n        \"\"\"Call function to drop point colors.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after color dropping,\n                'points' key is updated in the result dict.\n        \"\"\"\n        points = input_dict['points']\n        assert points.attribute_dims is not None and \\\n            'color' in points.attribute_dims, \\\n            'Expect points have color attribute'\n\n        # this if-expression is a bit strange\n        # `RandomDropPointsColor` is used in training 3D segmentor PAConv\n        # we discovered in our experiments that, using\n        # `if np.random.rand() > 1.0 - self.drop_ratio` consistently leads to\n        # better results than using `if np.random.rand() < self.drop_ratio`\n        # so we keep this hack in our codebase\n        if np.random.rand() > 1.0 - self.drop_ratio:\n            points.color = points.color * 0.0\n        return input_dict\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        repr_str = self.__class__.__name__\n        repr_str += f'(drop_ratio={self.drop_ratio})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass RandomFlip3D(RandomFlip):\n    \"\"\"Flip the points & bbox.\n\n    If the input dict contains the key \"flip\", then the flag will be used,\n    otherwise it will be randomly decided by a ratio specified in the init\n    method.\n\n    Args:\n        sync_2d (bool, optional): Whether to apply flip according to the 2D\n            images. If True, it will apply the same flip as that to 2D images.\n            If False, it will decide whether to flip randomly and independently\n            to that of 2D images. Defaults to True.\n        flip_ratio_bev_horizontal (float, optional): The flipping probability\n            in horizontal direction. Defaults to 0.0.\n        flip_ratio_bev_vertical (float, optional): The flipping probability\n            in vertical direction. Defaults to 0.0.\n    \"\"\"\n\n    def __init__(self,\n                 sync_2d=True,\n                 flip_ratio_bev_horizontal=0.0,\n                 flip_ratio_bev_vertical=0.0,\n                 **kwargs):\n        super(RandomFlip3D, self).__init__(\n            flip_ratio=flip_ratio_bev_horizontal, **kwargs)\n        self.sync_2d = sync_2d\n        self.flip_ratio_bev_vertical = flip_ratio_bev_vertical\n        if flip_ratio_bev_horizontal is not None:\n            assert isinstance(\n                flip_ratio_bev_horizontal,\n                (int, float)) and 0 <= flip_ratio_bev_horizontal <= 1\n        if flip_ratio_bev_vertical is not None:\n            assert isinstance(\n                flip_ratio_bev_vertical,\n                (int, float)) and 0 <= flip_ratio_bev_vertical <= 1\n\n    def random_flip_data_3d(self, input_dict, direction='horizontal'):\n        \"\"\"Flip 3D data randomly.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n            direction (str, optional): Flip direction.\n                Default: 'horizontal'.\n\n        Returns:\n            dict: Flipped results, 'points', 'bbox3d_fields' keys are\n                updated in the result dict.\n        \"\"\"\n        assert direction in ['horizontal', 'vertical']\n        # for semantic segmentation task, only points will be flipped.\n        if 'bbox3d_fields' not in input_dict:\n            input_dict['points'].flip(direction)\n            return\n        if len(input_dict['bbox3d_fields']) == 0:  # test mode\n            input_dict['bbox3d_fields'].append('empty_box3d')\n            input_dict['empty_box3d'] = input_dict['box_type_3d'](\n                np.array([], dtype=np.float32))\n        assert len(input_dict['bbox3d_fields']) == 1\n        for key in input_dict['bbox3d_fields']:\n            if 'points' in input_dict:\n                input_dict['points'] = input_dict[key].flip(\n                    direction, points=input_dict['points'])\n            else:\n                input_dict[key].flip(direction)\n        if 'centers2d' in input_dict:\n            assert self.sync_2d is True and direction == 'horizontal', \\\n                'Only support sync_2d=True and horizontal flip with images'\n            w = input_dict['ori_shape'][1]\n            input_dict['centers2d'][..., 0] = \\\n                w - input_dict['centers2d'][..., 0]\n            # need to modify the horizontal position of camera center\n            # along u-axis in the image (flip like centers2d)\n            # ['cam2img'][0][2] = c_u\n            # see more details and examples at\n            # https://github.com/open-mmlab/mmdetection3d/pull/744\n            input_dict['cam2img'][0][2] = w - input_dict['cam2img'][0][2]\n\n    def __call__(self, input_dict):\n        \"\"\"Call function to flip points, values in the ``bbox3d_fields`` and\n        also flip 2D image and its annotations.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Flipped results, 'flip', 'flip_direction',\n                'pcd_horizontal_flip' and 'pcd_vertical_flip' keys are added\n                into result dict.\n        \"\"\"\n        # flip 2D image and its annotations\n        super(RandomFlip3D, self).__call__(input_dict)\n\n        if self.sync_2d:\n            input_dict['pcd_horizontal_flip'] = input_dict['flip']\n            input_dict['pcd_vertical_flip'] = False\n        else:\n            if 'pcd_horizontal_flip' not in input_dict:\n                flip_horizontal = True if np.random.rand(\n                ) < self.flip_ratio else False\n                input_dict['pcd_horizontal_flip'] = flip_horizontal\n            if 'pcd_vertical_flip' not in input_dict:\n                flip_vertical = True if np.random.rand(\n                ) < self.flip_ratio_bev_vertical else False\n                input_dict['pcd_vertical_flip'] = flip_vertical\n\n        if 'transformation_3d_flow' not in input_dict:\n            input_dict['transformation_3d_flow'] = []\n\n        if input_dict['pcd_horizontal_flip']:\n            self.random_flip_data_3d(input_dict, 'horizontal')\n            input_dict['transformation_3d_flow'].extend(['HF'])\n        if input_dict['pcd_vertical_flip']:\n            self.random_flip_data_3d(input_dict, 'vertical')\n            input_dict['transformation_3d_flow'].extend(['VF'])\n        return input_dict\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        repr_str = self.__class__.__name__\n        repr_str += f'(sync_2d={self.sync_2d},'\n        repr_str += f' flip_ratio_bev_vertical={self.flip_ratio_bev_vertical})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass MultiViewWrapper(object):\n    \"\"\"Wrap transformation from single-view into multi-view.\n\n    The wrapper processes the images from multi-view one by one. For each\n    image, it constructs a pseudo dict according to the keys specified by the\n    'process_fields' parameter. After the transformation is finished, desired\n    information can be collected by specifying the keys in the 'collected_keys'\n    parameter. Multi-view images share the same transformation parameters\n    but do not share the same magnitude when a random transformation is\n    conducted.\n\n    Args:\n        transforms (list[dict]): A list of dict specifying the transformations\n            for the monocular situation.\n        process_fields (dict): Desired keys that the transformations should\n            be conducted on. Default to dict(img_fields=['img']).\n        collected_keys (list[str]): Collect information in transformation\n            like rotate angles, crop roi, and flip state.\n    \"\"\"\n\n    def __init__(self,\n                 transforms,\n                 process_fields=dict(img_fields=['img']),\n                 collected_keys=[]):\n        self.transform = Compose(transforms)\n        self.collected_keys = collected_keys\n        self.process_fields = process_fields\n\n    def __call__(self, input_dict):\n        for key in self.collected_keys:\n            input_dict[key] = []\n        for img_id in range(len(input_dict['img'])):\n            process_dict = self.process_fields.copy()\n            for field in self.process_fields:\n                for key in self.process_fields[field]:\n                    process_dict[key] = input_dict[key][img_id]\n            process_dict = self.transform(process_dict)\n            for field in self.process_fields:\n                for key in self.process_fields[field]:\n                    input_dict[key][img_id] = process_dict[key]\n            for key in self.collected_keys:\n                input_dict[key].append(process_dict[key])\n        return input_dict\n\n\n@PIPELINES.register_module()\nclass RangeLimitedRandomCrop(RandomCrop):\n    \"\"\"Randomly crop image-view objects under a limitation of range.\n\n    Args:\n        relative_x_offset_range (tuple[float]): Relative range of random crop\n            in x direction. (x_min, x_max) in [0, 1.0]. Default to (0.0, 1.0).\n        relative_y_offset_range (tuple[float]): Relative range of random crop\n            in y direction. (y_min, y_max) in [0, 1.0]. Default to (0.0, 1.0).\n    \"\"\"\n\n    def __init__(self,\n                 relative_x_offset_range=(0.0, 1.0),\n                 relative_y_offset_range=(0.0, 1.0),\n                 **kwargs):\n        super(RangeLimitedRandomCrop, self).__init__(**kwargs)\n        for range in [relative_x_offset_range, relative_y_offset_range]:\n            assert 0 <= range[0] <= range[1] <= 1\n        self.relative_x_offset_range = relative_x_offset_range\n        self.relative_y_offset_range = relative_y_offset_range\n\n    def _crop_data(self, results, crop_size, allow_negative_crop):\n        \"\"\"Function to randomly crop images.\n\n        Modified from RandomCrop in mmdet==2.25.0\n\n        Args:\n            results (dict): Result dict from loading pipeline.\n            crop_size (tuple): Expected absolute size after cropping, (h, w).\n\n        Returns:\n            dict: Randomly cropped results, 'img_shape' key in result dict is\n                updated according to crop size.\n        \"\"\"\n        assert crop_size[0] > 0 and crop_size[1] > 0\n        for key in results.get('img_fields', ['img']):\n            img = results[key]\n            margin_h = max(img.shape[0] - crop_size[0], 0)\n            margin_w = max(img.shape[1] - crop_size[1], 0)\n            offset_range_h = (margin_h * self.relative_y_offset_range[0],\n                              margin_h * self.relative_y_offset_range[1] + 1)\n            offset_h = np.random.randint(*offset_range_h)\n            offset_range_w = (margin_w * self.relative_x_offset_range[0],\n                              margin_w * self.relative_x_offset_range[1] + 1)\n            offset_w = np.random.randint(*offset_range_w)\n            crop_y1, crop_y2 = offset_h, offset_h + crop_size[0]\n            crop_x1, crop_x2 = offset_w, offset_w + crop_size[1]\n\n            # crop the image\n            img = img[crop_y1:crop_y2, crop_x1:crop_x2, ...]\n            img_shape = img.shape\n            results[key] = img\n            results['crop'] = (crop_x1, crop_y1, crop_x2, crop_y2)\n        results['img_shape'] = img_shape\n\n        # crop bboxes accordingly and clip to the image boundary\n        for key in results.get('bbox_fields', []):\n            # e.g. gt_bboxes and gt_bboxes_ignore\n            bbox_offset = np.array([offset_w, offset_h, offset_w, offset_h],\n                                   dtype=np.float32)\n            bboxes = results[key] - bbox_offset\n            if self.bbox_clip_border:\n                bboxes[:, 0::2] = np.clip(bboxes[:, 0::2], 0, img_shape[1])\n                bboxes[:, 1::2] = np.clip(bboxes[:, 1::2], 0, img_shape[0])\n            valid_inds = (bboxes[:, 2] > bboxes[:, 0]) & (\n                bboxes[:, 3] > bboxes[:, 1])\n            # If the crop does not contain any gt-bbox area and\n            # allow_negative_crop is False, skip this image.\n            if (key == 'gt_bboxes' and not valid_inds.any()\n                    and not allow_negative_crop):\n                return None\n            results[key] = bboxes[valid_inds, :]\n            # label fields. e.g. gt_labels and gt_labels_ignore\n            label_key = self.bbox2label.get(key)\n            if label_key in results:\n                results[label_key] = results[label_key][valid_inds]\n\n            # mask fields, e.g. gt_masks and gt_masks_ignore\n            mask_key = self.bbox2mask.get(key)\n            if mask_key in results:\n                results[mask_key] = results[mask_key][\n                    valid_inds.nonzero()[0]].crop(\n                        np.asarray([crop_x1, crop_y1, crop_x2, crop_y2]))\n                if self.recompute_bbox:\n                    results[key] = results[mask_key].get_bboxes()\n\n        # crop semantic seg\n        for key in results.get('seg_fields', []):\n            results[key] = results[key][crop_y1:crop_y2, crop_x1:crop_x2]\n\n        return results\n\n\n@PIPELINES.register_module()\nclass RandomRotate(Rotate):\n    \"\"\"Randomly rotate images.\n\n    The ratation angle is selected uniformly within the interval specified by\n    the 'range'  parameter.\n\n    Args:\n        range (tuple[float]): Define the range of random rotation.\n            (angle_min, angle_max) in angle.\n    \"\"\"\n\n    def __init__(self, range, **kwargs):\n        super(RandomRotate, self).__init__(**kwargs)\n        self.range = range\n\n    def __call__(self, results):\n        self.angle = np.random.uniform(self.range[0], self.range[1])\n        super(RandomRotate, self).__call__(results)\n        results['rotate'] = self.angle\n        return results\n\n\n@PIPELINES.register_module()\nclass RandomJitterPoints(object):\n    \"\"\"Randomly jitter point coordinates.\n\n    Different from the global translation in ``GlobalRotScaleTrans``, here we\n        apply different noises to each point in a scene.\n\n    Args:\n        jitter_std (list[float]): The standard deviation of jittering noise.\n            This applies random noise to all points in a 3D scene, which is\n            sampled from a gaussian distribution whose standard deviation is\n            set by ``jitter_std``. Defaults to [0.01, 0.01, 0.01]\n        clip_range (list[float]): Clip the randomly generated jitter\n            noise into this range. If None is given, don't perform clipping.\n            Defaults to [-0.05, 0.05]\n\n    Note:\n        This transform should only be used in point cloud segmentation tasks\n            because we don't transform ground-truth bboxes accordingly.\n        For similar transform in detection task, please refer to `ObjectNoise`.\n    \"\"\"\n\n    def __init__(self,\n                 jitter_std=[0.01, 0.01, 0.01],\n                 clip_range=[-0.05, 0.05]):\n        seq_types = (list, tuple, np.ndarray)\n        if not isinstance(jitter_std, seq_types):\n            assert isinstance(jitter_std, (int, float)), \\\n                f'unsupported jitter_std type {type(jitter_std)}'\n            jitter_std = [jitter_std, jitter_std, jitter_std]\n        self.jitter_std = jitter_std\n\n        if clip_range is not None:\n            if not isinstance(clip_range, seq_types):\n                assert isinstance(clip_range, (int, float)), \\\n                    f'unsupported clip_range type {type(clip_range)}'\n                clip_range = [-clip_range, clip_range]\n        self.clip_range = clip_range\n\n    def __call__(self, input_dict):\n        \"\"\"Call function to jitter all the points in the scene.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after adding noise to each point,\n                'points' key is updated in the result dict.\n        \"\"\"\n        points = input_dict['points']\n        jitter_std = np.array(self.jitter_std, dtype=np.float32)\n        jitter_noise = \\\n            np.random.randn(points.shape[0], 3) * jitter_std[None, :]\n        if self.clip_range is not None:\n            jitter_noise = np.clip(jitter_noise, self.clip_range[0],\n                                   self.clip_range[1])\n\n        points.translate(jitter_noise)\n        return input_dict\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        repr_str = self.__class__.__name__\n        repr_str += f'(jitter_std={self.jitter_std},'\n        repr_str += f' clip_range={self.clip_range})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass ObjectSample(object):\n    \"\"\"Sample GT objects to the data.\n\n    Args:\n        db_sampler (dict): Config dict of the database sampler.\n        sample_2d (bool): Whether to also paste 2D image patch to the images\n            This should be true when applying multi-modality cut-and-paste.\n            Defaults to False.\n        use_ground_plane (bool): Whether to use gound plane to adjust the\n            3D labels.\n    \"\"\"\n\n    def __init__(self, db_sampler, sample_2d=False, use_ground_plane=False):\n        self.sampler_cfg = db_sampler\n        self.sample_2d = sample_2d\n        if 'type' not in db_sampler.keys():\n            db_sampler['type'] = 'DataBaseSampler'\n        self.db_sampler = build_from_cfg(db_sampler, OBJECTSAMPLERS)\n        self.use_ground_plane = use_ground_plane\n\n    @staticmethod\n    def remove_points_in_boxes(points, boxes):\n        \"\"\"Remove the points in the sampled bounding boxes.\n\n        Args:\n            points (:obj:`BasePoints`): Input point cloud array.\n            boxes (np.ndarray): Sampled ground truth boxes.\n\n        Returns:\n            np.ndarray: Points with those in the boxes removed.\n        \"\"\"\n        masks = box_np_ops.points_in_rbbox(points.coord.numpy(), boxes)\n        points = points[np.logical_not(masks.any(-1))]\n        return points\n\n    def __call__(self, input_dict):\n        \"\"\"Call function to sample ground truth objects to the data.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after object sampling augmentation,\n                'points', 'gt_bboxes_3d', 'gt_labels_3d' keys are updated\n                in the result dict.\n        \"\"\"\n        gt_bboxes_3d = input_dict['gt_bboxes_3d']\n        gt_labels_3d = input_dict['gt_labels_3d']\n\n        if self.use_ground_plane and 'plane' in input_dict['ann_info']:\n            ground_plane = input_dict['ann_info']['plane']\n            input_dict['plane'] = ground_plane\n        else:\n            ground_plane = None\n        # change to float for blending operation\n        points = input_dict['points']\n        if self.sample_2d:\n            img = input_dict['img']\n            gt_bboxes_2d = input_dict['gt_bboxes']\n            # Assume for now 3D & 2D bboxes are the same\n            sampled_dict = self.db_sampler.sample_all(\n                gt_bboxes_3d.tensor.numpy(),\n                gt_labels_3d,\n                gt_bboxes_2d=gt_bboxes_2d,\n                img=img)\n        else:\n            sampled_dict = self.db_sampler.sample_all(\n                gt_bboxes_3d.tensor.numpy(),\n                gt_labels_3d,\n                img=None,\n                ground_plane=ground_plane)\n        num_exist = gt_labels_3d.shape[0]\n        if sampled_dict is not None:\n            sampled_gt_bboxes_3d = sampled_dict['gt_bboxes_3d']\n            sampled_points = sampled_dict['points']\n            sampled_gt_labels = sampled_dict['gt_labels_3d']\n\n            gt_labels_3d = np.concatenate([gt_labels_3d, sampled_gt_labels],\n                                          axis=0)\n            gt_bboxes_3d = gt_bboxes_3d.new_box(\n                np.concatenate(\n                    [gt_bboxes_3d.tensor.numpy(), sampled_gt_bboxes_3d]))\n\n            points = self.remove_points_in_boxes(points, sampled_gt_bboxes_3d)\n            # check the points dimension\n            points = points.cat([sampled_points, points])\n\n            if self.sample_2d:\n                sampled_gt_bboxes_2d = sampled_dict['gt_bboxes_2d']\n                gt_bboxes_2d = np.concatenate(\n                    [gt_bboxes_2d, sampled_gt_bboxes_2d]).astype(np.float32)\n\n                input_dict['gt_bboxes'] = gt_bboxes_2d\n                input_dict['img'] = sampled_dict['img']\n        gt_bboxes_ignore = np.ones_like(gt_labels_3d)\n        gt_bboxes_ignore[num_exist:] = 0\n        gt_bboxes_ignore = gt_bboxes_ignore.astype(np.bool)\n        input_dict['gt_bboxes_ignore'] = gt_bboxes_ignore\n        input_dict['gt_bboxes_3d'] = gt_bboxes_3d\n        input_dict['gt_labels_3d'] = gt_labels_3d.astype(np.int64)\n        input_dict['points'] = points\n\n        return input_dict\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        repr_str = self.__class__.__name__\n        repr_str += f' sample_2d={self.sample_2d},'\n        repr_str += f' data_root={self.sampler_cfg.data_root},'\n        repr_str += f' info_path={self.sampler_cfg.info_path},'\n        repr_str += f' rate={self.sampler_cfg.rate},'\n        repr_str += f' prepare={self.sampler_cfg.prepare},'\n        repr_str += f' classes={self.sampler_cfg.classes},'\n        repr_str += f' sample_groups={self.sampler_cfg.sample_groups}'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass ObjectNoise(object):\n    \"\"\"Apply noise to each GT objects in the scene.\n\n    Args:\n        translation_std (list[float], optional): Standard deviation of the\n            distribution where translation noise are sampled from.\n            Defaults to [0.25, 0.25, 0.25].\n        global_rot_range (list[float], optional): Global rotation to the scene.\n            Defaults to [0.0, 0.0].\n        rot_range (list[float], optional): Object rotation range.\n            Defaults to [-0.15707963267, 0.15707963267].\n        num_try (int, optional): Number of times to try if the noise applied is\n            invalid. Defaults to 100.\n    \"\"\"\n\n    def __init__(self,\n                 translation_std=[0.25, 0.25, 0.25],\n                 global_rot_range=[0.0, 0.0],\n                 rot_range=[-0.15707963267, 0.15707963267],\n                 num_try=100):\n        self.translation_std = translation_std\n        self.global_rot_range = global_rot_range\n        self.rot_range = rot_range\n        self.num_try = num_try\n\n    def __call__(self, input_dict):\n        \"\"\"Call function to apply noise to each ground truth in the scene.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after adding noise to each object,\n                'points', 'gt_bboxes_3d' keys are updated in the result dict.\n        \"\"\"\n        gt_bboxes_3d = input_dict['gt_bboxes_3d']\n        points = input_dict['points']\n\n        # TODO: this is inplace operation\n        numpy_box = gt_bboxes_3d.tensor.numpy()\n        numpy_points = points.tensor.numpy()\n\n        noise_per_object_v3_(\n            numpy_box,\n            numpy_points,\n            rotation_perturb=self.rot_range,\n            center_noise_std=self.translation_std,\n            global_random_rot_range=self.global_rot_range,\n            num_try=self.num_try)\n\n        input_dict['gt_bboxes_3d'] = gt_bboxes_3d.new_box(numpy_box)\n        input_dict['points'] = points.new_point(numpy_points)\n        return input_dict\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        repr_str = self.__class__.__name__\n        repr_str += f'(num_try={self.num_try},'\n        repr_str += f' translation_std={self.translation_std},'\n        repr_str += f' global_rot_range={self.global_rot_range},'\n        repr_str += f' rot_range={self.rot_range})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass GlobalAlignment(object):\n    \"\"\"Apply global alignment to 3D scene points by rotation and translation.\n\n    Args:\n        rotation_axis (int): Rotation axis for points and bboxes rotation.\n\n    Note:\n        We do not record the applied rotation and translation as in\n            GlobalRotScaleTrans. Because usually, we do not need to reverse\n            the alignment step.\n        For example, ScanNet 3D detection task uses aligned ground-truth\n            bounding boxes for evaluation.\n    \"\"\"\n\n    def __init__(self, rotation_axis):\n        self.rotation_axis = rotation_axis\n\n    def _trans_points(self, input_dict, trans_factor):\n        \"\"\"Private function to translate points.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n            trans_factor (np.ndarray): Translation vector to be applied.\n\n        Returns:\n            dict: Results after translation, 'points' is updated in the dict.\n        \"\"\"\n        input_dict['points'].translate(trans_factor)\n\n    def _rot_points(self, input_dict, rot_mat):\n        \"\"\"Private function to rotate bounding boxes and points.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n            rot_mat (np.ndarray): Rotation matrix to be applied.\n\n        Returns:\n            dict: Results after rotation, 'points' is updated in the dict.\n        \"\"\"\n        # input should be rot_mat_T so I transpose it here\n        input_dict['points'].rotate(rot_mat.T)\n\n    def _check_rot_mat(self, rot_mat):\n        \"\"\"Check if rotation matrix is valid for self.rotation_axis.\n\n        Args:\n            rot_mat (np.ndarray): Rotation matrix to be applied.\n        \"\"\"\n        is_valid = np.allclose(np.linalg.det(rot_mat), 1.0)\n        valid_array = np.zeros(3)\n        valid_array[self.rotation_axis] = 1.0\n        is_valid &= (rot_mat[self.rotation_axis, :] == valid_array).all()\n        is_valid &= (rot_mat[:, self.rotation_axis] == valid_array).all()\n        assert is_valid, f'invalid rotation matrix {rot_mat}'\n\n    def __call__(self, input_dict):\n        \"\"\"Call function to shuffle points.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after global alignment, 'points' and keys in\n                input_dict['bbox3d_fields'] are updated in the result dict.\n        \"\"\"\n        assert 'axis_align_matrix' in input_dict['ann_info'].keys(), \\\n            'axis_align_matrix is not provided in GlobalAlignment'\n\n        axis_align_matrix = input_dict['ann_info']['axis_align_matrix']\n        assert axis_align_matrix.shape == (4, 4), \\\n            f'invalid shape {axis_align_matrix.shape} for axis_align_matrix'\n        rot_mat = axis_align_matrix[:3, :3]\n        trans_vec = axis_align_matrix[:3, -1]\n\n        self._check_rot_mat(rot_mat)\n        self._rot_points(input_dict, rot_mat)\n        self._trans_points(input_dict, trans_vec)\n\n        return input_dict\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f'(rotation_axis={self.rotation_axis})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass GlobalRotScaleTrans(object):\n    \"\"\"Apply global rotation, scaling and translation to a 3D scene.\n\n    Args:\n        rot_range (list[float], optional): Range of rotation angle.\n            Defaults to [-0.78539816, 0.78539816] (close to [-pi/4, pi/4]).\n        scale_ratio_range (list[float], optional): Range of scale ratio.\n            Defaults to [0.95, 1.05].\n        translation_std (list[float], optional): The standard deviation of\n            translation noise applied to a scene, which\n            is sampled from a gaussian distribution whose standard deviation\n            is set by ``translation_std``. Defaults to [0, 0, 0]\n        shift_height (bool, optional): Whether to shift height.\n            (the fourth dimension of indoor points) when scaling.\n            Defaults to False.\n    \"\"\"\n\n    def __init__(self,\n                 rot_range=[-0.78539816, 0.78539816],\n                 scale_ratio_range=[0.95, 1.05],\n                 translation_std=[0, 0, 0],\n                 shift_height=False):\n        seq_types = (list, tuple, np.ndarray)\n        if not isinstance(rot_range, seq_types):\n            assert isinstance(rot_range, (int, float)), \\\n                f'unsupported rot_range type {type(rot_range)}'\n            rot_range = [-rot_range, rot_range]\n        self.rot_range = rot_range\n\n        assert isinstance(scale_ratio_range, seq_types), \\\n            f'unsupported scale_ratio_range type {type(scale_ratio_range)}'\n        self.scale_ratio_range = scale_ratio_range\n\n        if not isinstance(translation_std, seq_types):\n            assert isinstance(translation_std, (int, float)), \\\n                f'unsupported translation_std type {type(translation_std)}'\n            translation_std = [\n                translation_std, translation_std, translation_std\n            ]\n        assert all([std >= 0 for std in translation_std]), \\\n            'translation_std should be positive'\n        self.translation_std = translation_std\n        self.shift_height = shift_height\n\n    def _trans_bbox_points(self, input_dict):\n        \"\"\"Private function to translate bounding boxes and points.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after translation, 'points', 'pcd_trans'\n                and keys in input_dict['bbox3d_fields'] are updated\n                in the result dict.\n        \"\"\"\n        translation_std = np.array(self.translation_std, dtype=np.float32)\n        trans_factor = np.random.normal(scale=translation_std, size=3).T\n\n        input_dict['points'].translate(trans_factor)\n        input_dict['pcd_trans'] = trans_factor\n        for key in input_dict['bbox3d_fields']:\n            input_dict[key].translate(trans_factor)\n\n    def _rot_bbox_points(self, input_dict):\n        \"\"\"Private function to rotate bounding boxes and points.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after rotation, 'points', 'pcd_rotation'\n                and keys in input_dict['bbox3d_fields'] are updated\n                in the result dict.\n        \"\"\"\n        rotation = self.rot_range\n        noise_rotation = np.random.uniform(rotation[0], rotation[1])\n\n        # if no bbox in input_dict, only rotate points\n        if len(input_dict['bbox3d_fields']) == 0:\n            rot_mat_T = input_dict['points'].rotate(noise_rotation)\n            input_dict['pcd_rotation'] = rot_mat_T\n            input_dict['pcd_rotation_angle'] = noise_rotation\n            return\n\n        # rotate points with bboxes\n        for key in input_dict['bbox3d_fields']:\n            if len(input_dict[key].tensor) != 0:\n                points, rot_mat_T = input_dict[key].rotate(\n                    noise_rotation, input_dict['points'])\n                input_dict['points'] = points\n                input_dict['pcd_rotation'] = rot_mat_T\n                input_dict['pcd_rotation_angle'] = noise_rotation\n\n    def _scale_bbox_points(self, input_dict):\n        \"\"\"Private function to scale bounding boxes and points.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after scaling, 'points'and keys in\n                input_dict['bbox3d_fields'] are updated in the result dict.\n        \"\"\"\n        scale = input_dict['pcd_scale_factor']\n        points = input_dict['points']\n        points.scale(scale)\n        if self.shift_height:\n            assert 'height' in points.attribute_dims.keys(), \\\n                'setting shift_height=True but points have no height attribute'\n            points.tensor[:, points.attribute_dims['height']] *= scale\n        input_dict['points'] = points\n\n        for key in input_dict['bbox3d_fields']:\n            input_dict[key].scale(scale)\n\n    def _random_scale(self, input_dict):\n        \"\"\"Private function to randomly set the scale factor.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after scaling, 'pcd_scale_factor' are updated\n                in the result dict.\n        \"\"\"\n        scale_factor = np.random.uniform(self.scale_ratio_range[0],\n                                         self.scale_ratio_range[1])\n        input_dict['pcd_scale_factor'] = scale_factor\n\n    def __call__(self, input_dict):\n        \"\"\"Private function to rotate, scale and translate bounding boxes and\n        points.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after scaling, 'points', 'pcd_rotation',\n                'pcd_scale_factor', 'pcd_trans' and keys in\n                input_dict['bbox3d_fields'] are updated in the result dict.\n        \"\"\"\n        if 'transformation_3d_flow' not in input_dict:\n            input_dict['transformation_3d_flow'] = []\n\n        self._rot_bbox_points(input_dict)\n\n        if 'pcd_scale_factor' not in input_dict:\n            self._random_scale(input_dict)\n        self._scale_bbox_points(input_dict)\n\n        self._trans_bbox_points(input_dict)\n\n        input_dict['transformation_3d_flow'].extend(['R', 'S', 'T'])\n        return input_dict\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        repr_str = self.__class__.__name__\n        repr_str += f'(rot_range={self.rot_range},'\n        repr_str += f' scale_ratio_range={self.scale_ratio_range},'\n        repr_str += f' translation_std={self.translation_std},'\n        repr_str += f' shift_height={self.shift_height})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass PointShuffle(object):\n    \"\"\"Shuffle input points.\"\"\"\n\n    def __call__(self, input_dict):\n        \"\"\"Call function to shuffle points.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after filtering, 'points', 'pts_instance_mask'\n                and 'pts_semantic_mask' keys are updated in the result dict.\n        \"\"\"\n        idx = input_dict['points'].shuffle()\n        idx = idx.numpy()\n\n        pts_instance_mask = input_dict.get('pts_instance_mask', None)\n        pts_semantic_mask = input_dict.get('pts_semantic_mask', None)\n\n        if pts_instance_mask is not None:\n            input_dict['pts_instance_mask'] = pts_instance_mask[idx]\n\n        if pts_semantic_mask is not None:\n            input_dict['pts_semantic_mask'] = pts_semantic_mask[idx]\n\n        return input_dict\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n\n@PIPELINES.register_module()\nclass ObjectRangeFilter(object):\n    \"\"\"Filter objects by the range.\n\n    Args:\n        point_cloud_range (list[float]): Point cloud range.\n    \"\"\"\n\n    def __init__(self, point_cloud_range):\n        self.pcd_range = np.array(point_cloud_range, dtype=np.float32)\n\n    def __call__(self, input_dict):\n        \"\"\"Call function to filter objects by the range.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after filtering, 'gt_bboxes_3d', 'gt_labels_3d'\n                keys are updated in the result dict.\n        \"\"\"\n        # Check points instance type and initialise bev_range\n        if isinstance(input_dict['gt_bboxes_3d'],\n                      (LiDARInstance3DBoxes, DepthInstance3DBoxes)):\n            bev_range = self.pcd_range[[0, 1, 3, 4]]\n        elif isinstance(input_dict['gt_bboxes_3d'], CameraInstance3DBoxes):\n            bev_range = self.pcd_range[[0, 2, 3, 5]]\n\n        gt_bboxes_3d = input_dict['gt_bboxes_3d']\n        gt_labels_3d = input_dict['gt_labels_3d']\n        mask = gt_bboxes_3d.in_range_bev(bev_range)\n\n        if 'gt_bboxes_ignore' in input_dict:\n            gt_bboxes_ignore = input_dict['gt_bboxes_ignore']\n            gt_bboxes_ignore = gt_bboxes_ignore[mask.numpy().astype(np.bool)]\n            input_dict['gt_bboxes_ignore'] = gt_bboxes_ignore\n        gt_bboxes_3d = gt_bboxes_3d[mask]\n        # mask is a torch tensor but gt_labels_3d is still numpy array\n        # using mask to index gt_labels_3d will cause bug when\n        # len(gt_labels_3d) == 1, where mask=1 will be interpreted\n        # as gt_labels_3d[1] and cause out of index error\n        gt_labels_3d = gt_labels_3d[mask.numpy().astype(np.bool)]\n\n        # limit rad to [-pi, pi]\n        gt_bboxes_3d.limit_yaw(offset=0.5, period=2 * np.pi)\n        input_dict['gt_bboxes_3d'] = gt_bboxes_3d\n        input_dict['gt_labels_3d'] = gt_labels_3d\n\n        return input_dict\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        repr_str = self.__class__.__name__\n        repr_str += f'(point_cloud_range={self.pcd_range.tolist()})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass PointsRangeFilter(object):\n    \"\"\"Filter points by the range.\n\n    Args:\n        point_cloud_range (list[float]): Point cloud range.\n    \"\"\"\n\n    def __init__(self, point_cloud_range):\n        self.pcd_range = np.array(point_cloud_range, dtype=np.float32)\n\n    def __call__(self, input_dict):\n        \"\"\"Call function to filter points by the range.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after filtering, 'points', 'pts_instance_mask'\n                and 'pts_semantic_mask' keys are updated in the result dict.\n        \"\"\"\n        points = input_dict['points']\n        points_mask = points.in_range_3d(self.pcd_range)\n        clean_points = points[points_mask]\n        input_dict['points'] = clean_points\n        points_mask = points_mask.numpy()\n\n        pts_instance_mask = input_dict.get('pts_instance_mask', None)\n        pts_semantic_mask = input_dict.get('pts_semantic_mask', None)\n\n        if pts_instance_mask is not None:\n            input_dict['pts_instance_mask'] = pts_instance_mask[points_mask]\n\n        if pts_semantic_mask is not None:\n            input_dict['pts_semantic_mask'] = pts_semantic_mask[points_mask]\n\n        return input_dict\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        repr_str = self.__class__.__name__\n        repr_str += f'(point_cloud_range={self.pcd_range.tolist()})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass ObjectNameFilter(object):\n    \"\"\"Filter GT objects by their names.\n\n    Args:\n        classes (list[str]): List of class names to be kept for training.\n    \"\"\"\n\n    def __init__(self, classes):\n        self.classes = classes\n        self.labels = list(range(len(self.classes)))\n\n    def __call__(self, input_dict):\n        \"\"\"Call function to filter objects by their names.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after filtering, 'gt_bboxes_3d', 'gt_labels_3d'\n                keys are updated in the result dict.\n        \"\"\"\n        gt_labels_3d = input_dict['gt_labels_3d']\n        gt_bboxes_mask = np.array([n in self.labels for n in gt_labels_3d],\n                                  dtype=np.bool_)\n        input_dict['gt_bboxes_3d'] = input_dict['gt_bboxes_3d'][gt_bboxes_mask]\n        input_dict['gt_labels_3d'] = input_dict['gt_labels_3d'][gt_bboxes_mask]\n        if 'gt_bboxes_ignore' in input_dict:\n            gt_bboxes_ignore = input_dict['gt_bboxes_ignore']\n            gt_bboxes_ignore = gt_bboxes_ignore[gt_bboxes_mask]\n            input_dict['gt_bboxes_ignore'] = gt_bboxes_ignore\n        return input_dict\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        repr_str = self.__class__.__name__\n        repr_str += f'(classes={self.classes})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass PointSample(object):\n    \"\"\"Point sample.\n\n    Sampling data to a certain number.\n\n    Args:\n        num_points (int): Number of points to be sampled.\n        sample_range (float, optional): The range where to sample points.\n            If not None, the points with depth larger than `sample_range` are\n            prior to be sampled. Defaults to None.\n        replace (bool, optional): Whether the sampling is with or without\n            replacement. Defaults to False.\n    \"\"\"\n\n    def __init__(self, num_points, sample_range=None, replace=False):\n        self.num_points = num_points\n        self.sample_range = sample_range\n        self.replace = replace\n\n    def _points_random_sampling(self,\n                                points,\n                                num_samples,\n                                sample_range=None,\n                                replace=False,\n                                return_choices=False):\n        \"\"\"Points random sampling.\n\n        Sample points to a certain number.\n\n        Args:\n            points (np.ndarray | :obj:`BasePoints`): 3D Points.\n            num_samples (int): Number of samples to be sampled.\n            sample_range (float, optional): Indicating the range where the\n                points will be sampled. Defaults to None.\n            replace (bool, optional): Sampling with or without replacement.\n                Defaults to None.\n            return_choices (bool, optional): Whether return choice.\n                Defaults to False.\n        Returns:\n            tuple[np.ndarray] | np.ndarray:\n                - points (np.ndarray | :obj:`BasePoints`): 3D Points.\n                - choices (np.ndarray, optional): The generated random samples.\n        \"\"\"\n        if not replace:\n            replace = (points.shape[0] < num_samples)\n        point_range = range(len(points))\n        if sample_range is not None and not replace:\n            # Only sampling the near points when len(points) >= num_samples\n            dist = np.linalg.norm(points.tensor, axis=1)\n            far_inds = np.where(dist >= sample_range)[0]\n            near_inds = np.where(dist < sample_range)[0]\n            # in case there are too many far points\n            if len(far_inds) > num_samples:\n                far_inds = np.random.choice(\n                    far_inds, num_samples, replace=False)\n            point_range = near_inds\n            num_samples -= len(far_inds)\n        choices = np.random.choice(point_range, num_samples, replace=replace)\n        if sample_range is not None and not replace:\n            choices = np.concatenate((far_inds, choices))\n            # Shuffle points after sampling\n            np.random.shuffle(choices)\n        if return_choices:\n            return points[choices], choices\n        else:\n            return points[choices]\n\n    def __call__(self, results):\n        \"\"\"Call function to sample points to in indoor scenes.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n        Returns:\n            dict: Results after sampling, 'points', 'pts_instance_mask'\n                and 'pts_semantic_mask' keys are updated in the result dict.\n        \"\"\"\n        points = results['points']\n        points, choices = self._points_random_sampling(\n            points,\n            self.num_points,\n            self.sample_range,\n            self.replace,\n            return_choices=True)\n        results['points'] = points\n\n        pts_instance_mask = results.get('pts_instance_mask', None)\n        pts_semantic_mask = results.get('pts_semantic_mask', None)\n\n        if pts_instance_mask is not None:\n            pts_instance_mask = pts_instance_mask[choices]\n            results['pts_instance_mask'] = pts_instance_mask\n\n        if pts_semantic_mask is not None:\n            pts_semantic_mask = pts_semantic_mask[choices]\n            results['pts_semantic_mask'] = pts_semantic_mask\n\n        return results\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        repr_str = self.__class__.__name__\n        repr_str += f'(num_points={self.num_points},'\n        repr_str += f' sample_range={self.sample_range},'\n        repr_str += f' replace={self.replace})'\n\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass IndoorPointSample(PointSample):\n    \"\"\"Indoor point sample.\n\n    Sampling data to a certain number.\n    NOTE: IndoorPointSample is deprecated in favor of PointSample\n\n    Args:\n        num_points (int): Number of points to be sampled.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        warnings.warn(\n            'IndoorPointSample is deprecated in favor of PointSample')\n        super(IndoorPointSample, self).__init__(*args, **kwargs)\n\n\n@PIPELINES.register_module()\nclass IndoorPatchPointSample(object):\n    r\"\"\"Indoor point sample within a patch. Modified from `PointNet++ <https://\n    github.com/charlesq34/pointnet2/blob/master/scannet/scannet_dataset.py>`_.\n\n    Sampling data to a certain number for semantic segmentation.\n\n    Args:\n        num_points (int): Number of points to be sampled.\n        block_size (float, optional): Size of a block to sample points from.\n            Defaults to 1.5.\n        sample_rate (float, optional): Stride used in sliding patch generation.\n            This parameter is unused in `IndoorPatchPointSample` and thus has\n            been deprecated. We plan to remove it in the future.\n            Defaults to None.\n        ignore_index (int, optional): Label index that won't be used for the\n            segmentation task. This is set in PointSegClassMapping as neg_cls.\n            If not None, will be used as a patch selection criterion.\n            Defaults to None.\n        use_normalized_coord (bool, optional): Whether to use normalized xyz as\n            additional features. Defaults to False.\n        num_try (int, optional): Number of times to try if the patch selected\n            is invalid. Defaults to 10.\n        enlarge_size (float, optional): Enlarge the sampled patch to\n            [-block_size / 2 - enlarge_size, block_size / 2 + enlarge_size] as\n            an augmentation. If None, set it as 0. Defaults to 0.2.\n        min_unique_num (int, optional): Minimum number of unique points\n            the sampled patch should contain. If None, use PointNet++'s method\n            to judge uniqueness. Defaults to None.\n        eps (float, optional): A value added to patch boundary to guarantee\n            points coverage. Defaults to 1e-2.\n\n    Note:\n        This transform should only be used in the training process of point\n            cloud segmentation tasks. For the sliding patch generation and\n            inference process in testing, please refer to the `slide_inference`\n            function of `EncoderDecoder3D` class.\n    \"\"\"\n\n    def __init__(self,\n                 num_points,\n                 block_size=1.5,\n                 sample_rate=None,\n                 ignore_index=None,\n                 use_normalized_coord=False,\n                 num_try=10,\n                 enlarge_size=0.2,\n                 min_unique_num=None,\n                 eps=1e-2):\n        self.num_points = num_points\n        self.block_size = block_size\n        self.ignore_index = ignore_index\n        self.use_normalized_coord = use_normalized_coord\n        self.num_try = num_try\n        self.enlarge_size = enlarge_size if enlarge_size is not None else 0.0\n        self.min_unique_num = min_unique_num\n        self.eps = eps\n\n        if sample_rate is not None:\n            warnings.warn(\n                \"'sample_rate' has been deprecated and will be removed in \"\n                'the future. Please remove them from your code.')\n\n    def _input_generation(self, coords, patch_center, coord_max, attributes,\n                          attribute_dims, point_type):\n        \"\"\"Generating model input.\n\n        Generate input by subtracting patch center and adding additional\n            features. Currently support colors and normalized xyz as features.\n\n        Args:\n            coords (np.ndarray): Sampled 3D Points.\n            patch_center (np.ndarray): Center coordinate of the selected patch.\n            coord_max (np.ndarray): Max coordinate of all 3D Points.\n            attributes (np.ndarray): features of input points.\n            attribute_dims (dict): Dictionary to indicate the meaning of extra\n                dimension.\n            point_type (type): class of input points inherited from BasePoints.\n\n        Returns:\n            :obj:`BasePoints`: The generated input data.\n        \"\"\"\n        # subtract patch center, the z dimension is not centered\n        centered_coords = coords.copy()\n        centered_coords[:, 0] -= patch_center[0]\n        centered_coords[:, 1] -= patch_center[1]\n\n        if self.use_normalized_coord:\n            normalized_coord = coords / coord_max\n            attributes = np.concatenate([attributes, normalized_coord], axis=1)\n            if attribute_dims is None:\n                attribute_dims = dict()\n            attribute_dims.update(\n                dict(normalized_coord=[\n                    attributes.shape[1], attributes.shape[1] +\n                    1, attributes.shape[1] + 2\n                ]))\n\n        points = np.concatenate([centered_coords, attributes], axis=1)\n        points = point_type(\n            points, points_dim=points.shape[1], attribute_dims=attribute_dims)\n\n        return points\n\n    def _patch_points_sampling(self, points, sem_mask):\n        \"\"\"Patch points sampling.\n\n        First sample a valid patch.\n        Then sample points within that patch to a certain number.\n\n        Args:\n            points (:obj:`BasePoints`): 3D Points.\n            sem_mask (np.ndarray): semantic segmentation mask for input points.\n\n        Returns:\n            tuple[:obj:`BasePoints`, np.ndarray] | :obj:`BasePoints`:\n\n                - points (:obj:`BasePoints`): 3D Points.\n                - choices (np.ndarray): The generated random samples.\n        \"\"\"\n        coords = points.coord.numpy()\n        attributes = points.tensor[:, 3:].numpy()\n        attribute_dims = points.attribute_dims\n        point_type = type(points)\n\n        coord_max = np.amax(coords, axis=0)\n        coord_min = np.amin(coords, axis=0)\n\n        for _ in range(self.num_try):\n            # random sample a point as patch center\n            cur_center = coords[np.random.choice(coords.shape[0])]\n\n            # boundary of a patch, which would be enlarged by\n            # `self.enlarge_size` as an augmentation\n            cur_max = cur_center + np.array(\n                [self.block_size / 2.0, self.block_size / 2.0, 0.0])\n            cur_min = cur_center - np.array(\n                [self.block_size / 2.0, self.block_size / 2.0, 0.0])\n            cur_max[2] = coord_max[2]\n            cur_min[2] = coord_min[2]\n            cur_choice = np.sum(\n                (coords >= (cur_min - self.enlarge_size)) *\n                (coords <= (cur_max + self.enlarge_size)),\n                axis=1) == 3\n\n            if not cur_choice.any():  # no points in this patch\n                continue\n\n            cur_coords = coords[cur_choice, :]\n            cur_sem_mask = sem_mask[cur_choice]\n            point_idxs = np.where(cur_choice)[0]\n            mask = np.sum(\n                (cur_coords >= (cur_min - self.eps)) * (cur_coords <=\n                                                        (cur_max + self.eps)),\n                axis=1) == 3\n\n            # two criteria for patch sampling, adopted from PointNet++\n            # 1. selected patch should contain enough unique points\n            if self.min_unique_num is None:\n                # use PointNet++'s method as default\n                # [31, 31, 62] are just some big values used to transform\n                # coords from 3d array to 1d and then check their uniqueness\n                # this is used in all the ScanNet code following PointNet++\n                vidx = np.ceil(\n                    (cur_coords[mask, :] - cur_min) / (cur_max - cur_min) *\n                    np.array([31.0, 31.0, 62.0]))\n                vidx = np.unique(vidx[:, 0] * 31.0 * 62.0 + vidx[:, 1] * 62.0 +\n                                 vidx[:, 2])\n                flag1 = len(vidx) / 31.0 / 31.0 / 62.0 >= 0.02\n            else:\n                # if `min_unique_num` is provided, directly compare with it\n                flag1 = mask.sum() >= self.min_unique_num\n\n            # 2. selected patch should contain enough annotated points\n            if self.ignore_index is None:\n                flag2 = True\n            else:\n                flag2 = np.sum(cur_sem_mask != self.ignore_index) / \\\n                               len(cur_sem_mask) >= 0.7\n\n            if flag1 and flag2:\n                break\n\n        # sample idx to `self.num_points`\n        if point_idxs.size >= self.num_points:\n            # no duplicate in sub-sampling\n            choices = np.random.choice(\n                point_idxs, self.num_points, replace=False)\n        else:\n            # do not use random choice here to avoid some points not counted\n            dup = np.random.choice(point_idxs.size,\n                                   self.num_points - point_idxs.size)\n            idx_dup = np.concatenate(\n                [np.arange(point_idxs.size),\n                 np.array(dup)], 0)\n            choices = point_idxs[idx_dup]\n\n        # construct model input\n        points = self._input_generation(coords[choices], cur_center, coord_max,\n                                        attributes[choices], attribute_dims,\n                                        point_type)\n\n        return points, choices\n\n    def __call__(self, results):\n        \"\"\"Call function to sample points to in indoor scenes.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after sampling, 'points', 'pts_instance_mask'\n                and 'pts_semantic_mask' keys are updated in the result dict.\n        \"\"\"\n        points = results['points']\n\n        assert 'pts_semantic_mask' in results.keys(), \\\n            'semantic mask should be provided in training and evaluation'\n        pts_semantic_mask = results['pts_semantic_mask']\n\n        points, choices = self._patch_points_sampling(points,\n                                                      pts_semantic_mask)\n\n        results['points'] = points\n        results['pts_semantic_mask'] = pts_semantic_mask[choices]\n        pts_instance_mask = results.get('pts_instance_mask', None)\n        if pts_instance_mask is not None:\n            results['pts_instance_mask'] = pts_instance_mask[choices]\n\n        return results\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        repr_str = self.__class__.__name__\n        repr_str += f'(num_points={self.num_points},'\n        repr_str += f' block_size={self.block_size},'\n        repr_str += f' ignore_index={self.ignore_index},'\n        repr_str += f' use_normalized_coord={self.use_normalized_coord},'\n        repr_str += f' num_try={self.num_try},'\n        repr_str += f' enlarge_size={self.enlarge_size},'\n        repr_str += f' min_unique_num={self.min_unique_num},'\n        repr_str += f' eps={self.eps})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass BackgroundPointsFilter(object):\n    \"\"\"Filter background points near the bounding box.\n\n    Args:\n        bbox_enlarge_range (tuple[float], float): Bbox enlarge range.\n    \"\"\"\n\n    def __init__(self, bbox_enlarge_range):\n        assert (is_tuple_of(bbox_enlarge_range, float)\n                and len(bbox_enlarge_range) == 3) \\\n            or isinstance(bbox_enlarge_range, float), \\\n            f'Invalid arguments bbox_enlarge_range {bbox_enlarge_range}'\n\n        if isinstance(bbox_enlarge_range, float):\n            bbox_enlarge_range = [bbox_enlarge_range] * 3\n        self.bbox_enlarge_range = np.array(\n            bbox_enlarge_range, dtype=np.float32)[np.newaxis, :]\n\n    def __call__(self, input_dict):\n        \"\"\"Call function to filter points by the range.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after filtering, 'points', 'pts_instance_mask'\n                and 'pts_semantic_mask' keys are updated in the result dict.\n        \"\"\"\n        points = input_dict['points']\n        gt_bboxes_3d = input_dict['gt_bboxes_3d']\n\n        # avoid groundtruth being modified\n        gt_bboxes_3d_np = gt_bboxes_3d.tensor.clone().numpy()\n        gt_bboxes_3d_np[:, :3] = gt_bboxes_3d.gravity_center.clone().numpy()\n\n        enlarged_gt_bboxes_3d = gt_bboxes_3d_np.copy()\n        enlarged_gt_bboxes_3d[:, 3:6] += self.bbox_enlarge_range\n        points_numpy = points.tensor.clone().numpy()\n        foreground_masks = box_np_ops.points_in_rbbox(\n            points_numpy, gt_bboxes_3d_np, origin=(0.5, 0.5, 0.5))\n        enlarge_foreground_masks = box_np_ops.points_in_rbbox(\n            points_numpy, enlarged_gt_bboxes_3d, origin=(0.5, 0.5, 0.5))\n        foreground_masks = foreground_masks.max(1)\n        enlarge_foreground_masks = enlarge_foreground_masks.max(1)\n        valid_masks = ~np.logical_and(~foreground_masks,\n                                      enlarge_foreground_masks)\n\n        input_dict['points'] = points[valid_masks]\n        pts_instance_mask = input_dict.get('pts_instance_mask', None)\n        if pts_instance_mask is not None:\n            input_dict['pts_instance_mask'] = pts_instance_mask[valid_masks]\n\n        pts_semantic_mask = input_dict.get('pts_semantic_mask', None)\n        if pts_semantic_mask is not None:\n            input_dict['pts_semantic_mask'] = pts_semantic_mask[valid_masks]\n        return input_dict\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        repr_str = self.__class__.__name__\n        repr_str += f'(bbox_enlarge_range={self.bbox_enlarge_range.tolist()})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass VoxelBasedPointSampler(object):\n    \"\"\"Voxel based point sampler.\n\n    Apply voxel sampling to multiple sweep points.\n\n    Args:\n        cur_sweep_cfg (dict): Config for sampling current points.\n        prev_sweep_cfg (dict): Config for sampling previous points.\n        time_dim (int): Index that indicate the time dimension\n            for input points.\n    \"\"\"\n\n    def __init__(self, cur_sweep_cfg, prev_sweep_cfg=None, time_dim=3):\n        self.cur_voxel_generator = VoxelGenerator(**cur_sweep_cfg)\n        self.cur_voxel_num = self.cur_voxel_generator._max_voxels\n        self.time_dim = time_dim\n        if prev_sweep_cfg is not None:\n            assert prev_sweep_cfg['max_num_points'] == \\\n                cur_sweep_cfg['max_num_points']\n            self.prev_voxel_generator = VoxelGenerator(**prev_sweep_cfg)\n            self.prev_voxel_num = self.prev_voxel_generator._max_voxels\n        else:\n            self.prev_voxel_generator = None\n            self.prev_voxel_num = 0\n\n    def _sample_points(self, points, sampler, point_dim):\n        \"\"\"Sample points for each points subset.\n\n        Args:\n            points (np.ndarray): Points subset to be sampled.\n            sampler (VoxelGenerator): Voxel based sampler for\n                each points subset.\n            point_dim (int): The dimension of each points\n\n        Returns:\n            np.ndarray: Sampled points.\n        \"\"\"\n        voxels, coors, num_points_per_voxel = sampler.generate(points)\n        if voxels.shape[0] < sampler._max_voxels:\n            padding_points = np.zeros([\n                sampler._max_voxels - voxels.shape[0], sampler._max_num_points,\n                point_dim\n            ],\n                                      dtype=points.dtype)\n            padding_points[:] = voxels[0]\n            sample_points = np.concatenate([voxels, padding_points], axis=0)\n        else:\n            sample_points = voxels\n\n        return sample_points\n\n    def __call__(self, results):\n        \"\"\"Call function to sample points from multiple sweeps.\n\n        Args:\n            input_dict (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after sampling, 'points', 'pts_instance_mask'\n                and 'pts_semantic_mask' keys are updated in the result dict.\n        \"\"\"\n        points = results['points']\n        original_dim = points.shape[1]\n\n        # TODO: process instance and semantic mask while _max_num_points\n        # is larger than 1\n        # Extend points with seg and mask fields\n        map_fields2dim = []\n        start_dim = original_dim\n        points_numpy = points.tensor.numpy()\n        extra_channel = [points_numpy]\n        for idx, key in enumerate(results['pts_mask_fields']):\n            map_fields2dim.append((key, idx + start_dim))\n            extra_channel.append(results[key][..., None])\n\n        start_dim += len(results['pts_mask_fields'])\n        for idx, key in enumerate(results['pts_seg_fields']):\n            map_fields2dim.append((key, idx + start_dim))\n            extra_channel.append(results[key][..., None])\n\n        points_numpy = np.concatenate(extra_channel, axis=-1)\n\n        # Split points into two part, current sweep points and\n        # previous sweeps points.\n        # TODO: support different sampling methods for next sweeps points\n        # and previous sweeps points.\n        cur_points_flag = (points_numpy[:, self.time_dim] == 0)\n        cur_sweep_points = points_numpy[cur_points_flag]\n        prev_sweeps_points = points_numpy[~cur_points_flag]\n        if prev_sweeps_points.shape[0] == 0:\n            prev_sweeps_points = cur_sweep_points\n\n        # Shuffle points before sampling\n        np.random.shuffle(cur_sweep_points)\n        np.random.shuffle(prev_sweeps_points)\n\n        cur_sweep_points = self._sample_points(cur_sweep_points,\n                                               self.cur_voxel_generator,\n                                               points_numpy.shape[1])\n        if self.prev_voxel_generator is not None:\n            prev_sweeps_points = self._sample_points(prev_sweeps_points,\n                                                     self.prev_voxel_generator,\n                                                     points_numpy.shape[1])\n\n            points_numpy = np.concatenate(\n                [cur_sweep_points, prev_sweeps_points], 0)\n        else:\n            points_numpy = cur_sweep_points\n\n        if self.cur_voxel_generator._max_num_points == 1:\n            points_numpy = points_numpy.squeeze(1)\n        results['points'] = points.new_point(points_numpy[..., :original_dim])\n\n        # Restore the corresponding seg and mask fields\n        for key, dim_index in map_fields2dim:\n            results[key] = points_numpy[..., dim_index]\n\n        return results\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n\n        def _auto_indent(repr_str, indent):\n            repr_str = repr_str.split('\\n')\n            repr_str = [' ' * indent + t + '\\n' for t in repr_str]\n            repr_str = ''.join(repr_str)[:-1]\n            return repr_str\n\n        repr_str = self.__class__.__name__\n        indent = 4\n        repr_str += '(\\n'\n        repr_str += ' ' * indent + f'num_cur_sweep={self.cur_voxel_num},\\n'\n        repr_str += ' ' * indent + f'num_prev_sweep={self.prev_voxel_num},\\n'\n        repr_str += ' ' * indent + f'time_dim={self.time_dim},\\n'\n        repr_str += ' ' * indent + 'cur_voxel_generator=\\n'\n        repr_str += f'{_auto_indent(repr(self.cur_voxel_generator), 8)},\\n'\n        repr_str += ' ' * indent + 'prev_voxel_generator=\\n'\n        repr_str += f'{_auto_indent(repr(self.prev_voxel_generator), 8)})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass AffineResize(object):\n    \"\"\"Get the affine transform matrices to the target size.\n\n    Different from :class:`RandomAffine` in MMDetection, this class can\n    calculate the affine transform matrices while resizing the input image\n    to a fixed size. The affine transform matrices include: 1) matrix\n    transforming original image to the network input image size. 2) matrix\n    transforming original image to the network output feature map size.\n\n    Args:\n        img_scale (tuple): Images scales for resizing.\n        down_ratio (int): The down ratio of feature map.\n            Actually the arg should be >= 1.\n        bbox_clip_border (bool, optional): Whether clip the objects\n            outside the border of the image. Defaults to True.\n    \"\"\"\n\n    def __init__(self, img_scale, down_ratio, bbox_clip_border=True):\n\n        self.img_scale = img_scale\n        self.down_ratio = down_ratio\n        self.bbox_clip_border = bbox_clip_border\n\n    def __call__(self, results):\n        \"\"\"Call function to do affine transform to input image and labels.\n\n        Args:\n            results (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after affine resize, 'affine_aug', 'trans_mat'\n                keys are added in the result dict.\n        \"\"\"\n        # The results have gone through RandomShiftScale before AffineResize\n        if 'center' not in results:\n            img = results['img']\n            height, width = img.shape[:2]\n            center = np.array([width / 2, height / 2], dtype=np.float32)\n            size = np.array([width, height], dtype=np.float32)\n            results['affine_aug'] = False\n        else:\n            # The results did not go through RandomShiftScale before\n            # AffineResize\n            img = results['img']\n            center = results['center']\n            size = results['size']\n\n        trans_affine = self._get_transform_matrix(center, size, self.img_scale)\n\n        img = cv2.warpAffine(img, trans_affine[:2, :], self.img_scale)\n\n        if isinstance(self.down_ratio, tuple):\n            trans_mat = [\n                self._get_transform_matrix(\n                    center, size,\n                    (self.img_scale[0] // ratio, self.img_scale[1] // ratio))\n                for ratio in self.down_ratio\n            ]  # (3, 3)\n        else:\n            trans_mat = self._get_transform_matrix(\n                center, size, (self.img_scale[0] // self.down_ratio,\n                               self.img_scale[1] // self.down_ratio))\n\n        results['img'] = img\n        results['img_shape'] = img.shape\n        results['pad_shape'] = img.shape\n        results['trans_mat'] = trans_mat\n\n        self._affine_bboxes(results, trans_affine)\n\n        if 'centers2d' in results:\n            centers2d = self._affine_transform(results['centers2d'],\n                                               trans_affine)\n            valid_index = (centers2d[:, 0] >\n                           0) & (centers2d[:, 0] <\n                                 self.img_scale[0]) & (centers2d[:, 1] > 0) & (\n                                     centers2d[:, 1] < self.img_scale[1])\n            results['centers2d'] = centers2d[valid_index]\n\n            for key in results.get('bbox_fields', []):\n                if key in ['gt_bboxes']:\n                    results[key] = results[key][valid_index]\n                    if 'gt_labels' in results:\n                        results['gt_labels'] = results['gt_labels'][\n                            valid_index]\n                    if 'gt_masks' in results:\n                        raise NotImplementedError(\n                            'AffineResize only supports bbox.')\n\n            for key in results.get('bbox3d_fields', []):\n                if key in ['gt_bboxes_3d']:\n                    results[key].tensor = results[key].tensor[valid_index]\n                    if 'gt_labels_3d' in results:\n                        results['gt_labels_3d'] = results['gt_labels_3d'][\n                            valid_index]\n\n            results['depths'] = results['depths'][valid_index]\n\n        return results\n\n    def _affine_bboxes(self, results, matrix):\n        \"\"\"Affine transform bboxes to input image.\n\n        Args:\n            results (dict): Result dict from loading pipeline.\n            matrix (np.ndarray): Matrix transforming original\n                image to the network input image size.\n                shape: (3, 3)\n        \"\"\"\n\n        for key in results.get('bbox_fields', []):\n            bboxes = results[key]\n            bboxes[:, :2] = self._affine_transform(bboxes[:, :2], matrix)\n            bboxes[:, 2:] = self._affine_transform(bboxes[:, 2:], matrix)\n            if self.bbox_clip_border:\n                bboxes[:,\n                       [0, 2]] = bboxes[:,\n                                        [0, 2]].clip(0, self.img_scale[0] - 1)\n                bboxes[:,\n                       [1, 3]] = bboxes[:,\n                                        [1, 3]].clip(0, self.img_scale[1] - 1)\n            results[key] = bboxes\n\n    def _affine_transform(self, points, matrix):\n        \"\"\"Affine transform bbox points to input image.\n\n        Args:\n            points (np.ndarray): Points to be transformed.\n                shape: (N, 2)\n            matrix (np.ndarray): Affine transform matrix.\n                shape: (3, 3)\n\n        Returns:\n            np.ndarray: Transformed points.\n        \"\"\"\n        num_points = points.shape[0]\n        hom_points_2d = np.concatenate((points, np.ones((num_points, 1))),\n                                       axis=1)\n        hom_points_2d = hom_points_2d.T\n        affined_points = np.matmul(matrix, hom_points_2d).T\n        return affined_points[:, :2]\n\n    def _get_transform_matrix(self, center, scale, output_scale):\n        \"\"\"Get affine transform matrix.\n\n        Args:\n            center (tuple): Center of current image.\n            scale (tuple): Scale of current image.\n            output_scale (tuple[float]): The transform target image scales.\n\n        Returns:\n            np.ndarray: Affine transform matrix.\n        \"\"\"\n        # TODO: further add rot and shift here.\n        src_w = scale[0]\n        dst_w = output_scale[0]\n        dst_h = output_scale[1]\n\n        src_dir = np.array([0, src_w * -0.5])\n        dst_dir = np.array([0, dst_w * -0.5])\n\n        src = np.zeros((3, 2), dtype=np.float32)\n        dst = np.zeros((3, 2), dtype=np.float32)\n        src[0, :] = center\n        src[1, :] = center + src_dir\n        dst[0, :] = np.array([dst_w * 0.5, dst_h * 0.5])\n        dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir\n\n        src[2, :] = self._get_ref_point(src[0, :], src[1, :])\n        dst[2, :] = self._get_ref_point(dst[0, :], dst[1, :])\n\n        get_matrix = cv2.getAffineTransform(src, dst)\n\n        matrix = np.concatenate((get_matrix, [[0., 0., 1.]]))\n\n        return matrix.astype(np.float32)\n\n    def _get_ref_point(self, ref_point1, ref_point2):\n        \"\"\"Get reference point to calculate affine transform matrix.\n\n        While using opencv to calculate the affine matrix, we need at least\n        three corresponding points separately on original image and target\n        image. Here we use two points to get the the third reference point.\n        \"\"\"\n        d = ref_point1 - ref_point2\n        ref_point3 = ref_point2 + np.array([-d[1], d[0]])\n        return ref_point3\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f'(img_scale={self.img_scale}, '\n        repr_str += f'down_ratio={self.down_ratio}) '\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass RandomShiftScale(object):\n    \"\"\"Random shift scale.\n\n    Different from the normal shift and scale function, it doesn't\n    directly shift or scale image. It can record the shift and scale\n    infos into loading pipelines. It's designed to be used with\n    AffineResize together.\n\n    Args:\n        shift_scale (tuple[float]): Shift and scale range.\n        aug_prob (float): The shifting and scaling probability.\n    \"\"\"\n\n    def __init__(self, shift_scale, aug_prob):\n\n        self.shift_scale = shift_scale\n        self.aug_prob = aug_prob\n\n    def __call__(self, results):\n        \"\"\"Call function to record random shift and scale infos.\n\n        Args:\n            results (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Results after random shift and scale, 'center', 'size'\n                and 'affine_aug' keys are added in the result dict.\n        \"\"\"\n        img = results['img']\n\n        height, width = img.shape[:2]\n\n        center = np.array([width / 2, height / 2], dtype=np.float32)\n        size = np.array([width, height], dtype=np.float32)\n\n        if random.random() < self.aug_prob:\n            shift, scale = self.shift_scale[0], self.shift_scale[1]\n            shift_ranges = np.arange(-shift, shift + 0.1, 0.1)\n            center[0] += size[0] * random.choice(shift_ranges)\n            center[1] += size[1] * random.choice(shift_ranges)\n            scale_ranges = np.arange(1 - scale, 1 + scale + 0.1, 0.1)\n            size *= random.choice(scale_ranges)\n            results['affine_aug'] = True\n        else:\n            results['affine_aug'] = False\n\n        results['center'] = center\n        results['size'] = size\n\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f'(shift_scale={self.shift_scale}, '\n        repr_str += f'aug_prob={self.aug_prob}) '\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass ToEgo(object):\n    def __init__(self, ego_cam='CAM_FRONT',):\n        self.ego_cam=ego_cam\n\n    def __call__(self, results):\n        lidar2lidarego = np.eye(4, dtype=np.float32)\n        lidar2lidarego[:3, :3] = Quaternion(\n            results['curr']['lidar2ego_rotation']).rotation_matrix\n        lidar2lidarego[:3, 3] = results['curr']['lidar2ego_translation']\n\n        lidarego2global = np.eye(4, dtype=np.float32)\n        lidarego2global[:3, :3] = Quaternion(\n            results['curr']['ego2global_rotation']).rotation_matrix\n        lidarego2global[:3, 3] = results['curr']['ego2global_translation']\n\n        camego2global = np.eye(4, dtype=np.float32)\n        camego2global[:3, :3] = Quaternion(\n            results['curr']['cams'][self.ego_cam]\n            ['ego2global_rotation']).rotation_matrix\n        camego2global[:3, 3] = results['curr']['cams'][self.ego_cam][\n            'ego2global_translation']\n        lidar2camego = np.linalg.inv(camego2global) @ lidarego2global @ lidar2lidarego\n\n        points = results['points'].tensor.numpy()\n        points_ego = lidar2camego[:3,:3].reshape(1, 3, 3) @ \\\n                     points[:, :3].reshape(-1, 3, 1) + \\\n                     lidar2camego[:3, 3].reshape(1, 3, 1)\n        points[:, :3] = points_ego.squeeze(-1)\n        points = results['points'].new_point(points)\n        results['points'] = points\n        return results\n\n\n@PIPELINES.register_module()\nclass VelocityAug(object):\n    def __init__(self, rate=0.5, rate_vy=0.2, rate_rotation=-1, speed_range=None, thred_vy_by_vx=1.0,\n                 ego_cam='CAM_FRONT'):\n        # must be identical to that in tools/create_data_bevdet.py\n        self.cls = ['car', 'truck', 'construction_vehicle',\n                    'bus', 'trailer', 'barrier',\n                    'motorcycle', 'bicycle',\n                    'pedestrian', 'traffic_cone']\n        self.speed_range = dict(\n            car=[-10, 30, 6],\n            truck=[-10, 30, 6],\n            construction_vehicle=[-10, 30, 3],\n            bus=[-10, 30, 3],\n            trailer=[-10, 30, 3],\n            barrier=[-5, 5, 3],\n            motorcycle=[-2, 25, 3],\n            bicycle=[-2, 15, 2],\n            pedestrian=[-1, 10, 2]\n        ) if speed_range is None else speed_range\n        self.rate = rate\n        self.thred_vy_by_vx=thred_vy_by_vx\n        self.rate_vy = rate_vy\n        self.rate_rotation = rate_rotation\n        self.ego_cam = ego_cam\n\n    def interpolating(self, vx, vy, delta_t, box, rot):\n        delta_t_max = np.max(delta_t)\n        if vy ==0 or vx == 0:\n            delta_x = delta_t*vx\n            delta_y = np.zeros_like(delta_x)\n            rotation_interpolated = np.zeros_like(delta_x)\n        else:\n            theta = np.arctan2(abs(vy), abs(vx))\n            rotation = 2 * theta\n            radius = 0.5 * delta_t_max * np.sqrt(vx ** 2 + vy ** 2) / np.sin(theta)\n            rotation_interpolated = delta_t / delta_t_max * rotation\n            delta_y = radius - radius * np.cos(rotation_interpolated)\n            delta_x = radius * np.sin(rotation_interpolated)\n            if vy<0:\n                delta_y = - delta_y\n            if vx<0:\n                delta_x = - delta_x\n            if np.logical_xor(vx>0, vy>0):\n                rotation_interpolated = -rotation_interpolated\n        aug = np.zeros((delta_t.shape[0],3,3), dtype=np.float32)\n        aug[:, 2, 2] = 1.\n        sin = np.sin(-rotation_interpolated)\n        cos = np.cos(-rotation_interpolated)\n        aug[:,:2,:2] = np.stack([cos,sin,-sin,cos], axis=-1).reshape(delta_t.shape[0], 2, 2)\n        aug[:,:2, 2] = np.stack([delta_x, delta_y], axis=-1)\n\n        corner2center = np.eye(3)\n        corner2center[0, 2] = -0.5 * box[3]\n\n        instance2ego = np.eye(3)\n        yaw = -box[6]\n        s = np.sin(yaw)\n        c = np.cos(yaw)\n        instance2ego[:2,:2] = np.stack([c,s,-s,c]).reshape(2,2)\n        instance2ego[:2,2] = box[:2]\n        corner2ego = instance2ego @ corner2center\n        corner2ego = corner2ego[None, ...]\n        if not rot == 0:\n            t_rot = np.eye(3)\n            s_rot = np.sin(-rot)\n            c_rot = np.cos(-rot)\n            t_rot[:2,:2] = np.stack([c_rot, s_rot, -s_rot, c_rot]).reshape(2,2)\n\n            instance2ego_ = np.eye(3)\n            yaw_ = -box[6] - rot\n            s_ = np.sin(yaw_)\n            c_ = np.cos(yaw_)\n            instance2ego_[:2, :2] = np.stack([c_, s_, -s_, c_]).reshape(2, 2)\n            instance2ego_[:2, 2] = box[:2]\n            corner2ego_ = instance2ego_ @ corner2center\n            corner2ego_ = corner2ego_[None, ...]\n            t_rot = instance2ego @ t_rot @ np.linalg.inv(instance2ego)\n            aug = corner2ego_ @ aug @ np.linalg.inv(corner2ego_) @ t_rot[None, ...]\n        else:\n            aug = corner2ego @ aug @ np.linalg.inv(corner2ego)\n        return aug\n\n    def __call__(self, results):\n        gt_boxes = results['gt_bboxes_3d'].tensor.numpy().copy()\n        gt_velocity = gt_boxes[:,7:]\n        gt_velocity_norm = np.sum(np.square(gt_velocity), axis=1)\n        points = results['points'].tensor.numpy().copy()\n        point_indices = box_np_ops.points_in_rbbox(points, gt_boxes)\n\n        for bid in range(gt_boxes.shape[0]):\n            cls = self.cls[results['gt_labels_3d'][bid]]\n            points_all = points[point_indices[:, bid]]\n            delta_t = np.unique(points_all[:,4])\n            aug_rate_cls = self.rate if isinstance(self.rate, float) else self.rate[cls]\n            if points_all.shape[0]==0 or \\\n                    delta_t.shape[0]<3 or \\\n                    gt_velocity_norm[bid]>0.01 or \\\n                    cls not in self.speed_range or \\\n                    np.random.rand() > aug_rate_cls:\n                continue\n\n            # sampling speed vx,vy in instance coordinate\n            vx = np.random.rand() * (self.speed_range[cls][1] -\n                                     self.speed_range[cls][0]) + \\\n                 self.speed_range[cls][0]\n            if np.random.rand() < self.rate_vy:\n                max_vy = min(self.speed_range[cls][2]*2, abs(vx) * self.thred_vy_by_vx)\n                vy = (np.random.rand()-0.5) * max_vy\n            else:\n                vy = 0.0\n            vx = -vx\n\n            # if points_all.shape[0] == 0 or cls not in self.speed_range or gt_velocity_norm[bid]>0.01 or delta_t.shape[0]<3:\n            #     continue\n            # vx = 10\n            # vy = -2.\n\n            rot = 0.0\n            if np.random.rand() < self.rate_rotation:\n                rot = (np.random.rand()-0.5) * 1.57\n\n            aug = self.interpolating(vx, vy, delta_t, gt_boxes[bid], rot)\n\n            # update rotation\n            gt_boxes[bid, 6] += rot\n\n            # update velocity\n            delta_t_max = np.max(delta_t)\n            delta_t_max_index = np.argmax(delta_t)\n            center = gt_boxes[bid:bid+1, :2]\n            center_aug = center @ aug[delta_t_max_index, :2, :2].T + aug[delta_t_max_index, :2, 2]\n            vel = (center - center_aug) / delta_t_max\n            gt_boxes[bid, 7:] = vel\n\n            # update points\n            for fid in range(delta_t.shape[0]):\n                points_curr_frame_idxes = points_all[:,4] == delta_t[fid]\n\n                points_all[points_curr_frame_idxes, :2] = \\\n                    points_all[points_curr_frame_idxes, :2]  @ aug[fid,:2,:2].T + aug[fid,:2, 2:3].T\n            points[point_indices[:, bid]] = points_all\n\n\n        results['points'] = results['points'].new_point(points)\n        results['gt_bboxes_3d'] = results['gt_bboxes_3d'].new_box(gt_boxes)\n        return results\n\n    def adjust_adj_points(self, adj_points, point_indices_adj, bid, vx, vy, rot, gt_boxes_adj, info_adj, info):\n        ts_diff = info['timestamp'] / 1e6 - info_adj['timestamp'] / 1e6\n        points = adj_points.tensor.numpy().copy()\n        points_all_adj = points[point_indices_adj[:, bid]]\n        if points_all_adj.size>0:\n            delta_t_adj = np.unique(points_all_adj[:, 4]) + ts_diff\n            aug = self.interpolating(vx, vy, delta_t_adj, gt_boxes_adj[bid], rot)\n            for fid in range(delta_t_adj.shape[0]):\n                points_curr_frame_idxes = points_all_adj[:, 4] == delta_t_adj[fid]- ts_diff\n                points_all_adj[points_curr_frame_idxes, :2] = \\\n                    points_all_adj[points_curr_frame_idxes, :2] @ aug[fid, :2, :2].T + aug[fid, :2, 2:3].T\n            points[point_indices_adj[:, bid]] = points_all_adj\n        adj_points = adj_points.new_point(points)\n        return adj_points"
        }
    ]
}