{
    "sourceFile": "mmdet3d/datasets/pipelines/loading.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 40,
            "patches": [
                {
                    "date": 1716014482615,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1716018725700,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -366,8 +366,168 @@\n         return repr_str\n \n \n @PIPELINES.register_module()\n+class LoadRadarPointsMultiSweeps(object):\n+    \"\"\"Load radar points from multiple sweeps.\n+    This is usually used for nuScenes dataset to utilize previous sweeps.\n+    Args:\n+        sweeps_num (int): Number of sweeps. Defaults to 10.\n+        load_dim (int): Dimension number of the loaded points. Defaults to 5.\n+        use_dim (list[int]): Which dimension to use. Defaults to [0, 1, 2, 4].\n+        file_client_args (dict): Config dict of file clients, refer to\n+            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n+            for more details. Defaults to dict(backend='disk').\n+        pad_empty_sweeps (bool): Whether to repeat keyframe when\n+            sweeps is empty. Defaults to False.\n+        remove_close (bool): Whether to remove close points.\n+            Defaults to False.\n+        test_mode (bool): If test_model=True used for testing, it will not\n+            randomly sample sweeps but select the nearest N frames.\n+            Defaults to False.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        load_dim=18,\n+        use_dim=[0, 1, 2, 3, 4],\n+        sweeps_num=3,\n+        file_client_args=dict(backend=\"disk\"),\n+        max_num=300,\n+        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],\n+        test_mode=False,\n+    ):\n+        self.load_dim = load_dim\n+        self.use_dim = use_dim\n+        self.sweeps_num = sweeps_num\n+        self.file_client_args = file_client_args.copy()\n+        self.file_client = None\n+        self.max_num = max_num\n+        self.test_mode = test_mode\n+        self.pc_range = pc_range\n+\n+    def _load_points(self, pts_filename):\n+        \"\"\"Private function to load point clouds data.\n+        Args:\n+            pts_filename (str): Filename of point clouds data.\n+        Returns:\n+            np.ndarray: An array containing point clouds data.\n+            [N, 18]\n+        \"\"\"\n+        radar_obj = RadarPointCloud.from_file(pts_filename)\n+\n+        # [18, N]\n+        points = radar_obj.points\n+\n+        return points.transpose().astype(np.float32)\n+\n+    def _pad_or_drop(self, points):\n+        \"\"\"\n+        points: [N, 18]\n+        \"\"\"\n+\n+        num_points = points.shape[0]\n+\n+        if num_points == self.max_num:\n+            masks = np.ones((num_points, 1), dtype=points.dtype)\n+\n+            return points, masks\n+\n+        if num_points > self.max_num:\n+            points = np.random.permutation(points)[: self.max_num, :]\n+            masks = np.ones((self.max_num, 1), dtype=points.dtype)\n+\n+            return points, masks\n+\n+        if num_points < self.max_num:\n+            zeros = np.zeros(\n+                (self.max_num - num_points, points.shape[1]), dtype=points.dtype\n+            )\n+            masks = np.ones((num_points, 1), dtype=points.dtype)\n+\n+            points = np.concatenate((points, zeros), axis=0)\n+            masks = np.concatenate((masks, zeros.copy()[:, [0]]), axis=0)\n+\n+            return points, masks\n+\n+    def __call__(self, results):\n+        \"\"\"Call function to load multi-sweep point clouds from files.\n+        Args:\n+            results (dict): Result dict containing multi-sweep point cloud \\\n+                filenames.\n+        Returns:\n+            dict: The result dict containing the multi-sweep points data. \\\n+                Added key and value are described below.\n+                - points (np.ndarray | :obj:`BasePoints`): Multi-sweep point \\\n+                    cloud arrays.\n+        \"\"\"\n+        radars_dict = results[\"radar\"]\n+\n+        points_sweep_list = []\n+        for key, sweeps in radars_dict.items():\n+            if len(sweeps) < self.sweeps_num:\n+                idxes = list(range(len(sweeps)))\n+            else:\n+                idxes = list(range(self.sweeps_num))\n+\n+            ts = sweeps[0][\"timestamp\"] * 1e-6\n+            for idx in idxes:\n+                sweep = sweeps[idx]\n+\n+                points_sweep = self._load_points(sweep[\"data_path\"])\n+                points_sweep = np.copy(points_sweep).reshape(-1, self.load_dim)\n+\n+                timestamp = sweep[\"timestamp\"] * 1e-6\n+                time_diff = ts - timestamp\n+                time_diff = np.ones((points_sweep.shape[0], 1)) * time_diff\n+\n+                # velocity compensated by the ego motion in sensor frame\n+                velo_comp = points_sweep[:, 8:10]\n+                velo_comp = np.concatenate(\n+                    (velo_comp, np.zeros((velo_comp.shape[0], 1))), 1\n+                )\n+                velo_comp = velo_comp @ sweep[\"sensor2lidar_rotation\"].T\n+                velo_comp = velo_comp[:, :2]\n+\n+                # velocity in sensor frame\n+                velo = points_sweep[:, 6:8]\n+                velo = np.concatenate((velo, np.zeros((velo.shape[0], 1))), 1)\n+                velo = velo @ sweep[\"sensor2lidar_rotation\"].T\n+                velo = velo[:, :2]\n+\n+                points_sweep[:, :3] = (\n+                    points_sweep[:, :3] @ sweep[\"sensor2lidar_rotation\"].T\n+                )\n+                points_sweep[:, :3] += sweep[\"sensor2lidar_translation\"]\n+\n+                points_sweep_ = np.concatenate(\n+                    [\n+                        points_sweep[:, :6],\n+                        velo,\n+                        velo_comp,\n+                        points_sweep[:, 10:],\n+                        time_diff,\n+                    ],\n+                    axis=1,\n+                )\n+                points_sweep_list.append(points_sweep_)\n+\n+        points = np.concatenate(points_sweep_list, axis=0)\n+\n+        points = points[:, self.use_dim]\n+\n+        points = RadarPoints(points, points_dim=points.shape[-1], attribute_dims=None)\n+\n+        results[\"radar\"] = points\n+        return results\n+\n+    def __repr__(self):\n+        \"\"\"str: Return a string that describes the module.\"\"\"\n+        return f\"{self.__class__.__name__}(sweeps_num={self.sweeps_num})\"\n+\n+\n+\n+@PIPELINES.register_module()\n class LoadPointsFromFile(object):\n     \"\"\"Load Points From File.\n \n     Load points from file.\n"
                },
                {
                    "date": 1716018763140,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,9 +7,9 @@\n import torch\n from PIL import Image\n from pyquaternion import Quaternion\n \n-from mmdet3d.core.points import BasePoints, get_points_type\n+from mmdet3d.core.points import BasePoints, get_points_type,RadarPoints\n from mmdet.datasets.pipelines import LoadAnnotations, LoadImageFromFile\n from ...core.bbox import LiDARInstance3DBoxes\n from ..builder import PIPELINES\n \n"
                },
                {
                    "date": 1716018797975,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,28 +7,28 @@\n import torch\n from PIL import Image\n from pyquaternion import Quaternion\n \n-from mmdet3d.core.points import BasePoints, get_points_type,RadarPoints\n+from mmdet3d.core.points import BasePoints, get_points_type, RadarPoints\n from mmdet.datasets.pipelines import LoadAnnotations, LoadImageFromFile\n from ...core.bbox import LiDARInstance3DBoxes\n from ..builder import PIPELINES\n \n \n @PIPELINES.register_module()\n class LoadOccGTFromFile(object):\n     def __call__(self, results):\n-        occ_gt_path = results['occ_gt_path']\n+        occ_gt_path = results[\"occ_gt_path\"]\n         occ_gt_path = os.path.join(occ_gt_path, \"labels.npz\")\n \n         occ_labels = np.load(occ_gt_path)\n-        semantics = occ_labels['semantics']\n-        mask_lidar = occ_labels['mask_lidar']\n-        mask_camera = occ_labels['mask_camera']\n+        semantics = occ_labels[\"semantics\"]\n+        mask_lidar = occ_labels[\"mask_lidar\"]\n+        mask_camera = occ_labels[\"mask_camera\"]\n \n-        results['voxel_semantics'] = semantics\n-        results['mask_lidar'] = mask_lidar\n-        results['mask_camera'] = mask_camera\n+        results[\"voxel_semantics\"] = semantics\n+        results[\"mask_lidar\"] = mask_lidar\n+        results[\"mask_camera\"] = mask_camera\n         return results\n \n \n @PIPELINES.register_module()\n@@ -43,9 +43,9 @@\n         color_type (str, optional): Color type of the file.\n             Defaults to 'unchanged'.\n     \"\"\"\n \n-    def __init__(self, to_float32=False, color_type='unchanged'):\n+    def __init__(self, to_float32=False, color_type=\"unchanged\"):\n         self.to_float32 = to_float32\n         self.color_type = color_type\n \n     def __call__(self, results):\n@@ -65,34 +65,36 @@\n                 - pad_shape (tuple[int]): Shape of padded image arrays.\n                 - scale_factor (float): Scale factor.\n                 - img_norm_cfg (dict): Normalization configuration of images.\n         \"\"\"\n-        filename = results['img_filename']\n+        filename = results[\"img_filename\"]\n         # img is of shape (h, w, c, num_views)\n         img = np.stack(\n-            [mmcv.imread(name, self.color_type) for name in filename], axis=-1)\n+            [mmcv.imread(name, self.color_type) for name in filename], axis=-1\n+        )\n         if self.to_float32:\n             img = img.astype(np.float32)\n-        results['filename'] = filename\n+        results[\"filename\"] = filename\n         # unravel to list, see `DefaultFormatBundle` in formatting.py\n         # which will transpose each image separately and then stack into array\n-        results['img'] = [img[..., i] for i in range(img.shape[-1])]\n-        results['img_shape'] = img.shape\n-        results['ori_shape'] = img.shape\n+        results[\"img\"] = [img[..., i] for i in range(img.shape[-1])]\n+        results[\"img_shape\"] = img.shape\n+        results[\"ori_shape\"] = img.shape\n         # Set initial values for default meta_keys\n-        results['pad_shape'] = img.shape\n-        results['scale_factor'] = 1.0\n+        results[\"pad_shape\"] = img.shape\n+        results[\"scale_factor\"] = 1.0\n         num_channels = 1 if len(img.shape) < 3 else img.shape[2]\n-        results['img_norm_cfg'] = dict(\n+        results[\"img_norm_cfg\"] = dict(\n             mean=np.zeros(num_channels, dtype=np.float32),\n             std=np.ones(num_channels, dtype=np.float32),\n-            to_rgb=False)\n+            to_rgb=False,\n+        )\n         return results\n \n     def __repr__(self):\n         \"\"\"str: Return a string that describes the module.\"\"\"\n         repr_str = self.__class__.__name__\n-        repr_str += f'(to_float32={self.to_float32}, '\n+        repr_str += f\"(to_float32={self.to_float32}, \"\n         repr_str += f\"color_type='{self.color_type}')\"\n         return repr_str\n \n \n@@ -115,9 +117,9 @@\n         Returns:\n             dict: The dict contains loaded image and meta information.\n         \"\"\"\n         super().__call__(results)\n-        results['cam2img'] = results['img_info']['cam_intrinsic']\n+        results[\"cam2img\"] = results[\"img_info\"][\"cam_intrinsic\"]\n         return results\n \n \n @PIPELINES.register_module()\n@@ -146,30 +148,34 @@\n             randomly sample sweeps but select the nearest N frames.\n             Defaults to False.\n     \"\"\"\n \n-    def __init__(self,\n-                 sweeps_num=10,\n-                 load_dim=5,\n-                 use_dim=[0, 1, 2, 4],\n-                 time_dim=4,\n-                 file_client_args=dict(backend='disk'),\n-                 pad_empty_sweeps=False,\n-                 remove_close=False,\n-                 test_mode=False):\n+    def __init__(\n+        self,\n+        sweeps_num=10,\n+        load_dim=5,\n+        use_dim=[0, 1, 2, 4],\n+        time_dim=4,\n+        file_client_args=dict(backend=\"disk\"),\n+        pad_empty_sweeps=False,\n+        remove_close=False,\n+        test_mode=False,\n+    ):\n         self.load_dim = load_dim\n         self.sweeps_num = sweeps_num\n         self.use_dim = use_dim\n         self.time_dim = time_dim\n-        assert time_dim < load_dim, \\\n-            f'Expect the timestamp dimension < {load_dim}, got {time_dim}'\n+        assert (\n+            time_dim < load_dim\n+        ), f\"Expect the timestamp dimension < {load_dim}, got {time_dim}\"\n         self.file_client_args = file_client_args.copy()\n         self.file_client = None\n         self.pad_empty_sweeps = pad_empty_sweeps\n         self.remove_close = remove_close\n         self.test_mode = test_mode\n-        assert max(use_dim) < load_dim, \\\n-            f'Expect all used dimensions < {load_dim}, got {use_dim}'\n+        assert (\n+            max(use_dim) < load_dim\n+        ), f\"Expect all used dimensions < {load_dim}, got {use_dim}\"\n \n     def _load_points(self, pts_filename):\n         \"\"\"Private function to load point clouds data.\n \n@@ -185,9 +191,9 @@\n             pts_bytes = self.file_client.get(pts_filename)\n             points = np.frombuffer(pts_bytes, dtype=np.float32)\n         except ConnectionError:\n             mmcv.check_file_exist(pts_filename)\n-            if pts_filename.endswith('.npy'):\n+            if pts_filename.endswith(\".npy\"):\n                 points = np.load(pts_filename)\n             else:\n                 points = np.fromfile(pts_filename, dtype=np.float32)\n         return points\n@@ -227,48 +233,50 @@\n \n                 - points (np.ndarray | :obj:`BasePoints`): Multi-sweep point\n                     cloud arrays.\n         \"\"\"\n-        points = results['points']\n+        points = results[\"points\"]\n         points.tensor[:, self.time_dim] = 0\n         sweep_points_list = [points]\n-        ts = results['timestamp']\n-        if self.pad_empty_sweeps and len(results['sweeps']) == 0:\n+        ts = results[\"timestamp\"]\n+        if self.pad_empty_sweeps and len(results[\"sweeps\"]) == 0:\n             for i in range(self.sweeps_num):\n                 if self.remove_close:\n                     sweep_points_list.append(self._remove_close(points))\n                 else:\n                     sweep_points_list.append(points)\n         else:\n-            if len(results['sweeps']) <= self.sweeps_num:\n-                choices = np.arange(len(results['sweeps']))\n+            if len(results[\"sweeps\"]) <= self.sweeps_num:\n+                choices = np.arange(len(results[\"sweeps\"]))\n             elif self.test_mode:\n                 choices = np.arange(self.sweeps_num)\n             else:\n                 choices = np.random.choice(\n-                    len(results['sweeps']), self.sweeps_num, replace=False)\n+                    len(results[\"sweeps\"]), self.sweeps_num, replace=False\n+                )\n             for idx in choices:\n-                sweep = results['sweeps'][idx]\n-                points_sweep = self._load_points(sweep['data_path'])\n+                sweep = results[\"sweeps\"][idx]\n+                points_sweep = self._load_points(sweep[\"data_path\"])\n                 points_sweep = np.copy(points_sweep).reshape(-1, self.load_dim)\n                 if self.remove_close:\n                     points_sweep = self._remove_close(points_sweep)\n-                sweep_ts = sweep['timestamp'] / 1e6\n-                points_sweep[:, :3] = points_sweep[:, :3] @ sweep[\n-                    'sensor2lidar_rotation'].T\n-                points_sweep[:, :3] += sweep['sensor2lidar_translation']\n+                sweep_ts = sweep[\"timestamp\"] / 1e6\n+                points_sweep[:, :3] = (\n+                    points_sweep[:, :3] @ sweep[\"sensor2lidar_rotation\"].T\n+                )\n+                points_sweep[:, :3] += sweep[\"sensor2lidar_translation\"]\n                 points_sweep[:, self.time_dim] = ts - sweep_ts\n                 points_sweep = points.new_point(points_sweep)\n                 sweep_points_list.append(points_sweep)\n \n         points = points.cat(sweep_points_list)\n         points = points[:, self.use_dim]\n-        results['points'] = points\n+        results[\"points\"] = points\n         return results\n \n     def __repr__(self):\n         \"\"\"str: Return a string that describes the module.\"\"\"\n-        return f'{self.__class__.__name__}(sweeps_num={self.sweeps_num})'\n+        return f\"{self.__class__.__name__}(sweeps_num={self.sweeps_num})\"\n \n \n @PIPELINES.register_module()\n class PointSegClassMapping(object):\n@@ -283,18 +291,18 @@\n             segmentation mask. Defaults to 40.\n     \"\"\"\n \n     def __init__(self, valid_cat_ids, max_cat_id=40):\n-        assert max_cat_id >= np.max(valid_cat_ids), \\\n-            'max_cat_id should be greater than maximum id in valid_cat_ids'\n+        assert max_cat_id >= np.max(\n+            valid_cat_ids\n+        ), \"max_cat_id should be greater than maximum id in valid_cat_ids\"\n \n         self.valid_cat_ids = valid_cat_ids\n         self.max_cat_id = int(max_cat_id)\n \n         # build cat_id to class index mapping\n         neg_cls = len(valid_cat_ids)\n-        self.cat_id2class = np.ones(\n-            self.max_cat_id + 1, dtype=np.int) * neg_cls\n+        self.cat_id2class = np.ones(self.max_cat_id + 1, dtype=np.int) * neg_cls\n         for cls_idx, cat_id in enumerate(valid_cat_ids):\n             self.cat_id2class[cat_id] = cls_idx\n \n     def __call__(self, results):\n@@ -308,21 +316,21 @@\n                 Updated key and value are described below.\n \n                 - pts_semantic_mask (np.ndarray): Mapped semantic masks.\n         \"\"\"\n-        assert 'pts_semantic_mask' in results\n-        pts_semantic_mask = results['pts_semantic_mask']\n+        assert \"pts_semantic_mask\" in results\n+        pts_semantic_mask = results[\"pts_semantic_mask\"]\n \n         converted_pts_sem_mask = self.cat_id2class[pts_semantic_mask]\n \n-        results['pts_semantic_mask'] = converted_pts_sem_mask\n+        results[\"pts_semantic_mask\"] = converted_pts_sem_mask\n         return results\n \n     def __repr__(self):\n         \"\"\"str: Return a string that describes the module.\"\"\"\n         repr_str = self.__class__.__name__\n-        repr_str += f'(valid_cat_ids={self.valid_cat_ids}, '\n-        repr_str += f'max_cat_id={self.max_cat_id})'\n+        repr_str += f\"(valid_cat_ids={self.valid_cat_ids}, \"\n+        repr_str += f\"max_cat_id={self.max_cat_id})\"\n         return repr_str\n \n \n @PIPELINES.register_module()\n@@ -347,23 +355,23 @@\n                 Updated key and value are described below.\n \n                 - points (:obj:`BasePoints`): Points after color normalization.\n         \"\"\"\n-        points = results['points']\n-        assert points.attribute_dims is not None and \\\n-            'color' in points.attribute_dims.keys(), \\\n-            'Expect points have color attribute'\n+        points = results[\"points\"]\n+        assert (\n+            points.attribute_dims is not None\n+            and \"color\" in points.attribute_dims.keys()\n+        ), \"Expect points have color attribute\"\n         if self.color_mean is not None:\n-            points.color = points.color - \\\n-                points.color.new_tensor(self.color_mean)\n+            points.color = points.color - points.color.new_tensor(self.color_mean)\n         points.color = points.color / 255.0\n-        results['points'] = points\n+        results[\"points\"] = points\n         return results\n \n     def __repr__(self):\n         \"\"\"str: Return a string that describes the module.\"\"\"\n         repr_str = self.__class__.__name__\n-        repr_str += f'(color_mean={self.color_mean})'\n+        repr_str += f\"(color_mean={self.color_mean})\"\n         return repr_str\n \n \n @PIPELINES.register_module()\n@@ -524,9 +532,8 @@\n         \"\"\"str: Return a string that describes the module.\"\"\"\n         return f\"{self.__class__.__name__}(sweeps_num={self.sweeps_num})\"\n \n \n-\n @PIPELINES.register_module()\n class LoadPointsFromFile(object):\n     \"\"\"Load Points From File.\n \n@@ -552,22 +559,25 @@\n             https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n             for more details. Defaults to dict(backend='disk').\n     \"\"\"\n \n-    def __init__(self,\n-                 coord_type,\n-                 load_dim=6,\n-                 use_dim=[0, 1, 2],\n-                 shift_height=False,\n-                 use_color=False,\n-                 file_client_args=dict(backend='disk')):\n+    def __init__(\n+        self,\n+        coord_type,\n+        load_dim=6,\n+        use_dim=[0, 1, 2],\n+        shift_height=False,\n+        use_color=False,\n+        file_client_args=dict(backend=\"disk\"),\n+    ):\n         self.shift_height = shift_height\n         self.use_color = use_color\n         if isinstance(use_dim, int):\n             use_dim = list(range(use_dim))\n-        assert max(use_dim) < load_dim, \\\n-            f'Expect all used dimensions < {load_dim}, got {use_dim}'\n-        assert coord_type in ['CAMERA', 'LIDAR', 'DEPTH']\n+        assert (\n+            max(use_dim) < load_dim\n+        ), f\"Expect all used dimensions < {load_dim}, got {use_dim}\"\n+        assert coord_type in [\"CAMERA\", \"LIDAR\", \"DEPTH\"]\n \n         self.coord_type = coord_type\n         self.load_dim = load_dim\n         self.use_dim = use_dim\n@@ -589,9 +599,9 @@\n             pts_bytes = self.file_client.get(pts_filename)\n             points = np.frombuffer(pts_bytes, dtype=np.float32)\n         except ConnectionError:\n             mmcv.check_file_exist(pts_filename)\n-            if pts_filename.endswith('.npy'):\n+            if pts_filename.endswith(\".npy\"):\n                 points = np.load(pts_filename)\n             else:\n                 points = np.fromfile(pts_filename, dtype=np.float32)\n \n@@ -608,9 +618,9 @@\n                 Added key and value are described below.\n \n                 - points (:obj:`BasePoints`): Point clouds data.\n         \"\"\"\n-        pts_filename = results['pts_filename']\n+        pts_filename = results[\"pts_filename\"]\n         points = self._load_points(pts_filename)\n         points = points.reshape(-1, self.load_dim)\n         points = points[:, self.use_dim]\n         attribute_dims = None\n@@ -618,47 +628,51 @@\n         if self.shift_height:\n             floor_height = np.percentile(points[:, 2], 0.99)\n             height = points[:, 2] - floor_height\n             points = np.concatenate(\n-                [points[:, :3],\n-                 np.expand_dims(height, 1), points[:, 3:]], 1)\n+                [points[:, :3], np.expand_dims(height, 1), points[:, 3:]], 1\n+            )\n             attribute_dims = dict(height=3)\n \n         if self.use_color:\n             assert len(self.use_dim) >= 6\n             if attribute_dims is None:\n                 attribute_dims = dict()\n             attribute_dims.update(\n-                dict(color=[\n-                    points.shape[1] - 3,\n-                    points.shape[1] - 2,\n-                    points.shape[1] - 1,\n-                ]))\n+                dict(\n+                    color=[\n+                        points.shape[1] - 3,\n+                        points.shape[1] - 2,\n+                        points.shape[1] - 1,\n+                    ]\n+                )\n+            )\n \n         points_class = get_points_type(self.coord_type)\n         points = points_class(\n-            points, points_dim=points.shape[-1], attribute_dims=attribute_dims)\n-        results['points'] = points\n+            points, points_dim=points.shape[-1], attribute_dims=attribute_dims\n+        )\n+        results[\"points\"] = points\n \n         return results\n \n     def __repr__(self):\n         \"\"\"str: Return a string that describes the module.\"\"\"\n-        repr_str = self.__class__.__name__ + '('\n-        repr_str += f'shift_height={self.shift_height}, '\n-        repr_str += f'use_color={self.use_color}, '\n-        repr_str += f'file_client_args={self.file_client_args}, '\n-        repr_str += f'load_dim={self.load_dim}, '\n-        repr_str += f'use_dim={self.use_dim})'\n+        repr_str = self.__class__.__name__ + \"(\"\n+        repr_str += f\"shift_height={self.shift_height}, \"\n+        repr_str += f\"use_color={self.use_color}, \"\n+        repr_str += f\"file_client_args={self.file_client_args}, \"\n+        repr_str += f\"load_dim={self.load_dim}, \"\n+        repr_str += f\"use_dim={self.use_dim})\"\n         return repr_str\n \n \n @PIPELINES.register_module()\n class LoadPointsFromDict(LoadPointsFromFile):\n     \"\"\"Load Points From Dict.\"\"\"\n \n     def __call__(self, results):\n-        assert 'points' in results\n+        assert \"points\" in results\n         return results\n \n \n @PIPELINES.register_module()\n@@ -697,29 +711,32 @@\n             https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n             for more details.\n     \"\"\"\n \n-    def __init__(self,\n-                 with_bbox_3d=True,\n-                 with_label_3d=True,\n-                 with_attr_label=False,\n-                 with_mask_3d=False,\n-                 with_seg_3d=False,\n-                 with_bbox=False,\n-                 with_label=False,\n-                 with_mask=False,\n-                 with_seg=False,\n-                 with_bbox_depth=False,\n-                 poly2mask=True,\n-                 seg_3d_dtype=np.int64,\n-                 file_client_args=dict(backend='disk')):\n+    def __init__(\n+        self,\n+        with_bbox_3d=True,\n+        with_label_3d=True,\n+        with_attr_label=False,\n+        with_mask_3d=False,\n+        with_seg_3d=False,\n+        with_bbox=False,\n+        with_label=False,\n+        with_mask=False,\n+        with_seg=False,\n+        with_bbox_depth=False,\n+        poly2mask=True,\n+        seg_3d_dtype=np.int64,\n+        file_client_args=dict(backend=\"disk\"),\n+    ):\n         super().__init__(\n             with_bbox,\n             with_label,\n             with_mask,\n             with_seg,\n             poly2mask,\n-            file_client_args=file_client_args)\n+            file_client_args=file_client_args,\n+        )\n         self.with_bbox_3d = with_bbox_3d\n         self.with_bbox_depth = with_bbox_depth\n         self.with_label_3d = with_label_3d\n         self.with_attr_label = with_attr_label\n@@ -735,10 +752,10 @@\n \n         Returns:\n             dict: The dict containing loaded 3D bounding box annotations.\n         \"\"\"\n-        results['gt_bboxes_3d'] = results['ann_info']['gt_bboxes_3d']\n-        results['bbox3d_fields'].append('gt_bboxes_3d')\n+        results[\"gt_bboxes_3d\"] = results[\"ann_info\"][\"gt_bboxes_3d\"]\n+        results[\"bbox3d_fields\"].append(\"gt_bboxes_3d\")\n         return results\n \n     def _load_bboxes_depth(self, results):\n         \"\"\"Private function to load 2.5D bounding box annotations.\n@@ -748,10 +765,10 @@\n \n         Returns:\n             dict: The dict containing loaded 2.5D bounding box annotations.\n         \"\"\"\n-        results['centers2d'] = results['ann_info']['centers2d']\n-        results['depths'] = results['ann_info']['depths']\n+        results[\"centers2d\"] = results[\"ann_info\"][\"centers2d\"]\n+        results[\"depths\"] = results[\"ann_info\"][\"depths\"]\n         return results\n \n     def _load_labels_3d(self, results):\n         \"\"\"Private function to load label annotations.\n@@ -761,9 +778,9 @@\n \n         Returns:\n             dict: The dict containing loaded label annotations.\n         \"\"\"\n-        results['gt_labels_3d'] = results['ann_info']['gt_labels_3d']\n+        results[\"gt_labels_3d\"] = results[\"ann_info\"][\"gt_labels_3d\"]\n         return results\n \n     def _load_attr_labels(self, results):\n         \"\"\"Private function to load label annotations.\n@@ -773,9 +790,9 @@\n \n         Returns:\n             dict: The dict containing loaded label annotations.\n         \"\"\"\n-        results['attr_labels'] = results['ann_info']['attr_labels']\n+        results[\"attr_labels\"] = results[\"ann_info\"][\"attr_labels\"]\n         return results\n \n     def _load_masks_3d(self, results):\n         \"\"\"Private function to load 3D mask annotations.\n@@ -785,22 +802,21 @@\n \n         Returns:\n             dict: The dict containing loaded 3D mask annotations.\n         \"\"\"\n-        pts_instance_mask_path = results['ann_info']['pts_instance_mask_path']\n+        pts_instance_mask_path = results[\"ann_info\"][\"pts_instance_mask_path\"]\n \n         if self.file_client is None:\n             self.file_client = mmcv.FileClient(**self.file_client_args)\n         try:\n             mask_bytes = self.file_client.get(pts_instance_mask_path)\n             pts_instance_mask = np.frombuffer(mask_bytes, dtype=np.int64)\n         except ConnectionError:\n             mmcv.check_file_exist(pts_instance_mask_path)\n-            pts_instance_mask = np.fromfile(\n-                pts_instance_mask_path, dtype=np.int64)\n+            pts_instance_mask = np.fromfile(pts_instance_mask_path, dtype=np.int64)\n \n-        results['pts_instance_mask'] = pts_instance_mask\n-        results['pts_mask_fields'].append('pts_instance_mask')\n+        results[\"pts_instance_mask\"] = pts_instance_mask\n+        results[\"pts_mask_fields\"].append(\"pts_instance_mask\")\n         return results\n \n     def _load_semantic_seg_3d(self, results):\n         \"\"\"Private function to load 3D semantic segmentation annotations.\n@@ -810,24 +826,24 @@\n \n         Returns:\n             dict: The dict containing the semantic segmentation annotations.\n         \"\"\"\n-        pts_semantic_mask_path = results['ann_info']['pts_semantic_mask_path']\n+        pts_semantic_mask_path = results[\"ann_info\"][\"pts_semantic_mask_path\"]\n \n         if self.file_client is None:\n             self.file_client = mmcv.FileClient(**self.file_client_args)\n         try:\n             mask_bytes = self.file_client.get(pts_semantic_mask_path)\n             # add .copy() to fix read-only bug\n             pts_semantic_mask = np.frombuffer(\n-                mask_bytes, dtype=self.seg_3d_dtype).copy()\n+                mask_bytes, dtype=self.seg_3d_dtype\n+            ).copy()\n         except ConnectionError:\n             mmcv.check_file_exist(pts_semantic_mask_path)\n-            pts_semantic_mask = np.fromfile(\n-                pts_semantic_mask_path, dtype=np.int64)\n+            pts_semantic_mask = np.fromfile(pts_semantic_mask_path, dtype=np.int64)\n \n-        results['pts_semantic_mask'] = pts_semantic_mask\n-        results['pts_seg_fields'].append('pts_semantic_mask')\n+        results[\"pts_semantic_mask\"] = pts_semantic_mask\n+        results[\"pts_seg_fields\"].append(\"pts_semantic_mask\")\n         return results\n \n     def __call__(self, results):\n         \"\"\"Call function to load multiple types annotations.\n@@ -860,21 +876,21 @@\n         return results\n \n     def __repr__(self):\n         \"\"\"str: Return a string that describes the module.\"\"\"\n-        indent_str = '    '\n-        repr_str = self.__class__.__name__ + '(\\n'\n-        repr_str += f'{indent_str}with_bbox_3d={self.with_bbox_3d}, '\n-        repr_str += f'{indent_str}with_label_3d={self.with_label_3d}, '\n-        repr_str += f'{indent_str}with_attr_label={self.with_attr_label}, '\n-        repr_str += f'{indent_str}with_mask_3d={self.with_mask_3d}, '\n-        repr_str += f'{indent_str}with_seg_3d={self.with_seg_3d}, '\n-        repr_str += f'{indent_str}with_bbox={self.with_bbox}, '\n-        repr_str += f'{indent_str}with_label={self.with_label}, '\n-        repr_str += f'{indent_str}with_mask={self.with_mask}, '\n-        repr_str += f'{indent_str}with_seg={self.with_seg}, '\n-        repr_str += f'{indent_str}with_bbox_depth={self.with_bbox_depth}, '\n-        repr_str += f'{indent_str}poly2mask={self.poly2mask})'\n+        indent_str = \"    \"\n+        repr_str = self.__class__.__name__ + \"(\\n\"\n+        repr_str += f\"{indent_str}with_bbox_3d={self.with_bbox_3d}, \"\n+        repr_str += f\"{indent_str}with_label_3d={self.with_label_3d}, \"\n+        repr_str += f\"{indent_str}with_attr_label={self.with_attr_label}, \"\n+        repr_str += f\"{indent_str}with_mask_3d={self.with_mask_3d}, \"\n+        repr_str += f\"{indent_str}with_seg_3d={self.with_seg_3d}, \"\n+        repr_str += f\"{indent_str}with_bbox={self.with_bbox}, \"\n+        repr_str += f\"{indent_str}with_label={self.with_label}, \"\n+        repr_str += f\"{indent_str}with_mask={self.with_mask}, \"\n+        repr_str += f\"{indent_str}with_seg={self.with_seg}, \"\n+        repr_str += f\"{indent_str}with_bbox_depth={self.with_bbox_depth}, \"\n+        repr_str += f\"{indent_str}poly2mask={self.poly2mask})\"\n         return repr_str\n \n \n @PIPELINES.register_module()\n@@ -888,126 +904,139 @@\n         height, width = height // self.downsample, width // self.downsample\n         depth_map = torch.zeros((height, width), dtype=torch.float32)\n         coor = torch.round(points[:, :2] / self.downsample)\n         depth = points[:, 2]\n-        kept1 = (coor[:, 0] >= 0) & (coor[:, 0] < width) & (\n-            coor[:, 1] >= 0) & (coor[:, 1] < height) & (\n-                depth < self.grid_config['depth'][1]) & (\n-                    depth >= self.grid_config['depth'][0])\n+        kept1 = (\n+            (coor[:, 0] >= 0)\n+            & (coor[:, 0] < width)\n+            & (coor[:, 1] >= 0)\n+            & (coor[:, 1] < height)\n+            & (depth < self.grid_config[\"depth\"][1])\n+            & (depth >= self.grid_config[\"depth\"][0])\n+        )\n         coor, depth = coor[kept1], depth[kept1]\n         ranks = coor[:, 0] + coor[:, 1] * width\n-        sort = (ranks + depth / 100.).argsort()\n+        sort = (ranks + depth / 100.0).argsort()\n         coor, depth, ranks = coor[sort], depth[sort], ranks[sort]\n \n         kept2 = torch.ones(coor.shape[0], device=coor.device, dtype=torch.bool)\n-        kept2[1:] = (ranks[1:] != ranks[:-1])\n+        kept2[1:] = ranks[1:] != ranks[:-1]\n         coor, depth = coor[kept2], depth[kept2]\n         coor = coor.to(torch.long)\n         depth_map[coor[:, 1], coor[:, 0]] = depth\n         return depth_map\n \n     def __call__(self, results):\n-        points_lidar = results['points']\n-        imgs, rots, trans, intrins = results['img_inputs'][:4]\n-        post_rots, post_trans, bda = results['img_inputs'][4:]\n+        points_lidar = results[\"points\"]\n+        imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n+        post_rots, post_trans, bda = results[\"img_inputs\"][4:]\n         depth_map_list = []\n-        for cid in range(len(results['cam_names'])):\n-            cam_name = results['cam_names'][cid]\n+        for cid in range(len(results[\"cam_names\"])):\n+            cam_name = results[\"cam_names\"][cid]\n             lidar2lidarego = np.eye(4, dtype=np.float32)\n             lidar2lidarego[:3, :3] = Quaternion(\n-                results['curr']['lidar2ego_rotation']).rotation_matrix\n-            lidar2lidarego[:3, 3] = results['curr']['lidar2ego_translation']\n+                results[\"curr\"][\"lidar2ego_rotation\"]\n+            ).rotation_matrix\n+            lidar2lidarego[:3, 3] = results[\"curr\"][\"lidar2ego_translation\"]\n             lidar2lidarego = torch.from_numpy(lidar2lidarego)\n \n             lidarego2global = np.eye(4, dtype=np.float32)\n             lidarego2global[:3, :3] = Quaternion(\n-                results['curr']['ego2global_rotation']).rotation_matrix\n-            lidarego2global[:3, 3] = results['curr']['ego2global_translation']\n+                results[\"curr\"][\"ego2global_rotation\"]\n+            ).rotation_matrix\n+            lidarego2global[:3, 3] = results[\"curr\"][\"ego2global_translation\"]\n             lidarego2global = torch.from_numpy(lidarego2global)\n \n             cam2camego = np.eye(4, dtype=np.float32)\n             cam2camego[:3, :3] = Quaternion(\n-                results['curr']['cams'][cam_name]\n-                ['sensor2ego_rotation']).rotation_matrix\n-            cam2camego[:3, 3] = results['curr']['cams'][cam_name][\n-                'sensor2ego_translation']\n+                results[\"curr\"][\"cams\"][cam_name][\"sensor2ego_rotation\"]\n+            ).rotation_matrix\n+            cam2camego[:3, 3] = results[\"curr\"][\"cams\"][cam_name][\n+                \"sensor2ego_translation\"\n+            ]\n             cam2camego = torch.from_numpy(cam2camego)\n \n             camego2global = np.eye(4, dtype=np.float32)\n             camego2global[:3, :3] = Quaternion(\n-                results['curr']['cams'][cam_name]\n-                ['ego2global_rotation']).rotation_matrix\n-            camego2global[:3, 3] = results['curr']['cams'][cam_name][\n-                'ego2global_translation']\n+                results[\"curr\"][\"cams\"][cam_name][\"ego2global_rotation\"]\n+            ).rotation_matrix\n+            camego2global[:3, 3] = results[\"curr\"][\"cams\"][cam_name][\n+                \"ego2global_translation\"\n+            ]\n             camego2global = torch.from_numpy(camego2global)\n \n             cam2img = np.eye(4, dtype=np.float32)\n             cam2img = torch.from_numpy(cam2img)\n             cam2img[:3, :3] = intrins[cid]\n \n             lidar2cam = torch.inverse(camego2global.matmul(cam2camego)).matmul(\n-                lidarego2global.matmul(lidar2lidarego))\n+                lidarego2global.matmul(lidar2lidarego)\n+            )\n             lidar2img = cam2img.matmul(lidar2cam)\n             points_img = points_lidar.tensor[:, :3].matmul(\n-                lidar2img[:3, :3].T) + lidar2img[:3, 3].unsqueeze(0)\n+                lidar2img[:3, :3].T\n+            ) + lidar2img[:3, 3].unsqueeze(0)\n             points_img = torch.cat(\n-                [points_img[:, :2] / points_img[:, 2:3], points_img[:, 2:3]],\n-                1)\n-            points_img = points_img.matmul(\n-                post_rots[cid].T) + post_trans[cid:cid + 1, :]\n-            depth_map = self.points2depthmap(points_img, imgs.shape[2],\n-                                             imgs.shape[3])\n+                [points_img[:, :2] / points_img[:, 2:3], points_img[:, 2:3]], 1\n+            )\n+            points_img = (\n+                points_img.matmul(post_rots[cid].T) + post_trans[cid : cid + 1, :]\n+            )\n+            depth_map = self.points2depthmap(points_img, imgs.shape[2], imgs.shape[3])\n             depth_map_list.append(depth_map)\n         depth_map = torch.stack(depth_map_list)\n-        results['gt_depth'] = depth_map\n+        results[\"gt_depth\"] = depth_map\n         return results\n \n \n @PIPELINES.register_module()\n class PointToMultiViewDepthFusion(PointToMultiViewDepth):\n     def __call__(self, results):\n-        points_camego_aug = results['points'].tensor[:, :3]\n+        points_camego_aug = results[\"points\"].tensor[:, :3]\n         # print(points_lidar.shape)\n-        imgs, rots, trans, intrins = results['img_inputs'][:4]\n-        post_rots, post_trans, bda = results['img_inputs'][4:]\n-        points_camego = points_camego_aug - bda[:3, 3].view(1,3)\n-        points_camego = points_camego.matmul(torch.inverse(bda[:3,:3]).T)\n+        imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n+        post_rots, post_trans, bda = results[\"img_inputs\"][4:]\n+        points_camego = points_camego_aug - bda[:3, 3].view(1, 3)\n+        points_camego = points_camego.matmul(torch.inverse(bda[:3, :3]).T)\n \n         depth_map_list = []\n-        for cid in range(len(results['cam_names'])):\n-            cam_name = results['cam_names'][cid]\n+        for cid in range(len(results[\"cam_names\"])):\n+            cam_name = results[\"cam_names\"][cid]\n \n             cam2camego = np.eye(4, dtype=np.float32)\n             cam2camego[:3, :3] = Quaternion(\n-                results['curr']['cams'][cam_name]\n-                ['sensor2ego_rotation']).rotation_matrix\n-            cam2camego[:3, 3] = results['curr']['cams'][cam_name][\n-                'sensor2ego_translation']\n+                results[\"curr\"][\"cams\"][cam_name][\"sensor2ego_rotation\"]\n+            ).rotation_matrix\n+            cam2camego[:3, 3] = results[\"curr\"][\"cams\"][cam_name][\n+                \"sensor2ego_translation\"\n+            ]\n             cam2camego = torch.from_numpy(cam2camego)\n \n             cam2img = np.eye(4, dtype=np.float32)\n             cam2img = torch.from_numpy(cam2img)\n             cam2img[:3, :3] = intrins[cid]\n \n             camego2img = cam2img.matmul(torch.inverse(cam2camego))\n \n-            points_img = points_camego.matmul(\n-                camego2img[:3, :3].T) + camego2img[:3, 3].unsqueeze(0)\n+            points_img = points_camego.matmul(camego2img[:3, :3].T) + camego2img[\n+                :3, 3\n+            ].unsqueeze(0)\n             points_img = torch.cat(\n-                [points_img[:, :2] / points_img[:, 2:3], points_img[:, 2:3]],\n-                1)\n-            points_img = points_img.matmul(\n-                post_rots[cid].T) + post_trans[cid:cid + 1, :]\n-            depth_map = self.points2depthmap(points_img, imgs.shape[2],\n-                                             imgs.shape[3])\n+                [points_img[:, :2] / points_img[:, 2:3], points_img[:, 2:3]], 1\n+            )\n+            points_img = (\n+                points_img.matmul(post_rots[cid].T) + post_trans[cid : cid + 1, :]\n+            )\n+            depth_map = self.points2depthmap(points_img, imgs.shape[2], imgs.shape[3])\n             depth_map_list.append(depth_map)\n         depth_map = torch.stack(depth_map_list)\n-        results['gt_depth'] = depth_map\n+        results[\"gt_depth\"] = depth_map\n         return results\n \n \n def mmlabNormalize(img):\n     from mmcv.image.photometric import imnormalize\n+\n     mean = np.array([123.675, 116.28, 103.53], dtype=np.float32)\n     std = np.array([58.395, 57.12, 57.375], dtype=np.float32)\n     to_rgb = True\n     img = imnormalize(np.array(img), mean, std, to_rgb)\n@@ -1040,15 +1069,18 @@\n         self.sequential = sequential\n         self.opencv_pp = opencv_pp\n \n     def get_rot(self, h):\n-        return torch.Tensor([\n-            [np.cos(h), np.sin(h)],\n-            [-np.sin(h), np.cos(h)],\n-        ])\n+        return torch.Tensor(\n+            [\n+                [np.cos(h), np.sin(h)],\n+                [-np.sin(h), np.cos(h)],\n+            ]\n+        )\n \n-    def img_transform(self, img, post_rot, post_tran, resize, resize_dims,\n-                      crop, flip, rotate):\n+    def img_transform(\n+        self, img, post_rot, post_tran, resize, resize_dims, crop, flip, rotate\n+    ):\n         # adjust image\n         if not self.opencv_pp:\n             img = self.img_transform_core(img, resize_dims, crop, flip, rotate)\n \n@@ -1068,17 +1100,16 @@\n         if self.opencv_pp:\n             img = self.img_transform_core_opencv(img, post_rot, post_tran, crop)\n         return img, post_rot, post_tran\n \n-    def img_transform_core_opencv(self, img, post_rot, post_tran,\n-                                  crop):\n+    def img_transform_core_opencv(self, img, post_rot, post_tran, crop):\n         img = np.array(img).astype(np.float32)\n-        img = cv2.warpAffine(img,\n-                             np.concatenate([post_rot,\n-                                            post_tran.reshape(2,1)],\n-                                            axis=1),\n-                             (crop[2]-crop[0], crop[3]-crop[1]),\n-                             flags=cv2.INTER_LINEAR)\n+        img = cv2.warpAffine(\n+            img,\n+            np.concatenate([post_rot, post_tran.reshape(2, 1)], axis=1),\n+            (crop[2] - crop[0], crop[3] - crop[1]),\n+            flags=cv2.INTER_LINEAR,\n+        )\n         return img\n \n     def img_transform_core(self, img, resize_dims, crop, flip, rotate):\n         # adjust image\n@@ -1089,72 +1120,69 @@\n         img = img.rotate(rotate)\n         return img\n \n     def choose_cams(self):\n-        if self.is_train and self.data_config['Ncams'] < len(\n-                self.data_config['cams']):\n+        if self.is_train and self.data_config[\"Ncams\"] < len(self.data_config[\"cams\"]):\n             cam_names = np.random.choice(\n-                self.data_config['cams'],\n-                self.data_config['Ncams'],\n-                replace=False)\n+                self.data_config[\"cams\"], self.data_config[\"Ncams\"], replace=False\n+            )\n         else:\n-            cam_names = self.data_config['cams']\n+            cam_names = self.data_config[\"cams\"]\n         return cam_names\n \n     def sample_augmentation(self, H, W, flip=None, scale=None):\n-        fH, fW = self.data_config['input_size']\n+        fH, fW = self.data_config[\"input_size\"]\n         if self.is_train:\n             resize = float(fW) / float(W)\n-            resize += np.random.uniform(*self.data_config['resize'])\n+            resize += np.random.uniform(*self.data_config[\"resize\"])\n             resize_dims = (int(W * resize), int(H * resize))\n             newW, newH = resize_dims\n-            random_crop_height = \\\n-                self.data_config.get('random_crop_height', False)\n+            random_crop_height = self.data_config.get(\"random_crop_height\", False)\n             if random_crop_height:\n-                crop_h = int(np.random.uniform(max(0.3*newH, newH-fH),\n-                                               newH-fH))\n+                crop_h = int(np.random.uniform(max(0.3 * newH, newH - fH), newH - fH))\n             else:\n-                crop_h = \\\n-                    int((1 - np.random.uniform(*self.data_config['crop_h'])) *\n-                         newH) - fH\n+                crop_h = (\n+                    int((1 - np.random.uniform(*self.data_config[\"crop_h\"])) * newH)\n+                    - fH\n+                )\n             crop_w = int(np.random.uniform(0, max(0, newW - fW)))\n             crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n-            flip = self.data_config['flip'] and np.random.choice([0, 1])\n-            rotate = np.random.uniform(*self.data_config['rot'])\n-            if self.data_config.get('vflip', False) and np.random.choice([0, 1]):\n+            flip = self.data_config[\"flip\"] and np.random.choice([0, 1])\n+            rotate = np.random.uniform(*self.data_config[\"rot\"])\n+            if self.data_config.get(\"vflip\", False) and np.random.choice([0, 1]):\n                 rotate += 180\n         else:\n             resize = float(fW) / float(W)\n             if scale is not None:\n                 resize += scale\n             else:\n-                resize += self.data_config.get('resize_test', 0.0)\n+                resize += self.data_config.get(\"resize_test\", 0.0)\n             resize_dims = (int(W * resize), int(H * resize))\n             newW, newH = resize_dims\n-            crop_h = int((1 - np.mean(self.data_config['crop_h'])) * newH) - fH\n+            crop_h = int((1 - np.mean(self.data_config[\"crop_h\"])) * newH) - fH\n             crop_w = int(max(0, newW - fW) / 2)\n             crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n             flip = False if flip is None else flip\n             rotate = 0\n         return resize, resize_dims, crop, flip, rotate\n \n     def get_sensor_transforms(self, cam_info, cam_name):\n-        w, x, y, z = cam_info['cams'][cam_name]['sensor2ego_rotation']\n+        w, x, y, z = cam_info[\"cams\"][cam_name][\"sensor2ego_rotation\"]\n         # sweep sensor to sweep ego\n-        sensor2ego_rot = torch.Tensor(\n-            Quaternion(w, x, y, z).rotation_matrix)\n+        sensor2ego_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n         sensor2ego_tran = torch.Tensor(\n-            cam_info['cams'][cam_name]['sensor2ego_translation'])\n+            cam_info[\"cams\"][cam_name][\"sensor2ego_translation\"]\n+        )\n         sensor2ego = sensor2ego_rot.new_zeros((4, 4))\n         sensor2ego[3, 3] = 1\n         sensor2ego[:3, :3] = sensor2ego_rot\n         sensor2ego[:3, -1] = sensor2ego_tran\n         # sweep ego to global\n-        w, x, y, z = cam_info['cams'][cam_name]['ego2global_rotation']\n-        ego2global_rot = torch.Tensor(\n-            Quaternion(w, x, y, z).rotation_matrix)\n+        w, x, y, z = cam_info[\"cams\"][cam_name][\"ego2global_rotation\"]\n+        ego2global_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n         ego2global_tran = torch.Tensor(\n-            cam_info['cams'][cam_name]['ego2global_translation'])\n+            cam_info[\"cams\"][cam_name][\"ego2global_translation\"]\n+        )\n         ego2global = ego2global_rot.new_zeros((4, 4))\n         ego2global[3, 3] = 1\n         ego2global[:3, :3] = ego2global_rot\n         ego2global[:3, -1] = ego2global_tran\n@@ -1166,41 +1194,41 @@\n             results (dict): Result dict from loading pipeline.\n         Returns:\n             dict: Result dict with images distorted.\n         \"\"\"\n-        if np.random.rand()>pmd.get('rate', 1.0):\n+        if np.random.rand() > pmd.get(\"rate\", 1.0):\n             return img\n \n         img = np.array(img).astype(np.float32)\n-        assert img.dtype == np.float32, \\\n-            'PhotoMetricDistortion needs the input image of dtype np.float32,' \\\n+        assert img.dtype == np.float32, (\n+            \"PhotoMetricDistortion needs the input image of dtype np.float32,\"\n             ' please set \"to_float32=True\" in \"LoadImageFromFile\" pipeline'\n+        )\n         # random brightness\n         if np.random.randint(2):\n-            delta = np.random.uniform(-pmd['brightness_delta'],\n-                                   pmd['brightness_delta'])\n+            delta = np.random.uniform(-pmd[\"brightness_delta\"], pmd[\"brightness_delta\"])\n             img += delta\n \n         # mode == 0 --> do random contrast first\n         # mode == 1 --> do random contrast last\n         mode = np.random.randint(2)\n         if mode == 1:\n             if np.random.randint(2):\n-                alpha = np.random.uniform(pmd['contrast_lower'],\n-                                       pmd['contrast_upper'])\n+                alpha = np.random.uniform(pmd[\"contrast_lower\"], pmd[\"contrast_upper\"])\n                 img *= alpha\n \n         # convert color from BGR to HSV\n         img = mmcv.bgr2hsv(img)\n \n         # random saturation\n         if np.random.randint(2):\n-            img[..., 1] *= np.random.uniform(pmd['saturation_lower'],\n-                                          pmd['saturation_upper'])\n+            img[..., 1] *= np.random.uniform(\n+                pmd[\"saturation_lower\"], pmd[\"saturation_upper\"]\n+            )\n \n         # random hue\n         if np.random.randint(2):\n-            img[..., 0] += np.random.uniform(-pmd['hue_delta'], pmd['hue_delta'])\n+            img[..., 0] += np.random.uniform(-pmd[\"hue_delta\"], pmd[\"hue_delta\"])\n             img[..., 0][img[..., 0] > 360] -= 360\n             img[..., 0][img[..., 0] < 0] += 360\n \n         # convert color from HSV to BGR\n@@ -1208,10 +1236,9 @@\n \n         # random contrast\n         if mode == 0:\n             if np.random.randint(2):\n-                alpha = np.random.uniform(pmd['contrast_lower'],\n-                                       pmd['contrast_upper'])\n+                alpha = np.random.uniform(pmd[\"contrast_lower\"], pmd[\"contrast_upper\"])\n                 img *= alpha\n \n         # randomly swap channels\n         if np.random.randint(2):\n@@ -1225,82 +1252,85 @@\n         intrins = []\n         post_rots = []\n         post_trans = []\n         cam_names = self.choose_cams()\n-        results['cam_names'] = cam_names\n+        results[\"cam_names\"] = cam_names\n         canvas = []\n         for cam_name in cam_names:\n-            cam_data = results['curr']['cams'][cam_name]\n-            filename = cam_data['data_path']\n+            cam_data = results[\"curr\"][\"cams\"][cam_name]\n+            filename = cam_data[\"data_path\"]\n             img = Image.open(filename)\n             post_rot = torch.eye(2)\n             post_tran = torch.zeros(2)\n \n-            intrin = torch.Tensor(cam_data['cam_intrinsic'])\n+            intrin = torch.Tensor(cam_data[\"cam_intrinsic\"])\n \n-            sensor2ego, ego2global = \\\n-                self.get_sensor_transforms(results['curr'], cam_name)\n+            sensor2ego, ego2global = self.get_sensor_transforms(\n+                results[\"curr\"], cam_name\n+            )\n             # image view augmentation (resize, crop, horizontal flip, rotate)\n             img_augs = self.sample_augmentation(\n-                H=img.height, W=img.width, flip=flip, scale=scale)\n+                H=img.height, W=img.width, flip=flip, scale=scale\n+            )\n             resize, resize_dims, crop, flip, rotate = img_augs\n-            img, post_rot2, post_tran2 = \\\n-                self.img_transform(img, post_rot,\n-                                   post_tran,\n-                                   resize=resize,\n-                                   resize_dims=resize_dims,\n-                                   crop=crop,\n-                                   flip=flip,\n-                                   rotate=rotate)\n+            img, post_rot2, post_tran2 = self.img_transform(\n+                img,\n+                post_rot,\n+                post_tran,\n+                resize=resize,\n+                resize_dims=resize_dims,\n+                crop=crop,\n+                flip=flip,\n+                rotate=rotate,\n+            )\n \n             # for convenience, make augmentation matrices 3x3\n             post_tran = torch.zeros(3)\n             post_rot = torch.eye(3)\n             post_tran[:2] = post_tran2\n             post_rot[:2, :2] = post_rot2\n \n-            if self.is_train and self.data_config.get('pmd', None) is not None:\n-                img = self.photo_metric_distortion(img, self.data_config['pmd'])\n+            if self.is_train and self.data_config.get(\"pmd\", None) is not None:\n+                img = self.photo_metric_distortion(img, self.data_config[\"pmd\"])\n \n             canvas.append(np.array(img))\n             imgs.append(self.normalize_img(img))\n \n             if self.sequential:\n-                assert 'adjacent' in results\n-                for adj_info in results['adjacent']:\n-                    filename_adj = adj_info['cams'][cam_name]['data_path']\n+                assert \"adjacent\" in results\n+                for adj_info in results[\"adjacent\"]:\n+                    filename_adj = adj_info[\"cams\"][cam_name][\"data_path\"]\n                     img_adjacent = Image.open(filename_adj)\n                     if self.opencv_pp:\n-                        img_adjacent = \\\n-                            self.img_transform_core_opencv(\n-                                img_adjacent,\n-                                post_rot[:2, :2],\n-                                post_tran[:2],\n-                                crop)\n+                        img_adjacent = self.img_transform_core_opencv(\n+                            img_adjacent, post_rot[:2, :2], post_tran[:2], crop\n+                        )\n                     else:\n                         img_adjacent = self.img_transform_core(\n                             img_adjacent,\n                             resize_dims=resize_dims,\n                             crop=crop,\n                             flip=flip,\n-                            rotate=rotate)\n+                            rotate=rotate,\n+                        )\n                     imgs.append(self.normalize_img(img_adjacent))\n             intrins.append(intrin)\n             sensor2egos.append(sensor2ego)\n             ego2globals.append(ego2global)\n             post_rots.append(post_rot)\n             post_trans.append(post_tran)\n \n         if self.sequential:\n-            for adj_info in results['adjacent']:\n-                post_trans.extend(post_trans[:len(cam_names)])\n-                post_rots.extend(post_rots[:len(cam_names)])\n-                intrins.extend(intrins[:len(cam_names)])\n+            for adj_info in results[\"adjacent\"]:\n+                post_trans.extend(post_trans[: len(cam_names)])\n+                post_rots.extend(post_rots[: len(cam_names)])\n+                intrins.extend(intrins[: len(cam_names)])\n \n                 # align\n                 for cam_name in cam_names:\n-                    sensor2ego, ego2global = \\\n-                        self.get_sensor_transforms(adj_info, cam_name)\n+                    sensor2ego, ego2global = self.get_sensor_transforms(\n+                        adj_info, cam_name\n+                    )\n                     sensor2egos.append(sensor2ego)\n                     ego2globals.append(ego2global)\n \n         imgs = torch.stack(imgs)\n@@ -1309,28 +1339,28 @@\n         ego2globals = torch.stack(ego2globals)\n         intrins = torch.stack(intrins)\n         post_rots = torch.stack(post_rots)\n         post_trans = torch.stack(post_trans)\n-        results['canvas'] = canvas\n+        results[\"canvas\"] = canvas\n         return (imgs, sensor2egos, ego2globals, intrins, post_rots, post_trans)\n \n     def __call__(self, results):\n-        results['img_inputs'] = self.get_inputs(results)\n+        results[\"img_inputs\"] = self.get_inputs(results)\n         return results\n \n \n @PIPELINES.register_module()\n class LoadAnnotations(object):\n \n     def __call__(self, results):\n-        gt_boxes, gt_labels = results['ann_infos']\n+        gt_boxes, gt_labels = results[\"ann_infos\"]\n         gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n         if len(gt_boxes) == 0:\n             gt_boxes = torch.zeros(0, 9)\n-        results['gt_bboxes_3d'] = \\\n-            LiDARInstance3DBoxes(gt_boxes, box_dim=gt_boxes.shape[-1],\n-                                 origin=(0.5, 0.5, 0.5))\n-        results['gt_labels_3d'] = gt_labels\n+        results[\"gt_bboxes_3d\"] = LiDARInstance3DBoxes(\n+            gt_boxes, box_dim=gt_boxes.shape[-1], origin=(0.5, 0.5, 0.5)\n+        )\n+        results[\"gt_labels_3d\"] = gt_labels\n         return results\n \n \n @PIPELINES.register_module()\n@@ -1343,13 +1373,13 @@\n \n     def sample_bda_augmentation(self):\n         \"\"\"Generate bda augmentation values based on bda_config.\"\"\"\n         if self.is_train:\n-            rotate_bda = np.random.uniform(*self.bda_aug_conf['rot_lim'])\n-            scale_bda = np.random.uniform(*self.bda_aug_conf['scale_lim'])\n-            flip_dx = np.random.uniform() < self.bda_aug_conf['flip_dx_ratio']\n-            flip_dy = np.random.uniform() < self.bda_aug_conf['flip_dy_ratio']\n-            translation_std = self.bda_aug_conf.get('tran_lim', [0.0, 0.0, 0.0])\n+            rotate_bda = np.random.uniform(*self.bda_aug_conf[\"rot_lim\"])\n+            scale_bda = np.random.uniform(*self.bda_aug_conf[\"scale_lim\"])\n+            flip_dx = np.random.uniform() < self.bda_aug_conf[\"flip_dx_ratio\"]\n+            flip_dy = np.random.uniform() < self.bda_aug_conf[\"flip_dy_ratio\"]\n+            translation_std = self.bda_aug_conf.get(\"tran_lim\", [0.0, 0.0, 0.0])\n             tran_bda = np.random.normal(scale=translation_std, size=3).T\n         else:\n             rotate_bda = 0\n             scale_bda = 1.0\n@@ -1357,74 +1387,86 @@\n             flip_dy = False\n             tran_bda = np.zeros((1, 3), dtype=np.float32)\n         return rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda\n \n-    def bev_transform(self, gt_boxes, rotate_angle, scale_ratio, flip_dx,\n-                      flip_dy, tran_bda):\n+    def bev_transform(\n+        self, gt_boxes, rotate_angle, scale_ratio, flip_dx, flip_dy, tran_bda\n+    ):\n         rotate_angle = torch.tensor(rotate_angle / 180 * np.pi)\n         rot_sin = torch.sin(rotate_angle)\n         rot_cos = torch.cos(rotate_angle)\n-        rot_mat = torch.Tensor([[rot_cos, -rot_sin, 0], [rot_sin, rot_cos, 0],\n-                                [0, 0, 1]])\n-        scale_mat = torch.Tensor([[scale_ratio, 0, 0], [0, scale_ratio, 0],\n-                                  [0, 0, scale_ratio]])\n+        rot_mat = torch.Tensor(\n+            [[rot_cos, -rot_sin, 0], [rot_sin, rot_cos, 0], [0, 0, 1]]\n+        )\n+        scale_mat = torch.Tensor(\n+            [[scale_ratio, 0, 0], [0, scale_ratio, 0], [0, 0, scale_ratio]]\n+        )\n         flip_mat = torch.Tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n         if flip_dx:\n-            flip_mat = flip_mat @ torch.Tensor([[-1, 0, 0], [0, 1, 0],\n-                                                [0, 0, 1]])\n+            flip_mat = flip_mat @ torch.Tensor([[-1, 0, 0], [0, 1, 0], [0, 0, 1]])\n         if flip_dy:\n-            flip_mat = flip_mat @ torch.Tensor([[1, 0, 0], [0, -1, 0],\n-                                                [0, 0, 1]])\n+            flip_mat = flip_mat @ torch.Tensor([[1, 0, 0], [0, -1, 0], [0, 0, 1]])\n         rot_mat = flip_mat @ (scale_mat @ rot_mat)\n         if gt_boxes.shape[0] > 0:\n-            gt_boxes[:, :3] = (\n-                rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n+            gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n             gt_boxes[:, 3:6] *= scale_ratio\n             gt_boxes[:, 6] += rotate_angle\n             if flip_dx:\n-                gt_boxes[:,\n-                         6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:,\n-                                                                           6]\n+                gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n             if flip_dy:\n                 gt_boxes[:, 6] = -gt_boxes[:, 6]\n-            gt_boxes[:, 7:] = (\n-                rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(-1)\n+            gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(\n+                -1\n+            )\n             gt_boxes[:, :3] = gt_boxes[:, :3] + tran_bda\n         return gt_boxes, rot_mat\n \n     def __call__(self, results):\n-        gt_boxes = results['gt_bboxes_3d'].tensor\n-        gt_boxes[:,2] = gt_boxes[:,2] + 0.5*gt_boxes[:,5]\n-        rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda = \\\n+        gt_boxes = results[\"gt_bboxes_3d\"].tensor\n+        gt_boxes[:, 2] = gt_boxes[:, 2] + 0.5 * gt_boxes[:, 5]\n+        rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda = (\n             self.sample_bda_augmentation()\n+        )\n         bda_mat = torch.zeros(4, 4)\n         bda_mat[3, 3] = 1\n-        gt_boxes, bda_rot = self.bev_transform(gt_boxes, rotate_bda, scale_bda,\n-                                               flip_dx, flip_dy, tran_bda)\n-        if 'points' in results:\n-            points = results['points'].tensor\n+        gt_boxes, bda_rot = self.bev_transform(\n+            gt_boxes, rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda\n+        )\n+        if \"points\" in results:\n+            points = results[\"points\"].tensor\n             points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n-            points[:,:3] = points_aug + tran_bda\n-            points = results['points'].new_point(points)\n-            results['points'] = points\n+            points[:, :3] = points_aug + tran_bda\n+            points = results[\"points\"].new_point(points)\n+            results[\"points\"] = points\n         bda_mat[:3, :3] = bda_rot\n         bda_mat[:3, 3] = torch.from_numpy(tran_bda)\n         if len(gt_boxes) == 0:\n             gt_boxes = torch.zeros(0, 9)\n-        results['gt_bboxes_3d'] = \\\n-            LiDARInstance3DBoxes(gt_boxes, box_dim=gt_boxes.shape[-1],\n-                                 origin=(0.5, 0.5, 0.5))\n-        if 'img_inputs' in results:\n-            imgs, rots, trans, intrins = results['img_inputs'][:4]\n-            post_rots, post_trans = results['img_inputs'][4:]\n-            results['img_inputs'] = (imgs, rots, trans, intrins, post_rots,\n-                                     post_trans, bda_mat)\n-        if 'voxel_semantics' in results:\n+        results[\"gt_bboxes_3d\"] = LiDARInstance3DBoxes(\n+            gt_boxes, box_dim=gt_boxes.shape[-1], origin=(0.5, 0.5, 0.5)\n+        )\n+        if \"img_inputs\" in results:\n+            imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n+            post_rots, post_trans = results[\"img_inputs\"][4:]\n+            results[\"img_inputs\"] = (\n+                imgs,\n+                rots,\n+                trans,\n+                intrins,\n+                post_rots,\n+                post_trans,\n+                bda_mat,\n+            )\n+        if \"voxel_semantics\" in results:\n             if flip_dx:\n-                results['voxel_semantics'] = results['voxel_semantics'][::-1,...].copy()\n-                results['mask_lidar'] = results['mask_lidar'][::-1,...].copy()\n-                results['mask_camera'] = results['mask_camera'][::-1,...].copy()\n+                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n+                    ::-1, ...\n+                ].copy()\n+                results[\"mask_lidar\"] = results[\"mask_lidar\"][::-1, ...].copy()\n+                results[\"mask_camera\"] = results[\"mask_camera\"][::-1, ...].copy()\n             if flip_dy:\n-                results['voxel_semantics'] = results['voxel_semantics'][:,::-1,...].copy()\n-                results['mask_lidar'] = results['mask_lidar'][:,::-1,...].copy()\n-                results['mask_camera'] = results['mask_camera'][:,::-1,...].copy()\n+                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n+                    :, ::-1, ...\n+                ].copy()\n+                results[\"mask_lidar\"] = results[\"mask_lidar\"][:, ::-1, ...].copy()\n+                results[\"mask_camera\"] = results[\"mask_camera\"][:, ::-1, ...].copy()\n         return results\n"
                },
                {
                    "date": 1716018875336,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,8 +7,11 @@\n import torch\n from PIL import Image\n from pyquaternion import Quaternion\n \n+\n+from nuscenes.utils.data_classes import RadarPointCloud\n+\n from mmdet3d.core.points import BasePoints, get_points_type, RadarPoints\n from mmdet.datasets.pipelines import LoadAnnotations, LoadImageFromFile\n from ...core.bbox import LiDARInstance3DBoxes\n from ..builder import PIPELINES\n"
                },
                {
                    "date": 1716534508171,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1439,8 +1439,14 @@\n             points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n             points[:, :3] = points_aug + tran_bda\n             points = results[\"points\"].new_point(points)\n             results[\"points\"] = points\n+        if \"radar\" in results:\n+            points = results[\"points\"].tensor\n+            points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n+            points[:, :3] = points_aug + tran_bda\n+            points = results[\"points\"].new_point(points)\n+            results[\"points\"] = points\n         bda_mat[:3, :3] = bda_rot\n         bda_mat[:3, 3] = torch.from_numpy(tran_bda)\n         if len(gt_boxes) == 0:\n             gt_boxes = torch.zeros(0, 9)\n"
                },
                {
                    "date": 1716534550756,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1439,10 +1439,11 @@\n             points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n             points[:, :3] = points_aug + tran_bda\n             points = results[\"points\"].new_point(points)\n             results[\"points\"] = points\n+            \n         if \"radar\" in results:\n-            points = results[\"points\"].tensor\n+            points = results[\"radar\"].tensor\n             points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n             points[:, :3] = points_aug + tran_bda\n             points = results[\"points\"].new_point(points)\n             results[\"points\"] = points\n"
                },
                {
                    "date": 1716534614211,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1444,9 +1444,9 @@\n         if \"radar\" in results:\n             points = results[\"radar\"].tensor\n             points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n             points[:, :3] = points_aug + tran_bda\n-            points = results[\"points\"].new_point(points)\n+            points = results[\"radar\"].new_point(points)\n             results[\"points\"] = points\n         bda_mat[:3, :3] = bda_rot\n         bda_mat[:3, 3] = torch.from_numpy(tran_bda)\n         if len(gt_boxes) == 0:\n"
                },
                {
                    "date": 1716538321322,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1445,9 +1445,10 @@\n             points = results[\"radar\"].tensor\n             points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n             points[:, :3] = points_aug + tran_bda\n             points = results[\"radar\"].new_point(points)\n-            results[\"points\"] = points\n+            results[\"radar\"] = points\n+            \n         bda_mat[:3, :3] = bda_rot\n         bda_mat[:3, 3] = torch.from_numpy(tran_bda)\n         if len(gt_boxes) == 0:\n             gt_boxes = torch.zeros(0, 9)\n"
                },
                {
                    "date": 1716538380991,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -930,9 +930,10 @@\n \n     def __call__(self, results):\n         points_lidar = results[\"points\"]\n         imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n-        post_rots, post_trans, bda = results[\"img_inputs\"][4:]\n+        # post_rots, post_trans, bda = results[\"img_inputs\"][4:]\n+        post_rots, post_trans = results[\"img_inputs\"][4:]\n         depth_map_list = []\n         for cid in range(len(results[\"cam_names\"])):\n             cam_name = results[\"cam_names\"][cid]\n             lidar2lidarego = np.eye(4, dtype=np.float32)\n"
                },
                {
                    "date": 1716538509608,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1440,16 +1440,16 @@\n             points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n             points[:, :3] = points_aug + tran_bda\n             points = results[\"points\"].new_point(points)\n             results[\"points\"] = points\n-            \n+\n         if \"radar\" in results:\n             points = results[\"radar\"].tensor\n             points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n             points[:, :3] = points_aug + tran_bda\n             points = results[\"radar\"].new_point(points)\n             results[\"radar\"] = points\n-            \n+\n         bda_mat[:3, :3] = bda_rot\n         bda_mat[:3, 3] = torch.from_numpy(tran_bda)\n         if len(gt_boxes) == 0:\n             gt_boxes = torch.zeros(0, 9)\n"
                },
                {
                    "date": 1716644165278,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1367,8 +1367,101 @@\n         return results\n \n \n @PIPELINES.register_module()\n+class LoadAnnotationsBEVDepth(object):\n+    def __init__(self, bda_aug_conf, classes, is_train=True):\n+        self.bda_aug_conf = bda_aug_conf\n+        self.is_train = is_train\n+        self.classes = classes\n+\n+    def sample_bda_augmentation(self):\n+        \"\"\"Generate bda augmentation values based on bda_config.\"\"\"\n+        if self.is_train:\n+            rotate_bda = np.random.uniform(*self.bda_aug_conf[\"rot_lim\"])\n+            scale_bda = np.random.uniform(*self.bda_aug_conf[\"scale_lim\"])\n+            flip_dx = np.random.uniform() < self.bda_aug_conf[\"flip_dx_ratio\"]\n+            flip_dy = np.random.uniform() < self.bda_aug_conf[\"flip_dy_ratio\"]\n+        else:\n+            rotate_bda = 0\n+            scale_bda = 1.0\n+            flip_dx = False\n+            flip_dy = False\n+        return rotate_bda, scale_bda, flip_dx, flip_dy\n+\n+    def bev_transform(self, gt_boxes, rotate_angle, scale_ratio, flip_dx, flip_dy):\n+        rotate_angle = torch.tensor(rotate_angle / 180 * np.pi)\n+        rot_sin = torch.sin(rotate_angle)\n+        rot_cos = torch.cos(rotate_angle)\n+        rot_mat = torch.Tensor(\n+            [[rot_cos, -rot_sin, 0], [rot_sin, rot_cos, 0], [0, 0, 1]]\n+        )\n+        scale_mat = torch.Tensor(\n+            [[scale_ratio, 0, 0], [0, scale_ratio, 0], [0, 0, scale_ratio]]\n+        )\n+        flip_mat = torch.Tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        if flip_dx:\n+            flip_mat = flip_mat @ torch.Tensor([[-1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        if flip_dy:\n+            flip_mat = flip_mat @ torch.Tensor([[1, 0, 0], [0, -1, 0], [0, 0, 1]])\n+        rot_mat = flip_mat @ (scale_mat @ rot_mat)\n+        if gt_boxes.shape[0] > 0:\n+            gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n+            gt_boxes[:, 3:6] *= scale_ratio\n+            gt_boxes[:, 6] += rotate_angle\n+            if flip_dx:\n+                gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n+            if flip_dy:\n+                gt_boxes[:, 6] = -gt_boxes[:, 6]\n+            gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(\n+                -1\n+            )\n+        return gt_boxes, rot_mat\n+\n+    def __call__(self, results):\n+        gt_boxes, gt_labels = results[\"ann_infos\"]\n+        gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n+        rotate_bda, scale_bda, flip_dx, flip_dy = self.sample_bda_augmentation()\n+        bda_mat = torch.zeros(4, 4)\n+        bda_mat[3, 3] = 1\n+        gt_boxes, bda_rot = self.bev_transform(\n+            gt_boxes, rotate_bda, scale_bda, flip_dx, flip_dy\n+        )\n+        bda_mat[:3, :3] = bda_rot\n+        if len(gt_boxes) == 0:\n+            gt_boxes = torch.zeros(0, 9)\n+        results[\"gt_bboxes_3d\"] = LiDARInstance3DBoxes(\n+            gt_boxes, box_dim=gt_boxes.shape[-1], origin=(0.5, 0.5, 0.5)\n+        )\n+        results[\"gt_labels_3d\"] = gt_labels\n+        imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n+        post_rots, post_trans = results[\"img_inputs\"][4:]\n+        results[\"img_inputs\"] = (\n+            imgs,\n+            rots,\n+            trans,\n+            intrins,\n+            post_rots,\n+            post_trans,\n+            bda_rot,\n+        )\n+        if \"voxel_semantics\" in results:\n+            if flip_dx:\n+                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n+                    ::-1, ...\n+                ].copy()\n+                results[\"mask_lidar\"] = results[\"mask_lidar\"][::-1, ...].copy()\n+                results[\"mask_camera\"] = results[\"mask_camera\"][::-1, ...].copy()\n+            if flip_dy:\n+                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n+                    :, ::-1, ...\n+                ].copy()\n+                results[\"mask_lidar\"] = results[\"mask_lidar\"][:, ::-1, ...].copy()\n+                results[\"mask_camera\"] = results[\"mask_camera\"][:, ::-1, ...].copy()\n+        return results\n+\n+\n+@PIPELINES.register_module()\n class BEVAug(object):\n \n     def __init__(self, bda_aug_conf, classes, is_train=True):\n         self.bda_aug_conf = bda_aug_conf\n"
                },
                {
                    "date": 1716644760343,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1441,9 +1441,10 @@\n             trans,\n             intrins,\n             post_rots,\n             post_trans,\n-            bda_rot,\n+            # bda_rot,\n+            bda_mat,\n         )\n         if \"voxel_semantics\" in results:\n             if flip_dx:\n                 results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n"
                },
                {
                    "date": 1716795177946,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,1578 @@\n+# Copyright (c) OpenMMLab. All rights reserved.\n+import os\n+\n+import cv2\n+import mmcv\n+import numpy as np\n+import torch\n+from PIL import Image\n+from pyquaternion import Quaternion\n+\n+\n+from nuscenes.utils.data_classes import RadarPointCloud\n+\n+from mmdet3d.core.points import BasePoints, get_points_type, RadarPoints\n+from mmdet.datasets.pipelines import LoadAnnotations, LoadImageFromFile\n+from ...core.bbox import LiDARInstance3DBoxes\n+from ..builder import PIPELINES\n+\n+\n+@PIPELINES.register_module()\n+class LoadOccGTFromFile(object):\n+    def __call__(self, results):\n+        occ_gt_path = results[\"occ_gt_path\"]\n+        occ_gt_path = os.path.join(occ_gt_path, \"labels.npz\")\n+\n+        occ_labels = np.load(occ_gt_path)\n+        semantics = occ_labels[\"semantics\"]\n+        mask_lidar = occ_labels[\"mask_lidar\"]\n+        mask_camera = occ_labels[\"mask_camera\"]\n+\n+        results[\"voxel_semantics\"] = semantics\n+        results[\"mask_lidar\"] = mask_lidar\n+        results[\"mask_camera\"] = mask_camera\n+        return results\n+\n+\n+@PIPELINES.register_module()\n+class LoadMultiViewImageFromFiles(object):\n+    \"\"\"Load multi channel images from a list of separate channel files.\n+\n+    Expects results['img_filename'] to be a list of filenames.\n+\n+    Args:\n+        to_float32 (bool, optional): Whether to convert the img to float32.\n+            Defaults to False.\n+        color_type (str, optional): Color type of the file.\n+            Defaults to 'unchanged'.\n+    \"\"\"\n+\n+    def __init__(self, to_float32=False, color_type=\"unchanged\"):\n+        self.to_float32 = to_float32\n+        self.color_type = color_type\n+\n+    def __call__(self, results):\n+        \"\"\"Call function to load multi-view image from files.\n+\n+        Args:\n+            results (dict): Result dict containing multi-view image filenames.\n+\n+        Returns:\n+            dict: The result dict containing the multi-view image data.\n+                Added keys and values are described below.\n+\n+                - filename (str): Multi-view image filenames.\n+                - img (np.ndarray): Multi-view image arrays.\n+                - img_shape (tuple[int]): Shape of multi-view image arrays.\n+                - ori_shape (tuple[int]): Shape of original image arrays.\n+                - pad_shape (tuple[int]): Shape of padded image arrays.\n+                - scale_factor (float): Scale factor.\n+                - img_norm_cfg (dict): Normalization configuration of images.\n+        \"\"\"\n+        filename = results[\"img_filename\"]\n+        # img is of shape (h, w, c, num_views)\n+        img = np.stack(\n+            [mmcv.imread(name, self.color_type) for name in filename], axis=-1\n+        )\n+        if self.to_float32:\n+            img = img.astype(np.float32)\n+        results[\"filename\"] = filename\n+        # unravel to list, see `DefaultFormatBundle` in formatting.py\n+        # which will transpose each image separately and then stack into array\n+        results[\"img\"] = [img[..., i] for i in range(img.shape[-1])]\n+        results[\"img_shape\"] = img.shape\n+        results[\"ori_shape\"] = img.shape\n+        # Set initial values for default meta_keys\n+        results[\"pad_shape\"] = img.shape\n+        results[\"scale_factor\"] = 1.0\n+        num_channels = 1 if len(img.shape) < 3 else img.shape[2]\n+        results[\"img_norm_cfg\"] = dict(\n+            mean=np.zeros(num_channels, dtype=np.float32),\n+            std=np.ones(num_channels, dtype=np.float32),\n+            to_rgb=False,\n+        )\n+        return results\n+\n+    def __repr__(self):\n+        \"\"\"str: Return a string that describes the module.\"\"\"\n+        repr_str = self.__class__.__name__\n+        repr_str += f\"(to_float32={self.to_float32}, \"\n+        repr_str += f\"color_type='{self.color_type}')\"\n+        return repr_str\n+\n+\n+@PIPELINES.register_module()\n+class LoadImageFromFileMono3D(LoadImageFromFile):\n+    \"\"\"Load an image from file in monocular 3D object detection. Compared to 2D\n+    detection, additional camera parameters need to be loaded.\n+\n+    Args:\n+        kwargs (dict): Arguments are the same as those in\n+            :class:`LoadImageFromFile`.\n+    \"\"\"\n+\n+    def __call__(self, results):\n+        \"\"\"Call functions to load image and get image meta information.\n+\n+        Args:\n+            results (dict): Result dict from :obj:`mmdet.CustomDataset`.\n+\n+        Returns:\n+            dict: The dict contains loaded image and meta information.\n+        \"\"\"\n+        super().__call__(results)\n+        results[\"cam2img\"] = results[\"img_info\"][\"cam_intrinsic\"]\n+        return results\n+\n+\n+@PIPELINES.register_module()\n+class LoadPointsFromMultiSweeps(object):\n+    \"\"\"Load points from multiple sweeps.\n+\n+    This is usually used for nuScenes dataset to utilize previous sweeps.\n+\n+    Args:\n+        sweeps_num (int, optional): Number of sweeps. Defaults to 10.\n+        load_dim (int, optional): Dimension number of the loaded points.\n+            Defaults to 5.\n+        use_dim (list[int], optional): Which dimension to use.\n+            Defaults to [0, 1, 2, 4].\n+        time_dim (int, optional): Which dimension to represent the timestamps\n+            of each points. Defaults to 4.\n+        file_client_args (dict, optional): Config dict of file clients,\n+            refer to\n+            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n+            for more details. Defaults to dict(backend='disk').\n+        pad_empty_sweeps (bool, optional): Whether to repeat keyframe when\n+            sweeps is empty. Defaults to False.\n+        remove_close (bool, optional): Whether to remove close points.\n+            Defaults to False.\n+        test_mode (bool, optional): If `test_mode=True`, it will not\n+            randomly sample sweeps but select the nearest N frames.\n+            Defaults to False.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        sweeps_num=10,\n+        load_dim=5,\n+        use_dim=[0, 1, 2, 4],\n+        time_dim=4,\n+        file_client_args=dict(backend=\"disk\"),\n+        pad_empty_sweeps=False,\n+        remove_close=False,\n+        test_mode=False,\n+    ):\n+        self.load_dim = load_dim\n+        self.sweeps_num = sweeps_num\n+        self.use_dim = use_dim\n+        self.time_dim = time_dim\n+        assert (\n+            time_dim < load_dim\n+        ), f\"Expect the timestamp dimension < {load_dim}, got {time_dim}\"\n+        self.file_client_args = file_client_args.copy()\n+        self.file_client = None\n+        self.pad_empty_sweeps = pad_empty_sweeps\n+        self.remove_close = remove_close\n+        self.test_mode = test_mode\n+        assert (\n+            max(use_dim) < load_dim\n+        ), f\"Expect all used dimensions < {load_dim}, got {use_dim}\"\n+\n+    def _load_points(self, pts_filename):\n+        \"\"\"Private function to load point clouds data.\n+\n+        Args:\n+            pts_filename (str): Filename of point clouds data.\n+\n+        Returns:\n+            np.ndarray: An array containing point clouds data.\n+        \"\"\"\n+        if self.file_client is None:\n+            self.file_client = mmcv.FileClient(**self.file_client_args)\n+        try:\n+            pts_bytes = self.file_client.get(pts_filename)\n+            points = np.frombuffer(pts_bytes, dtype=np.float32)\n+        except ConnectionError:\n+            mmcv.check_file_exist(pts_filename)\n+            if pts_filename.endswith(\".npy\"):\n+                points = np.load(pts_filename)\n+            else:\n+                points = np.fromfile(pts_filename, dtype=np.float32)\n+        return points\n+\n+    def _remove_close(self, points, radius=1.0):\n+        \"\"\"Removes point too close within a certain radius from origin.\n+\n+        Args:\n+            points (np.ndarray | :obj:`BasePoints`): Sweep points.\n+            radius (float, optional): Radius below which points are removed.\n+                Defaults to 1.0.\n+\n+        Returns:\n+            np.ndarray: Points after removing.\n+        \"\"\"\n+        if isinstance(points, np.ndarray):\n+            points_numpy = points\n+        elif isinstance(points, BasePoints):\n+            points_numpy = points.tensor.numpy()\n+        else:\n+            raise NotImplementedError\n+        x_filt = np.abs(points_numpy[:, 0]) < radius\n+        y_filt = np.abs(points_numpy[:, 1]) < radius\n+        not_close = np.logical_not(np.logical_and(x_filt, y_filt))\n+        return points[not_close]\n+\n+    def __call__(self, results):\n+        \"\"\"Call function to load multi-sweep point clouds from files.\n+\n+        Args:\n+            results (dict): Result dict containing multi-sweep point cloud\n+                filenames.\n+\n+        Returns:\n+            dict: The result dict containing the multi-sweep points data.\n+                Added key and value are described below.\n+\n+                - points (np.ndarray | :obj:`BasePoints`): Multi-sweep point\n+                    cloud arrays.\n+        \"\"\"\n+        points = results[\"points\"]\n+        points.tensor[:, self.time_dim] = 0\n+        sweep_points_list = [points]\n+        ts = results[\"timestamp\"]\n+        if self.pad_empty_sweeps and len(results[\"sweeps\"]) == 0:\n+            for i in range(self.sweeps_num):\n+                if self.remove_close:\n+                    sweep_points_list.append(self._remove_close(points))\n+                else:\n+                    sweep_points_list.append(points)\n+        else:\n+            if len(results[\"sweeps\"]) <= self.sweeps_num:\n+                choices = np.arange(len(results[\"sweeps\"]))\n+            elif self.test_mode:\n+                choices = np.arange(self.sweeps_num)\n+            else:\n+                choices = np.random.choice(\n+                    len(results[\"sweeps\"]), self.sweeps_num, replace=False\n+                )\n+            for idx in choices:\n+                sweep = results[\"sweeps\"][idx]\n+                points_sweep = self._load_points(sweep[\"data_path\"])\n+                points_sweep = np.copy(points_sweep).reshape(-1, self.load_dim)\n+                if self.remove_close:\n+                    points_sweep = self._remove_close(points_sweep)\n+                sweep_ts = sweep[\"timestamp\"] / 1e6\n+                points_sweep[:, :3] = (\n+                    points_sweep[:, :3] @ sweep[\"sensor2lidar_rotation\"].T\n+                )\n+                points_sweep[:, :3] += sweep[\"sensor2lidar_translation\"]\n+                points_sweep[:, self.time_dim] = ts - sweep_ts\n+                points_sweep = points.new_point(points_sweep)\n+                sweep_points_list.append(points_sweep)\n+\n+        points = points.cat(sweep_points_list)\n+        points = points[:, self.use_dim]\n+        results[\"points\"] = points\n+        return results\n+\n+    def __repr__(self):\n+        \"\"\"str: Return a string that describes the module.\"\"\"\n+        return f\"{self.__class__.__name__}(sweeps_num={self.sweeps_num})\"\n+\n+\n+@PIPELINES.register_module()\n+class PointSegClassMapping(object):\n+    \"\"\"Map original semantic class to valid category ids.\n+\n+    Map valid classes as 0~len(valid_cat_ids)-1 and\n+    others as len(valid_cat_ids).\n+\n+    Args:\n+        valid_cat_ids (tuple[int]): A tuple of valid category.\n+        max_cat_id (int, optional): The max possible cat_id in input\n+            segmentation mask. Defaults to 40.\n+    \"\"\"\n+\n+    def __init__(self, valid_cat_ids, max_cat_id=40):\n+        assert max_cat_id >= np.max(\n+            valid_cat_ids\n+        ), \"max_cat_id should be greater than maximum id in valid_cat_ids\"\n+\n+        self.valid_cat_ids = valid_cat_ids\n+        self.max_cat_id = int(max_cat_id)\n+\n+        # build cat_id to class index mapping\n+        neg_cls = len(valid_cat_ids)\n+        self.cat_id2class = np.ones(self.max_cat_id + 1, dtype=np.int) * neg_cls\n+        for cls_idx, cat_id in enumerate(valid_cat_ids):\n+            self.cat_id2class[cat_id] = cls_idx\n+\n+    def __call__(self, results):\n+        \"\"\"Call function to map original semantic class to valid category ids.\n+\n+        Args:\n+            results (dict): Result dict containing point semantic masks.\n+\n+        Returns:\n+            dict: The result dict containing the mapped category ids.\n+                Updated key and value are described below.\n+\n+                - pts_semantic_mask (np.ndarray): Mapped semantic masks.\n+        \"\"\"\n+        assert \"pts_semantic_mask\" in results\n+        pts_semantic_mask = results[\"pts_semantic_mask\"]\n+\n+        converted_pts_sem_mask = self.cat_id2class[pts_semantic_mask]\n+\n+        results[\"pts_semantic_mask\"] = converted_pts_sem_mask\n+        return results\n+\n+    def __repr__(self):\n+        \"\"\"str: Return a string that describes the module.\"\"\"\n+        repr_str = self.__class__.__name__\n+        repr_str += f\"(valid_cat_ids={self.valid_cat_ids}, \"\n+        repr_str += f\"max_cat_id={self.max_cat_id})\"\n+        return repr_str\n+\n+\n+@PIPELINES.register_module()\n+class NormalizePointsColor(object):\n+    \"\"\"Normalize color of points.\n+\n+    Args:\n+        color_mean (list[float]): Mean color of the point cloud.\n+    \"\"\"\n+\n+    def __init__(self, color_mean):\n+        self.color_mean = color_mean\n+\n+    def __call__(self, results):\n+        \"\"\"Call function to normalize color of points.\n+\n+        Args:\n+            results (dict): Result dict containing point clouds data.\n+\n+        Returns:\n+            dict: The result dict containing the normalized points.\n+                Updated key and value are described below.\n+\n+                - points (:obj:`BasePoints`): Points after color normalization.\n+        \"\"\"\n+        points = results[\"points\"]\n+        assert (\n+            points.attribute_dims is not None\n+            and \"color\" in points.attribute_dims.keys()\n+        ), \"Expect points have color attribute\"\n+        if self.color_mean is not None:\n+            points.color = points.color - points.color.new_tensor(self.color_mean)\n+        points.color = points.color / 255.0\n+        results[\"points\"] = points\n+        return results\n+\n+    def __repr__(self):\n+        \"\"\"str: Return a string that describes the module.\"\"\"\n+        repr_str = self.__class__.__name__\n+        repr_str += f\"(color_mean={self.color_mean})\"\n+        return repr_str\n+\n+\n+@PIPELINES.register_module()\n+class LoadRadarPointsMultiSweeps(object):\n+    \"\"\"Load radar points from multiple sweeps.\n+    This is usually used for nuScenes dataset to utilize previous sweeps.\n+    Args:\n+        sweeps_num (int): Number of sweeps. Defaults to 10.\n+        load_dim (int): Dimension number of the loaded points. Defaults to 5.\n+        use_dim (list[int]): Which dimension to use. Defaults to [0, 1, 2, 4].\n+        file_client_args (dict): Config dict of file clients, refer to\n+            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n+            for more details. Defaults to dict(backend='disk').\n+        pad_empty_sweeps (bool): Whether to repeat keyframe when\n+            sweeps is empty. Defaults to False.\n+        remove_close (bool): Whether to remove close points.\n+            Defaults to False.\n+        test_mode (bool): If test_model=True used for testing, it will not\n+            randomly sample sweeps but select the nearest N frames.\n+            Defaults to False.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        load_dim=18,\n+        use_dim=[0, 1, 2, 3, 4],\n+        sweeps_num=3,\n+        file_client_args=dict(backend=\"disk\"),\n+        max_num=300,\n+        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],\n+        test_mode=False,\n+    ):\n+        self.load_dim = load_dim\n+        self.use_dim = use_dim\n+        self.sweeps_num = sweeps_num\n+        self.file_client_args = file_client_args.copy()\n+        self.file_client = None\n+        self.max_num = max_num\n+        self.test_mode = test_mode\n+        self.pc_range = pc_range\n+\n+    def _load_points(self, pts_filename):\n+        \"\"\"Private function to load point clouds data.\n+        Args:\n+            pts_filename (str): Filename of point clouds data.\n+        Returns:\n+            np.ndarray: An array containing point clouds data.\n+            [N, 18]\n+        \"\"\"\n+        radar_obj = RadarPointCloud.from_file(pts_filename)\n+\n+        # [18, N]\n+        points = radar_obj.points\n+\n+        return points.transpose().astype(np.float32)\n+\n+    def _pad_or_drop(self, points):\n+        \"\"\"\n+        points: [N, 18]\n+        \"\"\"\n+\n+        num_points = points.shape[0]\n+\n+        if num_points == self.max_num:\n+            masks = np.ones((num_points, 1), dtype=points.dtype)\n+\n+            return points, masks\n+\n+        if num_points > self.max_num:\n+            points = np.random.permutation(points)[: self.max_num, :]\n+            masks = np.ones((self.max_num, 1), dtype=points.dtype)\n+\n+            return points, masks\n+\n+        if num_points < self.max_num:\n+            zeros = np.zeros(\n+                (self.max_num - num_points, points.shape[1]), dtype=points.dtype\n+            )\n+            masks = np.ones((num_points, 1), dtype=points.dtype)\n+\n+            points = np.concatenate((points, zeros), axis=0)\n+            masks = np.concatenate((masks, zeros.copy()[:, [0]]), axis=0)\n+\n+            return points, masks\n+\n+    def __call__(self, results):\n+        \"\"\"Call function to load multi-sweep point clouds from files.\n+        Args:\n+            results (dict): Result dict containing multi-sweep point cloud \\\n+                filenames.\n+        Returns:\n+            dict: The result dict containing the multi-sweep points data. \\\n+                Added key and value are described below.\n+                - points (np.ndarray | :obj:`BasePoints`): Multi-sweep point \\\n+                    cloud arrays.\n+        \"\"\"\n+        radars_dict = results[\"radar\"]\n+\n+        points_sweep_list = []\n+        for key, sweeps in radars_dict.items():\n+            if len(sweeps) < self.sweeps_num:\n+                idxes = list(range(len(sweeps)))\n+            else:\n+                idxes = list(range(self.sweeps_num))\n+\n+            ts = sweeps[0][\"timestamp\"] * 1e-6\n+            for idx in idxes:\n+                sweep = sweeps[idx]\n+\n+                points_sweep = self._load_points(sweep[\"data_path\"])\n+                points_sweep = np.copy(points_sweep).reshape(-1, self.load_dim)\n+\n+                timestamp = sweep[\"timestamp\"] * 1e-6\n+                time_diff = ts - timestamp\n+                time_diff = np.ones((points_sweep.shape[0], 1)) * time_diff\n+\n+                # velocity compensated by the ego motion in sensor frame\n+                velo_comp = points_sweep[:, 8:10]\n+                velo_comp = np.concatenate(\n+                    (velo_comp, np.zeros((velo_comp.shape[0], 1))), 1\n+                )\n+                velo_comp = velo_comp @ sweep[\"sensor2lidar_rotation\"].T\n+                velo_comp = velo_comp[:, :2]\n+\n+                # velocity in sensor frame\n+                velo = points_sweep[:, 6:8]\n+                velo = np.concatenate((velo, np.zeros((velo.shape[0], 1))), 1)\n+                velo = velo @ sweep[\"sensor2lidar_rotation\"].T\n+                velo = velo[:, :2]\n+\n+                points_sweep[:, :3] = (\n+                    points_sweep[:, :3] @ sweep[\"sensor2lidar_rotation\"].T\n+                )\n+                points_sweep[:, :3] += sweep[\"sensor2lidar_translation\"]\n+\n+                points_sweep_ = np.concatenate(\n+                    [\n+                        points_sweep[:, :6],\n+                        velo,\n+                        velo_comp,\n+                        points_sweep[:, 10:],\n+                        time_diff,\n+                    ],\n+                    axis=1,\n+                )\n+                points_sweep_list.append(points_sweep_)\n+\n+        points = np.concatenate(points_sweep_list, axis=0)\n+\n+        points = points[:, self.use_dim]\n+\n+        points = RadarPoints(points, points_dim=points.shape[-1], attribute_dims=None)\n+\n+        results[\"radar\"] = points\n+        return results\n+\n+    def __repr__(self):\n+        \"\"\"str: Return a string that describes the module.\"\"\"\n+        return f\"{self.__class__.__name__}(sweeps_num={self.sweeps_num})\"\n+\n+\n+@PIPELINES.register_module()\n+class LoadPointsFromFile(object):\n+    \"\"\"Load Points From File.\n+\n+    Load points from file.\n+\n+    Args:\n+        coord_type (str): The type of coordinates of points cloud.\n+            Available options includes:\n+            - 'LIDAR': Points in LiDAR coordinates.\n+            - 'DEPTH': Points in depth coordinates, usually for indoor dataset.\n+            - 'CAMERA': Points in camera coordinates.\n+        load_dim (int, optional): The dimension of the loaded points.\n+            Defaults to 6.\n+        use_dim (list[int], optional): Which dimensions of the points to use.\n+            Defaults to [0, 1, 2]. For KITTI dataset, set use_dim=4\n+            or use_dim=[0, 1, 2, 3] to use the intensity dimension.\n+        shift_height (bool, optional): Whether to use shifted height.\n+            Defaults to False.\n+        use_color (bool, optional): Whether to use color features.\n+            Defaults to False.\n+        file_client_args (dict, optional): Config dict of file clients,\n+            refer to\n+            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n+            for more details. Defaults to dict(backend='disk').\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        coord_type,\n+        load_dim=6,\n+        use_dim=[0, 1, 2],\n+        shift_height=False,\n+        use_color=False,\n+        file_client_args=dict(backend=\"disk\"),\n+    ):\n+        self.shift_height = shift_height\n+        self.use_color = use_color\n+        if isinstance(use_dim, int):\n+            use_dim = list(range(use_dim))\n+        assert (\n+            max(use_dim) < load_dim\n+        ), f\"Expect all used dimensions < {load_dim}, got {use_dim}\"\n+        assert coord_type in [\"CAMERA\", \"LIDAR\", \"DEPTH\"]\n+\n+        self.coord_type = coord_type\n+        self.load_dim = load_dim\n+        self.use_dim = use_dim\n+        self.file_client_args = file_client_args.copy()\n+        self.file_client = None\n+\n+    def _load_points(self, pts_filename):\n+        \"\"\"Private function to load point clouds data.\n+\n+        Args:\n+            pts_filename (str): Filename of point clouds data.\n+\n+        Returns:\n+            np.ndarray: An array containing point clouds data.\n+        \"\"\"\n+        if self.file_client is None:\n+            self.file_client = mmcv.FileClient(**self.file_client_args)\n+        try:\n+            pts_bytes = self.file_client.get(pts_filename)\n+            points = np.frombuffer(pts_bytes, dtype=np.float32)\n+        except ConnectionError:\n+            mmcv.check_file_exist(pts_filename)\n+            if pts_filename.endswith(\".npy\"):\n+                points = np.load(pts_filename)\n+            else:\n+                points = np.fromfile(pts_filename, dtype=np.float32)\n+\n+        return points\n+\n+    def __call__(self, results):\n+        \"\"\"Call function to load points data from file.\n+\n+        Args:\n+            results (dict): Result dict containing point clouds data.\n+\n+        Returns:\n+            dict: The result dict containing the point clouds data.\n+                Added key and value are described below.\n+\n+                - points (:obj:`BasePoints`): Point clouds data.\n+        \"\"\"\n+        pts_filename = results[\"pts_filename\"]\n+        points = self._load_points(pts_filename)\n+        points = points.reshape(-1, self.load_dim)\n+        points = points[:, self.use_dim]\n+        attribute_dims = None\n+\n+        if self.shift_height:\n+            floor_height = np.percentile(points[:, 2], 0.99)\n+            height = points[:, 2] - floor_height\n+            points = np.concatenate(\n+                [points[:, :3], np.expand_dims(height, 1), points[:, 3:]], 1\n+            )\n+            attribute_dims = dict(height=3)\n+\n+        if self.use_color:\n+            assert len(self.use_dim) >= 6\n+            if attribute_dims is None:\n+                attribute_dims = dict()\n+            attribute_dims.update(\n+                dict(\n+                    color=[\n+                        points.shape[1] - 3,\n+                        points.shape[1] - 2,\n+                        points.shape[1] - 1,\n+                    ]\n+                )\n+            )\n+\n+        points_class = get_points_type(self.coord_type)\n+        points = points_class(\n+            points, points_dim=points.shape[-1], attribute_dims=attribute_dims\n+        )\n+        results[\"points\"] = points\n+\n+        return results\n+\n+    def __repr__(self):\n+        \"\"\"str: Return a string that describes the module.\"\"\"\n+        repr_str = self.__class__.__name__ + \"(\"\n+        repr_str += f\"shift_height={self.shift_height}, \"\n+        repr_str += f\"use_color={self.use_color}, \"\n+        repr_str += f\"file_client_args={self.file_client_args}, \"\n+        repr_str += f\"load_dim={self.load_dim}, \"\n+        repr_str += f\"use_dim={self.use_dim})\"\n+        return repr_str\n+\n+\n+@PIPELINES.register_module()\n+class LoadPointsFromDict(LoadPointsFromFile):\n+    \"\"\"Load Points From Dict.\"\"\"\n+\n+    def __call__(self, results):\n+        assert \"points\" in results\n+        return results\n+\n+\n+@PIPELINES.register_module()\n+class LoadAnnotations3D(LoadAnnotations):\n+    \"\"\"Load Annotations3D.\n+\n+    Load instance mask and semantic mask of points and\n+    encapsulate the items into related fields.\n+\n+    Args:\n+        with_bbox_3d (bool, optional): Whether to load 3D boxes.\n+            Defaults to True.\n+        with_label_3d (bool, optional): Whether to load 3D labels.\n+            Defaults to True.\n+        with_attr_label (bool, optional): Whether to load attribute label.\n+            Defaults to False.\n+        with_mask_3d (bool, optional): Whether to load 3D instance masks.\n+            for points. Defaults to False.\n+        with_seg_3d (bool, optional): Whether to load 3D semantic masks.\n+            for points. Defaults to False.\n+        with_bbox (bool, optional): Whether to load 2D boxes.\n+            Defaults to False.\n+        with_label (bool, optional): Whether to load 2D labels.\n+            Defaults to False.\n+        with_mask (bool, optional): Whether to load 2D instance masks.\n+            Defaults to False.\n+        with_seg (bool, optional): Whether to load 2D semantic masks.\n+            Defaults to False.\n+        with_bbox_depth (bool, optional): Whether to load 2.5D boxes.\n+            Defaults to False.\n+        poly2mask (bool, optional): Whether to convert polygon annotations\n+            to bitmasks. Defaults to True.\n+        seg_3d_dtype (dtype, optional): Dtype of 3D semantic masks.\n+            Defaults to int64\n+        file_client_args (dict): Config dict of file clients, refer to\n+            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n+            for more details.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        with_bbox_3d=True,\n+        with_label_3d=True,\n+        with_attr_label=False,\n+        with_mask_3d=False,\n+        with_seg_3d=False,\n+        with_bbox=False,\n+        with_label=False,\n+        with_mask=False,\n+        with_seg=False,\n+        with_bbox_depth=False,\n+        poly2mask=True,\n+        seg_3d_dtype=np.int64,\n+        file_client_args=dict(backend=\"disk\"),\n+    ):\n+        super().__init__(\n+            with_bbox,\n+            with_label,\n+            with_mask,\n+            with_seg,\n+            poly2mask,\n+            file_client_args=file_client_args,\n+        )\n+        self.with_bbox_3d = with_bbox_3d\n+        self.with_bbox_depth = with_bbox_depth\n+        self.with_label_3d = with_label_3d\n+        self.with_attr_label = with_attr_label\n+        self.with_mask_3d = with_mask_3d\n+        self.with_seg_3d = with_seg_3d\n+        self.seg_3d_dtype = seg_3d_dtype\n+\n+    def _load_bboxes_3d(self, results):\n+        \"\"\"Private function to load 3D bounding box annotations.\n+\n+        Args:\n+            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n+\n+        Returns:\n+            dict: The dict containing loaded 3D bounding box annotations.\n+        \"\"\"\n+        results[\"gt_bboxes_3d\"] = results[\"ann_info\"][\"gt_bboxes_3d\"]\n+        results[\"bbox3d_fields\"].append(\"gt_bboxes_3d\")\n+        return results\n+\n+    def _load_bboxes_depth(self, results):\n+        \"\"\"Private function to load 2.5D bounding box annotations.\n+\n+        Args:\n+            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n+\n+        Returns:\n+            dict: The dict containing loaded 2.5D bounding box annotations.\n+        \"\"\"\n+        results[\"centers2d\"] = results[\"ann_info\"][\"centers2d\"]\n+        results[\"depths\"] = results[\"ann_info\"][\"depths\"]\n+        return results\n+\n+    def _load_labels_3d(self, results):\n+        \"\"\"Private function to load label annotations.\n+\n+        Args:\n+            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n+\n+        Returns:\n+            dict: The dict containing loaded label annotations.\n+        \"\"\"\n+        results[\"gt_labels_3d\"] = results[\"ann_info\"][\"gt_labels_3d\"]\n+        return results\n+\n+    def _load_attr_labels(self, results):\n+        \"\"\"Private function to load label annotations.\n+\n+        Args:\n+            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n+\n+        Returns:\n+            dict: The dict containing loaded label annotations.\n+        \"\"\"\n+        results[\"attr_labels\"] = results[\"ann_info\"][\"attr_labels\"]\n+        return results\n+\n+    def _load_masks_3d(self, results):\n+        \"\"\"Private function to load 3D mask annotations.\n+\n+        Args:\n+            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n+\n+        Returns:\n+            dict: The dict containing loaded 3D mask annotations.\n+        \"\"\"\n+        pts_instance_mask_path = results[\"ann_info\"][\"pts_instance_mask_path\"]\n+\n+        if self.file_client is None:\n+            self.file_client = mmcv.FileClient(**self.file_client_args)\n+        try:\n+            mask_bytes = self.file_client.get(pts_instance_mask_path)\n+            pts_instance_mask = np.frombuffer(mask_bytes, dtype=np.int64)\n+        except ConnectionError:\n+            mmcv.check_file_exist(pts_instance_mask_path)\n+            pts_instance_mask = np.fromfile(pts_instance_mask_path, dtype=np.int64)\n+\n+        results[\"pts_instance_mask\"] = pts_instance_mask\n+        results[\"pts_mask_fields\"].append(\"pts_instance_mask\")\n+        return results\n+\n+    def _load_semantic_seg_3d(self, results):\n+        \"\"\"Private function to load 3D semantic segmentation annotations.\n+\n+        Args:\n+            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n+\n+        Returns:\n+            dict: The dict containing the semantic segmentation annotations.\n+        \"\"\"\n+        pts_semantic_mask_path = results[\"ann_info\"][\"pts_semantic_mask_path\"]\n+\n+        if self.file_client is None:\n+            self.file_client = mmcv.FileClient(**self.file_client_args)\n+        try:\n+            mask_bytes = self.file_client.get(pts_semantic_mask_path)\n+            # add .copy() to fix read-only bug\n+            pts_semantic_mask = np.frombuffer(\n+                mask_bytes, dtype=self.seg_3d_dtype\n+            ).copy()\n+        except ConnectionError:\n+            mmcv.check_file_exist(pts_semantic_mask_path)\n+            pts_semantic_mask = np.fromfile(pts_semantic_mask_path, dtype=np.int64)\n+\n+        results[\"pts_semantic_mask\"] = pts_semantic_mask\n+        results[\"pts_seg_fields\"].append(\"pts_semantic_mask\")\n+        return results\n+\n+    def __call__(self, results):\n+        \"\"\"Call function to load multiple types annotations.\n+\n+        Args:\n+            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n+\n+        Returns:\n+            dict: The dict containing loaded 3D bounding box, label, mask and\n+                semantic segmentation annotations.\n+        \"\"\"\n+        results = super().__call__(results)\n+        if self.with_bbox_3d:\n+            results = self._load_bboxes_3d(results)\n+            if results is None:\n+                return None\n+        if self.with_bbox_depth:\n+            results = self._load_bboxes_depth(results)\n+            if results is None:\n+                return None\n+        if self.with_label_3d:\n+            results = self._load_labels_3d(results)\n+        if self.with_attr_label:\n+            results = self._load_attr_labels(results)\n+        if self.with_mask_3d:\n+            results = self._load_masks_3d(results)\n+        if self.with_seg_3d:\n+            results = self._load_semantic_seg_3d(results)\n+\n+        return results\n+\n+    def __repr__(self):\n+        \"\"\"str: Return a string that describes the module.\"\"\"\n+        indent_str = \"    \"\n+        repr_str = self.__class__.__name__ + \"(\\n\"\n+        repr_str += f\"{indent_str}with_bbox_3d={self.with_bbox_3d}, \"\n+        repr_str += f\"{indent_str}with_label_3d={self.with_label_3d}, \"\n+        repr_str += f\"{indent_str}with_attr_label={self.with_attr_label}, \"\n+        repr_str += f\"{indent_str}with_mask_3d={self.with_mask_3d}, \"\n+        repr_str += f\"{indent_str}with_seg_3d={self.with_seg_3d}, \"\n+        repr_str += f\"{indent_str}with_bbox={self.with_bbox}, \"\n+        repr_str += f\"{indent_str}with_label={self.with_label}, \"\n+        repr_str += f\"{indent_str}with_mask={self.with_mask}, \"\n+        repr_str += f\"{indent_str}with_seg={self.with_seg}, \"\n+        repr_str += f\"{indent_str}with_bbox_depth={self.with_bbox_depth}, \"\n+        repr_str += f\"{indent_str}poly2mask={self.poly2mask})\"\n+        return repr_str\n+\n+\n+@PIPELINES.register_module()\n+class PointToMultiViewDepth(object):\n+\n+    def __init__(self, grid_config, downsample=1):\n+        self.downsample = downsample\n+        self.grid_config = grid_config\n+\n+    def points2depthmap(self, points, height, width):\n+        height, width = height // self.downsample, width // self.downsample\n+        depth_map = torch.zeros((height, width), dtype=torch.float32)\n+        coor = torch.round(points[:, :2] / self.downsample)\n+        depth = points[:, 2]\n+        kept1 = (\n+            (coor[:, 0] >= 0)\n+            & (coor[:, 0] < width)\n+            & (coor[:, 1] >= 0)\n+            & (coor[:, 1] < height)\n+            & (depth < self.grid_config[\"depth\"][1])\n+            & (depth >= self.grid_config[\"depth\"][0])\n+        )\n+        coor, depth = coor[kept1], depth[kept1]\n+        ranks = coor[:, 0] + coor[:, 1] * width\n+        sort = (ranks + depth / 100.0).argsort()\n+        coor, depth, ranks = coor[sort], depth[sort], ranks[sort]\n+\n+        kept2 = torch.ones(coor.shape[0], device=coor.device, dtype=torch.bool)\n+        kept2[1:] = ranks[1:] != ranks[:-1]\n+        coor, depth = coor[kept2], depth[kept2]\n+        coor = coor.to(torch.long)\n+        depth_map[coor[:, 1], coor[:, 0]] = depth\n+        return depth_map\n+\n+    def __call__(self, results):\n+        points_lidar = results[\"points\"]\n+        imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n+        # post_rots, post_trans, bda = results[\"img_inputs\"][4:]\n+        post_rots, post_trans = results[\"img_inputs\"][4:]\n+        depth_map_list = []\n+        for cid in range(len(results[\"cam_names\"])):\n+            cam_name = results[\"cam_names\"][cid]\n+            lidar2lidarego = np.eye(4, dtype=np.float32)\n+            lidar2lidarego[:3, :3] = Quaternion(\n+                results[\"curr\"][\"lidar2ego_rotation\"]\n+            ).rotation_matrix\n+            lidar2lidarego[:3, 3] = results[\"curr\"][\"lidar2ego_translation\"]\n+            lidar2lidarego = torch.from_numpy(lidar2lidarego)\n+\n+            lidarego2global = np.eye(4, dtype=np.float32)\n+            lidarego2global[:3, :3] = Quaternion(\n+                results[\"curr\"][\"ego2global_rotation\"]\n+            ).rotation_matrix\n+            lidarego2global[:3, 3] = results[\"curr\"][\"ego2global_translation\"]\n+            lidarego2global = torch.from_numpy(lidarego2global)\n+\n+            cam2camego = np.eye(4, dtype=np.float32)\n+            cam2camego[:3, :3] = Quaternion(\n+                results[\"curr\"][\"cams\"][cam_name][\"sensor2ego_rotation\"]\n+            ).rotation_matrix\n+            cam2camego[:3, 3] = results[\"curr\"][\"cams\"][cam_name][\n+                \"sensor2ego_translation\"\n+            ]\n+            cam2camego = torch.from_numpy(cam2camego)\n+\n+            camego2global = np.eye(4, dtype=np.float32)\n+            camego2global[:3, :3] = Quaternion(\n+                results[\"curr\"][\"cams\"][cam_name][\"ego2global_rotation\"]\n+            ).rotation_matrix\n+            camego2global[:3, 3] = results[\"curr\"][\"cams\"][cam_name][\n+                \"ego2global_translation\"\n+            ]\n+            camego2global = torch.from_numpy(camego2global)\n+\n+            cam2img = np.eye(4, dtype=np.float32)\n+            cam2img = torch.from_numpy(cam2img)\n+            cam2img[:3, :3] = intrins[cid]\n+\n+            lidar2cam = torch.inverse(camego2global.matmul(cam2camego)).matmul(\n+                lidarego2global.matmul(lidar2lidarego)\n+            )\n+            lidar2img = cam2img.matmul(lidar2cam)\n+            points_img = points_lidar.tensor[:, :3].matmul(\n+                lidar2img[:3, :3].T\n+            ) + lidar2img[:3, 3].unsqueeze(0)\n+            points_img = torch.cat(\n+                [points_img[:, :2] / points_img[:, 2:3], points_img[:, 2:3]], 1\n+            )\n+            points_img = (\n+                points_img.matmul(post_rots[cid].T) + post_trans[cid : cid + 1, :]\n+            )\n+            depth_map = self.points2depthmap(points_img, imgs.shape[2], imgs.shape[3])\n+            depth_map_list.append(depth_map)\n+        depth_map = torch.stack(depth_map_list)\n+        results[\"gt_depth\"] = depth_map\n+        return results\n+\n+\n+@PIPELINES.register_module()\n+class PointToMultiViewDepthFusion(PointToMultiViewDepth):\n+    def __call__(self, results):\n+        points_camego_aug = results[\"points\"].tensor[:, :3]\n+        # print(points_lidar.shape)\n+        imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n+        post_rots, post_trans, bda = results[\"img_inputs\"][4:]\n+        points_camego = points_camego_aug - bda[:3, 3].view(1, 3)\n+        points_camego = points_camego.matmul(torch.inverse(bda[:3, :3]).T)\n+\n+        depth_map_list = []\n+        for cid in range(len(results[\"cam_names\"])):\n+            cam_name = results[\"cam_names\"][cid]\n+\n+            cam2camego = np.eye(4, dtype=np.float32)\n+            cam2camego[:3, :3] = Quaternion(\n+                results[\"curr\"][\"cams\"][cam_name][\"sensor2ego_rotation\"]\n+            ).rotation_matrix\n+            cam2camego[:3, 3] = results[\"curr\"][\"cams\"][cam_name][\n+                \"sensor2ego_translation\"\n+            ]\n+            cam2camego = torch.from_numpy(cam2camego)\n+\n+            cam2img = np.eye(4, dtype=np.float32)\n+            cam2img = torch.from_numpy(cam2img)\n+            cam2img[:3, :3] = intrins[cid]\n+\n+            camego2img = cam2img.matmul(torch.inverse(cam2camego))\n+\n+            points_img = points_camego.matmul(camego2img[:3, :3].T) + camego2img[\n+                :3, 3\n+            ].unsqueeze(0)\n+            points_img = torch.cat(\n+                [points_img[:, :2] / points_img[:, 2:3], points_img[:, 2:3]], 1\n+            )\n+            points_img = (\n+                points_img.matmul(post_rots[cid].T) + post_trans[cid : cid + 1, :]\n+            )\n+            depth_map = self.points2depthmap(points_img, imgs.shape[2], imgs.shape[3])\n+            depth_map_list.append(depth_map)\n+        depth_map = torch.stack(depth_map_list)\n+        results[\"gt_depth\"] = depth_map\n+        return results\n+\n+\n+def mmlabNormalize(img):\n+    from mmcv.image.photometric import imnormalize\n+\n+    mean = np.array([123.675, 116.28, 103.53], dtype=np.float32)\n+    std = np.array([58.395, 57.12, 57.375], dtype=np.float32)\n+    to_rgb = True\n+    img = imnormalize(np.array(img), mean, std, to_rgb)\n+    img = torch.tensor(img).float().permute(2, 0, 1).contiguous()\n+    return img\n+\n+\n+@PIPELINES.register_module()\n+class PrepareImageInputs(object):\n+    \"\"\"Load multi channel images from a list of separate channel files.\n+\n+    Expects results['img_filename'] to be a list of filenames.\n+\n+    Args:\n+        to_float32 (bool): Whether to convert the img to float32.\n+            Defaults to False.\n+        color_type (str): Color type of the file. Defaults to 'unchanged'.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        data_config,\n+        is_train=False,\n+        sequential=False,\n+        opencv_pp=False,\n+    ):\n+        self.is_train = is_train\n+        self.data_config = data_config\n+        self.normalize_img = mmlabNormalize\n+        self.sequential = sequential\n+        self.opencv_pp = opencv_pp\n+\n+    def get_rot(self, h):\n+        return torch.Tensor(\n+            [\n+                [np.cos(h), np.sin(h)],\n+                [-np.sin(h), np.cos(h)],\n+            ]\n+        )\n+\n+    def img_transform(\n+        self, img, post_rot, post_tran, resize, resize_dims, crop, flip, rotate\n+    ):\n+        # adjust image\n+        if not self.opencv_pp:\n+            img = self.img_transform_core(img, resize_dims, crop, flip, rotate)\n+\n+        # post-homography transformation\n+        post_rot *= resize\n+        post_tran -= torch.Tensor(crop[:2])\n+        if flip:\n+            A = torch.Tensor([[-1, 0], [0, 1]])\n+            b = torch.Tensor([crop[2] - crop[0], 0])\n+            post_rot = A.matmul(post_rot)\n+            post_tran = A.matmul(post_tran) + b\n+        A = self.get_rot(rotate / 180 * np.pi)\n+        b = torch.Tensor([crop[2] - crop[0], crop[3] - crop[1]]) / 2\n+        b = A.matmul(-b) + b\n+        post_rot = A.matmul(post_rot)\n+        post_tran = A.matmul(post_tran) + b\n+        if self.opencv_pp:\n+            img = self.img_transform_core_opencv(img, post_rot, post_tran, crop)\n+        return img, post_rot, post_tran\n+\n+    def img_transform_core_opencv(self, img, post_rot, post_tran, crop):\n+        img = np.array(img).astype(np.float32)\n+        img = cv2.warpAffine(\n+            img,\n+            np.concatenate([post_rot, post_tran.reshape(2, 1)], axis=1),\n+            (crop[2] - crop[0], crop[3] - crop[1]),\n+            flags=cv2.INTER_LINEAR,\n+        )\n+        return img\n+\n+    def img_transform_core(self, img, resize_dims, crop, flip, rotate):\n+        # adjust image\n+        img = img.resize(resize_dims)\n+        img = img.crop(crop)\n+        if flip:\n+            img = img.transpose(method=Image.FLIP_LEFT_RIGHT)\n+        img = img.rotate(rotate)\n+        return img\n+\n+    def choose_cams(self):\n+        if self.is_train and self.data_config[\"Ncams\"] < len(self.data_config[\"cams\"]):\n+            cam_names = np.random.choice(\n+                self.data_config[\"cams\"], self.data_config[\"Ncams\"], replace=False\n+            )\n+        else:\n+            cam_names = self.data_config[\"cams\"]\n+        return cam_names\n+\n+    def sample_augmentation(self, H, W, flip=None, scale=None):\n+        fH, fW = self.data_config[\"input_size\"]\n+        if self.is_train:\n+            resize = float(fW) / float(W)\n+            resize += np.random.uniform(*self.data_config[\"resize\"])\n+            resize_dims = (int(W * resize), int(H * resize))\n+            newW, newH = resize_dims\n+            random_crop_height = self.data_config.get(\"random_crop_height\", False)\n+            if random_crop_height:\n+                crop_h = int(np.random.uniform(max(0.3 * newH, newH - fH), newH - fH))\n+            else:\n+                crop_h = (\n+                    int((1 - np.random.uniform(*self.data_config[\"crop_h\"])) * newH)\n+                    - fH\n+                )\n+            crop_w = int(np.random.uniform(0, max(0, newW - fW)))\n+            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n+            flip = self.data_config[\"flip\"] and np.random.choice([0, 1])\n+            rotate = np.random.uniform(*self.data_config[\"rot\"])\n+            if self.data_config.get(\"vflip\", False) and np.random.choice([0, 1]):\n+                rotate += 180\n+        else:\n+            resize = float(fW) / float(W)\n+            if scale is not None:\n+                resize += scale\n+            else:\n+                resize += self.data_config.get(\"resize_test\", 0.0)\n+            resize_dims = (int(W * resize), int(H * resize))\n+            newW, newH = resize_dims\n+            crop_h = int((1 - np.mean(self.data_config[\"crop_h\"])) * newH) - fH\n+            crop_w = int(max(0, newW - fW) / 2)\n+            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n+            flip = False if flip is None else flip\n+            rotate = 0\n+        return resize, resize_dims, crop, flip, rotate\n+\n+    def get_sensor_transforms(self, cam_info, cam_name):\n+        w, x, y, z = cam_info[\"cams\"][cam_name][\"sensor2ego_rotation\"]\n+        # sweep sensor to sweep ego\n+        sensor2ego_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n+        sensor2ego_tran = torch.Tensor(\n+            cam_info[\"cams\"][cam_name][\"sensor2ego_translation\"]\n+        )\n+        sensor2ego = sensor2ego_rot.new_zeros((4, 4))\n+        sensor2ego[3, 3] = 1\n+        sensor2ego[:3, :3] = sensor2ego_rot\n+        sensor2ego[:3, -1] = sensor2ego_tran\n+        # sweep ego to global\n+        w, x, y, z = cam_info[\"cams\"][cam_name][\"ego2global_rotation\"]\n+        ego2global_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n+        ego2global_tran = torch.Tensor(\n+            cam_info[\"cams\"][cam_name][\"ego2global_translation\"]\n+        )\n+        ego2global = ego2global_rot.new_zeros((4, 4))\n+        ego2global[3, 3] = 1\n+        ego2global[:3, :3] = ego2global_rot\n+        ego2global[:3, -1] = ego2global_tran\n+        return sensor2ego, ego2global\n+\n+    def photo_metric_distortion(self, img, pmd):\n+        \"\"\"Call function to perform photometric distortion on images.\n+        Args:\n+            results (dict): Result dict from loading pipeline.\n+        Returns:\n+            dict: Result dict with images distorted.\n+        \"\"\"\n+        if np.random.rand() > pmd.get(\"rate\", 1.0):\n+            return img\n+\n+        img = np.array(img).astype(np.float32)\n+        assert img.dtype == np.float32, (\n+            \"PhotoMetricDistortion needs the input image of dtype np.float32,\"\n+            ' please set \"to_float32=True\" in \"LoadImageFromFile\" pipeline'\n+        )\n+        # random brightness\n+        if np.random.randint(2):\n+            delta = np.random.uniform(-pmd[\"brightness_delta\"], pmd[\"brightness_delta\"])\n+            img += delta\n+\n+        # mode == 0 --> do random contrast first\n+        # mode == 1 --> do random contrast last\n+        mode = np.random.randint(2)\n+        if mode == 1:\n+            if np.random.randint(2):\n+                alpha = np.random.uniform(pmd[\"contrast_lower\"], pmd[\"contrast_upper\"])\n+                img *= alpha\n+\n+        # convert color from BGR to HSV\n+        img = mmcv.bgr2hsv(img)\n+\n+        # random saturation\n+        if np.random.randint(2):\n+            img[..., 1] *= np.random.uniform(\n+                pmd[\"saturation_lower\"], pmd[\"saturation_upper\"]\n+            )\n+\n+        # random hue\n+        if np.random.randint(2):\n+            img[..., 0] += np.random.uniform(-pmd[\"hue_delta\"], pmd[\"hue_delta\"])\n+            img[..., 0][img[..., 0] > 360] -= 360\n+            img[..., 0][img[..., 0] < 0] += 360\n+\n+        # convert color from HSV to BGR\n+        img = mmcv.hsv2bgr(img)\n+\n+        # random contrast\n+        if mode == 0:\n+            if np.random.randint(2):\n+                alpha = np.random.uniform(pmd[\"contrast_lower\"], pmd[\"contrast_upper\"])\n+                img *= alpha\n+\n+        # randomly swap channels\n+        if np.random.randint(2):\n+            img = img[..., np.random.permutation(3)]\n+        return Image.fromarray(img.astype(np.uint8))\n+\n+    def get_inputs(self, results, flip=None, scale=None):\n+        imgs = []\n+        sensor2egos = []\n+        ego2globals = []\n+        intrins = []\n+        post_rots = []\n+        post_trans = []\n+        cam_names = self.choose_cams()\n+        results[\"cam_names\"] = cam_names\n+        canvas = []\n+        for cam_name in cam_names:\n+            cam_data = results[\"curr\"][\"cams\"][cam_name]\n+            filename = cam_data[\"data_path\"]\n+            img = Image.open(filename)\n+            post_rot = torch.eye(2)\n+            post_tran = torch.zeros(2)\n+\n+            intrin = torch.Tensor(cam_data[\"cam_intrinsic\"])\n+\n+            sensor2ego, ego2global = self.get_sensor_transforms(\n+                results[\"curr\"], cam_name\n+            )\n+            # image view augmentation (resize, crop, horizontal flip, rotate)\n+            img_augs = self.sample_augmentation(\n+                H=img.height, W=img.width, flip=flip, scale=scale\n+            )\n+            resize, resize_dims, crop, flip, rotate = img_augs\n+            img, post_rot2, post_tran2 = self.img_transform(\n+                img,\n+                post_rot,\n+                post_tran,\n+                resize=resize,\n+                resize_dims=resize_dims,\n+                crop=crop,\n+                flip=flip,\n+                rotate=rotate,\n+            )\n+\n+            # for convenience, make augmentation matrices 3x3\n+            post_tran = torch.zeros(3)\n+            post_rot = torch.eye(3)\n+            post_tran[:2] = post_tran2\n+            post_rot[:2, :2] = post_rot2\n+\n+            if self.is_train and self.data_config.get(\"pmd\", None) is not None:\n+                img = self.photo_metric_distortion(img, self.data_config[\"pmd\"])\n+\n+            canvas.append(np.array(img))\n+            imgs.append(self.normalize_img(img))\n+\n+            if self.sequential:\n+                assert \"adjacent\" in results\n+                for adj_info in results[\"adjacent\"]:\n+                    filename_adj = adj_info[\"cams\"][cam_name][\"data_path\"]\n+                    img_adjacent = Image.open(filename_adj)\n+                    if self.opencv_pp:\n+                        img_adjacent = self.img_transform_core_opencv(\n+                            img_adjacent, post_rot[:2, :2], post_tran[:2], crop\n+                        )\n+                    else:\n+                        img_adjacent = self.img_transform_core(\n+                            img_adjacent,\n+                            resize_dims=resize_dims,\n+                            crop=crop,\n+                            flip=flip,\n+                            rotate=rotate,\n+                        )\n+                    imgs.append(self.normalize_img(img_adjacent))\n+            intrins.append(intrin)\n+            sensor2egos.append(sensor2ego)\n+            ego2globals.append(ego2global)\n+            post_rots.append(post_rot)\n+            post_trans.append(post_tran)\n+\n+        if self.sequential:\n+            for adj_info in results[\"adjacent\"]:\n+                post_trans.extend(post_trans[: len(cam_names)])\n+                post_rots.extend(post_rots[: len(cam_names)])\n+                intrins.extend(intrins[: len(cam_names)])\n+\n+                # align\n+                for cam_name in cam_names:\n+                    sensor2ego, ego2global = self.get_sensor_transforms(\n+                        adj_info, cam_name\n+                    )\n+                    sensor2egos.append(sensor2ego)\n+                    ego2globals.append(ego2global)\n+\n+        imgs = torch.stack(imgs)\n+\n+        sensor2egos = torch.stack(sensor2egos)\n+        ego2globals = torch.stack(ego2globals)\n+        intrins = torch.stack(intrins)\n+        post_rots = torch.stack(post_rots)\n+        post_trans = torch.stack(post_trans)\n+        results[\"canvas\"] = canvas\n+        return (imgs, sensor2egos, ego2globals, intrins, post_rots, post_trans)\n+\n+    def __call__(self, results):\n+        results[\"img_inputs\"] = self.get_inputs(results)\n+        return results\n+\n+\n+@PIPELINES.register_module()\n+class LoadAnnotations(object):\n+\n+    def __call__(self, results):\n+        gt_boxes, gt_labels = results[\"ann_infos\"]\n+        gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n+        if len(gt_boxes) == 0:\n+            gt_boxes = torch.zeros(0, 9)\n+        results[\"gt_bboxes_3d\"] = LiDARInstance3DBoxes(\n+            gt_boxes, box_dim=gt_boxes.shape[-1], origin=(0.5, 0.5, 0.5)\n+        )\n+        results[\"gt_labels_3d\"] = gt_labels\n+        return results\n+\n+\n+@PIPELINES.register_module()\n+class LoadAnnotationsBEVDepth(object):\n+    def __init__(self, bda_aug_conf, classes, is_train=True):\n+        self.bda_aug_conf = bda_aug_conf\n+        self.is_train = is_train\n+        self.classes = classes\n+\n+    def sample_bda_augmentation(self):\n+        \"\"\"Generate bda augmentation values based on bda_config.\"\"\"\n+        if self.is_train:\n+            rotate_bda = np.random.uniform(*self.bda_aug_conf[\"rot_lim\"])\n+            scale_bda = np.random.uniform(*self.bda_aug_conf[\"scale_lim\"])\n+            flip_dx = np.random.uniform() < self.bda_aug_conf[\"flip_dx_ratio\"]\n+            flip_dy = np.random.uniform() < self.bda_aug_conf[\"flip_dy_ratio\"]\n+        else:\n+            rotate_bda = 0\n+            scale_bda = 1.0\n+            flip_dx = False\n+            flip_dy = False\n+        return rotate_bda, scale_bda, flip_dx, flip_dy\n+\n+    def bev_transform(self, gt_boxes, rotate_angle, scale_ratio, flip_dx, flip_dy):\n+        rotate_angle = torch.tensor(rotate_angle / 180 * np.pi)\n+        rot_sin = torch.sin(rotate_angle)\n+        rot_cos = torch.cos(rotate_angle)\n+        rot_mat = torch.Tensor(\n+            [[rot_cos, -rot_sin, 0], [rot_sin, rot_cos, 0], [0, 0, 1]]\n+        )\n+        scale_mat = torch.Tensor(\n+            [[scale_ratio, 0, 0], [0, scale_ratio, 0], [0, 0, scale_ratio]]\n+        )\n+        flip_mat = torch.Tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        if flip_dx:\n+            flip_mat = flip_mat @ torch.Tensor([[-1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        if flip_dy:\n+            flip_mat = flip_mat @ torch.Tensor([[1, 0, 0], [0, -1, 0], [0, 0, 1]])\n+        rot_mat = flip_mat @ (scale_mat @ rot_mat)\n+        if gt_boxes.shape[0] > 0:\n+            gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n+            gt_boxes[:, 3:6] *= scale_ratio\n+            gt_boxes[:, 6] += rotate_angle\n+            if flip_dx:\n+                gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n+            if flip_dy:\n+                gt_boxes[:, 6] = -gt_boxes[:, 6]\n+            gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(\n+                -1\n+            )\n+        return gt_boxes, rot_mat\n+\n+    def __call__(self, results):\n+        gt_boxes, gt_labels = results[\"ann_infos\"]\n+        gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n+        rotate_bda, scale_bda, flip_dx, flip_dy = self.sample_bda_augmentation()\n+        bda_mat = torch.zeros(4, 4)\n+        bda_mat[3, 3] = 1\n+        gt_boxes, bda_rot = self.bev_transform(\n+            gt_boxes, rotate_bda, scale_bda, flip_dx, flip_dy\n+        )\n+        bda_mat[:3, :3] = bda_rot\n+        if len(gt_boxes) == 0:\n+            gt_boxes = torch.zeros(0, 9)\n+        results[\"gt_bboxes_3d\"] = LiDARInstance3DBoxes(\n+            gt_boxes, box_dim=gt_boxes.shape[-1], origin=(0.5, 0.5, 0.5)\n+        )\n+        results[\"gt_labels_3d\"] = gt_labels\n+        imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n+        post_rots, post_trans = results[\"img_inputs\"][4:]\n+        results[\"img_inputs\"] = (\n+            imgs,\n+            rots,\n+            trans,\n+            intrins,\n+            post_rots,\n+            post_trans,\n+            # bda_rot,\n+            bda_mat,\n+        )\n+        if \"voxel_semantics\" in results:\n+            if flip_dx:\n+                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n+                    ::-1, ...\n+                ].copy()\n+                results[\"mask_lidar\"] = results[\"mask_lidar\"][::-1, ...].copy()\n+                results[\"mask_camera\"] = results[\"mask_camera\"][::-1, ...].copy()\n+            if flip_dy:\n+                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n+                    :, ::-1, ...\n+                ].copy()\n+                results[\"mask_lidar\"] = results[\"mask_lidar\"][:, ::-1, ...].copy()\n+                results[\"mask_camera\"] = results[\"mask_camera\"][:, ::-1, ...].copy()\n+        return results\n+\n+\n+@PIPELINES.register_module()\n+class BEVAug(object):\n+\n+    def __init__(self, bda_aug_conf, classes, is_train=True):\n+        self.bda_aug_conf = bda_aug_conf\n+        self.is_train = is_train\n+        self.classes = classes\n+\n+    def sample_bda_augmentation(self):\n+        \"\"\"Generate bda augmentation values based on bda_config.\"\"\"\n+        if self.is_train:\n+            rotate_bda = np.random.uniform(*self.bda_aug_conf[\"rot_lim\"])\n+            scale_bda = np.random.uniform(*self.bda_aug_conf[\"scale_lim\"])\n+            flip_dx = np.random.uniform() < self.bda_aug_conf[\"flip_dx_ratio\"]\n+            flip_dy = np.random.uniform() < self.bda_aug_conf[\"flip_dy_ratio\"]\n+            translation_std = self.bda_aug_conf.get(\"tran_lim\", [0.0, 0.0, 0.0])\n+            tran_bda = np.random.normal(scale=translation_std, size=3).T\n+        else:\n+            rotate_bda = 0\n+            scale_bda = 1.0\n+            flip_dx = False\n+            flip_dy = False\n+            tran_bda = np.zeros((1, 3), dtype=np.float32)\n+        return rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda\n+\n+    def bev_transform(\n+        self, gt_boxes, rotate_angle, scale_ratio, flip_dx, flip_dy, tran_bda\n+    ):\n+        rotate_angle = torch.tensor(rotate_angle / 180 * np.pi)\n+        rot_sin = torch.sin(rotate_angle)\n+        rot_cos = torch.cos(rotate_angle)\n+        rot_mat = torch.Tensor(\n+            [[rot_cos, -rot_sin, 0], [rot_sin, rot_cos, 0], [0, 0, 1]]\n+        )\n+        scale_mat = torch.Tensor(\n+            [[scale_ratio, 0, 0], [0, scale_ratio, 0], [0, 0, scale_ratio]]\n+        )\n+        flip_mat = torch.Tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        if flip_dx:\n+            flip_mat = flip_mat @ torch.Tensor([[-1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        if flip_dy:\n+            flip_mat = flip_mat @ torch.Tensor([[1, 0, 0], [0, -1, 0], [0, 0, 1]])\n+        rot_mat = flip_mat @ (scale_mat @ rot_mat)\n+        if gt_boxes.shape[0] > 0:\n+            gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n+            gt_boxes[:, 3:6] *= scale_ratio\n+            gt_boxes[:, 6] += rotate_angle\n+            if flip_dx:\n+                gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n+            if flip_dy:\n+                gt_boxes[:, 6] = -gt_boxes[:, 6]\n+            gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(\n+                -1\n+            )\n+            gt_boxes[:, :3] = gt_boxes[:, :3] + tran_bda\n+        return gt_boxes, rot_mat\n+\n+    def __call__(self, results):\n+        gt_boxes = results[\"gt_bboxes_3d\"].tensor\n+        gt_boxes[:, 2] = gt_boxes[:, 2] + 0.5 * gt_boxes[:, 5]\n+        rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda = (\n+            self.sample_bda_augmentation()\n+        )\n+        bda_mat = torch.zeros(4, 4)\n+        bda_mat[3, 3] = 1\n+        gt_boxes, bda_rot = self.bev_transform(\n+            gt_boxes, rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda\n+        )\n+        if \"points\" in results:\n+            points = results[\"points\"].tensor\n+            points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n+            points[:, :3] = points_aug + tran_bda\n+            points = results[\"points\"].new_point(points)\n+            results[\"points\"] = points\n+\n+        if \"radar\" in results:\n+            points = results[\"radar\"].tensor\n+            points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n+            points[:, :3] = points_aug + tran_bda\n+            points = results[\"radar\"].new_point(points)\n+            results[\"radar\"] = points\n+\n+        bda_mat[:3, :3] = bda_rot\n+        bda_mat[:3, 3] = torch.from_numpy(tran_bda)\n+        if len(gt_boxes) == 0:\n+            gt_boxes = torch.zeros(0, 9)\n+        results[\"gt_bboxes_3d\"] = LiDARInstance3DBoxes(\n+            gt_boxes, box_dim=gt_boxes.shape[-1], origin=(0.5, 0.5, 0.5)\n+        )\n+        if \"img_inputs\" in results:\n+            imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n+            post_rots, post_trans = results[\"img_inputs\"][4:]\n+            results[\"img_inputs\"] = (\n+                imgs,\n+                rots,\n+                trans,\n+                intrins,\n+                post_rots,\n+                post_trans,\n+                bda_mat,\n+            )\n+        if \"voxel_semantics\" in results:\n+            if flip_dx:\n+                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n+                    ::-1, ...\n+                ].copy()\n+                results[\"mask_lidar\"] = results[\"mask_lidar\"][::-1, ...].copy()\n+                results[\"mask_camera\"] = results[\"mask_camera\"][::-1, ...].copy()\n+            if flip_dy:\n+                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n+                    :, ::-1, ...\n+                ].copy()\n+                results[\"mask_lidar\"] = results[\"mask_lidar\"][:, ::-1, ...].copy()\n+                results[\"mask_camera\"] = results[\"mask_camera\"][:, ::-1, ...].copy()\n+        return results\n"
                },
                {
                    "date": 1716795347174,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1575,1582 +1575,4 @@\n                 ].copy()\n                 results[\"mask_lidar\"] = results[\"mask_lidar\"][:, ::-1, ...].copy()\n                 results[\"mask_camera\"] = results[\"mask_camera\"][:, ::-1, ...].copy()\n         return results\n-# Copyright (c) OpenMMLab. All rights reserved.\n-import os\n-\n-import cv2\n-import mmcv\n-import numpy as np\n-import torch\n-from PIL import Image\n-from pyquaternion import Quaternion\n-\n-\n-from nuscenes.utils.data_classes import RadarPointCloud\n-\n-from mmdet3d.core.points import BasePoints, get_points_type, RadarPoints\n-from mmdet.datasets.pipelines import LoadAnnotations, LoadImageFromFile\n-from ...core.bbox import LiDARInstance3DBoxes\n-from ..builder import PIPELINES\n-\n-\n-@PIPELINES.register_module()\n-class LoadOccGTFromFile(object):\n-    def __call__(self, results):\n-        occ_gt_path = results[\"occ_gt_path\"]\n-        occ_gt_path = os.path.join(occ_gt_path, \"labels.npz\")\n-\n-        occ_labels = np.load(occ_gt_path)\n-        semantics = occ_labels[\"semantics\"]\n-        mask_lidar = occ_labels[\"mask_lidar\"]\n-        mask_camera = occ_labels[\"mask_camera\"]\n-\n-        results[\"voxel_semantics\"] = semantics\n-        results[\"mask_lidar\"] = mask_lidar\n-        results[\"mask_camera\"] = mask_camera\n-        return results\n-\n-\n-@PIPELINES.register_module()\n-class LoadMultiViewImageFromFiles(object):\n-    \"\"\"Load multi channel images from a list of separate channel files.\n-\n-    Expects results['img_filename'] to be a list of filenames.\n-\n-    Args:\n-        to_float32 (bool, optional): Whether to convert the img to float32.\n-            Defaults to False.\n-        color_type (str, optional): Color type of the file.\n-            Defaults to 'unchanged'.\n-    \"\"\"\n-\n-    def __init__(self, to_float32=False, color_type=\"unchanged\"):\n-        self.to_float32 = to_float32\n-        self.color_type = color_type\n-\n-    def __call__(self, results):\n-        \"\"\"Call function to load multi-view image from files.\n-\n-        Args:\n-            results (dict): Result dict containing multi-view image filenames.\n-\n-        Returns:\n-            dict: The result dict containing the multi-view image data.\n-                Added keys and values are described below.\n-\n-                - filename (str): Multi-view image filenames.\n-                - img (np.ndarray): Multi-view image arrays.\n-                - img_shape (tuple[int]): Shape of multi-view image arrays.\n-                - ori_shape (tuple[int]): Shape of original image arrays.\n-                - pad_shape (tuple[int]): Shape of padded image arrays.\n-                - scale_factor (float): Scale factor.\n-                - img_norm_cfg (dict): Normalization configuration of images.\n-        \"\"\"\n-        filename = results[\"img_filename\"]\n-        # img is of shape (h, w, c, num_views)\n-        img = np.stack(\n-            [mmcv.imread(name, self.color_type) for name in filename], axis=-1\n-        )\n-        if self.to_float32:\n-            img = img.astype(np.float32)\n-        results[\"filename\"] = filename\n-        # unravel to list, see `DefaultFormatBundle` in formatting.py\n-        # which will transpose each image separately and then stack into array\n-        results[\"img\"] = [img[..., i] for i in range(img.shape[-1])]\n-        results[\"img_shape\"] = img.shape\n-        results[\"ori_shape\"] = img.shape\n-        # Set initial values for default meta_keys\n-        results[\"pad_shape\"] = img.shape\n-        results[\"scale_factor\"] = 1.0\n-        num_channels = 1 if len(img.shape) < 3 else img.shape[2]\n-        results[\"img_norm_cfg\"] = dict(\n-            mean=np.zeros(num_channels, dtype=np.float32),\n-            std=np.ones(num_channels, dtype=np.float32),\n-            to_rgb=False,\n-        )\n-        return results\n-\n-    def __repr__(self):\n-        \"\"\"str: Return a string that describes the module.\"\"\"\n-        repr_str = self.__class__.__name__\n-        repr_str += f\"(to_float32={self.to_float32}, \"\n-        repr_str += f\"color_type='{self.color_type}')\"\n-        return repr_str\n-\n-\n-@PIPELINES.register_module()\n-class LoadImageFromFileMono3D(LoadImageFromFile):\n-    \"\"\"Load an image from file in monocular 3D object detection. Compared to 2D\n-    detection, additional camera parameters need to be loaded.\n-\n-    Args:\n-        kwargs (dict): Arguments are the same as those in\n-            :class:`LoadImageFromFile`.\n-    \"\"\"\n-\n-    def __call__(self, results):\n-        \"\"\"Call functions to load image and get image meta information.\n-\n-        Args:\n-            results (dict): Result dict from :obj:`mmdet.CustomDataset`.\n-\n-        Returns:\n-            dict: The dict contains loaded image and meta information.\n-        \"\"\"\n-        super().__call__(results)\n-        results[\"cam2img\"] = results[\"img_info\"][\"cam_intrinsic\"]\n-        return results\n-\n-\n-@PIPELINES.register_module()\n-class LoadPointsFromMultiSweeps(object):\n-    \"\"\"Load points from multiple sweeps.\n-\n-    This is usually used for nuScenes dataset to utilize previous sweeps.\n-\n-    Args:\n-        sweeps_num (int, optional): Number of sweeps. Defaults to 10.\n-        load_dim (int, optional): Dimension number of the loaded points.\n-            Defaults to 5.\n-        use_dim (list[int], optional): Which dimension to use.\n-            Defaults to [0, 1, 2, 4].\n-        time_dim (int, optional): Which dimension to represent the timestamps\n-            of each points. Defaults to 4.\n-        file_client_args (dict, optional): Config dict of file clients,\n-            refer to\n-            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n-            for more details. Defaults to dict(backend='disk').\n-        pad_empty_sweeps (bool, optional): Whether to repeat keyframe when\n-            sweeps is empty. Defaults to False.\n-        remove_close (bool, optional): Whether to remove close points.\n-            Defaults to False.\n-        test_mode (bool, optional): If `test_mode=True`, it will not\n-            randomly sample sweeps but select the nearest N frames.\n-            Defaults to False.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        sweeps_num=10,\n-        load_dim=5,\n-        use_dim=[0, 1, 2, 4],\n-        time_dim=4,\n-        file_client_args=dict(backend=\"disk\"),\n-        pad_empty_sweeps=False,\n-        remove_close=False,\n-        test_mode=False,\n-    ):\n-        self.load_dim = load_dim\n-        self.sweeps_num = sweeps_num\n-        self.use_dim = use_dim\n-        self.time_dim = time_dim\n-        assert (\n-            time_dim < load_dim\n-        ), f\"Expect the timestamp dimension < {load_dim}, got {time_dim}\"\n-        self.file_client_args = file_client_args.copy()\n-        self.file_client = None\n-        self.pad_empty_sweeps = pad_empty_sweeps\n-        self.remove_close = remove_close\n-        self.test_mode = test_mode\n-        assert (\n-            max(use_dim) < load_dim\n-        ), f\"Expect all used dimensions < {load_dim}, got {use_dim}\"\n-\n-    def _load_points(self, pts_filename):\n-        \"\"\"Private function to load point clouds data.\n-\n-        Args:\n-            pts_filename (str): Filename of point clouds data.\n-\n-        Returns:\n-            np.ndarray: An array containing point clouds data.\n-        \"\"\"\n-        if self.file_client is None:\n-            self.file_client = mmcv.FileClient(**self.file_client_args)\n-        try:\n-            pts_bytes = self.file_client.get(pts_filename)\n-            points = np.frombuffer(pts_bytes, dtype=np.float32)\n-        except ConnectionError:\n-            mmcv.check_file_exist(pts_filename)\n-            if pts_filename.endswith(\".npy\"):\n-                points = np.load(pts_filename)\n-            else:\n-                points = np.fromfile(pts_filename, dtype=np.float32)\n-        return points\n-\n-    def _remove_close(self, points, radius=1.0):\n-        \"\"\"Removes point too close within a certain radius from origin.\n-\n-        Args:\n-            points (np.ndarray | :obj:`BasePoints`): Sweep points.\n-            radius (float, optional): Radius below which points are removed.\n-                Defaults to 1.0.\n-\n-        Returns:\n-            np.ndarray: Points after removing.\n-        \"\"\"\n-        if isinstance(points, np.ndarray):\n-            points_numpy = points\n-        elif isinstance(points, BasePoints):\n-            points_numpy = points.tensor.numpy()\n-        else:\n-            raise NotImplementedError\n-        x_filt = np.abs(points_numpy[:, 0]) < radius\n-        y_filt = np.abs(points_numpy[:, 1]) < radius\n-        not_close = np.logical_not(np.logical_and(x_filt, y_filt))\n-        return points[not_close]\n-\n-    def __call__(self, results):\n-        \"\"\"Call function to load multi-sweep point clouds from files.\n-\n-        Args:\n-            results (dict): Result dict containing multi-sweep point cloud\n-                filenames.\n-\n-        Returns:\n-            dict: The result dict containing the multi-sweep points data.\n-                Added key and value are described below.\n-\n-                - points (np.ndarray | :obj:`BasePoints`): Multi-sweep point\n-                    cloud arrays.\n-        \"\"\"\n-        points = results[\"points\"]\n-        points.tensor[:, self.time_dim] = 0\n-        sweep_points_list = [points]\n-        ts = results[\"timestamp\"]\n-        if self.pad_empty_sweeps and len(results[\"sweeps\"]) == 0:\n-            for i in range(self.sweeps_num):\n-                if self.remove_close:\n-                    sweep_points_list.append(self._remove_close(points))\n-                else:\n-                    sweep_points_list.append(points)\n-        else:\n-            if len(results[\"sweeps\"]) <= self.sweeps_num:\n-                choices = np.arange(len(results[\"sweeps\"]))\n-            elif self.test_mode:\n-                choices = np.arange(self.sweeps_num)\n-            else:\n-                choices = np.random.choice(\n-                    len(results[\"sweeps\"]), self.sweeps_num, replace=False\n-                )\n-            for idx in choices:\n-                sweep = results[\"sweeps\"][idx]\n-                points_sweep = self._load_points(sweep[\"data_path\"])\n-                points_sweep = np.copy(points_sweep).reshape(-1, self.load_dim)\n-                if self.remove_close:\n-                    points_sweep = self._remove_close(points_sweep)\n-                sweep_ts = sweep[\"timestamp\"] / 1e6\n-                points_sweep[:, :3] = (\n-                    points_sweep[:, :3] @ sweep[\"sensor2lidar_rotation\"].T\n-                )\n-                points_sweep[:, :3] += sweep[\"sensor2lidar_translation\"]\n-                points_sweep[:, self.time_dim] = ts - sweep_ts\n-                points_sweep = points.new_point(points_sweep)\n-                sweep_points_list.append(points_sweep)\n-\n-        points = points.cat(sweep_points_list)\n-        points = points[:, self.use_dim]\n-        results[\"points\"] = points\n-        return results\n-\n-    def __repr__(self):\n-        \"\"\"str: Return a string that describes the module.\"\"\"\n-        return f\"{self.__class__.__name__}(sweeps_num={self.sweeps_num})\"\n-\n-\n-@PIPELINES.register_module()\n-class PointSegClassMapping(object):\n-    \"\"\"Map original semantic class to valid category ids.\n-\n-    Map valid classes as 0~len(valid_cat_ids)-1 and\n-    others as len(valid_cat_ids).\n-\n-    Args:\n-        valid_cat_ids (tuple[int]): A tuple of valid category.\n-        max_cat_id (int, optional): The max possible cat_id in input\n-            segmentation mask. Defaults to 40.\n-    \"\"\"\n-\n-    def __init__(self, valid_cat_ids, max_cat_id=40):\n-        assert max_cat_id >= np.max(\n-            valid_cat_ids\n-        ), \"max_cat_id should be greater than maximum id in valid_cat_ids\"\n-\n-        self.valid_cat_ids = valid_cat_ids\n-        self.max_cat_id = int(max_cat_id)\n-\n-        # build cat_id to class index mapping\n-        neg_cls = len(valid_cat_ids)\n-        self.cat_id2class = np.ones(self.max_cat_id + 1, dtype=np.int) * neg_cls\n-        for cls_idx, cat_id in enumerate(valid_cat_ids):\n-            self.cat_id2class[cat_id] = cls_idx\n-\n-    def __call__(self, results):\n-        \"\"\"Call function to map original semantic class to valid category ids.\n-\n-        Args:\n-            results (dict): Result dict containing point semantic masks.\n-\n-        Returns:\n-            dict: The result dict containing the mapped category ids.\n-                Updated key and value are described below.\n-\n-                - pts_semantic_mask (np.ndarray): Mapped semantic masks.\n-        \"\"\"\n-        assert \"pts_semantic_mask\" in results\n-        pts_semantic_mask = results[\"pts_semantic_mask\"]\n-\n-        converted_pts_sem_mask = self.cat_id2class[pts_semantic_mask]\n-\n-        results[\"pts_semantic_mask\"] = converted_pts_sem_mask\n-        return results\n-\n-    def __repr__(self):\n-        \"\"\"str: Return a string that describes the module.\"\"\"\n-        repr_str = self.__class__.__name__\n-        repr_str += f\"(valid_cat_ids={self.valid_cat_ids}, \"\n-        repr_str += f\"max_cat_id={self.max_cat_id})\"\n-        return repr_str\n-\n-\n-@PIPELINES.register_module()\n-class NormalizePointsColor(object):\n-    \"\"\"Normalize color of points.\n-\n-    Args:\n-        color_mean (list[float]): Mean color of the point cloud.\n-    \"\"\"\n-\n-    def __init__(self, color_mean):\n-        self.color_mean = color_mean\n-\n-    def __call__(self, results):\n-        \"\"\"Call function to normalize color of points.\n-\n-        Args:\n-            results (dict): Result dict containing point clouds data.\n-\n-        Returns:\n-            dict: The result dict containing the normalized points.\n-                Updated key and value are described below.\n-\n-                - points (:obj:`BasePoints`): Points after color normalization.\n-        \"\"\"\n-        points = results[\"points\"]\n-        assert (\n-            points.attribute_dims is not None\n-            and \"color\" in points.attribute_dims.keys()\n-        ), \"Expect points have color attribute\"\n-        if self.color_mean is not None:\n-            points.color = points.color - points.color.new_tensor(self.color_mean)\n-        points.color = points.color / 255.0\n-        results[\"points\"] = points\n-        return results\n-\n-    def __repr__(self):\n-        \"\"\"str: Return a string that describes the module.\"\"\"\n-        repr_str = self.__class__.__name__\n-        repr_str += f\"(color_mean={self.color_mean})\"\n-        return repr_str\n-\n-\n-@PIPELINES.register_module()\n-class LoadRadarPointsMultiSweeps(object):\n-    \"\"\"Load radar points from multiple sweeps.\n-    This is usually used for nuScenes dataset to utilize previous sweeps.\n-    Args:\n-        sweeps_num (int): Number of sweeps. Defaults to 10.\n-        load_dim (int): Dimension number of the loaded points. Defaults to 5.\n-        use_dim (list[int]): Which dimension to use. Defaults to [0, 1, 2, 4].\n-        file_client_args (dict): Config dict of file clients, refer to\n-            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n-            for more details. Defaults to dict(backend='disk').\n-        pad_empty_sweeps (bool): Whether to repeat keyframe when\n-            sweeps is empty. Defaults to False.\n-        remove_close (bool): Whether to remove close points.\n-            Defaults to False.\n-        test_mode (bool): If test_model=True used for testing, it will not\n-            randomly sample sweeps but select the nearest N frames.\n-            Defaults to False.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        load_dim=18,\n-        use_dim=[0, 1, 2, 3, 4],\n-        sweeps_num=3,\n-        file_client_args=dict(backend=\"disk\"),\n-        max_num=300,\n-        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],\n-        test_mode=False,\n-    ):\n-        self.load_dim = load_dim\n-        self.use_dim = use_dim\n-        self.sweeps_num = sweeps_num\n-        self.file_client_args = file_client_args.copy()\n-        self.file_client = None\n-        self.max_num = max_num\n-        self.test_mode = test_mode\n-        self.pc_range = pc_range\n-\n-    def _load_points(self, pts_filename):\n-        \"\"\"Private function to load point clouds data.\n-        Args:\n-            pts_filename (str): Filename of point clouds data.\n-        Returns:\n-            np.ndarray: An array containing point clouds data.\n-            [N, 18]\n-        \"\"\"\n-        radar_obj = RadarPointCloud.from_file(pts_filename)\n-\n-        # [18, N]\n-        points = radar_obj.points\n-\n-        return points.transpose().astype(np.float32)\n-\n-    def _pad_or_drop(self, points):\n-        \"\"\"\n-        points: [N, 18]\n-        \"\"\"\n-\n-        num_points = points.shape[0]\n-\n-        if num_points == self.max_num:\n-            masks = np.ones((num_points, 1), dtype=points.dtype)\n-\n-            return points, masks\n-\n-        if num_points > self.max_num:\n-            points = np.random.permutation(points)[: self.max_num, :]\n-            masks = np.ones((self.max_num, 1), dtype=points.dtype)\n-\n-            return points, masks\n-\n-        if num_points < self.max_num:\n-            zeros = np.zeros(\n-                (self.max_num - num_points, points.shape[1]), dtype=points.dtype\n-            )\n-            masks = np.ones((num_points, 1), dtype=points.dtype)\n-\n-            points = np.concatenate((points, zeros), axis=0)\n-            masks = np.concatenate((masks, zeros.copy()[:, [0]]), axis=0)\n-\n-            return points, masks\n-\n-    def __call__(self, results):\n-        \"\"\"Call function to load multi-sweep point clouds from files.\n-        Args:\n-            results (dict): Result dict containing multi-sweep point cloud \\\n-                filenames.\n-        Returns:\n-            dict: The result dict containing the multi-sweep points data. \\\n-                Added key and value are described below.\n-                - points (np.ndarray | :obj:`BasePoints`): Multi-sweep point \\\n-                    cloud arrays.\n-        \"\"\"\n-        radars_dict = results[\"radar\"]\n-\n-        points_sweep_list = []\n-        for key, sweeps in radars_dict.items():\n-            if len(sweeps) < self.sweeps_num:\n-                idxes = list(range(len(sweeps)))\n-            else:\n-                idxes = list(range(self.sweeps_num))\n-\n-            ts = sweeps[0][\"timestamp\"] * 1e-6\n-            for idx in idxes:\n-                sweep = sweeps[idx]\n-\n-                points_sweep = self._load_points(sweep[\"data_path\"])\n-                points_sweep = np.copy(points_sweep).reshape(-1, self.load_dim)\n-\n-                timestamp = sweep[\"timestamp\"] * 1e-6\n-                time_diff = ts - timestamp\n-                time_diff = np.ones((points_sweep.shape[0], 1)) * time_diff\n-\n-                # velocity compensated by the ego motion in sensor frame\n-                velo_comp = points_sweep[:, 8:10]\n-                velo_comp = np.concatenate(\n-                    (velo_comp, np.zeros((velo_comp.shape[0], 1))), 1\n-                )\n-                velo_comp = velo_comp @ sweep[\"sensor2lidar_rotation\"].T\n-                velo_comp = velo_comp[:, :2]\n-\n-                # velocity in sensor frame\n-                velo = points_sweep[:, 6:8]\n-                velo = np.concatenate((velo, np.zeros((velo.shape[0], 1))), 1)\n-                velo = velo @ sweep[\"sensor2lidar_rotation\"].T\n-                velo = velo[:, :2]\n-\n-                points_sweep[:, :3] = (\n-                    points_sweep[:, :3] @ sweep[\"sensor2lidar_rotation\"].T\n-                )\n-                points_sweep[:, :3] += sweep[\"sensor2lidar_translation\"]\n-\n-                points_sweep_ = np.concatenate(\n-                    [\n-                        points_sweep[:, :6],\n-                        velo,\n-                        velo_comp,\n-                        points_sweep[:, 10:],\n-                        time_diff,\n-                    ],\n-                    axis=1,\n-                )\n-                points_sweep_list.append(points_sweep_)\n-\n-        points = np.concatenate(points_sweep_list, axis=0)\n-\n-        points = points[:, self.use_dim]\n-\n-        points = RadarPoints(points, points_dim=points.shape[-1], attribute_dims=None)\n-\n-        results[\"radar\"] = points\n-        return results\n-\n-    def __repr__(self):\n-        \"\"\"str: Return a string that describes the module.\"\"\"\n-        return f\"{self.__class__.__name__}(sweeps_num={self.sweeps_num})\"\n-\n-\n-@PIPELINES.register_module()\n-class LoadPointsFromFile(object):\n-    \"\"\"Load Points From File.\n-\n-    Load points from file.\n-\n-    Args:\n-        coord_type (str): The type of coordinates of points cloud.\n-            Available options includes:\n-            - 'LIDAR': Points in LiDAR coordinates.\n-            - 'DEPTH': Points in depth coordinates, usually for indoor dataset.\n-            - 'CAMERA': Points in camera coordinates.\n-        load_dim (int, optional): The dimension of the loaded points.\n-            Defaults to 6.\n-        use_dim (list[int], optional): Which dimensions of the points to use.\n-            Defaults to [0, 1, 2]. For KITTI dataset, set use_dim=4\n-            or use_dim=[0, 1, 2, 3] to use the intensity dimension.\n-        shift_height (bool, optional): Whether to use shifted height.\n-            Defaults to False.\n-        use_color (bool, optional): Whether to use color features.\n-            Defaults to False.\n-        file_client_args (dict, optional): Config dict of file clients,\n-            refer to\n-            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n-            for more details. Defaults to dict(backend='disk').\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        coord_type,\n-        load_dim=6,\n-        use_dim=[0, 1, 2],\n-        shift_height=False,\n-        use_color=False,\n-        file_client_args=dict(backend=\"disk\"),\n-    ):\n-        self.shift_height = shift_height\n-        self.use_color = use_color\n-        if isinstance(use_dim, int):\n-            use_dim = list(range(use_dim))\n-        assert (\n-            max(use_dim) < load_dim\n-        ), f\"Expect all used dimensions < {load_dim}, got {use_dim}\"\n-        assert coord_type in [\"CAMERA\", \"LIDAR\", \"DEPTH\"]\n-\n-        self.coord_type = coord_type\n-        self.load_dim = load_dim\n-        self.use_dim = use_dim\n-        self.file_client_args = file_client_args.copy()\n-        self.file_client = None\n-\n-    def _load_points(self, pts_filename):\n-        \"\"\"Private function to load point clouds data.\n-\n-        Args:\n-            pts_filename (str): Filename of point clouds data.\n-\n-        Returns:\n-            np.ndarray: An array containing point clouds data.\n-        \"\"\"\n-        if self.file_client is None:\n-            self.file_client = mmcv.FileClient(**self.file_client_args)\n-        try:\n-            pts_bytes = self.file_client.get(pts_filename)\n-            points = np.frombuffer(pts_bytes, dtype=np.float32)\n-        except ConnectionError:\n-            mmcv.check_file_exist(pts_filename)\n-            if pts_filename.endswith(\".npy\"):\n-                points = np.load(pts_filename)\n-            else:\n-                points = np.fromfile(pts_filename, dtype=np.float32)\n-\n-        return points\n-\n-    def __call__(self, results):\n-        \"\"\"Call function to load points data from file.\n-\n-        Args:\n-            results (dict): Result dict containing point clouds data.\n-\n-        Returns:\n-            dict: The result dict containing the point clouds data.\n-                Added key and value are described below.\n-\n-                - points (:obj:`BasePoints`): Point clouds data.\n-        \"\"\"\n-        pts_filename = results[\"pts_filename\"]\n-        points = self._load_points(pts_filename)\n-        points = points.reshape(-1, self.load_dim)\n-        points = points[:, self.use_dim]\n-        attribute_dims = None\n-\n-        if self.shift_height:\n-            floor_height = np.percentile(points[:, 2], 0.99)\n-            height = points[:, 2] - floor_height\n-            points = np.concatenate(\n-                [points[:, :3], np.expand_dims(height, 1), points[:, 3:]], 1\n-            )\n-            attribute_dims = dict(height=3)\n-\n-        if self.use_color:\n-            assert len(self.use_dim) >= 6\n-            if attribute_dims is None:\n-                attribute_dims = dict()\n-            attribute_dims.update(\n-                dict(\n-                    color=[\n-                        points.shape[1] - 3,\n-                        points.shape[1] - 2,\n-                        points.shape[1] - 1,\n-                    ]\n-                )\n-            )\n-\n-        points_class = get_points_type(self.coord_type)\n-        points = points_class(\n-            points, points_dim=points.shape[-1], attribute_dims=attribute_dims\n-        )\n-        results[\"points\"] = points\n-\n-        return results\n-\n-    def __repr__(self):\n-        \"\"\"str: Return a string that describes the module.\"\"\"\n-        repr_str = self.__class__.__name__ + \"(\"\n-        repr_str += f\"shift_height={self.shift_height}, \"\n-        repr_str += f\"use_color={self.use_color}, \"\n-        repr_str += f\"file_client_args={self.file_client_args}, \"\n-        repr_str += f\"load_dim={self.load_dim}, \"\n-        repr_str += f\"use_dim={self.use_dim})\"\n-        return repr_str\n-\n-\n-@PIPELINES.register_module()\n-class LoadPointsFromDict(LoadPointsFromFile):\n-    \"\"\"Load Points From Dict.\"\"\"\n-\n-    def __call__(self, results):\n-        assert \"points\" in results\n-        return results\n-\n-\n-@PIPELINES.register_module()\n-class LoadAnnotations3D(LoadAnnotations):\n-    \"\"\"Load Annotations3D.\n-\n-    Load instance mask and semantic mask of points and\n-    encapsulate the items into related fields.\n-\n-    Args:\n-        with_bbox_3d (bool, optional): Whether to load 3D boxes.\n-            Defaults to True.\n-        with_label_3d (bool, optional): Whether to load 3D labels.\n-            Defaults to True.\n-        with_attr_label (bool, optional): Whether to load attribute label.\n-            Defaults to False.\n-        with_mask_3d (bool, optional): Whether to load 3D instance masks.\n-            for points. Defaults to False.\n-        with_seg_3d (bool, optional): Whether to load 3D semantic masks.\n-            for points. Defaults to False.\n-        with_bbox (bool, optional): Whether to load 2D boxes.\n-            Defaults to False.\n-        with_label (bool, optional): Whether to load 2D labels.\n-            Defaults to False.\n-        with_mask (bool, optional): Whether to load 2D instance masks.\n-            Defaults to False.\n-        with_seg (bool, optional): Whether to load 2D semantic masks.\n-            Defaults to False.\n-        with_bbox_depth (bool, optional): Whether to load 2.5D boxes.\n-            Defaults to False.\n-        poly2mask (bool, optional): Whether to convert polygon annotations\n-            to bitmasks. Defaults to True.\n-        seg_3d_dtype (dtype, optional): Dtype of 3D semantic masks.\n-            Defaults to int64\n-        file_client_args (dict): Config dict of file clients, refer to\n-            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n-            for more details.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        with_bbox_3d=True,\n-        with_label_3d=True,\n-        with_attr_label=False,\n-        with_mask_3d=False,\n-        with_seg_3d=False,\n-        with_bbox=False,\n-        with_label=False,\n-        with_mask=False,\n-        with_seg=False,\n-        with_bbox_depth=False,\n-        poly2mask=True,\n-        seg_3d_dtype=np.int64,\n-        file_client_args=dict(backend=\"disk\"),\n-    ):\n-        super().__init__(\n-            with_bbox,\n-            with_label,\n-            with_mask,\n-            with_seg,\n-            poly2mask,\n-            file_client_args=file_client_args,\n-        )\n-        self.with_bbox_3d = with_bbox_3d\n-        self.with_bbox_depth = with_bbox_depth\n-        self.with_label_3d = with_label_3d\n-        self.with_attr_label = with_attr_label\n-        self.with_mask_3d = with_mask_3d\n-        self.with_seg_3d = with_seg_3d\n-        self.seg_3d_dtype = seg_3d_dtype\n-\n-    def _load_bboxes_3d(self, results):\n-        \"\"\"Private function to load 3D bounding box annotations.\n-\n-        Args:\n-            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n-\n-        Returns:\n-            dict: The dict containing loaded 3D bounding box annotations.\n-        \"\"\"\n-        results[\"gt_bboxes_3d\"] = results[\"ann_info\"][\"gt_bboxes_3d\"]\n-        results[\"bbox3d_fields\"].append(\"gt_bboxes_3d\")\n-        return results\n-\n-    def _load_bboxes_depth(self, results):\n-        \"\"\"Private function to load 2.5D bounding box annotations.\n-\n-        Args:\n-            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n-\n-        Returns:\n-            dict: The dict containing loaded 2.5D bounding box annotations.\n-        \"\"\"\n-        results[\"centers2d\"] = results[\"ann_info\"][\"centers2d\"]\n-        results[\"depths\"] = results[\"ann_info\"][\"depths\"]\n-        return results\n-\n-    def _load_labels_3d(self, results):\n-        \"\"\"Private function to load label annotations.\n-\n-        Args:\n-            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n-\n-        Returns:\n-            dict: The dict containing loaded label annotations.\n-        \"\"\"\n-        results[\"gt_labels_3d\"] = results[\"ann_info\"][\"gt_labels_3d\"]\n-        return results\n-\n-    def _load_attr_labels(self, results):\n-        \"\"\"Private function to load label annotations.\n-\n-        Args:\n-            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n-\n-        Returns:\n-            dict: The dict containing loaded label annotations.\n-        \"\"\"\n-        results[\"attr_labels\"] = results[\"ann_info\"][\"attr_labels\"]\n-        return results\n-\n-    def _load_masks_3d(self, results):\n-        \"\"\"Private function to load 3D mask annotations.\n-\n-        Args:\n-            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n-\n-        Returns:\n-            dict: The dict containing loaded 3D mask annotations.\n-        \"\"\"\n-        pts_instance_mask_path = results[\"ann_info\"][\"pts_instance_mask_path\"]\n-\n-        if self.file_client is None:\n-            self.file_client = mmcv.FileClient(**self.file_client_args)\n-        try:\n-            mask_bytes = self.file_client.get(pts_instance_mask_path)\n-            pts_instance_mask = np.frombuffer(mask_bytes, dtype=np.int64)\n-        except ConnectionError:\n-            mmcv.check_file_exist(pts_instance_mask_path)\n-            pts_instance_mask = np.fromfile(pts_instance_mask_path, dtype=np.int64)\n-\n-        results[\"pts_instance_mask\"] = pts_instance_mask\n-        results[\"pts_mask_fields\"].append(\"pts_instance_mask\")\n-        return results\n-\n-    def _load_semantic_seg_3d(self, results):\n-        \"\"\"Private function to load 3D semantic segmentation annotations.\n-\n-        Args:\n-            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n-\n-        Returns:\n-            dict: The dict containing the semantic segmentation annotations.\n-        \"\"\"\n-        pts_semantic_mask_path = results[\"ann_info\"][\"pts_semantic_mask_path\"]\n-\n-        if self.file_client is None:\n-            self.file_client = mmcv.FileClient(**self.file_client_args)\n-        try:\n-            mask_bytes = self.file_client.get(pts_semantic_mask_path)\n-            # add .copy() to fix read-only bug\n-            pts_semantic_mask = np.frombuffer(\n-                mask_bytes, dtype=self.seg_3d_dtype\n-            ).copy()\n-        except ConnectionError:\n-            mmcv.check_file_exist(pts_semantic_mask_path)\n-            pts_semantic_mask = np.fromfile(pts_semantic_mask_path, dtype=np.int64)\n-\n-        results[\"pts_semantic_mask\"] = pts_semantic_mask\n-        results[\"pts_seg_fields\"].append(\"pts_semantic_mask\")\n-        return results\n-\n-    def __call__(self, results):\n-        \"\"\"Call function to load multiple types annotations.\n-\n-        Args:\n-            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n-\n-        Returns:\n-            dict: The dict containing loaded 3D bounding box, label, mask and\n-                semantic segmentation annotations.\n-        \"\"\"\n-        results = super().__call__(results)\n-        if self.with_bbox_3d:\n-            results = self._load_bboxes_3d(results)\n-            if results is None:\n-                return None\n-        if self.with_bbox_depth:\n-            results = self._load_bboxes_depth(results)\n-            if results is None:\n-                return None\n-        if self.with_label_3d:\n-            results = self._load_labels_3d(results)\n-        if self.with_attr_label:\n-            results = self._load_attr_labels(results)\n-        if self.with_mask_3d:\n-            results = self._load_masks_3d(results)\n-        if self.with_seg_3d:\n-            results = self._load_semantic_seg_3d(results)\n-\n-        return results\n-\n-    def __repr__(self):\n-        \"\"\"str: Return a string that describes the module.\"\"\"\n-        indent_str = \"    \"\n-        repr_str = self.__class__.__name__ + \"(\\n\"\n-        repr_str += f\"{indent_str}with_bbox_3d={self.with_bbox_3d}, \"\n-        repr_str += f\"{indent_str}with_label_3d={self.with_label_3d}, \"\n-        repr_str += f\"{indent_str}with_attr_label={self.with_attr_label}, \"\n-        repr_str += f\"{indent_str}with_mask_3d={self.with_mask_3d}, \"\n-        repr_str += f\"{indent_str}with_seg_3d={self.with_seg_3d}, \"\n-        repr_str += f\"{indent_str}with_bbox={self.with_bbox}, \"\n-        repr_str += f\"{indent_str}with_label={self.with_label}, \"\n-        repr_str += f\"{indent_str}with_mask={self.with_mask}, \"\n-        repr_str += f\"{indent_str}with_seg={self.with_seg}, \"\n-        repr_str += f\"{indent_str}with_bbox_depth={self.with_bbox_depth}, \"\n-        repr_str += f\"{indent_str}poly2mask={self.poly2mask})\"\n-        return repr_str\n-\n-\n-@PIPELINES.register_module()\n-class PointToMultiViewDepth(object):\n-\n-    def __init__(self, grid_config, downsample=1):\n-        self.downsample = downsample\n-        self.grid_config = grid_config\n-\n-    def points2depthmap(self, points, height, width):\n-        height, width = height // self.downsample, width // self.downsample\n-        depth_map = torch.zeros((height, width), dtype=torch.float32)\n-        coor = torch.round(points[:, :2] / self.downsample)\n-        depth = points[:, 2]\n-        kept1 = (\n-            (coor[:, 0] >= 0)\n-            & (coor[:, 0] < width)\n-            & (coor[:, 1] >= 0)\n-            & (coor[:, 1] < height)\n-            & (depth < self.grid_config[\"depth\"][1])\n-            & (depth >= self.grid_config[\"depth\"][0])\n-        )\n-        coor, depth = coor[kept1], depth[kept1]\n-        ranks = coor[:, 0] + coor[:, 1] * width\n-        sort = (ranks + depth / 100.0).argsort()\n-        coor, depth, ranks = coor[sort], depth[sort], ranks[sort]\n-\n-        kept2 = torch.ones(coor.shape[0], device=coor.device, dtype=torch.bool)\n-        kept2[1:] = ranks[1:] != ranks[:-1]\n-        coor, depth = coor[kept2], depth[kept2]\n-        coor = coor.to(torch.long)\n-        depth_map[coor[:, 1], coor[:, 0]] = depth\n-        return depth_map\n-\n-    def __call__(self, results):\n-        points_lidar = results[\"points\"]\n-        imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n-        # post_rots, post_trans, bda = results[\"img_inputs\"][4:]\n-        post_rots, post_trans = results[\"img_inputs\"][4:]\n-        depth_map_list = []\n-        for cid in range(len(results[\"cam_names\"])):\n-            cam_name = results[\"cam_names\"][cid]\n-            lidar2lidarego = np.eye(4, dtype=np.float32)\n-            lidar2lidarego[:3, :3] = Quaternion(\n-                results[\"curr\"][\"lidar2ego_rotation\"]\n-            ).rotation_matrix\n-            lidar2lidarego[:3, 3] = results[\"curr\"][\"lidar2ego_translation\"]\n-            lidar2lidarego = torch.from_numpy(lidar2lidarego)\n-\n-            lidarego2global = np.eye(4, dtype=np.float32)\n-            lidarego2global[:3, :3] = Quaternion(\n-                results[\"curr\"][\"ego2global_rotation\"]\n-            ).rotation_matrix\n-            lidarego2global[:3, 3] = results[\"curr\"][\"ego2global_translation\"]\n-            lidarego2global = torch.from_numpy(lidarego2global)\n-\n-            cam2camego = np.eye(4, dtype=np.float32)\n-            cam2camego[:3, :3] = Quaternion(\n-                results[\"curr\"][\"cams\"][cam_name][\"sensor2ego_rotation\"]\n-            ).rotation_matrix\n-            cam2camego[:3, 3] = results[\"curr\"][\"cams\"][cam_name][\n-                \"sensor2ego_translation\"\n-            ]\n-            cam2camego = torch.from_numpy(cam2camego)\n-\n-            camego2global = np.eye(4, dtype=np.float32)\n-            camego2global[:3, :3] = Quaternion(\n-                results[\"curr\"][\"cams\"][cam_name][\"ego2global_rotation\"]\n-            ).rotation_matrix\n-            camego2global[:3, 3] = results[\"curr\"][\"cams\"][cam_name][\n-                \"ego2global_translation\"\n-            ]\n-            camego2global = torch.from_numpy(camego2global)\n-\n-            cam2img = np.eye(4, dtype=np.float32)\n-            cam2img = torch.from_numpy(cam2img)\n-            cam2img[:3, :3] = intrins[cid]\n-\n-            lidar2cam = torch.inverse(camego2global.matmul(cam2camego)).matmul(\n-                lidarego2global.matmul(lidar2lidarego)\n-            )\n-            lidar2img = cam2img.matmul(lidar2cam)\n-            points_img = points_lidar.tensor[:, :3].matmul(\n-                lidar2img[:3, :3].T\n-            ) + lidar2img[:3, 3].unsqueeze(0)\n-            points_img = torch.cat(\n-                [points_img[:, :2] / points_img[:, 2:3], points_img[:, 2:3]], 1\n-            )\n-            points_img = (\n-                points_img.matmul(post_rots[cid].T) + post_trans[cid : cid + 1, :]\n-            )\n-            depth_map = self.points2depthmap(points_img, imgs.shape[2], imgs.shape[3])\n-            depth_map_list.append(depth_map)\n-        depth_map = torch.stack(depth_map_list)\n-        results[\"gt_depth\"] = depth_map\n-        return results\n-\n-\n-@PIPELINES.register_module()\n-class PointToMultiViewDepthFusion(PointToMultiViewDepth):\n-    def __call__(self, results):\n-        points_camego_aug = results[\"points\"].tensor[:, :3]\n-        # print(points_lidar.shape)\n-        imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n-        post_rots, post_trans, bda = results[\"img_inputs\"][4:]\n-        points_camego = points_camego_aug - bda[:3, 3].view(1, 3)\n-        points_camego = points_camego.matmul(torch.inverse(bda[:3, :3]).T)\n-\n-        depth_map_list = []\n-        for cid in range(len(results[\"cam_names\"])):\n-            cam_name = results[\"cam_names\"][cid]\n-\n-            cam2camego = np.eye(4, dtype=np.float32)\n-            cam2camego[:3, :3] = Quaternion(\n-                results[\"curr\"][\"cams\"][cam_name][\"sensor2ego_rotation\"]\n-            ).rotation_matrix\n-            cam2camego[:3, 3] = results[\"curr\"][\"cams\"][cam_name][\n-                \"sensor2ego_translation\"\n-            ]\n-            cam2camego = torch.from_numpy(cam2camego)\n-\n-            cam2img = np.eye(4, dtype=np.float32)\n-            cam2img = torch.from_numpy(cam2img)\n-            cam2img[:3, :3] = intrins[cid]\n-\n-            camego2img = cam2img.matmul(torch.inverse(cam2camego))\n-\n-            points_img = points_camego.matmul(camego2img[:3, :3].T) + camego2img[\n-                :3, 3\n-            ].unsqueeze(0)\n-            points_img = torch.cat(\n-                [points_img[:, :2] / points_img[:, 2:3], points_img[:, 2:3]], 1\n-            )\n-            points_img = (\n-                points_img.matmul(post_rots[cid].T) + post_trans[cid : cid + 1, :]\n-            )\n-            depth_map = self.points2depthmap(points_img, imgs.shape[2], imgs.shape[3])\n-            depth_map_list.append(depth_map)\n-        depth_map = torch.stack(depth_map_list)\n-        results[\"gt_depth\"] = depth_map\n-        return results\n-\n-\n-def mmlabNormalize(img):\n-    from mmcv.image.photometric import imnormalize\n-\n-    mean = np.array([123.675, 116.28, 103.53], dtype=np.float32)\n-    std = np.array([58.395, 57.12, 57.375], dtype=np.float32)\n-    to_rgb = True\n-    img = imnormalize(np.array(img), mean, std, to_rgb)\n-    img = torch.tensor(img).float().permute(2, 0, 1).contiguous()\n-    return img\n-\n-\n-@PIPELINES.register_module()\n-class PrepareImageInputs(object):\n-    \"\"\"Load multi channel images from a list of separate channel files.\n-\n-    Expects results['img_filename'] to be a list of filenames.\n-\n-    Args:\n-        to_float32 (bool): Whether to convert the img to float32.\n-            Defaults to False.\n-        color_type (str): Color type of the file. Defaults to 'unchanged'.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        data_config,\n-        is_train=False,\n-        sequential=False,\n-        opencv_pp=False,\n-    ):\n-        self.is_train = is_train\n-        self.data_config = data_config\n-        self.normalize_img = mmlabNormalize\n-        self.sequential = sequential\n-        self.opencv_pp = opencv_pp\n-\n-    def get_rot(self, h):\n-        return torch.Tensor(\n-            [\n-                [np.cos(h), np.sin(h)],\n-                [-np.sin(h), np.cos(h)],\n-            ]\n-        )\n-\n-    def img_transform(\n-        self, img, post_rot, post_tran, resize, resize_dims, crop, flip, rotate\n-    ):\n-        # adjust image\n-        if not self.opencv_pp:\n-            img = self.img_transform_core(img, resize_dims, crop, flip, rotate)\n-\n-        # post-homography transformation\n-        post_rot *= resize\n-        post_tran -= torch.Tensor(crop[:2])\n-        if flip:\n-            A = torch.Tensor([[-1, 0], [0, 1]])\n-            b = torch.Tensor([crop[2] - crop[0], 0])\n-            post_rot = A.matmul(post_rot)\n-            post_tran = A.matmul(post_tran) + b\n-        A = self.get_rot(rotate / 180 * np.pi)\n-        b = torch.Tensor([crop[2] - crop[0], crop[3] - crop[1]]) / 2\n-        b = A.matmul(-b) + b\n-        post_rot = A.matmul(post_rot)\n-        post_tran = A.matmul(post_tran) + b\n-        if self.opencv_pp:\n-            img = self.img_transform_core_opencv(img, post_rot, post_tran, crop)\n-        return img, post_rot, post_tran\n-\n-    def img_transform_core_opencv(self, img, post_rot, post_tran, crop):\n-        img = np.array(img).astype(np.float32)\n-        img = cv2.warpAffine(\n-            img,\n-            np.concatenate([post_rot, post_tran.reshape(2, 1)], axis=1),\n-            (crop[2] - crop[0], crop[3] - crop[1]),\n-            flags=cv2.INTER_LINEAR,\n-        )\n-        return img\n-\n-    def img_transform_core(self, img, resize_dims, crop, flip, rotate):\n-        # adjust image\n-        img = img.resize(resize_dims)\n-        img = img.crop(crop)\n-        if flip:\n-            img = img.transpose(method=Image.FLIP_LEFT_RIGHT)\n-        img = img.rotate(rotate)\n-        return img\n-\n-    def choose_cams(self):\n-        if self.is_train and self.data_config[\"Ncams\"] < len(self.data_config[\"cams\"]):\n-            cam_names = np.random.choice(\n-                self.data_config[\"cams\"], self.data_config[\"Ncams\"], replace=False\n-            )\n-        else:\n-            cam_names = self.data_config[\"cams\"]\n-        return cam_names\n-\n-    def sample_augmentation(self, H, W, flip=None, scale=None):\n-        fH, fW = self.data_config[\"input_size\"]\n-        if self.is_train:\n-            resize = float(fW) / float(W)\n-            resize += np.random.uniform(*self.data_config[\"resize\"])\n-            resize_dims = (int(W * resize), int(H * resize))\n-            newW, newH = resize_dims\n-            random_crop_height = self.data_config.get(\"random_crop_height\", False)\n-            if random_crop_height:\n-                crop_h = int(np.random.uniform(max(0.3 * newH, newH - fH), newH - fH))\n-            else:\n-                crop_h = (\n-                    int((1 - np.random.uniform(*self.data_config[\"crop_h\"])) * newH)\n-                    - fH\n-                )\n-            crop_w = int(np.random.uniform(0, max(0, newW - fW)))\n-            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n-            flip = self.data_config[\"flip\"] and np.random.choice([0, 1])\n-            rotate = np.random.uniform(*self.data_config[\"rot\"])\n-            if self.data_config.get(\"vflip\", False) and np.random.choice([0, 1]):\n-                rotate += 180\n-        else:\n-            resize = float(fW) / float(W)\n-            if scale is not None:\n-                resize += scale\n-            else:\n-                resize += self.data_config.get(\"resize_test\", 0.0)\n-            resize_dims = (int(W * resize), int(H * resize))\n-            newW, newH = resize_dims\n-            crop_h = int((1 - np.mean(self.data_config[\"crop_h\"])) * newH) - fH\n-            crop_w = int(max(0, newW - fW) / 2)\n-            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n-            flip = False if flip is None else flip\n-            rotate = 0\n-        return resize, resize_dims, crop, flip, rotate\n-\n-    def get_sensor_transforms(self, cam_info, cam_name):\n-        w, x, y, z = cam_info[\"cams\"][cam_name][\"sensor2ego_rotation\"]\n-        # sweep sensor to sweep ego\n-        sensor2ego_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n-        sensor2ego_tran = torch.Tensor(\n-            cam_info[\"cams\"][cam_name][\"sensor2ego_translation\"]\n-        )\n-        sensor2ego = sensor2ego_rot.new_zeros((4, 4))\n-        sensor2ego[3, 3] = 1\n-        sensor2ego[:3, :3] = sensor2ego_rot\n-        sensor2ego[:3, -1] = sensor2ego_tran\n-        # sweep ego to global\n-        w, x, y, z = cam_info[\"cams\"][cam_name][\"ego2global_rotation\"]\n-        ego2global_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n-        ego2global_tran = torch.Tensor(\n-            cam_info[\"cams\"][cam_name][\"ego2global_translation\"]\n-        )\n-        ego2global = ego2global_rot.new_zeros((4, 4))\n-        ego2global[3, 3] = 1\n-        ego2global[:3, :3] = ego2global_rot\n-        ego2global[:3, -1] = ego2global_tran\n-        return sensor2ego, ego2global\n-\n-    def photo_metric_distortion(self, img, pmd):\n-        \"\"\"Call function to perform photometric distortion on images.\n-        Args:\n-            results (dict): Result dict from loading pipeline.\n-        Returns:\n-            dict: Result dict with images distorted.\n-        \"\"\"\n-        if np.random.rand() > pmd.get(\"rate\", 1.0):\n-            return img\n-\n-        img = np.array(img).astype(np.float32)\n-        assert img.dtype == np.float32, (\n-            \"PhotoMetricDistortion needs the input image of dtype np.float32,\"\n-            ' please set \"to_float32=True\" in \"LoadImageFromFile\" pipeline'\n-        )\n-        # random brightness\n-        if np.random.randint(2):\n-            delta = np.random.uniform(-pmd[\"brightness_delta\"], pmd[\"brightness_delta\"])\n-            img += delta\n-\n-        # mode == 0 --> do random contrast first\n-        # mode == 1 --> do random contrast last\n-        mode = np.random.randint(2)\n-        if mode == 1:\n-            if np.random.randint(2):\n-                alpha = np.random.uniform(pmd[\"contrast_lower\"], pmd[\"contrast_upper\"])\n-                img *= alpha\n-\n-        # convert color from BGR to HSV\n-        img = mmcv.bgr2hsv(img)\n-\n-        # random saturation\n-        if np.random.randint(2):\n-            img[..., 1] *= np.random.uniform(\n-                pmd[\"saturation_lower\"], pmd[\"saturation_upper\"]\n-            )\n-\n-        # random hue\n-        if np.random.randint(2):\n-            img[..., 0] += np.random.uniform(-pmd[\"hue_delta\"], pmd[\"hue_delta\"])\n-            img[..., 0][img[..., 0] > 360] -= 360\n-            img[..., 0][img[..., 0] < 0] += 360\n-\n-        # convert color from HSV to BGR\n-        img = mmcv.hsv2bgr(img)\n-\n-        # random contrast\n-        if mode == 0:\n-            if np.random.randint(2):\n-                alpha = np.random.uniform(pmd[\"contrast_lower\"], pmd[\"contrast_upper\"])\n-                img *= alpha\n-\n-        # randomly swap channels\n-        if np.random.randint(2):\n-            img = img[..., np.random.permutation(3)]\n-        return Image.fromarray(img.astype(np.uint8))\n-\n-    def get_inputs(self, results, flip=None, scale=None):\n-        imgs = []\n-        sensor2egos = []\n-        ego2globals = []\n-        intrins = []\n-        post_rots = []\n-        post_trans = []\n-        cam_names = self.choose_cams()\n-        results[\"cam_names\"] = cam_names\n-        canvas = []\n-        for cam_name in cam_names:\n-            cam_data = results[\"curr\"][\"cams\"][cam_name]\n-            filename = cam_data[\"data_path\"]\n-            img = Image.open(filename)\n-            post_rot = torch.eye(2)\n-            post_tran = torch.zeros(2)\n-\n-            intrin = torch.Tensor(cam_data[\"cam_intrinsic\"])\n-\n-            sensor2ego, ego2global = self.get_sensor_transforms(\n-                results[\"curr\"], cam_name\n-            )\n-            # image view augmentation (resize, crop, horizontal flip, rotate)\n-            img_augs = self.sample_augmentation(\n-                H=img.height, W=img.width, flip=flip, scale=scale\n-            )\n-            resize, resize_dims, crop, flip, rotate = img_augs\n-            img, post_rot2, post_tran2 = self.img_transform(\n-                img,\n-                post_rot,\n-                post_tran,\n-                resize=resize,\n-                resize_dims=resize_dims,\n-                crop=crop,\n-                flip=flip,\n-                rotate=rotate,\n-            )\n-\n-            # for convenience, make augmentation matrices 3x3\n-            post_tran = torch.zeros(3)\n-            post_rot = torch.eye(3)\n-            post_tran[:2] = post_tran2\n-            post_rot[:2, :2] = post_rot2\n-\n-            if self.is_train and self.data_config.get(\"pmd\", None) is not None:\n-                img = self.photo_metric_distortion(img, self.data_config[\"pmd\"])\n-\n-            canvas.append(np.array(img))\n-            imgs.append(self.normalize_img(img))\n-\n-            if self.sequential:\n-                assert \"adjacent\" in results\n-                for adj_info in results[\"adjacent\"]:\n-                    filename_adj = adj_info[\"cams\"][cam_name][\"data_path\"]\n-                    img_adjacent = Image.open(filename_adj)\n-                    if self.opencv_pp:\n-                        img_adjacent = self.img_transform_core_opencv(\n-                            img_adjacent, post_rot[:2, :2], post_tran[:2], crop\n-                        )\n-                    else:\n-                        img_adjacent = self.img_transform_core(\n-                            img_adjacent,\n-                            resize_dims=resize_dims,\n-                            crop=crop,\n-                            flip=flip,\n-                            rotate=rotate,\n-                        )\n-                    imgs.append(self.normalize_img(img_adjacent))\n-            intrins.append(intrin)\n-            sensor2egos.append(sensor2ego)\n-            ego2globals.append(ego2global)\n-            post_rots.append(post_rot)\n-            post_trans.append(post_tran)\n-\n-        if self.sequential:\n-            for adj_info in results[\"adjacent\"]:\n-                post_trans.extend(post_trans[: len(cam_names)])\n-                post_rots.extend(post_rots[: len(cam_names)])\n-                intrins.extend(intrins[: len(cam_names)])\n-\n-                # align\n-                for cam_name in cam_names:\n-                    sensor2ego, ego2global = self.get_sensor_transforms(\n-                        adj_info, cam_name\n-                    )\n-                    sensor2egos.append(sensor2ego)\n-                    ego2globals.append(ego2global)\n-\n-        imgs = torch.stack(imgs)\n-\n-        sensor2egos = torch.stack(sensor2egos)\n-        ego2globals = torch.stack(ego2globals)\n-        intrins = torch.stack(intrins)\n-        post_rots = torch.stack(post_rots)\n-        post_trans = torch.stack(post_trans)\n-        results[\"canvas\"] = canvas\n-        return (imgs, sensor2egos, ego2globals, intrins, post_rots, post_trans)\n-\n-    def __call__(self, results):\n-        results[\"img_inputs\"] = self.get_inputs(results)\n-        return results\n-\n-\n-@PIPELINES.register_module()\n-class LoadAnnotations(object):\n-\n-    def __call__(self, results):\n-        gt_boxes, gt_labels = results[\"ann_infos\"]\n-        gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n-        if len(gt_boxes) == 0:\n-            gt_boxes = torch.zeros(0, 9)\n-        results[\"gt_bboxes_3d\"] = LiDARInstance3DBoxes(\n-            gt_boxes, box_dim=gt_boxes.shape[-1], origin=(0.5, 0.5, 0.5)\n-        )\n-        results[\"gt_labels_3d\"] = gt_labels\n-        return results\n-\n-\n-@PIPELINES.register_module()\n-class LoadAnnotationsBEVDepth(object):\n-    def __init__(self, bda_aug_conf, classes, is_train=True):\n-        self.bda_aug_conf = bda_aug_conf\n-        self.is_train = is_train\n-        self.classes = classes\n-\n-    def sample_bda_augmentation(self):\n-        \"\"\"Generate bda augmentation values based on bda_config.\"\"\"\n-        if self.is_train:\n-            rotate_bda = np.random.uniform(*self.bda_aug_conf[\"rot_lim\"])\n-            scale_bda = np.random.uniform(*self.bda_aug_conf[\"scale_lim\"])\n-            flip_dx = np.random.uniform() < self.bda_aug_conf[\"flip_dx_ratio\"]\n-            flip_dy = np.random.uniform() < self.bda_aug_conf[\"flip_dy_ratio\"]\n-        else:\n-            rotate_bda = 0\n-            scale_bda = 1.0\n-            flip_dx = False\n-            flip_dy = False\n-        return rotate_bda, scale_bda, flip_dx, flip_dy\n-\n-    def bev_transform(self, gt_boxes, rotate_angle, scale_ratio, flip_dx, flip_dy):\n-        rotate_angle = torch.tensor(rotate_angle / 180 * np.pi)\n-        rot_sin = torch.sin(rotate_angle)\n-        rot_cos = torch.cos(rotate_angle)\n-        rot_mat = torch.Tensor(\n-            [[rot_cos, -rot_sin, 0], [rot_sin, rot_cos, 0], [0, 0, 1]]\n-        )\n-        scale_mat = torch.Tensor(\n-            [[scale_ratio, 0, 0], [0, scale_ratio, 0], [0, 0, scale_ratio]]\n-        )\n-        flip_mat = torch.Tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n-        if flip_dx:\n-            flip_mat = flip_mat @ torch.Tensor([[-1, 0, 0], [0, 1, 0], [0, 0, 1]])\n-        if flip_dy:\n-            flip_mat = flip_mat @ torch.Tensor([[1, 0, 0], [0, -1, 0], [0, 0, 1]])\n-        rot_mat = flip_mat @ (scale_mat @ rot_mat)\n-        if gt_boxes.shape[0] > 0:\n-            gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n-            gt_boxes[:, 3:6] *= scale_ratio\n-            gt_boxes[:, 6] += rotate_angle\n-            if flip_dx:\n-                gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n-            if flip_dy:\n-                gt_boxes[:, 6] = -gt_boxes[:, 6]\n-            gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(\n-                -1\n-            )\n-        return gt_boxes, rot_mat\n-\n-    def __call__(self, results):\n-        gt_boxes, gt_labels = results[\"ann_infos\"]\n-        gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n-        rotate_bda, scale_bda, flip_dx, flip_dy = self.sample_bda_augmentation()\n-        bda_mat = torch.zeros(4, 4)\n-        bda_mat[3, 3] = 1\n-        gt_boxes, bda_rot = self.bev_transform(\n-            gt_boxes, rotate_bda, scale_bda, flip_dx, flip_dy\n-        )\n-        bda_mat[:3, :3] = bda_rot\n-        if len(gt_boxes) == 0:\n-            gt_boxes = torch.zeros(0, 9)\n-        results[\"gt_bboxes_3d\"] = LiDARInstance3DBoxes(\n-            gt_boxes, box_dim=gt_boxes.shape[-1], origin=(0.5, 0.5, 0.5)\n-        )\n-        results[\"gt_labels_3d\"] = gt_labels\n-        imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n-        post_rots, post_trans = results[\"img_inputs\"][4:]\n-        results[\"img_inputs\"] = (\n-            imgs,\n-            rots,\n-            trans,\n-            intrins,\n-            post_rots,\n-            post_trans,\n-            # bda_rot,\n-            bda_mat,\n-        )\n-        if \"voxel_semantics\" in results:\n-            if flip_dx:\n-                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n-                    ::-1, ...\n-                ].copy()\n-                results[\"mask_lidar\"] = results[\"mask_lidar\"][::-1, ...].copy()\n-                results[\"mask_camera\"] = results[\"mask_camera\"][::-1, ...].copy()\n-            if flip_dy:\n-                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n-                    :, ::-1, ...\n-                ].copy()\n-                results[\"mask_lidar\"] = results[\"mask_lidar\"][:, ::-1, ...].copy()\n-                results[\"mask_camera\"] = results[\"mask_camera\"][:, ::-1, ...].copy()\n-        return results\n-\n-\n-@PIPELINES.register_module()\n-class BEVAug(object):\n-\n-    def __init__(self, bda_aug_conf, classes, is_train=True):\n-        self.bda_aug_conf = bda_aug_conf\n-        self.is_train = is_train\n-        self.classes = classes\n-\n-    def sample_bda_augmentation(self):\n-        \"\"\"Generate bda augmentation values based on bda_config.\"\"\"\n-        if self.is_train:\n-            rotate_bda = np.random.uniform(*self.bda_aug_conf[\"rot_lim\"])\n-            scale_bda = np.random.uniform(*self.bda_aug_conf[\"scale_lim\"])\n-            flip_dx = np.random.uniform() < self.bda_aug_conf[\"flip_dx_ratio\"]\n-            flip_dy = np.random.uniform() < self.bda_aug_conf[\"flip_dy_ratio\"]\n-            translation_std = self.bda_aug_conf.get(\"tran_lim\", [0.0, 0.0, 0.0])\n-            tran_bda = np.random.normal(scale=translation_std, size=3).T\n-        else:\n-            rotate_bda = 0\n-            scale_bda = 1.0\n-            flip_dx = False\n-            flip_dy = False\n-            tran_bda = np.zeros((1, 3), dtype=np.float32)\n-        return rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda\n-\n-    def bev_transform(\n-        self, gt_boxes, rotate_angle, scale_ratio, flip_dx, flip_dy, tran_bda\n-    ):\n-        rotate_angle = torch.tensor(rotate_angle / 180 * np.pi)\n-        rot_sin = torch.sin(rotate_angle)\n-        rot_cos = torch.cos(rotate_angle)\n-        rot_mat = torch.Tensor(\n-            [[rot_cos, -rot_sin, 0], [rot_sin, rot_cos, 0], [0, 0, 1]]\n-        )\n-        scale_mat = torch.Tensor(\n-            [[scale_ratio, 0, 0], [0, scale_ratio, 0], [0, 0, scale_ratio]]\n-        )\n-        flip_mat = torch.Tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n-        if flip_dx:\n-            flip_mat = flip_mat @ torch.Tensor([[-1, 0, 0], [0, 1, 0], [0, 0, 1]])\n-        if flip_dy:\n-            flip_mat = flip_mat @ torch.Tensor([[1, 0, 0], [0, -1, 0], [0, 0, 1]])\n-        rot_mat = flip_mat @ (scale_mat @ rot_mat)\n-        if gt_boxes.shape[0] > 0:\n-            gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n-            gt_boxes[:, 3:6] *= scale_ratio\n-            gt_boxes[:, 6] += rotate_angle\n-            if flip_dx:\n-                gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n-            if flip_dy:\n-                gt_boxes[:, 6] = -gt_boxes[:, 6]\n-            gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(\n-                -1\n-            )\n-            gt_boxes[:, :3] = gt_boxes[:, :3] + tran_bda\n-        return gt_boxes, rot_mat\n-\n-    def __call__(self, results):\n-        gt_boxes = results[\"gt_bboxes_3d\"].tensor\n-        gt_boxes[:, 2] = gt_boxes[:, 2] + 0.5 * gt_boxes[:, 5]\n-        rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda = (\n-            self.sample_bda_augmentation()\n-        )\n-        bda_mat = torch.zeros(4, 4)\n-        bda_mat[3, 3] = 1\n-        gt_boxes, bda_rot = self.bev_transform(\n-            gt_boxes, rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda\n-        )\n-        if \"points\" in results:\n-            points = results[\"points\"].tensor\n-            points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n-            points[:, :3] = points_aug + tran_bda\n-            points = results[\"points\"].new_point(points)\n-            results[\"points\"] = points\n-\n-        if \"radar\" in results:\n-            points = results[\"radar\"].tensor\n-            points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n-            points[:, :3] = points_aug + tran_bda\n-            points = results[\"radar\"].new_point(points)\n-            results[\"radar\"] = points\n-\n-        bda_mat[:3, :3] = bda_rot\n-        bda_mat[:3, 3] = torch.from_numpy(tran_bda)\n-        if len(gt_boxes) == 0:\n-            gt_boxes = torch.zeros(0, 9)\n-        results[\"gt_bboxes_3d\"] = LiDARInstance3DBoxes(\n-            gt_boxes, box_dim=gt_boxes.shape[-1], origin=(0.5, 0.5, 0.5)\n-        )\n-        if \"img_inputs\" in results:\n-            imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n-            post_rots, post_trans = results[\"img_inputs\"][4:]\n-            results[\"img_inputs\"] = (\n-                imgs,\n-                rots,\n-                trans,\n-                intrins,\n-                post_rots,\n-                post_trans,\n-                bda_mat,\n-            )\n-        if \"voxel_semantics\" in results:\n-            if flip_dx:\n-                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n-                    ::-1, ...\n-                ].copy()\n-                results[\"mask_lidar\"] = results[\"mask_lidar\"][::-1, ...].copy()\n-                results[\"mask_camera\"] = results[\"mask_camera\"][::-1, ...].copy()\n-            if flip_dy:\n-                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n-                    :, ::-1, ...\n-                ].copy()\n-                results[\"mask_lidar\"] = results[\"mask_lidar\"][:, ::-1, ...].copy()\n-                results[\"mask_camera\"] = results[\"mask_camera\"][:, ::-1, ...].copy()\n-        return results\n"
                },
                {
                    "date": 1717330797573,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,1578 @@\n+# Copyright (c) OpenMMLab. All rights reserved.\n+import os\n+\n+import cv2\n+import mmcv\n+import numpy as np\n+import torch\n+from PIL import Image\n+from pyquaternion import Quaternion\n+\n+\n+from nuscenes.utils.data_classes import RadarPointCloud\n+\n+from mmdet3d.core.points import BasePoints, get_points_type, RadarPoints\n+from mmdet.datasets.pipelines import LoadAnnotations, LoadImageFromFile\n+from ...core.bbox import LiDARInstance3DBoxes\n+from ..builder import PIPELINES\n+\n+\n+@PIPELINES.register_module()\n+class LoadOccGTFromFile(object):\n+    def __call__(self, results):\n+        occ_gt_path = results[\"occ_gt_path\"]\n+        occ_gt_path = os.path.join(occ_gt_path, \"labels.npz\")\n+\n+        occ_labels = np.load(occ_gt_path)\n+        semantics = occ_labels[\"semantics\"]\n+        mask_lidar = occ_labels[\"mask_lidar\"]\n+        mask_camera = occ_labels[\"mask_camera\"]\n+\n+        results[\"voxel_semantics\"] = semantics\n+        results[\"mask_lidar\"] = mask_lidar\n+        results[\"mask_camera\"] = mask_camera\n+        return results\n+\n+\n+@PIPELINES.register_module()\n+class LoadMultiViewImageFromFiles(object):\n+    \"\"\"Load multi channel images from a list of separate channel files.\n+\n+    Expects results['img_filename'] to be a list of filenames.\n+\n+    Args:\n+        to_float32 (bool, optional): Whether to convert the img to float32.\n+            Defaults to False.\n+        color_type (str, optional): Color type of the file.\n+            Defaults to 'unchanged'.\n+    \"\"\"\n+\n+    def __init__(self, to_float32=False, color_type=\"unchanged\"):\n+        self.to_float32 = to_float32\n+        self.color_type = color_type\n+\n+    def __call__(self, results):\n+        \"\"\"Call function to load multi-view image from files.\n+\n+        Args:\n+            results (dict): Result dict containing multi-view image filenames.\n+\n+        Returns:\n+            dict: The result dict containing the multi-view image data.\n+                Added keys and values are described below.\n+\n+                - filename (str): Multi-view image filenames.\n+                - img (np.ndarray): Multi-view image arrays.\n+                - img_shape (tuple[int]): Shape of multi-view image arrays.\n+                - ori_shape (tuple[int]): Shape of original image arrays.\n+                - pad_shape (tuple[int]): Shape of padded image arrays.\n+                - scale_factor (float): Scale factor.\n+                - img_norm_cfg (dict): Normalization configuration of images.\n+        \"\"\"\n+        filename = results[\"img_filename\"]\n+        # img is of shape (h, w, c, num_views)\n+        img = np.stack(\n+            [mmcv.imread(name, self.color_type) for name in filename], axis=-1\n+        )\n+        if self.to_float32:\n+            img = img.astype(np.float32)\n+        results[\"filename\"] = filename\n+        # unravel to list, see `DefaultFormatBundle` in formatting.py\n+        # which will transpose each image separately and then stack into array\n+        results[\"img\"] = [img[..., i] for i in range(img.shape[-1])]\n+        results[\"img_shape\"] = img.shape\n+        results[\"ori_shape\"] = img.shape\n+        # Set initial values for default meta_keys\n+        results[\"pad_shape\"] = img.shape\n+        results[\"scale_factor\"] = 1.0\n+        num_channels = 1 if len(img.shape) < 3 else img.shape[2]\n+        results[\"img_norm_cfg\"] = dict(\n+            mean=np.zeros(num_channels, dtype=np.float32),\n+            std=np.ones(num_channels, dtype=np.float32),\n+            to_rgb=False,\n+        )\n+        return results\n+\n+    def __repr__(self):\n+        \"\"\"str: Return a string that describes the module.\"\"\"\n+        repr_str = self.__class__.__name__\n+        repr_str += f\"(to_float32={self.to_float32}, \"\n+        repr_str += f\"color_type='{self.color_type}')\"\n+        return repr_str\n+\n+\n+@PIPELINES.register_module()\n+class LoadImageFromFileMono3D(LoadImageFromFile):\n+    \"\"\"Load an image from file in monocular 3D object detection. Compared to 2D\n+    detection, additional camera parameters need to be loaded.\n+\n+    Args:\n+        kwargs (dict): Arguments are the same as those in\n+            :class:`LoadImageFromFile`.\n+    \"\"\"\n+\n+    def __call__(self, results):\n+        \"\"\"Call functions to load image and get image meta information.\n+\n+        Args:\n+            results (dict): Result dict from :obj:`mmdet.CustomDataset`.\n+\n+        Returns:\n+            dict: The dict contains loaded image and meta information.\n+        \"\"\"\n+        super().__call__(results)\n+        results[\"cam2img\"] = results[\"img_info\"][\"cam_intrinsic\"]\n+        return results\n+\n+\n+@PIPELINES.register_module()\n+class LoadPointsFromMultiSweeps(object):\n+    \"\"\"Load points from multiple sweeps.\n+\n+    This is usually used for nuScenes dataset to utilize previous sweeps.\n+\n+    Args:\n+        sweeps_num (int, optional): Number of sweeps. Defaults to 10.\n+        load_dim (int, optional): Dimension number of the loaded points.\n+            Defaults to 5.\n+        use_dim (list[int], optional): Which dimension to use.\n+            Defaults to [0, 1, 2, 4].\n+        time_dim (int, optional): Which dimension to represent the timestamps\n+            of each points. Defaults to 4.\n+        file_client_args (dict, optional): Config dict of file clients,\n+            refer to\n+            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n+            for more details. Defaults to dict(backend='disk').\n+        pad_empty_sweeps (bool, optional): Whether to repeat keyframe when\n+            sweeps is empty. Defaults to False.\n+        remove_close (bool, optional): Whether to remove close points.\n+            Defaults to False.\n+        test_mode (bool, optional): If `test_mode=True`, it will not\n+            randomly sample sweeps but select the nearest N frames.\n+            Defaults to False.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        sweeps_num=10,\n+        load_dim=5,\n+        use_dim=[0, 1, 2, 4],\n+        time_dim=4,\n+        file_client_args=dict(backend=\"disk\"),\n+        pad_empty_sweeps=False,\n+        remove_close=False,\n+        test_mode=False,\n+    ):\n+        self.load_dim = load_dim\n+        self.sweeps_num = sweeps_num\n+        self.use_dim = use_dim\n+        self.time_dim = time_dim\n+        assert (\n+            time_dim < load_dim\n+        ), f\"Expect the timestamp dimension < {load_dim}, got {time_dim}\"\n+        self.file_client_args = file_client_args.copy()\n+        self.file_client = None\n+        self.pad_empty_sweeps = pad_empty_sweeps\n+        self.remove_close = remove_close\n+        self.test_mode = test_mode\n+        assert (\n+            max(use_dim) < load_dim\n+        ), f\"Expect all used dimensions < {load_dim}, got {use_dim}\"\n+\n+    def _load_points(self, pts_filename):\n+        \"\"\"Private function to load point clouds data.\n+\n+        Args:\n+            pts_filename (str): Filename of point clouds data.\n+\n+        Returns:\n+            np.ndarray: An array containing point clouds data.\n+        \"\"\"\n+        if self.file_client is None:\n+            self.file_client = mmcv.FileClient(**self.file_client_args)\n+        try:\n+            pts_bytes = self.file_client.get(pts_filename)\n+            points = np.frombuffer(pts_bytes, dtype=np.float32)\n+        except ConnectionError:\n+            mmcv.check_file_exist(pts_filename)\n+            if pts_filename.endswith(\".npy\"):\n+                points = np.load(pts_filename)\n+            else:\n+                points = np.fromfile(pts_filename, dtype=np.float32)\n+        return points\n+\n+    def _remove_close(self, points, radius=1.0):\n+        \"\"\"Removes point too close within a certain radius from origin.\n+\n+        Args:\n+            points (np.ndarray | :obj:`BasePoints`): Sweep points.\n+            radius (float, optional): Radius below which points are removed.\n+                Defaults to 1.0.\n+\n+        Returns:\n+            np.ndarray: Points after removing.\n+        \"\"\"\n+        if isinstance(points, np.ndarray):\n+            points_numpy = points\n+        elif isinstance(points, BasePoints):\n+            points_numpy = points.tensor.numpy()\n+        else:\n+            raise NotImplementedError\n+        x_filt = np.abs(points_numpy[:, 0]) < radius\n+        y_filt = np.abs(points_numpy[:, 1]) < radius\n+        not_close = np.logical_not(np.logical_and(x_filt, y_filt))\n+        return points[not_close]\n+\n+    def __call__(self, results):\n+        \"\"\"Call function to load multi-sweep point clouds from files.\n+\n+        Args:\n+            results (dict): Result dict containing multi-sweep point cloud\n+                filenames.\n+\n+        Returns:\n+            dict: The result dict containing the multi-sweep points data.\n+                Added key and value are described below.\n+\n+                - points (np.ndarray | :obj:`BasePoints`): Multi-sweep point\n+                    cloud arrays.\n+        \"\"\"\n+        points = results[\"points\"]\n+        points.tensor[:, self.time_dim] = 0\n+        sweep_points_list = [points]\n+        ts = results[\"timestamp\"]\n+        if self.pad_empty_sweeps and len(results[\"sweeps\"]) == 0:\n+            for i in range(self.sweeps_num):\n+                if self.remove_close:\n+                    sweep_points_list.append(self._remove_close(points))\n+                else:\n+                    sweep_points_list.append(points)\n+        else:\n+            if len(results[\"sweeps\"]) <= self.sweeps_num:\n+                choices = np.arange(len(results[\"sweeps\"]))\n+            elif self.test_mode:\n+                choices = np.arange(self.sweeps_num)\n+            else:\n+                choices = np.random.choice(\n+                    len(results[\"sweeps\"]), self.sweeps_num, replace=False\n+                )\n+            for idx in choices:\n+                sweep = results[\"sweeps\"][idx]\n+                points_sweep = self._load_points(sweep[\"data_path\"])\n+                points_sweep = np.copy(points_sweep).reshape(-1, self.load_dim)\n+                if self.remove_close:\n+                    points_sweep = self._remove_close(points_sweep)\n+                sweep_ts = sweep[\"timestamp\"] / 1e6\n+                points_sweep[:, :3] = (\n+                    points_sweep[:, :3] @ sweep[\"sensor2lidar_rotation\"].T\n+                )\n+                points_sweep[:, :3] += sweep[\"sensor2lidar_translation\"]\n+                points_sweep[:, self.time_dim] = ts - sweep_ts\n+                points_sweep = points.new_point(points_sweep)\n+                sweep_points_list.append(points_sweep)\n+\n+        points = points.cat(sweep_points_list)\n+        points = points[:, self.use_dim]\n+        results[\"points\"] = points\n+        return results\n+\n+    def __repr__(self):\n+        \"\"\"str: Return a string that describes the module.\"\"\"\n+        return f\"{self.__class__.__name__}(sweeps_num={self.sweeps_num})\"\n+\n+\n+@PIPELINES.register_module()\n+class PointSegClassMapping(object):\n+    \"\"\"Map original semantic class to valid category ids.\n+\n+    Map valid classes as 0~len(valid_cat_ids)-1 and\n+    others as len(valid_cat_ids).\n+\n+    Args:\n+        valid_cat_ids (tuple[int]): A tuple of valid category.\n+        max_cat_id (int, optional): The max possible cat_id in input\n+            segmentation mask. Defaults to 40.\n+    \"\"\"\n+\n+    def __init__(self, valid_cat_ids, max_cat_id=40):\n+        assert max_cat_id >= np.max(\n+            valid_cat_ids\n+        ), \"max_cat_id should be greater than maximum id in valid_cat_ids\"\n+\n+        self.valid_cat_ids = valid_cat_ids\n+        self.max_cat_id = int(max_cat_id)\n+\n+        # build cat_id to class index mapping\n+        neg_cls = len(valid_cat_ids)\n+        self.cat_id2class = np.ones(self.max_cat_id + 1, dtype=np.int) * neg_cls\n+        for cls_idx, cat_id in enumerate(valid_cat_ids):\n+            self.cat_id2class[cat_id] = cls_idx\n+\n+    def __call__(self, results):\n+        \"\"\"Call function to map original semantic class to valid category ids.\n+\n+        Args:\n+            results (dict): Result dict containing point semantic masks.\n+\n+        Returns:\n+            dict: The result dict containing the mapped category ids.\n+                Updated key and value are described below.\n+\n+                - pts_semantic_mask (np.ndarray): Mapped semantic masks.\n+        \"\"\"\n+        assert \"pts_semantic_mask\" in results\n+        pts_semantic_mask = results[\"pts_semantic_mask\"]\n+\n+        converted_pts_sem_mask = self.cat_id2class[pts_semantic_mask]\n+\n+        results[\"pts_semantic_mask\"] = converted_pts_sem_mask\n+        return results\n+\n+    def __repr__(self):\n+        \"\"\"str: Return a string that describes the module.\"\"\"\n+        repr_str = self.__class__.__name__\n+        repr_str += f\"(valid_cat_ids={self.valid_cat_ids}, \"\n+        repr_str += f\"max_cat_id={self.max_cat_id})\"\n+        return repr_str\n+\n+\n+@PIPELINES.register_module()\n+class NormalizePointsColor(object):\n+    \"\"\"Normalize color of points.\n+\n+    Args:\n+        color_mean (list[float]): Mean color of the point cloud.\n+    \"\"\"\n+\n+    def __init__(self, color_mean):\n+        self.color_mean = color_mean\n+\n+    def __call__(self, results):\n+        \"\"\"Call function to normalize color of points.\n+\n+        Args:\n+            results (dict): Result dict containing point clouds data.\n+\n+        Returns:\n+            dict: The result dict containing the normalized points.\n+                Updated key and value are described below.\n+\n+                - points (:obj:`BasePoints`): Points after color normalization.\n+        \"\"\"\n+        points = results[\"points\"]\n+        assert (\n+            points.attribute_dims is not None\n+            and \"color\" in points.attribute_dims.keys()\n+        ), \"Expect points have color attribute\"\n+        if self.color_mean is not None:\n+            points.color = points.color - points.color.new_tensor(self.color_mean)\n+        points.color = points.color / 255.0\n+        results[\"points\"] = points\n+        return results\n+\n+    def __repr__(self):\n+        \"\"\"str: Return a string that describes the module.\"\"\"\n+        repr_str = self.__class__.__name__\n+        repr_str += f\"(color_mean={self.color_mean})\"\n+        return repr_str\n+\n+\n+@PIPELINES.register_module()\n+class LoadRadarPointsMultiSweeps(object):\n+    \"\"\"Load radar points from multiple sweeps.\n+    This is usually used for nuScenes dataset to utilize previous sweeps.\n+    Args:\n+        sweeps_num (int): Number of sweeps. Defaults to 10.\n+        load_dim (int): Dimension number of the loaded points. Defaults to 5.\n+        use_dim (list[int]): Which dimension to use. Defaults to [0, 1, 2, 4].\n+        file_client_args (dict): Config dict of file clients, refer to\n+            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n+            for more details. Defaults to dict(backend='disk').\n+        pad_empty_sweeps (bool): Whether to repeat keyframe when\n+            sweeps is empty. Defaults to False.\n+        remove_close (bool): Whether to remove close points.\n+            Defaults to False.\n+        test_mode (bool): If test_model=True used for testing, it will not\n+            randomly sample sweeps but select the nearest N frames.\n+            Defaults to False.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        load_dim=18,\n+        use_dim=[0, 1, 2, 3, 4],\n+        sweeps_num=3,\n+        file_client_args=dict(backend=\"disk\"),\n+        max_num=300,\n+        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],\n+        test_mode=False,\n+    ):\n+        self.load_dim = load_dim\n+        self.use_dim = use_dim\n+        self.sweeps_num = sweeps_num\n+        self.file_client_args = file_client_args.copy()\n+        self.file_client = None\n+        self.max_num = max_num\n+        self.test_mode = test_mode\n+        self.pc_range = pc_range\n+\n+    def _load_points(self, pts_filename):\n+        \"\"\"Private function to load point clouds data.\n+        Args:\n+            pts_filename (str): Filename of point clouds data.\n+        Returns:\n+            np.ndarray: An array containing point clouds data.\n+            [N, 18]\n+        \"\"\"\n+        radar_obj = RadarPointCloud.from_file(pts_filename)\n+\n+        # [18, N]\n+        points = radar_obj.points\n+\n+        return points.transpose().astype(np.float32)\n+\n+    def _pad_or_drop(self, points):\n+        \"\"\"\n+        points: [N, 18]\n+        \"\"\"\n+\n+        num_points = points.shape[0]\n+\n+        if num_points == self.max_num:\n+            masks = np.ones((num_points, 1), dtype=points.dtype)\n+\n+            return points, masks\n+\n+        if num_points > self.max_num:\n+            points = np.random.permutation(points)[: self.max_num, :]\n+            masks = np.ones((self.max_num, 1), dtype=points.dtype)\n+\n+            return points, masks\n+\n+        if num_points < self.max_num:\n+            zeros = np.zeros(\n+                (self.max_num - num_points, points.shape[1]), dtype=points.dtype\n+            )\n+            masks = np.ones((num_points, 1), dtype=points.dtype)\n+\n+            points = np.concatenate((points, zeros), axis=0)\n+            masks = np.concatenate((masks, zeros.copy()[:, [0]]), axis=0)\n+\n+            return points, masks\n+\n+    def __call__(self, results):\n+        \"\"\"Call function to load multi-sweep point clouds from files.\n+        Args:\n+            results (dict): Result dict containing multi-sweep point cloud \\\n+                filenames.\n+        Returns:\n+            dict: The result dict containing the multi-sweep points data. \\\n+                Added key and value are described below.\n+                - points (np.ndarray | :obj:`BasePoints`): Multi-sweep point \\\n+                    cloud arrays.\n+        \"\"\"\n+        radars_dict = results[\"radar\"]\n+\n+        points_sweep_list = []\n+        for key, sweeps in radars_dict.items():\n+            if len(sweeps) < self.sweeps_num:\n+                idxes = list(range(len(sweeps)))\n+            else:\n+                idxes = list(range(self.sweeps_num))\n+\n+            ts = sweeps[0][\"timestamp\"] * 1e-6\n+            for idx in idxes:\n+                sweep = sweeps[idx]\n+\n+                points_sweep = self._load_points(sweep[\"data_path\"])\n+                points_sweep = np.copy(points_sweep).reshape(-1, self.load_dim)\n+\n+                timestamp = sweep[\"timestamp\"] * 1e-6\n+                time_diff = ts - timestamp\n+                time_diff = np.ones((points_sweep.shape[0], 1)) * time_diff\n+\n+                # velocity compensated by the ego motion in sensor frame\n+                velo_comp = points_sweep[:, 8:10]\n+                velo_comp = np.concatenate(\n+                    (velo_comp, np.zeros((velo_comp.shape[0], 1))), 1\n+                )\n+                velo_comp = velo_comp @ sweep[\"sensor2lidar_rotation\"].T\n+                velo_comp = velo_comp[:, :2]\n+\n+                # velocity in sensor frame\n+                velo = points_sweep[:, 6:8]\n+                velo = np.concatenate((velo, np.zeros((velo.shape[0], 1))), 1)\n+                velo = velo @ sweep[\"sensor2lidar_rotation\"].T\n+                velo = velo[:, :2]\n+\n+                points_sweep[:, :3] = (\n+                    points_sweep[:, :3] @ sweep[\"sensor2lidar_rotation\"].T\n+                )\n+                points_sweep[:, :3] += sweep[\"sensor2lidar_translation\"]\n+\n+                points_sweep_ = np.concatenate(\n+                    [\n+                        points_sweep[:, :6],\n+                        velo,\n+                        velo_comp,\n+                        points_sweep[:, 10:],\n+                        time_diff,\n+                    ],\n+                    axis=1,\n+                )\n+                points_sweep_list.append(points_sweep_)\n+\n+        points = np.concatenate(points_sweep_list, axis=0)\n+\n+        points = points[:, self.use_dim]\n+\n+        points = RadarPoints(points, points_dim=points.shape[-1], attribute_dims=None)\n+\n+        results[\"radar\"] = points\n+        return results\n+\n+    def __repr__(self):\n+        \"\"\"str: Return a string that describes the module.\"\"\"\n+        return f\"{self.__class__.__name__}(sweeps_num={self.sweeps_num})\"\n+\n+\n+@PIPELINES.register_module()\n+class LoadPointsFromFile(object):\n+    \"\"\"Load Points From File.\n+\n+    Load points from file.\n+\n+    Args:\n+        coord_type (str): The type of coordinates of points cloud.\n+            Available options includes:\n+            - 'LIDAR': Points in LiDAR coordinates.\n+            - 'DEPTH': Points in depth coordinates, usually for indoor dataset.\n+            - 'CAMERA': Points in camera coordinates.\n+        load_dim (int, optional): The dimension of the loaded points.\n+            Defaults to 6.\n+        use_dim (list[int], optional): Which dimensions of the points to use.\n+            Defaults to [0, 1, 2]. For KITTI dataset, set use_dim=4\n+            or use_dim=[0, 1, 2, 3] to use the intensity dimension.\n+        shift_height (bool, optional): Whether to use shifted height.\n+            Defaults to False.\n+        use_color (bool, optional): Whether to use color features.\n+            Defaults to False.\n+        file_client_args (dict, optional): Config dict of file clients,\n+            refer to\n+            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n+            for more details. Defaults to dict(backend='disk').\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        coord_type,\n+        load_dim=6,\n+        use_dim=[0, 1, 2],\n+        shift_height=False,\n+        use_color=False,\n+        file_client_args=dict(backend=\"disk\"),\n+    ):\n+        self.shift_height = shift_height\n+        self.use_color = use_color\n+        if isinstance(use_dim, int):\n+            use_dim = list(range(use_dim))\n+        assert (\n+            max(use_dim) < load_dim\n+        ), f\"Expect all used dimensions < {load_dim}, got {use_dim}\"\n+        assert coord_type in [\"CAMERA\", \"LIDAR\", \"DEPTH\"]\n+\n+        self.coord_type = coord_type\n+        self.load_dim = load_dim\n+        self.use_dim = use_dim\n+        self.file_client_args = file_client_args.copy()\n+        self.file_client = None\n+\n+    def _load_points(self, pts_filename):\n+        \"\"\"Private function to load point clouds data.\n+\n+        Args:\n+            pts_filename (str): Filename of point clouds data.\n+\n+        Returns:\n+            np.ndarray: An array containing point clouds data.\n+        \"\"\"\n+        if self.file_client is None:\n+            self.file_client = mmcv.FileClient(**self.file_client_args)\n+        try:\n+            pts_bytes = self.file_client.get(pts_filename)\n+            points = np.frombuffer(pts_bytes, dtype=np.float32)\n+        except ConnectionError:\n+            mmcv.check_file_exist(pts_filename)\n+            if pts_filename.endswith(\".npy\"):\n+                points = np.load(pts_filename)\n+            else:\n+                points = np.fromfile(pts_filename, dtype=np.float32)\n+\n+        return points\n+\n+    def __call__(self, results):\n+        \"\"\"Call function to load points data from file.\n+\n+        Args:\n+            results (dict): Result dict containing point clouds data.\n+\n+        Returns:\n+            dict: The result dict containing the point clouds data.\n+                Added key and value are described below.\n+\n+                - points (:obj:`BasePoints`): Point clouds data.\n+        \"\"\"\n+        pts_filename = results[\"pts_filename\"]\n+        points = self._load_points(pts_filename)\n+        points = points.reshape(-1, self.load_dim)\n+        points = points[:, self.use_dim]\n+        attribute_dims = None\n+\n+        if self.shift_height:\n+            floor_height = np.percentile(points[:, 2], 0.99)\n+            height = points[:, 2] - floor_height\n+            points = np.concatenate(\n+                [points[:, :3], np.expand_dims(height, 1), points[:, 3:]], 1\n+            )\n+            attribute_dims = dict(height=3)\n+\n+        if self.use_color:\n+            assert len(self.use_dim) >= 6\n+            if attribute_dims is None:\n+                attribute_dims = dict()\n+            attribute_dims.update(\n+                dict(\n+                    color=[\n+                        points.shape[1] - 3,\n+                        points.shape[1] - 2,\n+                        points.shape[1] - 1,\n+                    ]\n+                )\n+            )\n+\n+        points_class = get_points_type(self.coord_type)\n+        points = points_class(\n+            points, points_dim=points.shape[-1], attribute_dims=attribute_dims\n+        )\n+        results[\"points\"] = points\n+\n+        return results\n+\n+    def __repr__(self):\n+        \"\"\"str: Return a string that describes the module.\"\"\"\n+        repr_str = self.__class__.__name__ + \"(\"\n+        repr_str += f\"shift_height={self.shift_height}, \"\n+        repr_str += f\"use_color={self.use_color}, \"\n+        repr_str += f\"file_client_args={self.file_client_args}, \"\n+        repr_str += f\"load_dim={self.load_dim}, \"\n+        repr_str += f\"use_dim={self.use_dim})\"\n+        return repr_str\n+\n+\n+@PIPELINES.register_module()\n+class LoadPointsFromDict(LoadPointsFromFile):\n+    \"\"\"Load Points From Dict.\"\"\"\n+\n+    def __call__(self, results):\n+        assert \"points\" in results\n+        return results\n+\n+\n+@PIPELINES.register_module()\n+class LoadAnnotations3D(LoadAnnotations):\n+    \"\"\"Load Annotations3D.\n+\n+    Load instance mask and semantic mask of points and\n+    encapsulate the items into related fields.\n+\n+    Args:\n+        with_bbox_3d (bool, optional): Whether to load 3D boxes.\n+            Defaults to True.\n+        with_label_3d (bool, optional): Whether to load 3D labels.\n+            Defaults to True.\n+        with_attr_label (bool, optional): Whether to load attribute label.\n+            Defaults to False.\n+        with_mask_3d (bool, optional): Whether to load 3D instance masks.\n+            for points. Defaults to False.\n+        with_seg_3d (bool, optional): Whether to load 3D semantic masks.\n+            for points. Defaults to False.\n+        with_bbox (bool, optional): Whether to load 2D boxes.\n+            Defaults to False.\n+        with_label (bool, optional): Whether to load 2D labels.\n+            Defaults to False.\n+        with_mask (bool, optional): Whether to load 2D instance masks.\n+            Defaults to False.\n+        with_seg (bool, optional): Whether to load 2D semantic masks.\n+            Defaults to False.\n+        with_bbox_depth (bool, optional): Whether to load 2.5D boxes.\n+            Defaults to False.\n+        poly2mask (bool, optional): Whether to convert polygon annotations\n+            to bitmasks. Defaults to True.\n+        seg_3d_dtype (dtype, optional): Dtype of 3D semantic masks.\n+            Defaults to int64\n+        file_client_args (dict): Config dict of file clients, refer to\n+            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n+            for more details.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        with_bbox_3d=True,\n+        with_label_3d=True,\n+        with_attr_label=False,\n+        with_mask_3d=False,\n+        with_seg_3d=False,\n+        with_bbox=False,\n+        with_label=False,\n+        with_mask=False,\n+        with_seg=False,\n+        with_bbox_depth=False,\n+        poly2mask=True,\n+        seg_3d_dtype=np.int64,\n+        file_client_args=dict(backend=\"disk\"),\n+    ):\n+        super().__init__(\n+            with_bbox,\n+            with_label,\n+            with_mask,\n+            with_seg,\n+            poly2mask,\n+            file_client_args=file_client_args,\n+        )\n+        self.with_bbox_3d = with_bbox_3d\n+        self.with_bbox_depth = with_bbox_depth\n+        self.with_label_3d = with_label_3d\n+        self.with_attr_label = with_attr_label\n+        self.with_mask_3d = with_mask_3d\n+        self.with_seg_3d = with_seg_3d\n+        self.seg_3d_dtype = seg_3d_dtype\n+\n+    def _load_bboxes_3d(self, results):\n+        \"\"\"Private function to load 3D bounding box annotations.\n+\n+        Args:\n+            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n+\n+        Returns:\n+            dict: The dict containing loaded 3D bounding box annotations.\n+        \"\"\"\n+        results[\"gt_bboxes_3d\"] = results[\"ann_info\"][\"gt_bboxes_3d\"]\n+        results[\"bbox3d_fields\"].append(\"gt_bboxes_3d\")\n+        return results\n+\n+    def _load_bboxes_depth(self, results):\n+        \"\"\"Private function to load 2.5D bounding box annotations.\n+\n+        Args:\n+            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n+\n+        Returns:\n+            dict: The dict containing loaded 2.5D bounding box annotations.\n+        \"\"\"\n+        results[\"centers2d\"] = results[\"ann_info\"][\"centers2d\"]\n+        results[\"depths\"] = results[\"ann_info\"][\"depths\"]\n+        return results\n+\n+    def _load_labels_3d(self, results):\n+        \"\"\"Private function to load label annotations.\n+\n+        Args:\n+            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n+\n+        Returns:\n+            dict: The dict containing loaded label annotations.\n+        \"\"\"\n+        results[\"gt_labels_3d\"] = results[\"ann_info\"][\"gt_labels_3d\"]\n+        return results\n+\n+    def _load_attr_labels(self, results):\n+        \"\"\"Private function to load label annotations.\n+\n+        Args:\n+            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n+\n+        Returns:\n+            dict: The dict containing loaded label annotations.\n+        \"\"\"\n+        results[\"attr_labels\"] = results[\"ann_info\"][\"attr_labels\"]\n+        return results\n+\n+    def _load_masks_3d(self, results):\n+        \"\"\"Private function to load 3D mask annotations.\n+\n+        Args:\n+            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n+\n+        Returns:\n+            dict: The dict containing loaded 3D mask annotations.\n+        \"\"\"\n+        pts_instance_mask_path = results[\"ann_info\"][\"pts_instance_mask_path\"]\n+\n+        if self.file_client is None:\n+            self.file_client = mmcv.FileClient(**self.file_client_args)\n+        try:\n+            mask_bytes = self.file_client.get(pts_instance_mask_path)\n+            pts_instance_mask = np.frombuffer(mask_bytes, dtype=np.int64)\n+        except ConnectionError:\n+            mmcv.check_file_exist(pts_instance_mask_path)\n+            pts_instance_mask = np.fromfile(pts_instance_mask_path, dtype=np.int64)\n+\n+        results[\"pts_instance_mask\"] = pts_instance_mask\n+        results[\"pts_mask_fields\"].append(\"pts_instance_mask\")\n+        return results\n+\n+    def _load_semantic_seg_3d(self, results):\n+        \"\"\"Private function to load 3D semantic segmentation annotations.\n+\n+        Args:\n+            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n+\n+        Returns:\n+            dict: The dict containing the semantic segmentation annotations.\n+        \"\"\"\n+        pts_semantic_mask_path = results[\"ann_info\"][\"pts_semantic_mask_path\"]\n+\n+        if self.file_client is None:\n+            self.file_client = mmcv.FileClient(**self.file_client_args)\n+        try:\n+            mask_bytes = self.file_client.get(pts_semantic_mask_path)\n+            # add .copy() to fix read-only bug\n+            pts_semantic_mask = np.frombuffer(\n+                mask_bytes, dtype=self.seg_3d_dtype\n+            ).copy()\n+        except ConnectionError:\n+            mmcv.check_file_exist(pts_semantic_mask_path)\n+            pts_semantic_mask = np.fromfile(pts_semantic_mask_path, dtype=np.int64)\n+\n+        results[\"pts_semantic_mask\"] = pts_semantic_mask\n+        results[\"pts_seg_fields\"].append(\"pts_semantic_mask\")\n+        return results\n+\n+    def __call__(self, results):\n+        \"\"\"Call function to load multiple types annotations.\n+\n+        Args:\n+            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n+\n+        Returns:\n+            dict: The dict containing loaded 3D bounding box, label, mask and\n+                semantic segmentation annotations.\n+        \"\"\"\n+        results = super().__call__(results)\n+        if self.with_bbox_3d:\n+            results = self._load_bboxes_3d(results)\n+            if results is None:\n+                return None\n+        if self.with_bbox_depth:\n+            results = self._load_bboxes_depth(results)\n+            if results is None:\n+                return None\n+        if self.with_label_3d:\n+            results = self._load_labels_3d(results)\n+        if self.with_attr_label:\n+            results = self._load_attr_labels(results)\n+        if self.with_mask_3d:\n+            results = self._load_masks_3d(results)\n+        if self.with_seg_3d:\n+            results = self._load_semantic_seg_3d(results)\n+\n+        return results\n+\n+    def __repr__(self):\n+        \"\"\"str: Return a string that describes the module.\"\"\"\n+        indent_str = \"    \"\n+        repr_str = self.__class__.__name__ + \"(\\n\"\n+        repr_str += f\"{indent_str}with_bbox_3d={self.with_bbox_3d}, \"\n+        repr_str += f\"{indent_str}with_label_3d={self.with_label_3d}, \"\n+        repr_str += f\"{indent_str}with_attr_label={self.with_attr_label}, \"\n+        repr_str += f\"{indent_str}with_mask_3d={self.with_mask_3d}, \"\n+        repr_str += f\"{indent_str}with_seg_3d={self.with_seg_3d}, \"\n+        repr_str += f\"{indent_str}with_bbox={self.with_bbox}, \"\n+        repr_str += f\"{indent_str}with_label={self.with_label}, \"\n+        repr_str += f\"{indent_str}with_mask={self.with_mask}, \"\n+        repr_str += f\"{indent_str}with_seg={self.with_seg}, \"\n+        repr_str += f\"{indent_str}with_bbox_depth={self.with_bbox_depth}, \"\n+        repr_str += f\"{indent_str}poly2mask={self.poly2mask})\"\n+        return repr_str\n+\n+\n+@PIPELINES.register_module()\n+class PointToMultiViewDepth(object):\n+\n+    def __init__(self, grid_config, downsample=1):\n+        self.downsample = downsample\n+        self.grid_config = grid_config\n+\n+    def points2depthmap(self, points, height, width):\n+        height, width = height // self.downsample, width // self.downsample\n+        depth_map = torch.zeros((height, width), dtype=torch.float32)\n+        coor = torch.round(points[:, :2] / self.downsample)\n+        depth = points[:, 2]\n+        kept1 = (\n+            (coor[:, 0] >= 0)\n+            & (coor[:, 0] < width)\n+            & (coor[:, 1] >= 0)\n+            & (coor[:, 1] < height)\n+            & (depth < self.grid_config[\"depth\"][1])\n+            & (depth >= self.grid_config[\"depth\"][0])\n+        )\n+        coor, depth = coor[kept1], depth[kept1]\n+        ranks = coor[:, 0] + coor[:, 1] * width\n+        sort = (ranks + depth / 100.0).argsort()\n+        coor, depth, ranks = coor[sort], depth[sort], ranks[sort]\n+\n+        kept2 = torch.ones(coor.shape[0], device=coor.device, dtype=torch.bool)\n+        kept2[1:] = ranks[1:] != ranks[:-1]\n+        coor, depth = coor[kept2], depth[kept2]\n+        coor = coor.to(torch.long)\n+        depth_map[coor[:, 1], coor[:, 0]] = depth\n+        return depth_map\n+\n+    def __call__(self, results):\n+        points_lidar = results[\"points\"]\n+        imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n+        # post_rots, post_trans, bda = results[\"img_inputs\"][4:]\n+        post_rots, post_trans = results[\"img_inputs\"][4:]\n+        depth_map_list = []\n+        for cid in range(len(results[\"cam_names\"])):\n+            cam_name = results[\"cam_names\"][cid]\n+            lidar2lidarego = np.eye(4, dtype=np.float32)\n+            lidar2lidarego[:3, :3] = Quaternion(\n+                results[\"curr\"][\"lidar2ego_rotation\"]\n+            ).rotation_matrix\n+            lidar2lidarego[:3, 3] = results[\"curr\"][\"lidar2ego_translation\"]\n+            lidar2lidarego = torch.from_numpy(lidar2lidarego)\n+\n+            lidarego2global = np.eye(4, dtype=np.float32)\n+            lidarego2global[:3, :3] = Quaternion(\n+                results[\"curr\"][\"ego2global_rotation\"]\n+            ).rotation_matrix\n+            lidarego2global[:3, 3] = results[\"curr\"][\"ego2global_translation\"]\n+            lidarego2global = torch.from_numpy(lidarego2global)\n+\n+            cam2camego = np.eye(4, dtype=np.float32)\n+            cam2camego[:3, :3] = Quaternion(\n+                results[\"curr\"][\"cams\"][cam_name][\"sensor2ego_rotation\"]\n+            ).rotation_matrix\n+            cam2camego[:3, 3] = results[\"curr\"][\"cams\"][cam_name][\n+                \"sensor2ego_translation\"\n+            ]\n+            cam2camego = torch.from_numpy(cam2camego)\n+\n+            camego2global = np.eye(4, dtype=np.float32)\n+            camego2global[:3, :3] = Quaternion(\n+                results[\"curr\"][\"cams\"][cam_name][\"ego2global_rotation\"]\n+            ).rotation_matrix\n+            camego2global[:3, 3] = results[\"curr\"][\"cams\"][cam_name][\n+                \"ego2global_translation\"\n+            ]\n+            camego2global = torch.from_numpy(camego2global)\n+\n+            cam2img = np.eye(4, dtype=np.float32)\n+            cam2img = torch.from_numpy(cam2img)\n+            cam2img[:3, :3] = intrins[cid]\n+\n+            lidar2cam = torch.inverse(camego2global.matmul(cam2camego)).matmul(\n+                lidarego2global.matmul(lidar2lidarego)\n+            )\n+            lidar2img = cam2img.matmul(lidar2cam)\n+            points_img = points_lidar.tensor[:, :3].matmul(\n+                lidar2img[:3, :3].T\n+            ) + lidar2img[:3, 3].unsqueeze(0)\n+            points_img = torch.cat(\n+                [points_img[:, :2] / points_img[:, 2:3], points_img[:, 2:3]], 1\n+            )\n+            points_img = (\n+                points_img.matmul(post_rots[cid].T) + post_trans[cid : cid + 1, :]\n+            )\n+            depth_map = self.points2depthmap(points_img, imgs.shape[2], imgs.shape[3])\n+            depth_map_list.append(depth_map)\n+        depth_map = torch.stack(depth_map_list)\n+        results[\"gt_depth\"] = depth_map\n+        return results\n+\n+\n+@PIPELINES.register_module()\n+class PointToMultiViewDepthFusion(PointToMultiViewDepth):\n+    def __call__(self, results):\n+        points_camego_aug = results[\"points\"].tensor[:, :3]\n+        # print(points_lidar.shape)\n+        imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n+        post_rots, post_trans, bda = results[\"img_inputs\"][4:]\n+        points_camego = points_camego_aug - bda[:3, 3].view(1, 3)\n+        points_camego = points_camego.matmul(torch.inverse(bda[:3, :3]).T)\n+\n+        depth_map_list = []\n+        for cid in range(len(results[\"cam_names\"])):\n+            cam_name = results[\"cam_names\"][cid]\n+\n+            cam2camego = np.eye(4, dtype=np.float32)\n+            cam2camego[:3, :3] = Quaternion(\n+                results[\"curr\"][\"cams\"][cam_name][\"sensor2ego_rotation\"]\n+            ).rotation_matrix\n+            cam2camego[:3, 3] = results[\"curr\"][\"cams\"][cam_name][\n+                \"sensor2ego_translation\"\n+            ]\n+            cam2camego = torch.from_numpy(cam2camego)\n+\n+            cam2img = np.eye(4, dtype=np.float32)\n+            cam2img = torch.from_numpy(cam2img)\n+            cam2img[:3, :3] = intrins[cid]\n+\n+            camego2img = cam2img.matmul(torch.inverse(cam2camego))\n+\n+            points_img = points_camego.matmul(camego2img[:3, :3].T) + camego2img[\n+                :3, 3\n+            ].unsqueeze(0)\n+            points_img = torch.cat(\n+                [points_img[:, :2] / points_img[:, 2:3], points_img[:, 2:3]], 1\n+            )\n+            points_img = (\n+                points_img.matmul(post_rots[cid].T) + post_trans[cid : cid + 1, :]\n+            )\n+            depth_map = self.points2depthmap(points_img, imgs.shape[2], imgs.shape[3])\n+            depth_map_list.append(depth_map)\n+        depth_map = torch.stack(depth_map_list)\n+        results[\"gt_depth\"] = depth_map\n+        return results\n+\n+\n+def mmlabNormalize(img):\n+    from mmcv.image.photometric import imnormalize\n+\n+    mean = np.array([123.675, 116.28, 103.53], dtype=np.float32)\n+    std = np.array([58.395, 57.12, 57.375], dtype=np.float32)\n+    to_rgb = True\n+    img = imnormalize(np.array(img), mean, std, to_rgb)\n+    img = torch.tensor(img).float().permute(2, 0, 1).contiguous()\n+    return img\n+\n+\n+@PIPELINES.register_module()\n+class PrepareImageInputs(object):\n+    \"\"\"Load multi channel images from a list of separate channel files.\n+\n+    Expects results['img_filename'] to be a list of filenames.\n+\n+    Args:\n+        to_float32 (bool): Whether to convert the img to float32.\n+            Defaults to False.\n+        color_type (str): Color type of the file. Defaults to 'unchanged'.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        data_config,\n+        is_train=False,\n+        sequential=False,\n+        opencv_pp=False,\n+    ):\n+        self.is_train = is_train\n+        self.data_config = data_config\n+        self.normalize_img = mmlabNormalize\n+        self.sequential = sequential\n+        self.opencv_pp = opencv_pp\n+\n+    def get_rot(self, h):\n+        return torch.Tensor(\n+            [\n+                [np.cos(h), np.sin(h)],\n+                [-np.sin(h), np.cos(h)],\n+            ]\n+        )\n+\n+    def img_transform(\n+        self, img, post_rot, post_tran, resize, resize_dims, crop, flip, rotate\n+    ):\n+        # adjust image\n+        if not self.opencv_pp:\n+            img = self.img_transform_core(img, resize_dims, crop, flip, rotate)\n+\n+        # post-homography transformation\n+        post_rot *= resize\n+        post_tran -= torch.Tensor(crop[:2])\n+        if flip:\n+            A = torch.Tensor([[-1, 0], [0, 1]])\n+            b = torch.Tensor([crop[2] - crop[0], 0])\n+            post_rot = A.matmul(post_rot)\n+            post_tran = A.matmul(post_tran) + b\n+        A = self.get_rot(rotate / 180 * np.pi)\n+        b = torch.Tensor([crop[2] - crop[0], crop[3] - crop[1]]) / 2\n+        b = A.matmul(-b) + b\n+        post_rot = A.matmul(post_rot)\n+        post_tran = A.matmul(post_tran) + b\n+        if self.opencv_pp:\n+            img = self.img_transform_core_opencv(img, post_rot, post_tran, crop)\n+        return img, post_rot, post_tran\n+\n+    def img_transform_core_opencv(self, img, post_rot, post_tran, crop):\n+        img = np.array(img).astype(np.float32)\n+        img = cv2.warpAffine(\n+            img,\n+            np.concatenate([post_rot, post_tran.reshape(2, 1)], axis=1),\n+            (crop[2] - crop[0], crop[3] - crop[1]),\n+            flags=cv2.INTER_LINEAR,\n+        )\n+        return img\n+\n+    def img_transform_core(self, img, resize_dims, crop, flip, rotate):\n+        # adjust image\n+        img = img.resize(resize_dims)\n+        img = img.crop(crop)\n+        if flip:\n+            img = img.transpose(method=Image.FLIP_LEFT_RIGHT)\n+        img = img.rotate(rotate)\n+        return img\n+\n+    def choose_cams(self):\n+        if self.is_train and self.data_config[\"Ncams\"] < len(self.data_config[\"cams\"]):\n+            cam_names = np.random.choice(\n+                self.data_config[\"cams\"], self.data_config[\"Ncams\"], replace=False\n+            )\n+        else:\n+            cam_names = self.data_config[\"cams\"]\n+        return cam_names\n+\n+    def sample_augmentation(self, H, W, flip=None, scale=None):\n+        fH, fW = self.data_config[\"input_size\"]\n+        if self.is_train:\n+            resize = float(fW) / float(W)\n+            resize += np.random.uniform(*self.data_config[\"resize\"])\n+            resize_dims = (int(W * resize), int(H * resize))\n+            newW, newH = resize_dims\n+            random_crop_height = self.data_config.get(\"random_crop_height\", False)\n+            if random_crop_height:\n+                crop_h = int(np.random.uniform(max(0.3 * newH, newH - fH), newH - fH))\n+            else:\n+                crop_h = (\n+                    int((1 - np.random.uniform(*self.data_config[\"crop_h\"])) * newH)\n+                    - fH\n+                )\n+            crop_w = int(np.random.uniform(0, max(0, newW - fW)))\n+            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n+            flip = self.data_config[\"flip\"] and np.random.choice([0, 1])\n+            rotate = np.random.uniform(*self.data_config[\"rot\"])\n+            if self.data_config.get(\"vflip\", False) and np.random.choice([0, 1]):\n+                rotate += 180\n+        else:\n+            resize = float(fW) / float(W)\n+            if scale is not None:\n+                resize += scale\n+            else:\n+                resize += self.data_config.get(\"resize_test\", 0.0)\n+            resize_dims = (int(W * resize), int(H * resize))\n+            newW, newH = resize_dims\n+            crop_h = int((1 - np.mean(self.data_config[\"crop_h\"])) * newH) - fH\n+            crop_w = int(max(0, newW - fW) / 2)\n+            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n+            flip = False if flip is None else flip\n+            rotate = 0\n+        return resize, resize_dims, crop, flip, rotate\n+\n+    def get_sensor_transforms(self, cam_info, cam_name):\n+        w, x, y, z = cam_info[\"cams\"][cam_name][\"sensor2ego_rotation\"]\n+        # sweep sensor to sweep ego\n+        sensor2ego_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n+        sensor2ego_tran = torch.Tensor(\n+            cam_info[\"cams\"][cam_name][\"sensor2ego_translation\"]\n+        )\n+        sensor2ego = sensor2ego_rot.new_zeros((4, 4))\n+        sensor2ego[3, 3] = 1\n+        sensor2ego[:3, :3] = sensor2ego_rot\n+        sensor2ego[:3, -1] = sensor2ego_tran\n+        # sweep ego to global\n+        w, x, y, z = cam_info[\"cams\"][cam_name][\"ego2global_rotation\"]\n+        ego2global_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n+        ego2global_tran = torch.Tensor(\n+            cam_info[\"cams\"][cam_name][\"ego2global_translation\"]\n+        )\n+        ego2global = ego2global_rot.new_zeros((4, 4))\n+        ego2global[3, 3] = 1\n+        ego2global[:3, :3] = ego2global_rot\n+        ego2global[:3, -1] = ego2global_tran\n+        return sensor2ego, ego2global\n+\n+    def photo_metric_distortion(self, img, pmd):\n+        \"\"\"Call function to perform photometric distortion on images.\n+        Args:\n+            results (dict): Result dict from loading pipeline.\n+        Returns:\n+            dict: Result dict with images distorted.\n+        \"\"\"\n+        if np.random.rand() > pmd.get(\"rate\", 1.0):\n+            return img\n+\n+        img = np.array(img).astype(np.float32)\n+        assert img.dtype == np.float32, (\n+            \"PhotoMetricDistortion needs the input image of dtype np.float32,\"\n+            ' please set \"to_float32=True\" in \"LoadImageFromFile\" pipeline'\n+        )\n+        # random brightness\n+        if np.random.randint(2):\n+            delta = np.random.uniform(-pmd[\"brightness_delta\"], pmd[\"brightness_delta\"])\n+            img += delta\n+\n+        # mode == 0 --> do random contrast first\n+        # mode == 1 --> do random contrast last\n+        mode = np.random.randint(2)\n+        if mode == 1:\n+            if np.random.randint(2):\n+                alpha = np.random.uniform(pmd[\"contrast_lower\"], pmd[\"contrast_upper\"])\n+                img *= alpha\n+\n+        # convert color from BGR to HSV\n+        img = mmcv.bgr2hsv(img)\n+\n+        # random saturation\n+        if np.random.randint(2):\n+            img[..., 1] *= np.random.uniform(\n+                pmd[\"saturation_lower\"], pmd[\"saturation_upper\"]\n+            )\n+\n+        # random hue\n+        if np.random.randint(2):\n+            img[..., 0] += np.random.uniform(-pmd[\"hue_delta\"], pmd[\"hue_delta\"])\n+            img[..., 0][img[..., 0] > 360] -= 360\n+            img[..., 0][img[..., 0] < 0] += 360\n+\n+        # convert color from HSV to BGR\n+        img = mmcv.hsv2bgr(img)\n+\n+        # random contrast\n+        if mode == 0:\n+            if np.random.randint(2):\n+                alpha = np.random.uniform(pmd[\"contrast_lower\"], pmd[\"contrast_upper\"])\n+                img *= alpha\n+\n+        # randomly swap channels\n+        if np.random.randint(2):\n+            img = img[..., np.random.permutation(3)]\n+        return Image.fromarray(img.astype(np.uint8))\n+\n+    def get_inputs(self, results, flip=None, scale=None):\n+        imgs = []\n+        sensor2egos = []\n+        ego2globals = []\n+        intrins = []\n+        post_rots = []\n+        post_trans = []\n+        cam_names = self.choose_cams()\n+        results[\"cam_names\"] = cam_names\n+        canvas = []\n+        for cam_name in cam_names:\n+            cam_data = results[\"curr\"][\"cams\"][cam_name]\n+            filename = cam_data[\"data_path\"]\n+            img = Image.open(filename)\n+            post_rot = torch.eye(2)\n+            post_tran = torch.zeros(2)\n+\n+            intrin = torch.Tensor(cam_data[\"cam_intrinsic\"])\n+\n+            sensor2ego, ego2global = self.get_sensor_transforms(\n+                results[\"curr\"], cam_name\n+            )\n+            # image view augmentation (resize, crop, horizontal flip, rotate)\n+            img_augs = self.sample_augmentation(\n+                H=img.height, W=img.width, flip=flip, scale=scale\n+            )\n+            resize, resize_dims, crop, flip, rotate = img_augs\n+            img, post_rot2, post_tran2 = self.img_transform(\n+                img,\n+                post_rot,\n+                post_tran,\n+                resize=resize,\n+                resize_dims=resize_dims,\n+                crop=crop,\n+                flip=flip,\n+                rotate=rotate,\n+            )\n+\n+            # for convenience, make augmentation matrices 3x3\n+            post_tran = torch.zeros(3)\n+            post_rot = torch.eye(3)\n+            post_tran[:2] = post_tran2\n+            post_rot[:2, :2] = post_rot2\n+\n+            if self.is_train and self.data_config.get(\"pmd\", None) is not None:\n+                img = self.photo_metric_distortion(img, self.data_config[\"pmd\"])\n+\n+            canvas.append(np.array(img))\n+            imgs.append(self.normalize_img(img))\n+\n+            if self.sequential:\n+                assert \"adjacent\" in results\n+                for adj_info in results[\"adjacent\"]:\n+                    filename_adj = adj_info[\"cams\"][cam_name][\"data_path\"]\n+                    img_adjacent = Image.open(filename_adj)\n+                    if self.opencv_pp:\n+                        img_adjacent = self.img_transform_core_opencv(\n+                            img_adjacent, post_rot[:2, :2], post_tran[:2], crop\n+                        )\n+                    else:\n+                        img_adjacent = self.img_transform_core(\n+                            img_adjacent,\n+                            resize_dims=resize_dims,\n+                            crop=crop,\n+                            flip=flip,\n+                            rotate=rotate,\n+                        )\n+                    imgs.append(self.normalize_img(img_adjacent))\n+            intrins.append(intrin)\n+            sensor2egos.append(sensor2ego)\n+            ego2globals.append(ego2global)\n+            post_rots.append(post_rot)\n+            post_trans.append(post_tran)\n+\n+        if self.sequential:\n+            for adj_info in results[\"adjacent\"]:\n+                post_trans.extend(post_trans[: len(cam_names)])\n+                post_rots.extend(post_rots[: len(cam_names)])\n+                intrins.extend(intrins[: len(cam_names)])\n+\n+                # align\n+                for cam_name in cam_names:\n+                    sensor2ego, ego2global = self.get_sensor_transforms(\n+                        adj_info, cam_name\n+                    )\n+                    sensor2egos.append(sensor2ego)\n+                    ego2globals.append(ego2global)\n+\n+        imgs = torch.stack(imgs)\n+\n+        sensor2egos = torch.stack(sensor2egos)\n+        ego2globals = torch.stack(ego2globals)\n+        intrins = torch.stack(intrins)\n+        post_rots = torch.stack(post_rots)\n+        post_trans = torch.stack(post_trans)\n+        results[\"canvas\"] = canvas\n+        return (imgs, sensor2egos, ego2globals, intrins, post_rots, post_trans)\n+\n+    def __call__(self, results):\n+        results[\"img_inputs\"] = self.get_inputs(results)\n+        return results\n+\n+\n+@PIPELINES.register_module()\n+class LoadAnnotations(object):\n+\n+    def __call__(self, results):\n+        gt_boxes, gt_labels = results[\"ann_infos\"]\n+        gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n+        if len(gt_boxes) == 0:\n+            gt_boxes = torch.zeros(0, 9)\n+        results[\"gt_bboxes_3d\"] = LiDARInstance3DBoxes(\n+            gt_boxes, box_dim=gt_boxes.shape[-1], origin=(0.5, 0.5, 0.5)\n+        )\n+        results[\"gt_labels_3d\"] = gt_labels\n+        return results\n+\n+\n+@PIPELINES.register_module()\n+class LoadAnnotationsBEVDepth(object):\n+    def __init__(self, bda_aug_conf, classes, is_train=True):\n+        self.bda_aug_conf = bda_aug_conf\n+        self.is_train = is_train\n+        self.classes = classes\n+\n+    def sample_bda_augmentation(self):\n+        \"\"\"Generate bda augmentation values based on bda_config.\"\"\"\n+        if self.is_train:\n+            rotate_bda = np.random.uniform(*self.bda_aug_conf[\"rot_lim\"])\n+            scale_bda = np.random.uniform(*self.bda_aug_conf[\"scale_lim\"])\n+            flip_dx = np.random.uniform() < self.bda_aug_conf[\"flip_dx_ratio\"]\n+            flip_dy = np.random.uniform() < self.bda_aug_conf[\"flip_dy_ratio\"]\n+        else:\n+            rotate_bda = 0\n+            scale_bda = 1.0\n+            flip_dx = False\n+            flip_dy = False\n+        return rotate_bda, scale_bda, flip_dx, flip_dy\n+\n+    def bev_transform(self, gt_boxes, rotate_angle, scale_ratio, flip_dx, flip_dy):\n+        rotate_angle = torch.tensor(rotate_angle / 180 * np.pi)\n+        rot_sin = torch.sin(rotate_angle)\n+        rot_cos = torch.cos(rotate_angle)\n+        rot_mat = torch.Tensor(\n+            [[rot_cos, -rot_sin, 0], [rot_sin, rot_cos, 0], [0, 0, 1]]\n+        )\n+        scale_mat = torch.Tensor(\n+            [[scale_ratio, 0, 0], [0, scale_ratio, 0], [0, 0, scale_ratio]]\n+        )\n+        flip_mat = torch.Tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        if flip_dx:\n+            flip_mat = flip_mat @ torch.Tensor([[-1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        if flip_dy:\n+            flip_mat = flip_mat @ torch.Tensor([[1, 0, 0], [0, -1, 0], [0, 0, 1]])\n+        rot_mat = flip_mat @ (scale_mat @ rot_mat)\n+        if gt_boxes.shape[0] > 0:\n+            gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n+            gt_boxes[:, 3:6] *= scale_ratio\n+            gt_boxes[:, 6] += rotate_angle\n+            if flip_dx:\n+                gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n+            if flip_dy:\n+                gt_boxes[:, 6] = -gt_boxes[:, 6]\n+            gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(\n+                -1\n+            )\n+        return gt_boxes, rot_mat\n+\n+    def __call__(self, results):\n+        gt_boxes, gt_labels = results[\"ann_infos\"]\n+        gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n+        rotate_bda, scale_bda, flip_dx, flip_dy = self.sample_bda_augmentation()\n+        bda_mat = torch.zeros(4, 4)\n+        bda_mat[3, 3] = 1\n+        gt_boxes, bda_rot = self.bev_transform(\n+            gt_boxes, rotate_bda, scale_bda, flip_dx, flip_dy\n+        )\n+        bda_mat[:3, :3] = bda_rot\n+        if len(gt_boxes) == 0:\n+            gt_boxes = torch.zeros(0, 9)\n+        results[\"gt_bboxes_3d\"] = LiDARInstance3DBoxes(\n+            gt_boxes, box_dim=gt_boxes.shape[-1], origin=(0.5, 0.5, 0.5)\n+        )\n+        results[\"gt_labels_3d\"] = gt_labels\n+        imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n+        post_rots, post_trans = results[\"img_inputs\"][4:]\n+        results[\"img_inputs\"] = (\n+            imgs,\n+            rots,\n+            trans,\n+            intrins,\n+            post_rots,\n+            post_trans,\n+            # bda_rot,\n+            bda_mat,\n+        )\n+        if \"voxel_semantics\" in results:\n+            if flip_dx:\n+                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n+                    ::-1, ...\n+                ].copy()\n+                results[\"mask_lidar\"] = results[\"mask_lidar\"][::-1, ...].copy()\n+                results[\"mask_camera\"] = results[\"mask_camera\"][::-1, ...].copy()\n+            if flip_dy:\n+                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n+                    :, ::-1, ...\n+                ].copy()\n+                results[\"mask_lidar\"] = results[\"mask_lidar\"][:, ::-1, ...].copy()\n+                results[\"mask_camera\"] = results[\"mask_camera\"][:, ::-1, ...].copy()\n+        return results\n+\n+\n+@PIPELINES.register_module()\n+class BEVAug(object):\n+\n+    def __init__(self, bda_aug_conf, classes, is_train=True):\n+        self.bda_aug_conf = bda_aug_conf\n+        self.is_train = is_train\n+        self.classes = classes\n+\n+    def sample_bda_augmentation(self):\n+        \"\"\"Generate bda augmentation values based on bda_config.\"\"\"\n+        if self.is_train:\n+            rotate_bda = np.random.uniform(*self.bda_aug_conf[\"rot_lim\"])\n+            scale_bda = np.random.uniform(*self.bda_aug_conf[\"scale_lim\"])\n+            flip_dx = np.random.uniform() < self.bda_aug_conf[\"flip_dx_ratio\"]\n+            flip_dy = np.random.uniform() < self.bda_aug_conf[\"flip_dy_ratio\"]\n+            translation_std = self.bda_aug_conf.get(\"tran_lim\", [0.0, 0.0, 0.0])\n+            tran_bda = np.random.normal(scale=translation_std, size=3).T\n+        else:\n+            rotate_bda = 0\n+            scale_bda = 1.0\n+            flip_dx = False\n+            flip_dy = False\n+            tran_bda = np.zeros((1, 3), dtype=np.float32)\n+        return rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda\n+\n+    def bev_transform(\n+        self, gt_boxes, rotate_angle, scale_ratio, flip_dx, flip_dy, tran_bda\n+    ):\n+        rotate_angle = torch.tensor(rotate_angle / 180 * np.pi)\n+        rot_sin = torch.sin(rotate_angle)\n+        rot_cos = torch.cos(rotate_angle)\n+        rot_mat = torch.Tensor(\n+            [[rot_cos, -rot_sin, 0], [rot_sin, rot_cos, 0], [0, 0, 1]]\n+        )\n+        scale_mat = torch.Tensor(\n+            [[scale_ratio, 0, 0], [0, scale_ratio, 0], [0, 0, scale_ratio]]\n+        )\n+        flip_mat = torch.Tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        if flip_dx:\n+            flip_mat = flip_mat @ torch.Tensor([[-1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        if flip_dy:\n+            flip_mat = flip_mat @ torch.Tensor([[1, 0, 0], [0, -1, 0], [0, 0, 1]])\n+        rot_mat = flip_mat @ (scale_mat @ rot_mat)\n+        if gt_boxes.shape[0] > 0:\n+            gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n+            gt_boxes[:, 3:6] *= scale_ratio\n+            gt_boxes[:, 6] += rotate_angle\n+            if flip_dx:\n+                gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n+            if flip_dy:\n+                gt_boxes[:, 6] = -gt_boxes[:, 6]\n+            gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(\n+                -1\n+            )\n+            gt_boxes[:, :3] = gt_boxes[:, :3] + tran_bda\n+        return gt_boxes, rot_mat\n+\n+    def __call__(self, results):\n+        gt_boxes = results[\"gt_bboxes_3d\"].tensor\n+        gt_boxes[:, 2] = gt_boxes[:, 2] + 0.5 * gt_boxes[:, 5]\n+        rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda = (\n+            self.sample_bda_augmentation()\n+        )\n+        bda_mat = torch.zeros(4, 4)\n+        bda_mat[3, 3] = 1\n+        gt_boxes, bda_rot = self.bev_transform(\n+            gt_boxes, rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda\n+        )\n+        if \"points\" in results:\n+            points = results[\"points\"].tensor\n+            points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n+            points[:, :3] = points_aug + tran_bda\n+            points = results[\"points\"].new_point(points)\n+            results[\"points\"] = points\n+\n+        if \"radar\" in results:\n+            points = results[\"radar\"].tensor\n+            points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n+            points[:, :3] = points_aug + tran_bda\n+            points = results[\"radar\"].new_point(points)\n+            results[\"radar\"] = points\n+\n+        bda_mat[:3, :3] = bda_rot\n+        bda_mat[:3, 3] = torch.from_numpy(tran_bda)\n+        if len(gt_boxes) == 0:\n+            gt_boxes = torch.zeros(0, 9)\n+        results[\"gt_bboxes_3d\"] = LiDARInstance3DBoxes(\n+            gt_boxes, box_dim=gt_boxes.shape[-1], origin=(0.5, 0.5, 0.5)\n+        )\n+        if \"img_inputs\" in results:\n+            imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n+            post_rots, post_trans = results[\"img_inputs\"][4:]\n+            results[\"img_inputs\"] = (\n+                imgs,\n+                rots,\n+                trans,\n+                intrins,\n+                post_rots,\n+                post_trans,\n+                bda_mat,\n+            )\n+        if \"voxel_semantics\" in results:\n+            if flip_dx:\n+                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n+                    ::-1, ...\n+                ].copy()\n+                results[\"mask_lidar\"] = results[\"mask_lidar\"][::-1, ...].copy()\n+                results[\"mask_camera\"] = results[\"mask_camera\"][::-1, ...].copy()\n+            if flip_dy:\n+                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n+                    :, ::-1, ...\n+                ].copy()\n+                results[\"mask_lidar\"] = results[\"mask_lidar\"][:, ::-1, ...].copy()\n+                results[\"mask_camera\"] = results[\"mask_camera\"][:, ::-1, ...].copy()\n+        return results\n"
                },
                {
                    "date": 1717330805202,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1575,1582 +1575,4 @@\n                 ].copy()\n                 results[\"mask_lidar\"] = results[\"mask_lidar\"][:, ::-1, ...].copy()\n                 results[\"mask_camera\"] = results[\"mask_camera\"][:, ::-1, ...].copy()\n         return results\n-# Copyright (c) OpenMMLab. All rights reserved.\n-import os\n-\n-import cv2\n-import mmcv\n-import numpy as np\n-import torch\n-from PIL import Image\n-from pyquaternion import Quaternion\n-\n-\n-from nuscenes.utils.data_classes import RadarPointCloud\n-\n-from mmdet3d.core.points import BasePoints, get_points_type, RadarPoints\n-from mmdet.datasets.pipelines import LoadAnnotations, LoadImageFromFile\n-from ...core.bbox import LiDARInstance3DBoxes\n-from ..builder import PIPELINES\n-\n-\n-@PIPELINES.register_module()\n-class LoadOccGTFromFile(object):\n-    def __call__(self, results):\n-        occ_gt_path = results[\"occ_gt_path\"]\n-        occ_gt_path = os.path.join(occ_gt_path, \"labels.npz\")\n-\n-        occ_labels = np.load(occ_gt_path)\n-        semantics = occ_labels[\"semantics\"]\n-        mask_lidar = occ_labels[\"mask_lidar\"]\n-        mask_camera = occ_labels[\"mask_camera\"]\n-\n-        results[\"voxel_semantics\"] = semantics\n-        results[\"mask_lidar\"] = mask_lidar\n-        results[\"mask_camera\"] = mask_camera\n-        return results\n-\n-\n-@PIPELINES.register_module()\n-class LoadMultiViewImageFromFiles(object):\n-    \"\"\"Load multi channel images from a list of separate channel files.\n-\n-    Expects results['img_filename'] to be a list of filenames.\n-\n-    Args:\n-        to_float32 (bool, optional): Whether to convert the img to float32.\n-            Defaults to False.\n-        color_type (str, optional): Color type of the file.\n-            Defaults to 'unchanged'.\n-    \"\"\"\n-\n-    def __init__(self, to_float32=False, color_type=\"unchanged\"):\n-        self.to_float32 = to_float32\n-        self.color_type = color_type\n-\n-    def __call__(self, results):\n-        \"\"\"Call function to load multi-view image from files.\n-\n-        Args:\n-            results (dict): Result dict containing multi-view image filenames.\n-\n-        Returns:\n-            dict: The result dict containing the multi-view image data.\n-                Added keys and values are described below.\n-\n-                - filename (str): Multi-view image filenames.\n-                - img (np.ndarray): Multi-view image arrays.\n-                - img_shape (tuple[int]): Shape of multi-view image arrays.\n-                - ori_shape (tuple[int]): Shape of original image arrays.\n-                - pad_shape (tuple[int]): Shape of padded image arrays.\n-                - scale_factor (float): Scale factor.\n-                - img_norm_cfg (dict): Normalization configuration of images.\n-        \"\"\"\n-        filename = results[\"img_filename\"]\n-        # img is of shape (h, w, c, num_views)\n-        img = np.stack(\n-            [mmcv.imread(name, self.color_type) for name in filename], axis=-1\n-        )\n-        if self.to_float32:\n-            img = img.astype(np.float32)\n-        results[\"filename\"] = filename\n-        # unravel to list, see `DefaultFormatBundle` in formatting.py\n-        # which will transpose each image separately and then stack into array\n-        results[\"img\"] = [img[..., i] for i in range(img.shape[-1])]\n-        results[\"img_shape\"] = img.shape\n-        results[\"ori_shape\"] = img.shape\n-        # Set initial values for default meta_keys\n-        results[\"pad_shape\"] = img.shape\n-        results[\"scale_factor\"] = 1.0\n-        num_channels = 1 if len(img.shape) < 3 else img.shape[2]\n-        results[\"img_norm_cfg\"] = dict(\n-            mean=np.zeros(num_channels, dtype=np.float32),\n-            std=np.ones(num_channels, dtype=np.float32),\n-            to_rgb=False,\n-        )\n-        return results\n-\n-    def __repr__(self):\n-        \"\"\"str: Return a string that describes the module.\"\"\"\n-        repr_str = self.__class__.__name__\n-        repr_str += f\"(to_float32={self.to_float32}, \"\n-        repr_str += f\"color_type='{self.color_type}')\"\n-        return repr_str\n-\n-\n-@PIPELINES.register_module()\n-class LoadImageFromFileMono3D(LoadImageFromFile):\n-    \"\"\"Load an image from file in monocular 3D object detection. Compared to 2D\n-    detection, additional camera parameters need to be loaded.\n-\n-    Args:\n-        kwargs (dict): Arguments are the same as those in\n-            :class:`LoadImageFromFile`.\n-    \"\"\"\n-\n-    def __call__(self, results):\n-        \"\"\"Call functions to load image and get image meta information.\n-\n-        Args:\n-            results (dict): Result dict from :obj:`mmdet.CustomDataset`.\n-\n-        Returns:\n-            dict: The dict contains loaded image and meta information.\n-        \"\"\"\n-        super().__call__(results)\n-        results[\"cam2img\"] = results[\"img_info\"][\"cam_intrinsic\"]\n-        return results\n-\n-\n-@PIPELINES.register_module()\n-class LoadPointsFromMultiSweeps(object):\n-    \"\"\"Load points from multiple sweeps.\n-\n-    This is usually used for nuScenes dataset to utilize previous sweeps.\n-\n-    Args:\n-        sweeps_num (int, optional): Number of sweeps. Defaults to 10.\n-        load_dim (int, optional): Dimension number of the loaded points.\n-            Defaults to 5.\n-        use_dim (list[int], optional): Which dimension to use.\n-            Defaults to [0, 1, 2, 4].\n-        time_dim (int, optional): Which dimension to represent the timestamps\n-            of each points. Defaults to 4.\n-        file_client_args (dict, optional): Config dict of file clients,\n-            refer to\n-            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n-            for more details. Defaults to dict(backend='disk').\n-        pad_empty_sweeps (bool, optional): Whether to repeat keyframe when\n-            sweeps is empty. Defaults to False.\n-        remove_close (bool, optional): Whether to remove close points.\n-            Defaults to False.\n-        test_mode (bool, optional): If `test_mode=True`, it will not\n-            randomly sample sweeps but select the nearest N frames.\n-            Defaults to False.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        sweeps_num=10,\n-        load_dim=5,\n-        use_dim=[0, 1, 2, 4],\n-        time_dim=4,\n-        file_client_args=dict(backend=\"disk\"),\n-        pad_empty_sweeps=False,\n-        remove_close=False,\n-        test_mode=False,\n-    ):\n-        self.load_dim = load_dim\n-        self.sweeps_num = sweeps_num\n-        self.use_dim = use_dim\n-        self.time_dim = time_dim\n-        assert (\n-            time_dim < load_dim\n-        ), f\"Expect the timestamp dimension < {load_dim}, got {time_dim}\"\n-        self.file_client_args = file_client_args.copy()\n-        self.file_client = None\n-        self.pad_empty_sweeps = pad_empty_sweeps\n-        self.remove_close = remove_close\n-        self.test_mode = test_mode\n-        assert (\n-            max(use_dim) < load_dim\n-        ), f\"Expect all used dimensions < {load_dim}, got {use_dim}\"\n-\n-    def _load_points(self, pts_filename):\n-        \"\"\"Private function to load point clouds data.\n-\n-        Args:\n-            pts_filename (str): Filename of point clouds data.\n-\n-        Returns:\n-            np.ndarray: An array containing point clouds data.\n-        \"\"\"\n-        if self.file_client is None:\n-            self.file_client = mmcv.FileClient(**self.file_client_args)\n-        try:\n-            pts_bytes = self.file_client.get(pts_filename)\n-            points = np.frombuffer(pts_bytes, dtype=np.float32)\n-        except ConnectionError:\n-            mmcv.check_file_exist(pts_filename)\n-            if pts_filename.endswith(\".npy\"):\n-                points = np.load(pts_filename)\n-            else:\n-                points = np.fromfile(pts_filename, dtype=np.float32)\n-        return points\n-\n-    def _remove_close(self, points, radius=1.0):\n-        \"\"\"Removes point too close within a certain radius from origin.\n-\n-        Args:\n-            points (np.ndarray | :obj:`BasePoints`): Sweep points.\n-            radius (float, optional): Radius below which points are removed.\n-                Defaults to 1.0.\n-\n-        Returns:\n-            np.ndarray: Points after removing.\n-        \"\"\"\n-        if isinstance(points, np.ndarray):\n-            points_numpy = points\n-        elif isinstance(points, BasePoints):\n-            points_numpy = points.tensor.numpy()\n-        else:\n-            raise NotImplementedError\n-        x_filt = np.abs(points_numpy[:, 0]) < radius\n-        y_filt = np.abs(points_numpy[:, 1]) < radius\n-        not_close = np.logical_not(np.logical_and(x_filt, y_filt))\n-        return points[not_close]\n-\n-    def __call__(self, results):\n-        \"\"\"Call function to load multi-sweep point clouds from files.\n-\n-        Args:\n-            results (dict): Result dict containing multi-sweep point cloud\n-                filenames.\n-\n-        Returns:\n-            dict: The result dict containing the multi-sweep points data.\n-                Added key and value are described below.\n-\n-                - points (np.ndarray | :obj:`BasePoints`): Multi-sweep point\n-                    cloud arrays.\n-        \"\"\"\n-        points = results[\"points\"]\n-        points.tensor[:, self.time_dim] = 0\n-        sweep_points_list = [points]\n-        ts = results[\"timestamp\"]\n-        if self.pad_empty_sweeps and len(results[\"sweeps\"]) == 0:\n-            for i in range(self.sweeps_num):\n-                if self.remove_close:\n-                    sweep_points_list.append(self._remove_close(points))\n-                else:\n-                    sweep_points_list.append(points)\n-        else:\n-            if len(results[\"sweeps\"]) <= self.sweeps_num:\n-                choices = np.arange(len(results[\"sweeps\"]))\n-            elif self.test_mode:\n-                choices = np.arange(self.sweeps_num)\n-            else:\n-                choices = np.random.choice(\n-                    len(results[\"sweeps\"]), self.sweeps_num, replace=False\n-                )\n-            for idx in choices:\n-                sweep = results[\"sweeps\"][idx]\n-                points_sweep = self._load_points(sweep[\"data_path\"])\n-                points_sweep = np.copy(points_sweep).reshape(-1, self.load_dim)\n-                if self.remove_close:\n-                    points_sweep = self._remove_close(points_sweep)\n-                sweep_ts = sweep[\"timestamp\"] / 1e6\n-                points_sweep[:, :3] = (\n-                    points_sweep[:, :3] @ sweep[\"sensor2lidar_rotation\"].T\n-                )\n-                points_sweep[:, :3] += sweep[\"sensor2lidar_translation\"]\n-                points_sweep[:, self.time_dim] = ts - sweep_ts\n-                points_sweep = points.new_point(points_sweep)\n-                sweep_points_list.append(points_sweep)\n-\n-        points = points.cat(sweep_points_list)\n-        points = points[:, self.use_dim]\n-        results[\"points\"] = points\n-        return results\n-\n-    def __repr__(self):\n-        \"\"\"str: Return a string that describes the module.\"\"\"\n-        return f\"{self.__class__.__name__}(sweeps_num={self.sweeps_num})\"\n-\n-\n-@PIPELINES.register_module()\n-class PointSegClassMapping(object):\n-    \"\"\"Map original semantic class to valid category ids.\n-\n-    Map valid classes as 0~len(valid_cat_ids)-1 and\n-    others as len(valid_cat_ids).\n-\n-    Args:\n-        valid_cat_ids (tuple[int]): A tuple of valid category.\n-        max_cat_id (int, optional): The max possible cat_id in input\n-            segmentation mask. Defaults to 40.\n-    \"\"\"\n-\n-    def __init__(self, valid_cat_ids, max_cat_id=40):\n-        assert max_cat_id >= np.max(\n-            valid_cat_ids\n-        ), \"max_cat_id should be greater than maximum id in valid_cat_ids\"\n-\n-        self.valid_cat_ids = valid_cat_ids\n-        self.max_cat_id = int(max_cat_id)\n-\n-        # build cat_id to class index mapping\n-        neg_cls = len(valid_cat_ids)\n-        self.cat_id2class = np.ones(self.max_cat_id + 1, dtype=np.int) * neg_cls\n-        for cls_idx, cat_id in enumerate(valid_cat_ids):\n-            self.cat_id2class[cat_id] = cls_idx\n-\n-    def __call__(self, results):\n-        \"\"\"Call function to map original semantic class to valid category ids.\n-\n-        Args:\n-            results (dict): Result dict containing point semantic masks.\n-\n-        Returns:\n-            dict: The result dict containing the mapped category ids.\n-                Updated key and value are described below.\n-\n-                - pts_semantic_mask (np.ndarray): Mapped semantic masks.\n-        \"\"\"\n-        assert \"pts_semantic_mask\" in results\n-        pts_semantic_mask = results[\"pts_semantic_mask\"]\n-\n-        converted_pts_sem_mask = self.cat_id2class[pts_semantic_mask]\n-\n-        results[\"pts_semantic_mask\"] = converted_pts_sem_mask\n-        return results\n-\n-    def __repr__(self):\n-        \"\"\"str: Return a string that describes the module.\"\"\"\n-        repr_str = self.__class__.__name__\n-        repr_str += f\"(valid_cat_ids={self.valid_cat_ids}, \"\n-        repr_str += f\"max_cat_id={self.max_cat_id})\"\n-        return repr_str\n-\n-\n-@PIPELINES.register_module()\n-class NormalizePointsColor(object):\n-    \"\"\"Normalize color of points.\n-\n-    Args:\n-        color_mean (list[float]): Mean color of the point cloud.\n-    \"\"\"\n-\n-    def __init__(self, color_mean):\n-        self.color_mean = color_mean\n-\n-    def __call__(self, results):\n-        \"\"\"Call function to normalize color of points.\n-\n-        Args:\n-            results (dict): Result dict containing point clouds data.\n-\n-        Returns:\n-            dict: The result dict containing the normalized points.\n-                Updated key and value are described below.\n-\n-                - points (:obj:`BasePoints`): Points after color normalization.\n-        \"\"\"\n-        points = results[\"points\"]\n-        assert (\n-            points.attribute_dims is not None\n-            and \"color\" in points.attribute_dims.keys()\n-        ), \"Expect points have color attribute\"\n-        if self.color_mean is not None:\n-            points.color = points.color - points.color.new_tensor(self.color_mean)\n-        points.color = points.color / 255.0\n-        results[\"points\"] = points\n-        return results\n-\n-    def __repr__(self):\n-        \"\"\"str: Return a string that describes the module.\"\"\"\n-        repr_str = self.__class__.__name__\n-        repr_str += f\"(color_mean={self.color_mean})\"\n-        return repr_str\n-\n-\n-@PIPELINES.register_module()\n-class LoadRadarPointsMultiSweeps(object):\n-    \"\"\"Load radar points from multiple sweeps.\n-    This is usually used for nuScenes dataset to utilize previous sweeps.\n-    Args:\n-        sweeps_num (int): Number of sweeps. Defaults to 10.\n-        load_dim (int): Dimension number of the loaded points. Defaults to 5.\n-        use_dim (list[int]): Which dimension to use. Defaults to [0, 1, 2, 4].\n-        file_client_args (dict): Config dict of file clients, refer to\n-            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n-            for more details. Defaults to dict(backend='disk').\n-        pad_empty_sweeps (bool): Whether to repeat keyframe when\n-            sweeps is empty. Defaults to False.\n-        remove_close (bool): Whether to remove close points.\n-            Defaults to False.\n-        test_mode (bool): If test_model=True used for testing, it will not\n-            randomly sample sweeps but select the nearest N frames.\n-            Defaults to False.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        load_dim=18,\n-        use_dim=[0, 1, 2, 3, 4],\n-        sweeps_num=3,\n-        file_client_args=dict(backend=\"disk\"),\n-        max_num=300,\n-        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],\n-        test_mode=False,\n-    ):\n-        self.load_dim = load_dim\n-        self.use_dim = use_dim\n-        self.sweeps_num = sweeps_num\n-        self.file_client_args = file_client_args.copy()\n-        self.file_client = None\n-        self.max_num = max_num\n-        self.test_mode = test_mode\n-        self.pc_range = pc_range\n-\n-    def _load_points(self, pts_filename):\n-        \"\"\"Private function to load point clouds data.\n-        Args:\n-            pts_filename (str): Filename of point clouds data.\n-        Returns:\n-            np.ndarray: An array containing point clouds data.\n-            [N, 18]\n-        \"\"\"\n-        radar_obj = RadarPointCloud.from_file(pts_filename)\n-\n-        # [18, N]\n-        points = radar_obj.points\n-\n-        return points.transpose().astype(np.float32)\n-\n-    def _pad_or_drop(self, points):\n-        \"\"\"\n-        points: [N, 18]\n-        \"\"\"\n-\n-        num_points = points.shape[0]\n-\n-        if num_points == self.max_num:\n-            masks = np.ones((num_points, 1), dtype=points.dtype)\n-\n-            return points, masks\n-\n-        if num_points > self.max_num:\n-            points = np.random.permutation(points)[: self.max_num, :]\n-            masks = np.ones((self.max_num, 1), dtype=points.dtype)\n-\n-            return points, masks\n-\n-        if num_points < self.max_num:\n-            zeros = np.zeros(\n-                (self.max_num - num_points, points.shape[1]), dtype=points.dtype\n-            )\n-            masks = np.ones((num_points, 1), dtype=points.dtype)\n-\n-            points = np.concatenate((points, zeros), axis=0)\n-            masks = np.concatenate((masks, zeros.copy()[:, [0]]), axis=0)\n-\n-            return points, masks\n-\n-    def __call__(self, results):\n-        \"\"\"Call function to load multi-sweep point clouds from files.\n-        Args:\n-            results (dict): Result dict containing multi-sweep point cloud \\\n-                filenames.\n-        Returns:\n-            dict: The result dict containing the multi-sweep points data. \\\n-                Added key and value are described below.\n-                - points (np.ndarray | :obj:`BasePoints`): Multi-sweep point \\\n-                    cloud arrays.\n-        \"\"\"\n-        radars_dict = results[\"radar\"]\n-\n-        points_sweep_list = []\n-        for key, sweeps in radars_dict.items():\n-            if len(sweeps) < self.sweeps_num:\n-                idxes = list(range(len(sweeps)))\n-            else:\n-                idxes = list(range(self.sweeps_num))\n-\n-            ts = sweeps[0][\"timestamp\"] * 1e-6\n-            for idx in idxes:\n-                sweep = sweeps[idx]\n-\n-                points_sweep = self._load_points(sweep[\"data_path\"])\n-                points_sweep = np.copy(points_sweep).reshape(-1, self.load_dim)\n-\n-                timestamp = sweep[\"timestamp\"] * 1e-6\n-                time_diff = ts - timestamp\n-                time_diff = np.ones((points_sweep.shape[0], 1)) * time_diff\n-\n-                # velocity compensated by the ego motion in sensor frame\n-                velo_comp = points_sweep[:, 8:10]\n-                velo_comp = np.concatenate(\n-                    (velo_comp, np.zeros((velo_comp.shape[0], 1))), 1\n-                )\n-                velo_comp = velo_comp @ sweep[\"sensor2lidar_rotation\"].T\n-                velo_comp = velo_comp[:, :2]\n-\n-                # velocity in sensor frame\n-                velo = points_sweep[:, 6:8]\n-                velo = np.concatenate((velo, np.zeros((velo.shape[0], 1))), 1)\n-                velo = velo @ sweep[\"sensor2lidar_rotation\"].T\n-                velo = velo[:, :2]\n-\n-                points_sweep[:, :3] = (\n-                    points_sweep[:, :3] @ sweep[\"sensor2lidar_rotation\"].T\n-                )\n-                points_sweep[:, :3] += sweep[\"sensor2lidar_translation\"]\n-\n-                points_sweep_ = np.concatenate(\n-                    [\n-                        points_sweep[:, :6],\n-                        velo,\n-                        velo_comp,\n-                        points_sweep[:, 10:],\n-                        time_diff,\n-                    ],\n-                    axis=1,\n-                )\n-                points_sweep_list.append(points_sweep_)\n-\n-        points = np.concatenate(points_sweep_list, axis=0)\n-\n-        points = points[:, self.use_dim]\n-\n-        points = RadarPoints(points, points_dim=points.shape[-1], attribute_dims=None)\n-\n-        results[\"radar\"] = points\n-        return results\n-\n-    def __repr__(self):\n-        \"\"\"str: Return a string that describes the module.\"\"\"\n-        return f\"{self.__class__.__name__}(sweeps_num={self.sweeps_num})\"\n-\n-\n-@PIPELINES.register_module()\n-class LoadPointsFromFile(object):\n-    \"\"\"Load Points From File.\n-\n-    Load points from file.\n-\n-    Args:\n-        coord_type (str): The type of coordinates of points cloud.\n-            Available options includes:\n-            - 'LIDAR': Points in LiDAR coordinates.\n-            - 'DEPTH': Points in depth coordinates, usually for indoor dataset.\n-            - 'CAMERA': Points in camera coordinates.\n-        load_dim (int, optional): The dimension of the loaded points.\n-            Defaults to 6.\n-        use_dim (list[int], optional): Which dimensions of the points to use.\n-            Defaults to [0, 1, 2]. For KITTI dataset, set use_dim=4\n-            or use_dim=[0, 1, 2, 3] to use the intensity dimension.\n-        shift_height (bool, optional): Whether to use shifted height.\n-            Defaults to False.\n-        use_color (bool, optional): Whether to use color features.\n-            Defaults to False.\n-        file_client_args (dict, optional): Config dict of file clients,\n-            refer to\n-            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n-            for more details. Defaults to dict(backend='disk').\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        coord_type,\n-        load_dim=6,\n-        use_dim=[0, 1, 2],\n-        shift_height=False,\n-        use_color=False,\n-        file_client_args=dict(backend=\"disk\"),\n-    ):\n-        self.shift_height = shift_height\n-        self.use_color = use_color\n-        if isinstance(use_dim, int):\n-            use_dim = list(range(use_dim))\n-        assert (\n-            max(use_dim) < load_dim\n-        ), f\"Expect all used dimensions < {load_dim}, got {use_dim}\"\n-        assert coord_type in [\"CAMERA\", \"LIDAR\", \"DEPTH\"]\n-\n-        self.coord_type = coord_type\n-        self.load_dim = load_dim\n-        self.use_dim = use_dim\n-        self.file_client_args = file_client_args.copy()\n-        self.file_client = None\n-\n-    def _load_points(self, pts_filename):\n-        \"\"\"Private function to load point clouds data.\n-\n-        Args:\n-            pts_filename (str): Filename of point clouds data.\n-\n-        Returns:\n-            np.ndarray: An array containing point clouds data.\n-        \"\"\"\n-        if self.file_client is None:\n-            self.file_client = mmcv.FileClient(**self.file_client_args)\n-        try:\n-            pts_bytes = self.file_client.get(pts_filename)\n-            points = np.frombuffer(pts_bytes, dtype=np.float32)\n-        except ConnectionError:\n-            mmcv.check_file_exist(pts_filename)\n-            if pts_filename.endswith(\".npy\"):\n-                points = np.load(pts_filename)\n-            else:\n-                points = np.fromfile(pts_filename, dtype=np.float32)\n-\n-        return points\n-\n-    def __call__(self, results):\n-        \"\"\"Call function to load points data from file.\n-\n-        Args:\n-            results (dict): Result dict containing point clouds data.\n-\n-        Returns:\n-            dict: The result dict containing the point clouds data.\n-                Added key and value are described below.\n-\n-                - points (:obj:`BasePoints`): Point clouds data.\n-        \"\"\"\n-        pts_filename = results[\"pts_filename\"]\n-        points = self._load_points(pts_filename)\n-        points = points.reshape(-1, self.load_dim)\n-        points = points[:, self.use_dim]\n-        attribute_dims = None\n-\n-        if self.shift_height:\n-            floor_height = np.percentile(points[:, 2], 0.99)\n-            height = points[:, 2] - floor_height\n-            points = np.concatenate(\n-                [points[:, :3], np.expand_dims(height, 1), points[:, 3:]], 1\n-            )\n-            attribute_dims = dict(height=3)\n-\n-        if self.use_color:\n-            assert len(self.use_dim) >= 6\n-            if attribute_dims is None:\n-                attribute_dims = dict()\n-            attribute_dims.update(\n-                dict(\n-                    color=[\n-                        points.shape[1] - 3,\n-                        points.shape[1] - 2,\n-                        points.shape[1] - 1,\n-                    ]\n-                )\n-            )\n-\n-        points_class = get_points_type(self.coord_type)\n-        points = points_class(\n-            points, points_dim=points.shape[-1], attribute_dims=attribute_dims\n-        )\n-        results[\"points\"] = points\n-\n-        return results\n-\n-    def __repr__(self):\n-        \"\"\"str: Return a string that describes the module.\"\"\"\n-        repr_str = self.__class__.__name__ + \"(\"\n-        repr_str += f\"shift_height={self.shift_height}, \"\n-        repr_str += f\"use_color={self.use_color}, \"\n-        repr_str += f\"file_client_args={self.file_client_args}, \"\n-        repr_str += f\"load_dim={self.load_dim}, \"\n-        repr_str += f\"use_dim={self.use_dim})\"\n-        return repr_str\n-\n-\n-@PIPELINES.register_module()\n-class LoadPointsFromDict(LoadPointsFromFile):\n-    \"\"\"Load Points From Dict.\"\"\"\n-\n-    def __call__(self, results):\n-        assert \"points\" in results\n-        return results\n-\n-\n-@PIPELINES.register_module()\n-class LoadAnnotations3D(LoadAnnotations):\n-    \"\"\"Load Annotations3D.\n-\n-    Load instance mask and semantic mask of points and\n-    encapsulate the items into related fields.\n-\n-    Args:\n-        with_bbox_3d (bool, optional): Whether to load 3D boxes.\n-            Defaults to True.\n-        with_label_3d (bool, optional): Whether to load 3D labels.\n-            Defaults to True.\n-        with_attr_label (bool, optional): Whether to load attribute label.\n-            Defaults to False.\n-        with_mask_3d (bool, optional): Whether to load 3D instance masks.\n-            for points. Defaults to False.\n-        with_seg_3d (bool, optional): Whether to load 3D semantic masks.\n-            for points. Defaults to False.\n-        with_bbox (bool, optional): Whether to load 2D boxes.\n-            Defaults to False.\n-        with_label (bool, optional): Whether to load 2D labels.\n-            Defaults to False.\n-        with_mask (bool, optional): Whether to load 2D instance masks.\n-            Defaults to False.\n-        with_seg (bool, optional): Whether to load 2D semantic masks.\n-            Defaults to False.\n-        with_bbox_depth (bool, optional): Whether to load 2.5D boxes.\n-            Defaults to False.\n-        poly2mask (bool, optional): Whether to convert polygon annotations\n-            to bitmasks. Defaults to True.\n-        seg_3d_dtype (dtype, optional): Dtype of 3D semantic masks.\n-            Defaults to int64\n-        file_client_args (dict): Config dict of file clients, refer to\n-            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n-            for more details.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        with_bbox_3d=True,\n-        with_label_3d=True,\n-        with_attr_label=False,\n-        with_mask_3d=False,\n-        with_seg_3d=False,\n-        with_bbox=False,\n-        with_label=False,\n-        with_mask=False,\n-        with_seg=False,\n-        with_bbox_depth=False,\n-        poly2mask=True,\n-        seg_3d_dtype=np.int64,\n-        file_client_args=dict(backend=\"disk\"),\n-    ):\n-        super().__init__(\n-            with_bbox,\n-            with_label,\n-            with_mask,\n-            with_seg,\n-            poly2mask,\n-            file_client_args=file_client_args,\n-        )\n-        self.with_bbox_3d = with_bbox_3d\n-        self.with_bbox_depth = with_bbox_depth\n-        self.with_label_3d = with_label_3d\n-        self.with_attr_label = with_attr_label\n-        self.with_mask_3d = with_mask_3d\n-        self.with_seg_3d = with_seg_3d\n-        self.seg_3d_dtype = seg_3d_dtype\n-\n-    def _load_bboxes_3d(self, results):\n-        \"\"\"Private function to load 3D bounding box annotations.\n-\n-        Args:\n-            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n-\n-        Returns:\n-            dict: The dict containing loaded 3D bounding box annotations.\n-        \"\"\"\n-        results[\"gt_bboxes_3d\"] = results[\"ann_info\"][\"gt_bboxes_3d\"]\n-        results[\"bbox3d_fields\"].append(\"gt_bboxes_3d\")\n-        return results\n-\n-    def _load_bboxes_depth(self, results):\n-        \"\"\"Private function to load 2.5D bounding box annotations.\n-\n-        Args:\n-            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n-\n-        Returns:\n-            dict: The dict containing loaded 2.5D bounding box annotations.\n-        \"\"\"\n-        results[\"centers2d\"] = results[\"ann_info\"][\"centers2d\"]\n-        results[\"depths\"] = results[\"ann_info\"][\"depths\"]\n-        return results\n-\n-    def _load_labels_3d(self, results):\n-        \"\"\"Private function to load label annotations.\n-\n-        Args:\n-            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n-\n-        Returns:\n-            dict: The dict containing loaded label annotations.\n-        \"\"\"\n-        results[\"gt_labels_3d\"] = results[\"ann_info\"][\"gt_labels_3d\"]\n-        return results\n-\n-    def _load_attr_labels(self, results):\n-        \"\"\"Private function to load label annotations.\n-\n-        Args:\n-            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n-\n-        Returns:\n-            dict: The dict containing loaded label annotations.\n-        \"\"\"\n-        results[\"attr_labels\"] = results[\"ann_info\"][\"attr_labels\"]\n-        return results\n-\n-    def _load_masks_3d(self, results):\n-        \"\"\"Private function to load 3D mask annotations.\n-\n-        Args:\n-            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n-\n-        Returns:\n-            dict: The dict containing loaded 3D mask annotations.\n-        \"\"\"\n-        pts_instance_mask_path = results[\"ann_info\"][\"pts_instance_mask_path\"]\n-\n-        if self.file_client is None:\n-            self.file_client = mmcv.FileClient(**self.file_client_args)\n-        try:\n-            mask_bytes = self.file_client.get(pts_instance_mask_path)\n-            pts_instance_mask = np.frombuffer(mask_bytes, dtype=np.int64)\n-        except ConnectionError:\n-            mmcv.check_file_exist(pts_instance_mask_path)\n-            pts_instance_mask = np.fromfile(pts_instance_mask_path, dtype=np.int64)\n-\n-        results[\"pts_instance_mask\"] = pts_instance_mask\n-        results[\"pts_mask_fields\"].append(\"pts_instance_mask\")\n-        return results\n-\n-    def _load_semantic_seg_3d(self, results):\n-        \"\"\"Private function to load 3D semantic segmentation annotations.\n-\n-        Args:\n-            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n-\n-        Returns:\n-            dict: The dict containing the semantic segmentation annotations.\n-        \"\"\"\n-        pts_semantic_mask_path = results[\"ann_info\"][\"pts_semantic_mask_path\"]\n-\n-        if self.file_client is None:\n-            self.file_client = mmcv.FileClient(**self.file_client_args)\n-        try:\n-            mask_bytes = self.file_client.get(pts_semantic_mask_path)\n-            # add .copy() to fix read-only bug\n-            pts_semantic_mask = np.frombuffer(\n-                mask_bytes, dtype=self.seg_3d_dtype\n-            ).copy()\n-        except ConnectionError:\n-            mmcv.check_file_exist(pts_semantic_mask_path)\n-            pts_semantic_mask = np.fromfile(pts_semantic_mask_path, dtype=np.int64)\n-\n-        results[\"pts_semantic_mask\"] = pts_semantic_mask\n-        results[\"pts_seg_fields\"].append(\"pts_semantic_mask\")\n-        return results\n-\n-    def __call__(self, results):\n-        \"\"\"Call function to load multiple types annotations.\n-\n-        Args:\n-            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n-\n-        Returns:\n-            dict: The dict containing loaded 3D bounding box, label, mask and\n-                semantic segmentation annotations.\n-        \"\"\"\n-        results = super().__call__(results)\n-        if self.with_bbox_3d:\n-            results = self._load_bboxes_3d(results)\n-            if results is None:\n-                return None\n-        if self.with_bbox_depth:\n-            results = self._load_bboxes_depth(results)\n-            if results is None:\n-                return None\n-        if self.with_label_3d:\n-            results = self._load_labels_3d(results)\n-        if self.with_attr_label:\n-            results = self._load_attr_labels(results)\n-        if self.with_mask_3d:\n-            results = self._load_masks_3d(results)\n-        if self.with_seg_3d:\n-            results = self._load_semantic_seg_3d(results)\n-\n-        return results\n-\n-    def __repr__(self):\n-        \"\"\"str: Return a string that describes the module.\"\"\"\n-        indent_str = \"    \"\n-        repr_str = self.__class__.__name__ + \"(\\n\"\n-        repr_str += f\"{indent_str}with_bbox_3d={self.with_bbox_3d}, \"\n-        repr_str += f\"{indent_str}with_label_3d={self.with_label_3d}, \"\n-        repr_str += f\"{indent_str}with_attr_label={self.with_attr_label}, \"\n-        repr_str += f\"{indent_str}with_mask_3d={self.with_mask_3d}, \"\n-        repr_str += f\"{indent_str}with_seg_3d={self.with_seg_3d}, \"\n-        repr_str += f\"{indent_str}with_bbox={self.with_bbox}, \"\n-        repr_str += f\"{indent_str}with_label={self.with_label}, \"\n-        repr_str += f\"{indent_str}with_mask={self.with_mask}, \"\n-        repr_str += f\"{indent_str}with_seg={self.with_seg}, \"\n-        repr_str += f\"{indent_str}with_bbox_depth={self.with_bbox_depth}, \"\n-        repr_str += f\"{indent_str}poly2mask={self.poly2mask})\"\n-        return repr_str\n-\n-\n-@PIPELINES.register_module()\n-class PointToMultiViewDepth(object):\n-\n-    def __init__(self, grid_config, downsample=1):\n-        self.downsample = downsample\n-        self.grid_config = grid_config\n-\n-    def points2depthmap(self, points, height, width):\n-        height, width = height // self.downsample, width // self.downsample\n-        depth_map = torch.zeros((height, width), dtype=torch.float32)\n-        coor = torch.round(points[:, :2] / self.downsample)\n-        depth = points[:, 2]\n-        kept1 = (\n-            (coor[:, 0] >= 0)\n-            & (coor[:, 0] < width)\n-            & (coor[:, 1] >= 0)\n-            & (coor[:, 1] < height)\n-            & (depth < self.grid_config[\"depth\"][1])\n-            & (depth >= self.grid_config[\"depth\"][0])\n-        )\n-        coor, depth = coor[kept1], depth[kept1]\n-        ranks = coor[:, 0] + coor[:, 1] * width\n-        sort = (ranks + depth / 100.0).argsort()\n-        coor, depth, ranks = coor[sort], depth[sort], ranks[sort]\n-\n-        kept2 = torch.ones(coor.shape[0], device=coor.device, dtype=torch.bool)\n-        kept2[1:] = ranks[1:] != ranks[:-1]\n-        coor, depth = coor[kept2], depth[kept2]\n-        coor = coor.to(torch.long)\n-        depth_map[coor[:, 1], coor[:, 0]] = depth\n-        return depth_map\n-\n-    def __call__(self, results):\n-        points_lidar = results[\"points\"]\n-        imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n-        # post_rots, post_trans, bda = results[\"img_inputs\"][4:]\n-        post_rots, post_trans = results[\"img_inputs\"][4:]\n-        depth_map_list = []\n-        for cid in range(len(results[\"cam_names\"])):\n-            cam_name = results[\"cam_names\"][cid]\n-            lidar2lidarego = np.eye(4, dtype=np.float32)\n-            lidar2lidarego[:3, :3] = Quaternion(\n-                results[\"curr\"][\"lidar2ego_rotation\"]\n-            ).rotation_matrix\n-            lidar2lidarego[:3, 3] = results[\"curr\"][\"lidar2ego_translation\"]\n-            lidar2lidarego = torch.from_numpy(lidar2lidarego)\n-\n-            lidarego2global = np.eye(4, dtype=np.float32)\n-            lidarego2global[:3, :3] = Quaternion(\n-                results[\"curr\"][\"ego2global_rotation\"]\n-            ).rotation_matrix\n-            lidarego2global[:3, 3] = results[\"curr\"][\"ego2global_translation\"]\n-            lidarego2global = torch.from_numpy(lidarego2global)\n-\n-            cam2camego = np.eye(4, dtype=np.float32)\n-            cam2camego[:3, :3] = Quaternion(\n-                results[\"curr\"][\"cams\"][cam_name][\"sensor2ego_rotation\"]\n-            ).rotation_matrix\n-            cam2camego[:3, 3] = results[\"curr\"][\"cams\"][cam_name][\n-                \"sensor2ego_translation\"\n-            ]\n-            cam2camego = torch.from_numpy(cam2camego)\n-\n-            camego2global = np.eye(4, dtype=np.float32)\n-            camego2global[:3, :3] = Quaternion(\n-                results[\"curr\"][\"cams\"][cam_name][\"ego2global_rotation\"]\n-            ).rotation_matrix\n-            camego2global[:3, 3] = results[\"curr\"][\"cams\"][cam_name][\n-                \"ego2global_translation\"\n-            ]\n-            camego2global = torch.from_numpy(camego2global)\n-\n-            cam2img = np.eye(4, dtype=np.float32)\n-            cam2img = torch.from_numpy(cam2img)\n-            cam2img[:3, :3] = intrins[cid]\n-\n-            lidar2cam = torch.inverse(camego2global.matmul(cam2camego)).matmul(\n-                lidarego2global.matmul(lidar2lidarego)\n-            )\n-            lidar2img = cam2img.matmul(lidar2cam)\n-            points_img = points_lidar.tensor[:, :3].matmul(\n-                lidar2img[:3, :3].T\n-            ) + lidar2img[:3, 3].unsqueeze(0)\n-            points_img = torch.cat(\n-                [points_img[:, :2] / points_img[:, 2:3], points_img[:, 2:3]], 1\n-            )\n-            points_img = (\n-                points_img.matmul(post_rots[cid].T) + post_trans[cid : cid + 1, :]\n-            )\n-            depth_map = self.points2depthmap(points_img, imgs.shape[2], imgs.shape[3])\n-            depth_map_list.append(depth_map)\n-        depth_map = torch.stack(depth_map_list)\n-        results[\"gt_depth\"] = depth_map\n-        return results\n-\n-\n-@PIPELINES.register_module()\n-class PointToMultiViewDepthFusion(PointToMultiViewDepth):\n-    def __call__(self, results):\n-        points_camego_aug = results[\"points\"].tensor[:, :3]\n-        # print(points_lidar.shape)\n-        imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n-        post_rots, post_trans, bda = results[\"img_inputs\"][4:]\n-        points_camego = points_camego_aug - bda[:3, 3].view(1, 3)\n-        points_camego = points_camego.matmul(torch.inverse(bda[:3, :3]).T)\n-\n-        depth_map_list = []\n-        for cid in range(len(results[\"cam_names\"])):\n-            cam_name = results[\"cam_names\"][cid]\n-\n-            cam2camego = np.eye(4, dtype=np.float32)\n-            cam2camego[:3, :3] = Quaternion(\n-                results[\"curr\"][\"cams\"][cam_name][\"sensor2ego_rotation\"]\n-            ).rotation_matrix\n-            cam2camego[:3, 3] = results[\"curr\"][\"cams\"][cam_name][\n-                \"sensor2ego_translation\"\n-            ]\n-            cam2camego = torch.from_numpy(cam2camego)\n-\n-            cam2img = np.eye(4, dtype=np.float32)\n-            cam2img = torch.from_numpy(cam2img)\n-            cam2img[:3, :3] = intrins[cid]\n-\n-            camego2img = cam2img.matmul(torch.inverse(cam2camego))\n-\n-            points_img = points_camego.matmul(camego2img[:3, :3].T) + camego2img[\n-                :3, 3\n-            ].unsqueeze(0)\n-            points_img = torch.cat(\n-                [points_img[:, :2] / points_img[:, 2:3], points_img[:, 2:3]], 1\n-            )\n-            points_img = (\n-                points_img.matmul(post_rots[cid].T) + post_trans[cid : cid + 1, :]\n-            )\n-            depth_map = self.points2depthmap(points_img, imgs.shape[2], imgs.shape[3])\n-            depth_map_list.append(depth_map)\n-        depth_map = torch.stack(depth_map_list)\n-        results[\"gt_depth\"] = depth_map\n-        return results\n-\n-\n-def mmlabNormalize(img):\n-    from mmcv.image.photometric import imnormalize\n-\n-    mean = np.array([123.675, 116.28, 103.53], dtype=np.float32)\n-    std = np.array([58.395, 57.12, 57.375], dtype=np.float32)\n-    to_rgb = True\n-    img = imnormalize(np.array(img), mean, std, to_rgb)\n-    img = torch.tensor(img).float().permute(2, 0, 1).contiguous()\n-    return img\n-\n-\n-@PIPELINES.register_module()\n-class PrepareImageInputs(object):\n-    \"\"\"Load multi channel images from a list of separate channel files.\n-\n-    Expects results['img_filename'] to be a list of filenames.\n-\n-    Args:\n-        to_float32 (bool): Whether to convert the img to float32.\n-            Defaults to False.\n-        color_type (str): Color type of the file. Defaults to 'unchanged'.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        data_config,\n-        is_train=False,\n-        sequential=False,\n-        opencv_pp=False,\n-    ):\n-        self.is_train = is_train\n-        self.data_config = data_config\n-        self.normalize_img = mmlabNormalize\n-        self.sequential = sequential\n-        self.opencv_pp = opencv_pp\n-\n-    def get_rot(self, h):\n-        return torch.Tensor(\n-            [\n-                [np.cos(h), np.sin(h)],\n-                [-np.sin(h), np.cos(h)],\n-            ]\n-        )\n-\n-    def img_transform(\n-        self, img, post_rot, post_tran, resize, resize_dims, crop, flip, rotate\n-    ):\n-        # adjust image\n-        if not self.opencv_pp:\n-            img = self.img_transform_core(img, resize_dims, crop, flip, rotate)\n-\n-        # post-homography transformation\n-        post_rot *= resize\n-        post_tran -= torch.Tensor(crop[:2])\n-        if flip:\n-            A = torch.Tensor([[-1, 0], [0, 1]])\n-            b = torch.Tensor([crop[2] - crop[0], 0])\n-            post_rot = A.matmul(post_rot)\n-            post_tran = A.matmul(post_tran) + b\n-        A = self.get_rot(rotate / 180 * np.pi)\n-        b = torch.Tensor([crop[2] - crop[0], crop[3] - crop[1]]) / 2\n-        b = A.matmul(-b) + b\n-        post_rot = A.matmul(post_rot)\n-        post_tran = A.matmul(post_tran) + b\n-        if self.opencv_pp:\n-            img = self.img_transform_core_opencv(img, post_rot, post_tran, crop)\n-        return img, post_rot, post_tran\n-\n-    def img_transform_core_opencv(self, img, post_rot, post_tran, crop):\n-        img = np.array(img).astype(np.float32)\n-        img = cv2.warpAffine(\n-            img,\n-            np.concatenate([post_rot, post_tran.reshape(2, 1)], axis=1),\n-            (crop[2] - crop[0], crop[3] - crop[1]),\n-            flags=cv2.INTER_LINEAR,\n-        )\n-        return img\n-\n-    def img_transform_core(self, img, resize_dims, crop, flip, rotate):\n-        # adjust image\n-        img = img.resize(resize_dims)\n-        img = img.crop(crop)\n-        if flip:\n-            img = img.transpose(method=Image.FLIP_LEFT_RIGHT)\n-        img = img.rotate(rotate)\n-        return img\n-\n-    def choose_cams(self):\n-        if self.is_train and self.data_config[\"Ncams\"] < len(self.data_config[\"cams\"]):\n-            cam_names = np.random.choice(\n-                self.data_config[\"cams\"], self.data_config[\"Ncams\"], replace=False\n-            )\n-        else:\n-            cam_names = self.data_config[\"cams\"]\n-        return cam_names\n-\n-    def sample_augmentation(self, H, W, flip=None, scale=None):\n-        fH, fW = self.data_config[\"input_size\"]\n-        if self.is_train:\n-            resize = float(fW) / float(W)\n-            resize += np.random.uniform(*self.data_config[\"resize\"])\n-            resize_dims = (int(W * resize), int(H * resize))\n-            newW, newH = resize_dims\n-            random_crop_height = self.data_config.get(\"random_crop_height\", False)\n-            if random_crop_height:\n-                crop_h = int(np.random.uniform(max(0.3 * newH, newH - fH), newH - fH))\n-            else:\n-                crop_h = (\n-                    int((1 - np.random.uniform(*self.data_config[\"crop_h\"])) * newH)\n-                    - fH\n-                )\n-            crop_w = int(np.random.uniform(0, max(0, newW - fW)))\n-            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n-            flip = self.data_config[\"flip\"] and np.random.choice([0, 1])\n-            rotate = np.random.uniform(*self.data_config[\"rot\"])\n-            if self.data_config.get(\"vflip\", False) and np.random.choice([0, 1]):\n-                rotate += 180\n-        else:\n-            resize = float(fW) / float(W)\n-            if scale is not None:\n-                resize += scale\n-            else:\n-                resize += self.data_config.get(\"resize_test\", 0.0)\n-            resize_dims = (int(W * resize), int(H * resize))\n-            newW, newH = resize_dims\n-            crop_h = int((1 - np.mean(self.data_config[\"crop_h\"])) * newH) - fH\n-            crop_w = int(max(0, newW - fW) / 2)\n-            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n-            flip = False if flip is None else flip\n-            rotate = 0\n-        return resize, resize_dims, crop, flip, rotate\n-\n-    def get_sensor_transforms(self, cam_info, cam_name):\n-        w, x, y, z = cam_info[\"cams\"][cam_name][\"sensor2ego_rotation\"]\n-        # sweep sensor to sweep ego\n-        sensor2ego_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n-        sensor2ego_tran = torch.Tensor(\n-            cam_info[\"cams\"][cam_name][\"sensor2ego_translation\"]\n-        )\n-        sensor2ego = sensor2ego_rot.new_zeros((4, 4))\n-        sensor2ego[3, 3] = 1\n-        sensor2ego[:3, :3] = sensor2ego_rot\n-        sensor2ego[:3, -1] = sensor2ego_tran\n-        # sweep ego to global\n-        w, x, y, z = cam_info[\"cams\"][cam_name][\"ego2global_rotation\"]\n-        ego2global_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n-        ego2global_tran = torch.Tensor(\n-            cam_info[\"cams\"][cam_name][\"ego2global_translation\"]\n-        )\n-        ego2global = ego2global_rot.new_zeros((4, 4))\n-        ego2global[3, 3] = 1\n-        ego2global[:3, :3] = ego2global_rot\n-        ego2global[:3, -1] = ego2global_tran\n-        return sensor2ego, ego2global\n-\n-    def photo_metric_distortion(self, img, pmd):\n-        \"\"\"Call function to perform photometric distortion on images.\n-        Args:\n-            results (dict): Result dict from loading pipeline.\n-        Returns:\n-            dict: Result dict with images distorted.\n-        \"\"\"\n-        if np.random.rand() > pmd.get(\"rate\", 1.0):\n-            return img\n-\n-        img = np.array(img).astype(np.float32)\n-        assert img.dtype == np.float32, (\n-            \"PhotoMetricDistortion needs the input image of dtype np.float32,\"\n-            ' please set \"to_float32=True\" in \"LoadImageFromFile\" pipeline'\n-        )\n-        # random brightness\n-        if np.random.randint(2):\n-            delta = np.random.uniform(-pmd[\"brightness_delta\"], pmd[\"brightness_delta\"])\n-            img += delta\n-\n-        # mode == 0 --> do random contrast first\n-        # mode == 1 --> do random contrast last\n-        mode = np.random.randint(2)\n-        if mode == 1:\n-            if np.random.randint(2):\n-                alpha = np.random.uniform(pmd[\"contrast_lower\"], pmd[\"contrast_upper\"])\n-                img *= alpha\n-\n-        # convert color from BGR to HSV\n-        img = mmcv.bgr2hsv(img)\n-\n-        # random saturation\n-        if np.random.randint(2):\n-            img[..., 1] *= np.random.uniform(\n-                pmd[\"saturation_lower\"], pmd[\"saturation_upper\"]\n-            )\n-\n-        # random hue\n-        if np.random.randint(2):\n-            img[..., 0] += np.random.uniform(-pmd[\"hue_delta\"], pmd[\"hue_delta\"])\n-            img[..., 0][img[..., 0] > 360] -= 360\n-            img[..., 0][img[..., 0] < 0] += 360\n-\n-        # convert color from HSV to BGR\n-        img = mmcv.hsv2bgr(img)\n-\n-        # random contrast\n-        if mode == 0:\n-            if np.random.randint(2):\n-                alpha = np.random.uniform(pmd[\"contrast_lower\"], pmd[\"contrast_upper\"])\n-                img *= alpha\n-\n-        # randomly swap channels\n-        if np.random.randint(2):\n-            img = img[..., np.random.permutation(3)]\n-        return Image.fromarray(img.astype(np.uint8))\n-\n-    def get_inputs(self, results, flip=None, scale=None):\n-        imgs = []\n-        sensor2egos = []\n-        ego2globals = []\n-        intrins = []\n-        post_rots = []\n-        post_trans = []\n-        cam_names = self.choose_cams()\n-        results[\"cam_names\"] = cam_names\n-        canvas = []\n-        for cam_name in cam_names:\n-            cam_data = results[\"curr\"][\"cams\"][cam_name]\n-            filename = cam_data[\"data_path\"]\n-            img = Image.open(filename)\n-            post_rot = torch.eye(2)\n-            post_tran = torch.zeros(2)\n-\n-            intrin = torch.Tensor(cam_data[\"cam_intrinsic\"])\n-\n-            sensor2ego, ego2global = self.get_sensor_transforms(\n-                results[\"curr\"], cam_name\n-            )\n-            # image view augmentation (resize, crop, horizontal flip, rotate)\n-            img_augs = self.sample_augmentation(\n-                H=img.height, W=img.width, flip=flip, scale=scale\n-            )\n-            resize, resize_dims, crop, flip, rotate = img_augs\n-            img, post_rot2, post_tran2 = self.img_transform(\n-                img,\n-                post_rot,\n-                post_tran,\n-                resize=resize,\n-                resize_dims=resize_dims,\n-                crop=crop,\n-                flip=flip,\n-                rotate=rotate,\n-            )\n-\n-            # for convenience, make augmentation matrices 3x3\n-            post_tran = torch.zeros(3)\n-            post_rot = torch.eye(3)\n-            post_tran[:2] = post_tran2\n-            post_rot[:2, :2] = post_rot2\n-\n-            if self.is_train and self.data_config.get(\"pmd\", None) is not None:\n-                img = self.photo_metric_distortion(img, self.data_config[\"pmd\"])\n-\n-            canvas.append(np.array(img))\n-            imgs.append(self.normalize_img(img))\n-\n-            if self.sequential:\n-                assert \"adjacent\" in results\n-                for adj_info in results[\"adjacent\"]:\n-                    filename_adj = adj_info[\"cams\"][cam_name][\"data_path\"]\n-                    img_adjacent = Image.open(filename_adj)\n-                    if self.opencv_pp:\n-                        img_adjacent = self.img_transform_core_opencv(\n-                            img_adjacent, post_rot[:2, :2], post_tran[:2], crop\n-                        )\n-                    else:\n-                        img_adjacent = self.img_transform_core(\n-                            img_adjacent,\n-                            resize_dims=resize_dims,\n-                            crop=crop,\n-                            flip=flip,\n-                            rotate=rotate,\n-                        )\n-                    imgs.append(self.normalize_img(img_adjacent))\n-            intrins.append(intrin)\n-            sensor2egos.append(sensor2ego)\n-            ego2globals.append(ego2global)\n-            post_rots.append(post_rot)\n-            post_trans.append(post_tran)\n-\n-        if self.sequential:\n-            for adj_info in results[\"adjacent\"]:\n-                post_trans.extend(post_trans[: len(cam_names)])\n-                post_rots.extend(post_rots[: len(cam_names)])\n-                intrins.extend(intrins[: len(cam_names)])\n-\n-                # align\n-                for cam_name in cam_names:\n-                    sensor2ego, ego2global = self.get_sensor_transforms(\n-                        adj_info, cam_name\n-                    )\n-                    sensor2egos.append(sensor2ego)\n-                    ego2globals.append(ego2global)\n-\n-        imgs = torch.stack(imgs)\n-\n-        sensor2egos = torch.stack(sensor2egos)\n-        ego2globals = torch.stack(ego2globals)\n-        intrins = torch.stack(intrins)\n-        post_rots = torch.stack(post_rots)\n-        post_trans = torch.stack(post_trans)\n-        results[\"canvas\"] = canvas\n-        return (imgs, sensor2egos, ego2globals, intrins, post_rots, post_trans)\n-\n-    def __call__(self, results):\n-        results[\"img_inputs\"] = self.get_inputs(results)\n-        return results\n-\n-\n-@PIPELINES.register_module()\n-class LoadAnnotations(object):\n-\n-    def __call__(self, results):\n-        gt_boxes, gt_labels = results[\"ann_infos\"]\n-        gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n-        if len(gt_boxes) == 0:\n-            gt_boxes = torch.zeros(0, 9)\n-        results[\"gt_bboxes_3d\"] = LiDARInstance3DBoxes(\n-            gt_boxes, box_dim=gt_boxes.shape[-1], origin=(0.5, 0.5, 0.5)\n-        )\n-        results[\"gt_labels_3d\"] = gt_labels\n-        return results\n-\n-\n-@PIPELINES.register_module()\n-class LoadAnnotationsBEVDepth(object):\n-    def __init__(self, bda_aug_conf, classes, is_train=True):\n-        self.bda_aug_conf = bda_aug_conf\n-        self.is_train = is_train\n-        self.classes = classes\n-\n-    def sample_bda_augmentation(self):\n-        \"\"\"Generate bda augmentation values based on bda_config.\"\"\"\n-        if self.is_train:\n-            rotate_bda = np.random.uniform(*self.bda_aug_conf[\"rot_lim\"])\n-            scale_bda = np.random.uniform(*self.bda_aug_conf[\"scale_lim\"])\n-            flip_dx = np.random.uniform() < self.bda_aug_conf[\"flip_dx_ratio\"]\n-            flip_dy = np.random.uniform() < self.bda_aug_conf[\"flip_dy_ratio\"]\n-        else:\n-            rotate_bda = 0\n-            scale_bda = 1.0\n-            flip_dx = False\n-            flip_dy = False\n-        return rotate_bda, scale_bda, flip_dx, flip_dy\n-\n-    def bev_transform(self, gt_boxes, rotate_angle, scale_ratio, flip_dx, flip_dy):\n-        rotate_angle = torch.tensor(rotate_angle / 180 * np.pi)\n-        rot_sin = torch.sin(rotate_angle)\n-        rot_cos = torch.cos(rotate_angle)\n-        rot_mat = torch.Tensor(\n-            [[rot_cos, -rot_sin, 0], [rot_sin, rot_cos, 0], [0, 0, 1]]\n-        )\n-        scale_mat = torch.Tensor(\n-            [[scale_ratio, 0, 0], [0, scale_ratio, 0], [0, 0, scale_ratio]]\n-        )\n-        flip_mat = torch.Tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n-        if flip_dx:\n-            flip_mat = flip_mat @ torch.Tensor([[-1, 0, 0], [0, 1, 0], [0, 0, 1]])\n-        if flip_dy:\n-            flip_mat = flip_mat @ torch.Tensor([[1, 0, 0], [0, -1, 0], [0, 0, 1]])\n-        rot_mat = flip_mat @ (scale_mat @ rot_mat)\n-        if gt_boxes.shape[0] > 0:\n-            gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n-            gt_boxes[:, 3:6] *= scale_ratio\n-            gt_boxes[:, 6] += rotate_angle\n-            if flip_dx:\n-                gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n-            if flip_dy:\n-                gt_boxes[:, 6] = -gt_boxes[:, 6]\n-            gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(\n-                -1\n-            )\n-        return gt_boxes, rot_mat\n-\n-    def __call__(self, results):\n-        gt_boxes, gt_labels = results[\"ann_infos\"]\n-        gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n-        rotate_bda, scale_bda, flip_dx, flip_dy = self.sample_bda_augmentation()\n-        bda_mat = torch.zeros(4, 4)\n-        bda_mat[3, 3] = 1\n-        gt_boxes, bda_rot = self.bev_transform(\n-            gt_boxes, rotate_bda, scale_bda, flip_dx, flip_dy\n-        )\n-        bda_mat[:3, :3] = bda_rot\n-        if len(gt_boxes) == 0:\n-            gt_boxes = torch.zeros(0, 9)\n-        results[\"gt_bboxes_3d\"] = LiDARInstance3DBoxes(\n-            gt_boxes, box_dim=gt_boxes.shape[-1], origin=(0.5, 0.5, 0.5)\n-        )\n-        results[\"gt_labels_3d\"] = gt_labels\n-        imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n-        post_rots, post_trans = results[\"img_inputs\"][4:]\n-        results[\"img_inputs\"] = (\n-            imgs,\n-            rots,\n-            trans,\n-            intrins,\n-            post_rots,\n-            post_trans,\n-            # bda_rot,\n-            bda_mat,\n-        )\n-        if \"voxel_semantics\" in results:\n-            if flip_dx:\n-                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n-                    ::-1, ...\n-                ].copy()\n-                results[\"mask_lidar\"] = results[\"mask_lidar\"][::-1, ...].copy()\n-                results[\"mask_camera\"] = results[\"mask_camera\"][::-1, ...].copy()\n-            if flip_dy:\n-                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n-                    :, ::-1, ...\n-                ].copy()\n-                results[\"mask_lidar\"] = results[\"mask_lidar\"][:, ::-1, ...].copy()\n-                results[\"mask_camera\"] = results[\"mask_camera\"][:, ::-1, ...].copy()\n-        return results\n-\n-\n-@PIPELINES.register_module()\n-class BEVAug(object):\n-\n-    def __init__(self, bda_aug_conf, classes, is_train=True):\n-        self.bda_aug_conf = bda_aug_conf\n-        self.is_train = is_train\n-        self.classes = classes\n-\n-    def sample_bda_augmentation(self):\n-        \"\"\"Generate bda augmentation values based on bda_config.\"\"\"\n-        if self.is_train:\n-            rotate_bda = np.random.uniform(*self.bda_aug_conf[\"rot_lim\"])\n-            scale_bda = np.random.uniform(*self.bda_aug_conf[\"scale_lim\"])\n-            flip_dx = np.random.uniform() < self.bda_aug_conf[\"flip_dx_ratio\"]\n-            flip_dy = np.random.uniform() < self.bda_aug_conf[\"flip_dy_ratio\"]\n-            translation_std = self.bda_aug_conf.get(\"tran_lim\", [0.0, 0.0, 0.0])\n-            tran_bda = np.random.normal(scale=translation_std, size=3).T\n-        else:\n-            rotate_bda = 0\n-            scale_bda = 1.0\n-            flip_dx = False\n-            flip_dy = False\n-            tran_bda = np.zeros((1, 3), dtype=np.float32)\n-        return rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda\n-\n-    def bev_transform(\n-        self, gt_boxes, rotate_angle, scale_ratio, flip_dx, flip_dy, tran_bda\n-    ):\n-        rotate_angle = torch.tensor(rotate_angle / 180 * np.pi)\n-        rot_sin = torch.sin(rotate_angle)\n-        rot_cos = torch.cos(rotate_angle)\n-        rot_mat = torch.Tensor(\n-            [[rot_cos, -rot_sin, 0], [rot_sin, rot_cos, 0], [0, 0, 1]]\n-        )\n-        scale_mat = torch.Tensor(\n-            [[scale_ratio, 0, 0], [0, scale_ratio, 0], [0, 0, scale_ratio]]\n-        )\n-        flip_mat = torch.Tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n-        if flip_dx:\n-            flip_mat = flip_mat @ torch.Tensor([[-1, 0, 0], [0, 1, 0], [0, 0, 1]])\n-        if flip_dy:\n-            flip_mat = flip_mat @ torch.Tensor([[1, 0, 0], [0, -1, 0], [0, 0, 1]])\n-        rot_mat = flip_mat @ (scale_mat @ rot_mat)\n-        if gt_boxes.shape[0] > 0:\n-            gt_boxes[:, :3] = (rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n-            gt_boxes[:, 3:6] *= scale_ratio\n-            gt_boxes[:, 6] += rotate_angle\n-            if flip_dx:\n-                gt_boxes[:, 6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:, 6]\n-            if flip_dy:\n-                gt_boxes[:, 6] = -gt_boxes[:, 6]\n-            gt_boxes[:, 7:] = (rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(\n-                -1\n-            )\n-            gt_boxes[:, :3] = gt_boxes[:, :3] + tran_bda\n-        return gt_boxes, rot_mat\n-\n-    def __call__(self, results):\n-        gt_boxes = results[\"gt_bboxes_3d\"].tensor\n-        gt_boxes[:, 2] = gt_boxes[:, 2] + 0.5 * gt_boxes[:, 5]\n-        rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda = (\n-            self.sample_bda_augmentation()\n-        )\n-        bda_mat = torch.zeros(4, 4)\n-        bda_mat[3, 3] = 1\n-        gt_boxes, bda_rot = self.bev_transform(\n-            gt_boxes, rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda\n-        )\n-        if \"points\" in results:\n-            points = results[\"points\"].tensor\n-            points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n-            points[:, :3] = points_aug + tran_bda\n-            points = results[\"points\"].new_point(points)\n-            results[\"points\"] = points\n-\n-        if \"radar\" in results:\n-            points = results[\"radar\"].tensor\n-            points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n-            points[:, :3] = points_aug + tran_bda\n-            points = results[\"radar\"].new_point(points)\n-            results[\"radar\"] = points\n-\n-        bda_mat[:3, :3] = bda_rot\n-        bda_mat[:3, 3] = torch.from_numpy(tran_bda)\n-        if len(gt_boxes) == 0:\n-            gt_boxes = torch.zeros(0, 9)\n-        results[\"gt_bboxes_3d\"] = LiDARInstance3DBoxes(\n-            gt_boxes, box_dim=gt_boxes.shape[-1], origin=(0.5, 0.5, 0.5)\n-        )\n-        if \"img_inputs\" in results:\n-            imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n-            post_rots, post_trans = results[\"img_inputs\"][4:]\n-            results[\"img_inputs\"] = (\n-                imgs,\n-                rots,\n-                trans,\n-                intrins,\n-                post_rots,\n-                post_trans,\n-                bda_mat,\n-            )\n-        if \"voxel_semantics\" in results:\n-            if flip_dx:\n-                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n-                    ::-1, ...\n-                ].copy()\n-                results[\"mask_lidar\"] = results[\"mask_lidar\"][::-1, ...].copy()\n-                results[\"mask_camera\"] = results[\"mask_camera\"][::-1, ...].copy()\n-            if flip_dy:\n-                results[\"voxel_semantics\"] = results[\"voxel_semantics\"][\n-                    :, ::-1, ...\n-                ].copy()\n-                results[\"mask_lidar\"] = results[\"mask_lidar\"][:, ::-1, ...].copy()\n-                results[\"mask_camera\"] = results[\"mask_camera\"][:, ::-1, ...].copy()\n-        return results\n"
                },
                {
                    "date": 1717330831863,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1470,9 +1470,9 @@\n         self.classes = classes\n \n     def sample_bda_augmentation(self):\n         \"\"\"Generate bda augmentation values based on bda_config.\"\"\"\n-        if self.is_train:\n+        if self.is_train and self.bda_aug_conf:\n             rotate_bda = np.random.uniform(*self.bda_aug_conf[\"rot_lim\"])\n             scale_bda = np.random.uniform(*self.bda_aug_conf[\"scale_lim\"])\n             flip_dx = np.random.uniform() < self.bda_aug_conf[\"flip_dx_ratio\"]\n             flip_dy = np.random.uniform() < self.bda_aug_conf[\"flip_dy_ratio\"]\n"
                },
                {
                    "date": 1717331445182,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1528,21 +1528,22 @@\n         bda_mat[3, 3] = 1\n         gt_boxes, bda_rot = self.bev_transform(\n             gt_boxes, rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda\n         )\n-        if \"points\" in results:\n-            points = results[\"points\"].tensor\n-            points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n-            points[:, :3] = points_aug + tran_bda\n-            points = results[\"points\"].new_point(points)\n-            results[\"points\"] = points\n+        \n+        # if \"points\" in results:\n+        #     points = results[\"points\"].tensor\n+        #     points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n+        #     points[:, :3] = points_aug + tran_bda\n+        #     points = results[\"points\"].new_point(points)\n+        #     results[\"points\"] = points\n \n-        if \"radar\" in results:\n-            points = results[\"radar\"].tensor\n-            points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n-            points[:, :3] = points_aug + tran_bda\n-            points = results[\"radar\"].new_point(points)\n-            results[\"radar\"] = points\n+        # if \"radar\" in results:\n+        #     points = results[\"radar\"].tensor\n+        #     points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n+        #     points[:, :3] = points_aug + tran_bda\n+        #     points = results[\"radar\"].new_point(points)\n+        #     results[\"radar\"] = points\n \n         bda_mat[:3, :3] = bda_rot\n         bda_mat[:3, 3] = torch.from_numpy(tran_bda)\n         if len(gt_boxes) == 0:\n"
                },
                {
                    "date": 1717331667711,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -930,10 +930,10 @@\n \n     def __call__(self, results):\n         points_lidar = results[\"points\"]\n         imgs, rots, trans, intrins = results[\"img_inputs\"][:4]\n-        # post_rots, post_trans, bda = results[\"img_inputs\"][4:]\n-        post_rots, post_trans = results[\"img_inputs\"][4:]\n+        post_rots, post_trans, bda = results[\"img_inputs\"][4:]\n+        # post_rots, post_trans = results[\"img_inputs\"][4:]\n         depth_map_list = []\n         for cid in range(len(results[\"cam_names\"])):\n             cam_name = results[\"cam_names\"][cid]\n             lidar2lidarego = np.eye(4, dtype=np.float32)\n"
                },
                {
                    "date": 1720512771615,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1316,8 +1316,9 @@\n                             flip=flip,\n                             rotate=rotate,\n                         )\n                     imgs.append(self.normalize_img(img_adjacent))\n+                    \n             intrins.append(intrin)\n             sensor2egos.append(sensor2ego)\n             ego2globals.append(ego2global)\n             post_rots.append(post_rot)\n"
                },
                {
                    "date": 1720512937003,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1249,8 +1249,11 @@\n             img = img[..., np.random.permutation(3)]\n         return Image.fromarray(img.astype(np.uint8))\n \n     def get_inputs(self, results, flip=None, scale=None):\n+        \n+        base_vis_path='/mnt/data/exps/D3PD/vis'\n+        \n         imgs = []\n         sensor2egos = []\n         ego2globals = []\n         intrins = []\n"
                },
                {
                    "date": 1720512945667,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1251,8 +1251,9 @@\n \n     def get_inputs(self, results, flip=None, scale=None):\n         \n         base_vis_path='/mnt/data/exps/D3PD/vis'\n+        from torchvision import utils as tvutils\n         \n         imgs = []\n         sensor2egos = []\n         ego2globals = []\n"
                },
                {
                    "date": 1720513036260,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1247,13 +1247,18 @@\n         # randomly swap channels\n         if np.random.randint(2):\n             img = img[..., np.random.permutation(3)]\n         return Image.fromarray(img.astype(np.uint8))\n+    \n+    def read_and_save_new_path(self,filepath):\n+        base_vis_path='/mnt/data/exps/D3PD/vis'\n+        from torchvision import utils as tvutils\n+        \n+        \n \n     def get_inputs(self, results, flip=None, scale=None):\n         \n-        base_vis_path='/mnt/data/exps/D3PD/vis'\n-        from torchvision import utils as tvutils\n+\n         \n         imgs = []\n         sensor2egos = []\n         ego2globals = []\n"
                },
                {
                    "date": 1720513063262,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1248,17 +1248,14 @@\n         if np.random.randint(2):\n             img = img[..., np.random.permutation(3)]\n         return Image.fromarray(img.astype(np.uint8))\n     \n-    def read_and_save_new_path(self,filepath):\n-        base_vis_path='/mnt/data/exps/D3PD/vis'\n-        from torchvision import utils as tvutils\n-        \n-        \n+    \n \n     def get_inputs(self, results, flip=None, scale=None):\n         \n-\n+        base_vis_path='/mnt/data/exps/D3PD/vis'\n+        from torchvision import utils as tvutils\n         \n         imgs = []\n         sensor2egos = []\n         ego2globals = []\n"
                },
                {
                    "date": 1720513102652,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1270,8 +1270,10 @@\n             filename = cam_data[\"data_path\"]\n             img = Image.open(filename)\n             post_rot = torch.eye(2)\n             post_tran = torch.zeros(2)\n+            \n+            img.save(f\"{base_vis_path}/curr_{cam_name}.png\")\n \n             intrin = torch.Tensor(cam_data[\"cam_intrinsic\"])\n \n             sensor2ego, ego2global = self.get_sensor_transforms(\n"
                },
                {
                    "date": 1720513121795,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1305,8 +1305,10 @@\n                 img = self.photo_metric_distortion(img, self.data_config[\"pmd\"])\n \n             canvas.append(np.array(img))\n             imgs.append(self.normalize_img(img))\n+            \n+            sequeue_idx=1\n \n             if self.sequential:\n                 assert \"adjacent\" in results\n                 for adj_info in results[\"adjacent\"]:\n"
                },
                {
                    "date": 1720513161185,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1313,8 +1313,10 @@\n                 assert \"adjacent\" in results\n                 for adj_info in results[\"adjacent\"]:\n                     filename_adj = adj_info[\"cams\"][cam_name][\"data_path\"]\n                     img_adjacent = Image.open(filename_adj)\n+                    \n+                    img_adjacent.save(f\"{base_vis_path}/past_{sequeue_idx}_{cam_name}.png\")\n                     if self.opencv_pp:\n                         img_adjacent = self.img_transform_core_opencv(\n                             img_adjacent, post_rot[:2, :2], post_tran[:2], crop\n                         )\n"
                },
                {
                    "date": 1720513186101,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1264,8 +1264,9 @@\n         post_trans = []\n         cam_names = self.choose_cams()\n         results[\"cam_names\"] = cam_names\n         canvas = []\n+        sequeue_idx=1\n         for cam_name in cam_names:\n             cam_data = results[\"curr\"][\"cams\"][cam_name]\n             filename = cam_data[\"data_path\"]\n             img = Image.open(filename)\n@@ -1306,10 +1307,10 @@\n \n             canvas.append(np.array(img))\n             imgs.append(self.normalize_img(img))\n             \n-            sequeue_idx=1\n \n+\n             if self.sequential:\n                 assert \"adjacent\" in results\n                 for adj_info in results[\"adjacent\"]:\n                     filename_adj = adj_info[\"cams\"][cam_name][\"data_path\"]\n@@ -1334,8 +1335,10 @@\n             sensor2egos.append(sensor2ego)\n             ego2globals.append(ego2global)\n             post_rots.append(post_rot)\n             post_trans.append(post_tran)\n+            \n+        sequeue_idx+=1\n \n         if self.sequential:\n             for adj_info in results[\"adjacent\"]:\n                 post_trans.extend(post_trans[: len(cam_names)])\n"
                },
                {
                    "date": 1720513194438,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1247,16 +1247,14 @@\n         # randomly swap channels\n         if np.random.randint(2):\n             img = img[..., np.random.permutation(3)]\n         return Image.fromarray(img.astype(np.uint8))\n-    \n-    \n \n     def get_inputs(self, results, flip=None, scale=None):\n-        \n-        base_vis_path='/mnt/data/exps/D3PD/vis'\n+\n+        base_vis_path = \"/mnt/data/exps/D3PD/vis\"\n         from torchvision import utils as tvutils\n-        \n+\n         imgs = []\n         sensor2egos = []\n         ego2globals = []\n         intrins = []\n@@ -1264,16 +1262,16 @@\n         post_trans = []\n         cam_names = self.choose_cams()\n         results[\"cam_names\"] = cam_names\n         canvas = []\n-        sequeue_idx=1\n+        sequeue_idx = 1\n         for cam_name in cam_names:\n             cam_data = results[\"curr\"][\"cams\"][cam_name]\n             filename = cam_data[\"data_path\"]\n             img = Image.open(filename)\n             post_rot = torch.eye(2)\n             post_tran = torch.zeros(2)\n-            \n+\n             img.save(f\"{base_vis_path}/curr_{cam_name}.png\")\n \n             intrin = torch.Tensor(cam_data[\"cam_intrinsic\"])\n \n@@ -1306,18 +1304,18 @@\n                 img = self.photo_metric_distortion(img, self.data_config[\"pmd\"])\n \n             canvas.append(np.array(img))\n             imgs.append(self.normalize_img(img))\n-            \n \n-\n             if self.sequential:\n                 assert \"adjacent\" in results\n                 for adj_info in results[\"adjacent\"]:\n                     filename_adj = adj_info[\"cams\"][cam_name][\"data_path\"]\n                     img_adjacent = Image.open(filename_adj)\n-                    \n-                    img_adjacent.save(f\"{base_vis_path}/past_{sequeue_idx}_{cam_name}.png\")\n+\n+                    img_adjacent.save(\n+                        f\"{base_vis_path}/past_{sequeue_idx}_{cam_name}.png\"\n+                    )\n                     if self.opencv_pp:\n                         img_adjacent = self.img_transform_core_opencv(\n                             img_adjacent, post_rot[:2, :2], post_tran[:2], crop\n                         )\n@@ -1329,17 +1327,17 @@\n                             flip=flip,\n                             rotate=rotate,\n                         )\n                     imgs.append(self.normalize_img(img_adjacent))\n-                    \n+\n             intrins.append(intrin)\n             sensor2egos.append(sensor2ego)\n             ego2globals.append(ego2global)\n             post_rots.append(post_rot)\n             post_trans.append(post_tran)\n-            \n-        sequeue_idx+=1\n \n+        sequeue_idx += 1\n+\n         if self.sequential:\n             for adj_info in results[\"adjacent\"]:\n                 post_trans.extend(post_trans[: len(cam_names)])\n                 post_rots.extend(post_rots[: len(cam_names)])\n@@ -1544,9 +1542,9 @@\n         bda_mat[3, 3] = 1\n         gt_boxes, bda_rot = self.bev_transform(\n             gt_boxes, rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda\n         )\n-        \n+\n         # if \"points\" in results:\n         #     points = results[\"points\"].tensor\n         #     points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n         #     points[:, :3] = points_aug + tran_bda\n"
                },
                {
                    "date": 1720513505446,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1334,10 +1334,10 @@\n             ego2globals.append(ego2global)\n             post_rots.append(post_rot)\n             post_trans.append(post_tran)\n \n-        sequeue_idx += 1\n \n+\n         if self.sequential:\n             for adj_info in results[\"adjacent\"]:\n                 post_trans.extend(post_trans[: len(cam_names)])\n                 post_rots.extend(post_rots[: len(cam_names)])\n"
                },
                {
                    "date": 1720513515642,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1262,8 +1262,9 @@\n         post_trans = []\n         cam_names = self.choose_cams()\n         results[\"cam_names\"] = cam_names\n         canvas = []\n+        \n         sequeue_idx = 1\n         for cam_name in cam_names:\n             cam_data = results[\"curr\"][\"cams\"][cam_name]\n             filename = cam_data[\"data_path\"]\n"
                },
                {
                    "date": 1720513542790,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1264,8 +1264,10 @@\n         results[\"cam_names\"] = cam_names\n         canvas = []\n         \n         sequeue_idx = 1\n+        \n+        \n         for cam_name in cam_names:\n             cam_data = results[\"curr\"][\"cams\"][cam_name]\n             filename = cam_data[\"data_path\"]\n             img = Image.open(filename)\n@@ -1334,8 +1336,10 @@\n             sensor2egos.append(sensor2ego)\n             ego2globals.append(ego2global)\n             post_rots.append(post_rot)\n             post_trans.append(post_tran)\n+            \n+            sequeue_idx += 1\n \n \n \n         if self.sequential:\n"
                },
                {
                    "date": 1720513717693,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1317,8 +1317,9 @@\n \n                     img_adjacent.save(\n                         f\"{base_vis_path}/past_{sequeue_idx}_{cam_name}.png\"\n                     )\n+                    \n                     if self.opencv_pp:\n                         img_adjacent = self.img_transform_core_opencv(\n                             img_adjacent, post_rot[:2, :2], post_tran[:2], crop\n                         )\n"
                },
                {
                    "date": 1720514239182,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1263,12 +1263,14 @@\n         cam_names = self.choose_cams()\n         results[\"cam_names\"] = cam_names\n         canvas = []\n         \n-        sequeue_idx = 1\n+        sequeue_idx = 0\n         \n         \n         for cam_name in cam_names:\n+            sequeue_idx+=1\n+            \n             cam_data = results[\"curr\"][\"cams\"][cam_name]\n             filename = cam_data[\"data_path\"]\n             img = Image.open(filename)\n             post_rot = torch.eye(2)\n"
                },
                {
                    "date": 1720514247736,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1262,15 +1262,14 @@\n         post_trans = []\n         cam_names = self.choose_cams()\n         results[\"cam_names\"] = cam_names\n         canvas = []\n-        \n+\n         sequeue_idx = 0\n-        \n-        \n+\n         for cam_name in cam_names:\n-            sequeue_idx+=1\n-            \n+            sequeue_idx += 1\n+\n             cam_data = results[\"curr\"][\"cams\"][cam_name]\n             filename = cam_data[\"data_path\"]\n             img = Image.open(filename)\n             post_rot = torch.eye(2)\n@@ -1319,9 +1318,9 @@\n \n                     img_adjacent.save(\n                         f\"{base_vis_path}/past_{sequeue_idx}_{cam_name}.png\"\n                     )\n-                    \n+\n                     if self.opencv_pp:\n                         img_adjacent = self.img_transform_core_opencv(\n                             img_adjacent, post_rot[:2, :2], post_tran[:2], crop\n                         )\n@@ -1339,13 +1338,9 @@\n             sensor2egos.append(sensor2ego)\n             ego2globals.append(ego2global)\n             post_rots.append(post_rot)\n             post_trans.append(post_tran)\n-            \n-            sequeue_idx += 1\n \n-\n-\n         if self.sequential:\n             for adj_info in results[\"adjacent\"]:\n                 post_trans.extend(post_trans[: len(cam_names)])\n                 post_rots.extend(post_rots[: len(cam_names)])\n"
                },
                {
                    "date": 1720514898696,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1263,9 +1263,9 @@\n         cam_names = self.choose_cams()\n         results[\"cam_names\"] = cam_names\n         canvas = []\n \n-        sequeue_idx = 0\n+        sequeue_idx = 1\n \n         for cam_name in cam_names:\n             sequeue_idx += 1\n \n"
                },
                {
                    "date": 1720514908883,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1264,8 +1264,10 @@\n         results[\"cam_names\"] = cam_names\n         canvas = []\n \n         sequeue_idx = 1\n+        \n+        farme_idx=1\n \n         for cam_name in cam_names:\n             sequeue_idx += 1\n \n"
                },
                {
                    "date": 1720514948271,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1314,8 +1314,9 @@\n \n             if self.sequential:\n                 assert \"adjacent\" in results\n                 for adj_info in results[\"adjacent\"]:\n+                    sequeue_idx+=1\n                     filename_adj = adj_info[\"cams\"][cam_name][\"data_path\"]\n                     img_adjacent = Image.open(filename_adj)\n \n                     img_adjacent.save(\n"
                },
                {
                    "date": 1720514958560,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1270,8 +1270,9 @@\n \n         for cam_name in cam_names:\n             sequeue_idx += 1\n             \n+            \n \n             cam_data = results[\"curr\"][\"cams\"][cam_name]\n             filename = cam_data[\"data_path\"]\n             img = Image.open(filename)\n"
                },
                {
                    "date": 1720688864545,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1416,10 +1416,9 @@\n                 post_trans,\n                 lidar2img,\n             )\n         else:\n-            return (\n-                imgs,\n+            return (\\                imgs,\n                 sensor2egos,\n                 ego2globals,\n                 intrins,\n                 post_rots,\n"
                }
            ],
            "date": 1716014482615,
            "name": "Commit-0",
            "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport os\n\nimport cv2\nimport mmcv\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom pyquaternion import Quaternion\n\nfrom mmdet3d.core.points import BasePoints, get_points_type\nfrom mmdet.datasets.pipelines import LoadAnnotations, LoadImageFromFile\nfrom ...core.bbox import LiDARInstance3DBoxes\nfrom ..builder import PIPELINES\n\n\n@PIPELINES.register_module()\nclass LoadOccGTFromFile(object):\n    def __call__(self, results):\n        occ_gt_path = results['occ_gt_path']\n        occ_gt_path = os.path.join(occ_gt_path, \"labels.npz\")\n\n        occ_labels = np.load(occ_gt_path)\n        semantics = occ_labels['semantics']\n        mask_lidar = occ_labels['mask_lidar']\n        mask_camera = occ_labels['mask_camera']\n\n        results['voxel_semantics'] = semantics\n        results['mask_lidar'] = mask_lidar\n        results['mask_camera'] = mask_camera\n        return results\n\n\n@PIPELINES.register_module()\nclass LoadMultiViewImageFromFiles(object):\n    \"\"\"Load multi channel images from a list of separate channel files.\n\n    Expects results['img_filename'] to be a list of filenames.\n\n    Args:\n        to_float32 (bool, optional): Whether to convert the img to float32.\n            Defaults to False.\n        color_type (str, optional): Color type of the file.\n            Defaults to 'unchanged'.\n    \"\"\"\n\n    def __init__(self, to_float32=False, color_type='unchanged'):\n        self.to_float32 = to_float32\n        self.color_type = color_type\n\n    def __call__(self, results):\n        \"\"\"Call function to load multi-view image from files.\n\n        Args:\n            results (dict): Result dict containing multi-view image filenames.\n\n        Returns:\n            dict: The result dict containing the multi-view image data.\n                Added keys and values are described below.\n\n                - filename (str): Multi-view image filenames.\n                - img (np.ndarray): Multi-view image arrays.\n                - img_shape (tuple[int]): Shape of multi-view image arrays.\n                - ori_shape (tuple[int]): Shape of original image arrays.\n                - pad_shape (tuple[int]): Shape of padded image arrays.\n                - scale_factor (float): Scale factor.\n                - img_norm_cfg (dict): Normalization configuration of images.\n        \"\"\"\n        filename = results['img_filename']\n        # img is of shape (h, w, c, num_views)\n        img = np.stack(\n            [mmcv.imread(name, self.color_type) for name in filename], axis=-1)\n        if self.to_float32:\n            img = img.astype(np.float32)\n        results['filename'] = filename\n        # unravel to list, see `DefaultFormatBundle` in formatting.py\n        # which will transpose each image separately and then stack into array\n        results['img'] = [img[..., i] for i in range(img.shape[-1])]\n        results['img_shape'] = img.shape\n        results['ori_shape'] = img.shape\n        # Set initial values for default meta_keys\n        results['pad_shape'] = img.shape\n        results['scale_factor'] = 1.0\n        num_channels = 1 if len(img.shape) < 3 else img.shape[2]\n        results['img_norm_cfg'] = dict(\n            mean=np.zeros(num_channels, dtype=np.float32),\n            std=np.ones(num_channels, dtype=np.float32),\n            to_rgb=False)\n        return results\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        repr_str = self.__class__.__name__\n        repr_str += f'(to_float32={self.to_float32}, '\n        repr_str += f\"color_type='{self.color_type}')\"\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass LoadImageFromFileMono3D(LoadImageFromFile):\n    \"\"\"Load an image from file in monocular 3D object detection. Compared to 2D\n    detection, additional camera parameters need to be loaded.\n\n    Args:\n        kwargs (dict): Arguments are the same as those in\n            :class:`LoadImageFromFile`.\n    \"\"\"\n\n    def __call__(self, results):\n        \"\"\"Call functions to load image and get image meta information.\n\n        Args:\n            results (dict): Result dict from :obj:`mmdet.CustomDataset`.\n\n        Returns:\n            dict: The dict contains loaded image and meta information.\n        \"\"\"\n        super().__call__(results)\n        results['cam2img'] = results['img_info']['cam_intrinsic']\n        return results\n\n\n@PIPELINES.register_module()\nclass LoadPointsFromMultiSweeps(object):\n    \"\"\"Load points from multiple sweeps.\n\n    This is usually used for nuScenes dataset to utilize previous sweeps.\n\n    Args:\n        sweeps_num (int, optional): Number of sweeps. Defaults to 10.\n        load_dim (int, optional): Dimension number of the loaded points.\n            Defaults to 5.\n        use_dim (list[int], optional): Which dimension to use.\n            Defaults to [0, 1, 2, 4].\n        time_dim (int, optional): Which dimension to represent the timestamps\n            of each points. Defaults to 4.\n        file_client_args (dict, optional): Config dict of file clients,\n            refer to\n            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n            for more details. Defaults to dict(backend='disk').\n        pad_empty_sweeps (bool, optional): Whether to repeat keyframe when\n            sweeps is empty. Defaults to False.\n        remove_close (bool, optional): Whether to remove close points.\n            Defaults to False.\n        test_mode (bool, optional): If `test_mode=True`, it will not\n            randomly sample sweeps but select the nearest N frames.\n            Defaults to False.\n    \"\"\"\n\n    def __init__(self,\n                 sweeps_num=10,\n                 load_dim=5,\n                 use_dim=[0, 1, 2, 4],\n                 time_dim=4,\n                 file_client_args=dict(backend='disk'),\n                 pad_empty_sweeps=False,\n                 remove_close=False,\n                 test_mode=False):\n        self.load_dim = load_dim\n        self.sweeps_num = sweeps_num\n        self.use_dim = use_dim\n        self.time_dim = time_dim\n        assert time_dim < load_dim, \\\n            f'Expect the timestamp dimension < {load_dim}, got {time_dim}'\n        self.file_client_args = file_client_args.copy()\n        self.file_client = None\n        self.pad_empty_sweeps = pad_empty_sweeps\n        self.remove_close = remove_close\n        self.test_mode = test_mode\n        assert max(use_dim) < load_dim, \\\n            f'Expect all used dimensions < {load_dim}, got {use_dim}'\n\n    def _load_points(self, pts_filename):\n        \"\"\"Private function to load point clouds data.\n\n        Args:\n            pts_filename (str): Filename of point clouds data.\n\n        Returns:\n            np.ndarray: An array containing point clouds data.\n        \"\"\"\n        if self.file_client is None:\n            self.file_client = mmcv.FileClient(**self.file_client_args)\n        try:\n            pts_bytes = self.file_client.get(pts_filename)\n            points = np.frombuffer(pts_bytes, dtype=np.float32)\n        except ConnectionError:\n            mmcv.check_file_exist(pts_filename)\n            if pts_filename.endswith('.npy'):\n                points = np.load(pts_filename)\n            else:\n                points = np.fromfile(pts_filename, dtype=np.float32)\n        return points\n\n    def _remove_close(self, points, radius=1.0):\n        \"\"\"Removes point too close within a certain radius from origin.\n\n        Args:\n            points (np.ndarray | :obj:`BasePoints`): Sweep points.\n            radius (float, optional): Radius below which points are removed.\n                Defaults to 1.0.\n\n        Returns:\n            np.ndarray: Points after removing.\n        \"\"\"\n        if isinstance(points, np.ndarray):\n            points_numpy = points\n        elif isinstance(points, BasePoints):\n            points_numpy = points.tensor.numpy()\n        else:\n            raise NotImplementedError\n        x_filt = np.abs(points_numpy[:, 0]) < radius\n        y_filt = np.abs(points_numpy[:, 1]) < radius\n        not_close = np.logical_not(np.logical_and(x_filt, y_filt))\n        return points[not_close]\n\n    def __call__(self, results):\n        \"\"\"Call function to load multi-sweep point clouds from files.\n\n        Args:\n            results (dict): Result dict containing multi-sweep point cloud\n                filenames.\n\n        Returns:\n            dict: The result dict containing the multi-sweep points data.\n                Added key and value are described below.\n\n                - points (np.ndarray | :obj:`BasePoints`): Multi-sweep point\n                    cloud arrays.\n        \"\"\"\n        points = results['points']\n        points.tensor[:, self.time_dim] = 0\n        sweep_points_list = [points]\n        ts = results['timestamp']\n        if self.pad_empty_sweeps and len(results['sweeps']) == 0:\n            for i in range(self.sweeps_num):\n                if self.remove_close:\n                    sweep_points_list.append(self._remove_close(points))\n                else:\n                    sweep_points_list.append(points)\n        else:\n            if len(results['sweeps']) <= self.sweeps_num:\n                choices = np.arange(len(results['sweeps']))\n            elif self.test_mode:\n                choices = np.arange(self.sweeps_num)\n            else:\n                choices = np.random.choice(\n                    len(results['sweeps']), self.sweeps_num, replace=False)\n            for idx in choices:\n                sweep = results['sweeps'][idx]\n                points_sweep = self._load_points(sweep['data_path'])\n                points_sweep = np.copy(points_sweep).reshape(-1, self.load_dim)\n                if self.remove_close:\n                    points_sweep = self._remove_close(points_sweep)\n                sweep_ts = sweep['timestamp'] / 1e6\n                points_sweep[:, :3] = points_sweep[:, :3] @ sweep[\n                    'sensor2lidar_rotation'].T\n                points_sweep[:, :3] += sweep['sensor2lidar_translation']\n                points_sweep[:, self.time_dim] = ts - sweep_ts\n                points_sweep = points.new_point(points_sweep)\n                sweep_points_list.append(points_sweep)\n\n        points = points.cat(sweep_points_list)\n        points = points[:, self.use_dim]\n        results['points'] = points\n        return results\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        return f'{self.__class__.__name__}(sweeps_num={self.sweeps_num})'\n\n\n@PIPELINES.register_module()\nclass PointSegClassMapping(object):\n    \"\"\"Map original semantic class to valid category ids.\n\n    Map valid classes as 0~len(valid_cat_ids)-1 and\n    others as len(valid_cat_ids).\n\n    Args:\n        valid_cat_ids (tuple[int]): A tuple of valid category.\n        max_cat_id (int, optional): The max possible cat_id in input\n            segmentation mask. Defaults to 40.\n    \"\"\"\n\n    def __init__(self, valid_cat_ids, max_cat_id=40):\n        assert max_cat_id >= np.max(valid_cat_ids), \\\n            'max_cat_id should be greater than maximum id in valid_cat_ids'\n\n        self.valid_cat_ids = valid_cat_ids\n        self.max_cat_id = int(max_cat_id)\n\n        # build cat_id to class index mapping\n        neg_cls = len(valid_cat_ids)\n        self.cat_id2class = np.ones(\n            self.max_cat_id + 1, dtype=np.int) * neg_cls\n        for cls_idx, cat_id in enumerate(valid_cat_ids):\n            self.cat_id2class[cat_id] = cls_idx\n\n    def __call__(self, results):\n        \"\"\"Call function to map original semantic class to valid category ids.\n\n        Args:\n            results (dict): Result dict containing point semantic masks.\n\n        Returns:\n            dict: The result dict containing the mapped category ids.\n                Updated key and value are described below.\n\n                - pts_semantic_mask (np.ndarray): Mapped semantic masks.\n        \"\"\"\n        assert 'pts_semantic_mask' in results\n        pts_semantic_mask = results['pts_semantic_mask']\n\n        converted_pts_sem_mask = self.cat_id2class[pts_semantic_mask]\n\n        results['pts_semantic_mask'] = converted_pts_sem_mask\n        return results\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        repr_str = self.__class__.__name__\n        repr_str += f'(valid_cat_ids={self.valid_cat_ids}, '\n        repr_str += f'max_cat_id={self.max_cat_id})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass NormalizePointsColor(object):\n    \"\"\"Normalize color of points.\n\n    Args:\n        color_mean (list[float]): Mean color of the point cloud.\n    \"\"\"\n\n    def __init__(self, color_mean):\n        self.color_mean = color_mean\n\n    def __call__(self, results):\n        \"\"\"Call function to normalize color of points.\n\n        Args:\n            results (dict): Result dict containing point clouds data.\n\n        Returns:\n            dict: The result dict containing the normalized points.\n                Updated key and value are described below.\n\n                - points (:obj:`BasePoints`): Points after color normalization.\n        \"\"\"\n        points = results['points']\n        assert points.attribute_dims is not None and \\\n            'color' in points.attribute_dims.keys(), \\\n            'Expect points have color attribute'\n        if self.color_mean is not None:\n            points.color = points.color - \\\n                points.color.new_tensor(self.color_mean)\n        points.color = points.color / 255.0\n        results['points'] = points\n        return results\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        repr_str = self.__class__.__name__\n        repr_str += f'(color_mean={self.color_mean})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass LoadPointsFromFile(object):\n    \"\"\"Load Points From File.\n\n    Load points from file.\n\n    Args:\n        coord_type (str): The type of coordinates of points cloud.\n            Available options includes:\n            - 'LIDAR': Points in LiDAR coordinates.\n            - 'DEPTH': Points in depth coordinates, usually for indoor dataset.\n            - 'CAMERA': Points in camera coordinates.\n        load_dim (int, optional): The dimension of the loaded points.\n            Defaults to 6.\n        use_dim (list[int], optional): Which dimensions of the points to use.\n            Defaults to [0, 1, 2]. For KITTI dataset, set use_dim=4\n            or use_dim=[0, 1, 2, 3] to use the intensity dimension.\n        shift_height (bool, optional): Whether to use shifted height.\n            Defaults to False.\n        use_color (bool, optional): Whether to use color features.\n            Defaults to False.\n        file_client_args (dict, optional): Config dict of file clients,\n            refer to\n            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n            for more details. Defaults to dict(backend='disk').\n    \"\"\"\n\n    def __init__(self,\n                 coord_type,\n                 load_dim=6,\n                 use_dim=[0, 1, 2],\n                 shift_height=False,\n                 use_color=False,\n                 file_client_args=dict(backend='disk')):\n        self.shift_height = shift_height\n        self.use_color = use_color\n        if isinstance(use_dim, int):\n            use_dim = list(range(use_dim))\n        assert max(use_dim) < load_dim, \\\n            f'Expect all used dimensions < {load_dim}, got {use_dim}'\n        assert coord_type in ['CAMERA', 'LIDAR', 'DEPTH']\n\n        self.coord_type = coord_type\n        self.load_dim = load_dim\n        self.use_dim = use_dim\n        self.file_client_args = file_client_args.copy()\n        self.file_client = None\n\n    def _load_points(self, pts_filename):\n        \"\"\"Private function to load point clouds data.\n\n        Args:\n            pts_filename (str): Filename of point clouds data.\n\n        Returns:\n            np.ndarray: An array containing point clouds data.\n        \"\"\"\n        if self.file_client is None:\n            self.file_client = mmcv.FileClient(**self.file_client_args)\n        try:\n            pts_bytes = self.file_client.get(pts_filename)\n            points = np.frombuffer(pts_bytes, dtype=np.float32)\n        except ConnectionError:\n            mmcv.check_file_exist(pts_filename)\n            if pts_filename.endswith('.npy'):\n                points = np.load(pts_filename)\n            else:\n                points = np.fromfile(pts_filename, dtype=np.float32)\n\n        return points\n\n    def __call__(self, results):\n        \"\"\"Call function to load points data from file.\n\n        Args:\n            results (dict): Result dict containing point clouds data.\n\n        Returns:\n            dict: The result dict containing the point clouds data.\n                Added key and value are described below.\n\n                - points (:obj:`BasePoints`): Point clouds data.\n        \"\"\"\n        pts_filename = results['pts_filename']\n        points = self._load_points(pts_filename)\n        points = points.reshape(-1, self.load_dim)\n        points = points[:, self.use_dim]\n        attribute_dims = None\n\n        if self.shift_height:\n            floor_height = np.percentile(points[:, 2], 0.99)\n            height = points[:, 2] - floor_height\n            points = np.concatenate(\n                [points[:, :3],\n                 np.expand_dims(height, 1), points[:, 3:]], 1)\n            attribute_dims = dict(height=3)\n\n        if self.use_color:\n            assert len(self.use_dim) >= 6\n            if attribute_dims is None:\n                attribute_dims = dict()\n            attribute_dims.update(\n                dict(color=[\n                    points.shape[1] - 3,\n                    points.shape[1] - 2,\n                    points.shape[1] - 1,\n                ]))\n\n        points_class = get_points_type(self.coord_type)\n        points = points_class(\n            points, points_dim=points.shape[-1], attribute_dims=attribute_dims)\n        results['points'] = points\n\n        return results\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        repr_str = self.__class__.__name__ + '('\n        repr_str += f'shift_height={self.shift_height}, '\n        repr_str += f'use_color={self.use_color}, '\n        repr_str += f'file_client_args={self.file_client_args}, '\n        repr_str += f'load_dim={self.load_dim}, '\n        repr_str += f'use_dim={self.use_dim})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass LoadPointsFromDict(LoadPointsFromFile):\n    \"\"\"Load Points From Dict.\"\"\"\n\n    def __call__(self, results):\n        assert 'points' in results\n        return results\n\n\n@PIPELINES.register_module()\nclass LoadAnnotations3D(LoadAnnotations):\n    \"\"\"Load Annotations3D.\n\n    Load instance mask and semantic mask of points and\n    encapsulate the items into related fields.\n\n    Args:\n        with_bbox_3d (bool, optional): Whether to load 3D boxes.\n            Defaults to True.\n        with_label_3d (bool, optional): Whether to load 3D labels.\n            Defaults to True.\n        with_attr_label (bool, optional): Whether to load attribute label.\n            Defaults to False.\n        with_mask_3d (bool, optional): Whether to load 3D instance masks.\n            for points. Defaults to False.\n        with_seg_3d (bool, optional): Whether to load 3D semantic masks.\n            for points. Defaults to False.\n        with_bbox (bool, optional): Whether to load 2D boxes.\n            Defaults to False.\n        with_label (bool, optional): Whether to load 2D labels.\n            Defaults to False.\n        with_mask (bool, optional): Whether to load 2D instance masks.\n            Defaults to False.\n        with_seg (bool, optional): Whether to load 2D semantic masks.\n            Defaults to False.\n        with_bbox_depth (bool, optional): Whether to load 2.5D boxes.\n            Defaults to False.\n        poly2mask (bool, optional): Whether to convert polygon annotations\n            to bitmasks. Defaults to True.\n        seg_3d_dtype (dtype, optional): Dtype of 3D semantic masks.\n            Defaults to int64\n        file_client_args (dict): Config dict of file clients, refer to\n            https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py\n            for more details.\n    \"\"\"\n\n    def __init__(self,\n                 with_bbox_3d=True,\n                 with_label_3d=True,\n                 with_attr_label=False,\n                 with_mask_3d=False,\n                 with_seg_3d=False,\n                 with_bbox=False,\n                 with_label=False,\n                 with_mask=False,\n                 with_seg=False,\n                 with_bbox_depth=False,\n                 poly2mask=True,\n                 seg_3d_dtype=np.int64,\n                 file_client_args=dict(backend='disk')):\n        super().__init__(\n            with_bbox,\n            with_label,\n            with_mask,\n            with_seg,\n            poly2mask,\n            file_client_args=file_client_args)\n        self.with_bbox_3d = with_bbox_3d\n        self.with_bbox_depth = with_bbox_depth\n        self.with_label_3d = with_label_3d\n        self.with_attr_label = with_attr_label\n        self.with_mask_3d = with_mask_3d\n        self.with_seg_3d = with_seg_3d\n        self.seg_3d_dtype = seg_3d_dtype\n\n    def _load_bboxes_3d(self, results):\n        \"\"\"Private function to load 3D bounding box annotations.\n\n        Args:\n            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n\n        Returns:\n            dict: The dict containing loaded 3D bounding box annotations.\n        \"\"\"\n        results['gt_bboxes_3d'] = results['ann_info']['gt_bboxes_3d']\n        results['bbox3d_fields'].append('gt_bboxes_3d')\n        return results\n\n    def _load_bboxes_depth(self, results):\n        \"\"\"Private function to load 2.5D bounding box annotations.\n\n        Args:\n            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n\n        Returns:\n            dict: The dict containing loaded 2.5D bounding box annotations.\n        \"\"\"\n        results['centers2d'] = results['ann_info']['centers2d']\n        results['depths'] = results['ann_info']['depths']\n        return results\n\n    def _load_labels_3d(self, results):\n        \"\"\"Private function to load label annotations.\n\n        Args:\n            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n\n        Returns:\n            dict: The dict containing loaded label annotations.\n        \"\"\"\n        results['gt_labels_3d'] = results['ann_info']['gt_labels_3d']\n        return results\n\n    def _load_attr_labels(self, results):\n        \"\"\"Private function to load label annotations.\n\n        Args:\n            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n\n        Returns:\n            dict: The dict containing loaded label annotations.\n        \"\"\"\n        results['attr_labels'] = results['ann_info']['attr_labels']\n        return results\n\n    def _load_masks_3d(self, results):\n        \"\"\"Private function to load 3D mask annotations.\n\n        Args:\n            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n\n        Returns:\n            dict: The dict containing loaded 3D mask annotations.\n        \"\"\"\n        pts_instance_mask_path = results['ann_info']['pts_instance_mask_path']\n\n        if self.file_client is None:\n            self.file_client = mmcv.FileClient(**self.file_client_args)\n        try:\n            mask_bytes = self.file_client.get(pts_instance_mask_path)\n            pts_instance_mask = np.frombuffer(mask_bytes, dtype=np.int64)\n        except ConnectionError:\n            mmcv.check_file_exist(pts_instance_mask_path)\n            pts_instance_mask = np.fromfile(\n                pts_instance_mask_path, dtype=np.int64)\n\n        results['pts_instance_mask'] = pts_instance_mask\n        results['pts_mask_fields'].append('pts_instance_mask')\n        return results\n\n    def _load_semantic_seg_3d(self, results):\n        \"\"\"Private function to load 3D semantic segmentation annotations.\n\n        Args:\n            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n\n        Returns:\n            dict: The dict containing the semantic segmentation annotations.\n        \"\"\"\n        pts_semantic_mask_path = results['ann_info']['pts_semantic_mask_path']\n\n        if self.file_client is None:\n            self.file_client = mmcv.FileClient(**self.file_client_args)\n        try:\n            mask_bytes = self.file_client.get(pts_semantic_mask_path)\n            # add .copy() to fix read-only bug\n            pts_semantic_mask = np.frombuffer(\n                mask_bytes, dtype=self.seg_3d_dtype).copy()\n        except ConnectionError:\n            mmcv.check_file_exist(pts_semantic_mask_path)\n            pts_semantic_mask = np.fromfile(\n                pts_semantic_mask_path, dtype=np.int64)\n\n        results['pts_semantic_mask'] = pts_semantic_mask\n        results['pts_seg_fields'].append('pts_semantic_mask')\n        return results\n\n    def __call__(self, results):\n        \"\"\"Call function to load multiple types annotations.\n\n        Args:\n            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.\n\n        Returns:\n            dict: The dict containing loaded 3D bounding box, label, mask and\n                semantic segmentation annotations.\n        \"\"\"\n        results = super().__call__(results)\n        if self.with_bbox_3d:\n            results = self._load_bboxes_3d(results)\n            if results is None:\n                return None\n        if self.with_bbox_depth:\n            results = self._load_bboxes_depth(results)\n            if results is None:\n                return None\n        if self.with_label_3d:\n            results = self._load_labels_3d(results)\n        if self.with_attr_label:\n            results = self._load_attr_labels(results)\n        if self.with_mask_3d:\n            results = self._load_masks_3d(results)\n        if self.with_seg_3d:\n            results = self._load_semantic_seg_3d(results)\n\n        return results\n\n    def __repr__(self):\n        \"\"\"str: Return a string that describes the module.\"\"\"\n        indent_str = '    '\n        repr_str = self.__class__.__name__ + '(\\n'\n        repr_str += f'{indent_str}with_bbox_3d={self.with_bbox_3d}, '\n        repr_str += f'{indent_str}with_label_3d={self.with_label_3d}, '\n        repr_str += f'{indent_str}with_attr_label={self.with_attr_label}, '\n        repr_str += f'{indent_str}with_mask_3d={self.with_mask_3d}, '\n        repr_str += f'{indent_str}with_seg_3d={self.with_seg_3d}, '\n        repr_str += f'{indent_str}with_bbox={self.with_bbox}, '\n        repr_str += f'{indent_str}with_label={self.with_label}, '\n        repr_str += f'{indent_str}with_mask={self.with_mask}, '\n        repr_str += f'{indent_str}with_seg={self.with_seg}, '\n        repr_str += f'{indent_str}with_bbox_depth={self.with_bbox_depth}, '\n        repr_str += f'{indent_str}poly2mask={self.poly2mask})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass PointToMultiViewDepth(object):\n\n    def __init__(self, grid_config, downsample=1):\n        self.downsample = downsample\n        self.grid_config = grid_config\n\n    def points2depthmap(self, points, height, width):\n        height, width = height // self.downsample, width // self.downsample\n        depth_map = torch.zeros((height, width), dtype=torch.float32)\n        coor = torch.round(points[:, :2] / self.downsample)\n        depth = points[:, 2]\n        kept1 = (coor[:, 0] >= 0) & (coor[:, 0] < width) & (\n            coor[:, 1] >= 0) & (coor[:, 1] < height) & (\n                depth < self.grid_config['depth'][1]) & (\n                    depth >= self.grid_config['depth'][0])\n        coor, depth = coor[kept1], depth[kept1]\n        ranks = coor[:, 0] + coor[:, 1] * width\n        sort = (ranks + depth / 100.).argsort()\n        coor, depth, ranks = coor[sort], depth[sort], ranks[sort]\n\n        kept2 = torch.ones(coor.shape[0], device=coor.device, dtype=torch.bool)\n        kept2[1:] = (ranks[1:] != ranks[:-1])\n        coor, depth = coor[kept2], depth[kept2]\n        coor = coor.to(torch.long)\n        depth_map[coor[:, 1], coor[:, 0]] = depth\n        return depth_map\n\n    def __call__(self, results):\n        points_lidar = results['points']\n        imgs, rots, trans, intrins = results['img_inputs'][:4]\n        post_rots, post_trans, bda = results['img_inputs'][4:]\n        depth_map_list = []\n        for cid in range(len(results['cam_names'])):\n            cam_name = results['cam_names'][cid]\n            lidar2lidarego = np.eye(4, dtype=np.float32)\n            lidar2lidarego[:3, :3] = Quaternion(\n                results['curr']['lidar2ego_rotation']).rotation_matrix\n            lidar2lidarego[:3, 3] = results['curr']['lidar2ego_translation']\n            lidar2lidarego = torch.from_numpy(lidar2lidarego)\n\n            lidarego2global = np.eye(4, dtype=np.float32)\n            lidarego2global[:3, :3] = Quaternion(\n                results['curr']['ego2global_rotation']).rotation_matrix\n            lidarego2global[:3, 3] = results['curr']['ego2global_translation']\n            lidarego2global = torch.from_numpy(lidarego2global)\n\n            cam2camego = np.eye(4, dtype=np.float32)\n            cam2camego[:3, :3] = Quaternion(\n                results['curr']['cams'][cam_name]\n                ['sensor2ego_rotation']).rotation_matrix\n            cam2camego[:3, 3] = results['curr']['cams'][cam_name][\n                'sensor2ego_translation']\n            cam2camego = torch.from_numpy(cam2camego)\n\n            camego2global = np.eye(4, dtype=np.float32)\n            camego2global[:3, :3] = Quaternion(\n                results['curr']['cams'][cam_name]\n                ['ego2global_rotation']).rotation_matrix\n            camego2global[:3, 3] = results['curr']['cams'][cam_name][\n                'ego2global_translation']\n            camego2global = torch.from_numpy(camego2global)\n\n            cam2img = np.eye(4, dtype=np.float32)\n            cam2img = torch.from_numpy(cam2img)\n            cam2img[:3, :3] = intrins[cid]\n\n            lidar2cam = torch.inverse(camego2global.matmul(cam2camego)).matmul(\n                lidarego2global.matmul(lidar2lidarego))\n            lidar2img = cam2img.matmul(lidar2cam)\n            points_img = points_lidar.tensor[:, :3].matmul(\n                lidar2img[:3, :3].T) + lidar2img[:3, 3].unsqueeze(0)\n            points_img = torch.cat(\n                [points_img[:, :2] / points_img[:, 2:3], points_img[:, 2:3]],\n                1)\n            points_img = points_img.matmul(\n                post_rots[cid].T) + post_trans[cid:cid + 1, :]\n            depth_map = self.points2depthmap(points_img, imgs.shape[2],\n                                             imgs.shape[3])\n            depth_map_list.append(depth_map)\n        depth_map = torch.stack(depth_map_list)\n        results['gt_depth'] = depth_map\n        return results\n\n\n@PIPELINES.register_module()\nclass PointToMultiViewDepthFusion(PointToMultiViewDepth):\n    def __call__(self, results):\n        points_camego_aug = results['points'].tensor[:, :3]\n        # print(points_lidar.shape)\n        imgs, rots, trans, intrins = results['img_inputs'][:4]\n        post_rots, post_trans, bda = results['img_inputs'][4:]\n        points_camego = points_camego_aug - bda[:3, 3].view(1,3)\n        points_camego = points_camego.matmul(torch.inverse(bda[:3,:3]).T)\n\n        depth_map_list = []\n        for cid in range(len(results['cam_names'])):\n            cam_name = results['cam_names'][cid]\n\n            cam2camego = np.eye(4, dtype=np.float32)\n            cam2camego[:3, :3] = Quaternion(\n                results['curr']['cams'][cam_name]\n                ['sensor2ego_rotation']).rotation_matrix\n            cam2camego[:3, 3] = results['curr']['cams'][cam_name][\n                'sensor2ego_translation']\n            cam2camego = torch.from_numpy(cam2camego)\n\n            cam2img = np.eye(4, dtype=np.float32)\n            cam2img = torch.from_numpy(cam2img)\n            cam2img[:3, :3] = intrins[cid]\n\n            camego2img = cam2img.matmul(torch.inverse(cam2camego))\n\n            points_img = points_camego.matmul(\n                camego2img[:3, :3].T) + camego2img[:3, 3].unsqueeze(0)\n            points_img = torch.cat(\n                [points_img[:, :2] / points_img[:, 2:3], points_img[:, 2:3]],\n                1)\n            points_img = points_img.matmul(\n                post_rots[cid].T) + post_trans[cid:cid + 1, :]\n            depth_map = self.points2depthmap(points_img, imgs.shape[2],\n                                             imgs.shape[3])\n            depth_map_list.append(depth_map)\n        depth_map = torch.stack(depth_map_list)\n        results['gt_depth'] = depth_map\n        return results\n\n\ndef mmlabNormalize(img):\n    from mmcv.image.photometric import imnormalize\n    mean = np.array([123.675, 116.28, 103.53], dtype=np.float32)\n    std = np.array([58.395, 57.12, 57.375], dtype=np.float32)\n    to_rgb = True\n    img = imnormalize(np.array(img), mean, std, to_rgb)\n    img = torch.tensor(img).float().permute(2, 0, 1).contiguous()\n    return img\n\n\n@PIPELINES.register_module()\nclass PrepareImageInputs(object):\n    \"\"\"Load multi channel images from a list of separate channel files.\n\n    Expects results['img_filename'] to be a list of filenames.\n\n    Args:\n        to_float32 (bool): Whether to convert the img to float32.\n            Defaults to False.\n        color_type (str): Color type of the file. Defaults to 'unchanged'.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_config,\n        is_train=False,\n        sequential=False,\n        opencv_pp=False,\n    ):\n        self.is_train = is_train\n        self.data_config = data_config\n        self.normalize_img = mmlabNormalize\n        self.sequential = sequential\n        self.opencv_pp = opencv_pp\n\n    def get_rot(self, h):\n        return torch.Tensor([\n            [np.cos(h), np.sin(h)],\n            [-np.sin(h), np.cos(h)],\n        ])\n\n    def img_transform(self, img, post_rot, post_tran, resize, resize_dims,\n                      crop, flip, rotate):\n        # adjust image\n        if not self.opencv_pp:\n            img = self.img_transform_core(img, resize_dims, crop, flip, rotate)\n\n        # post-homography transformation\n        post_rot *= resize\n        post_tran -= torch.Tensor(crop[:2])\n        if flip:\n            A = torch.Tensor([[-1, 0], [0, 1]])\n            b = torch.Tensor([crop[2] - crop[0], 0])\n            post_rot = A.matmul(post_rot)\n            post_tran = A.matmul(post_tran) + b\n        A = self.get_rot(rotate / 180 * np.pi)\n        b = torch.Tensor([crop[2] - crop[0], crop[3] - crop[1]]) / 2\n        b = A.matmul(-b) + b\n        post_rot = A.matmul(post_rot)\n        post_tran = A.matmul(post_tran) + b\n        if self.opencv_pp:\n            img = self.img_transform_core_opencv(img, post_rot, post_tran, crop)\n        return img, post_rot, post_tran\n\n    def img_transform_core_opencv(self, img, post_rot, post_tran,\n                                  crop):\n        img = np.array(img).astype(np.float32)\n        img = cv2.warpAffine(img,\n                             np.concatenate([post_rot,\n                                            post_tran.reshape(2,1)],\n                                            axis=1),\n                             (crop[2]-crop[0], crop[3]-crop[1]),\n                             flags=cv2.INTER_LINEAR)\n        return img\n\n    def img_transform_core(self, img, resize_dims, crop, flip, rotate):\n        # adjust image\n        img = img.resize(resize_dims)\n        img = img.crop(crop)\n        if flip:\n            img = img.transpose(method=Image.FLIP_LEFT_RIGHT)\n        img = img.rotate(rotate)\n        return img\n\n    def choose_cams(self):\n        if self.is_train and self.data_config['Ncams'] < len(\n                self.data_config['cams']):\n            cam_names = np.random.choice(\n                self.data_config['cams'],\n                self.data_config['Ncams'],\n                replace=False)\n        else:\n            cam_names = self.data_config['cams']\n        return cam_names\n\n    def sample_augmentation(self, H, W, flip=None, scale=None):\n        fH, fW = self.data_config['input_size']\n        if self.is_train:\n            resize = float(fW) / float(W)\n            resize += np.random.uniform(*self.data_config['resize'])\n            resize_dims = (int(W * resize), int(H * resize))\n            newW, newH = resize_dims\n            random_crop_height = \\\n                self.data_config.get('random_crop_height', False)\n            if random_crop_height:\n                crop_h = int(np.random.uniform(max(0.3*newH, newH-fH),\n                                               newH-fH))\n            else:\n                crop_h = \\\n                    int((1 - np.random.uniform(*self.data_config['crop_h'])) *\n                         newH) - fH\n            crop_w = int(np.random.uniform(0, max(0, newW - fW)))\n            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n            flip = self.data_config['flip'] and np.random.choice([0, 1])\n            rotate = np.random.uniform(*self.data_config['rot'])\n            if self.data_config.get('vflip', False) and np.random.choice([0, 1]):\n                rotate += 180\n        else:\n            resize = float(fW) / float(W)\n            if scale is not None:\n                resize += scale\n            else:\n                resize += self.data_config.get('resize_test', 0.0)\n            resize_dims = (int(W * resize), int(H * resize))\n            newW, newH = resize_dims\n            crop_h = int((1 - np.mean(self.data_config['crop_h'])) * newH) - fH\n            crop_w = int(max(0, newW - fW) / 2)\n            crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n            flip = False if flip is None else flip\n            rotate = 0\n        return resize, resize_dims, crop, flip, rotate\n\n    def get_sensor_transforms(self, cam_info, cam_name):\n        w, x, y, z = cam_info['cams'][cam_name]['sensor2ego_rotation']\n        # sweep sensor to sweep ego\n        sensor2ego_rot = torch.Tensor(\n            Quaternion(w, x, y, z).rotation_matrix)\n        sensor2ego_tran = torch.Tensor(\n            cam_info['cams'][cam_name]['sensor2ego_translation'])\n        sensor2ego = sensor2ego_rot.new_zeros((4, 4))\n        sensor2ego[3, 3] = 1\n        sensor2ego[:3, :3] = sensor2ego_rot\n        sensor2ego[:3, -1] = sensor2ego_tran\n        # sweep ego to global\n        w, x, y, z = cam_info['cams'][cam_name]['ego2global_rotation']\n        ego2global_rot = torch.Tensor(\n            Quaternion(w, x, y, z).rotation_matrix)\n        ego2global_tran = torch.Tensor(\n            cam_info['cams'][cam_name]['ego2global_translation'])\n        ego2global = ego2global_rot.new_zeros((4, 4))\n        ego2global[3, 3] = 1\n        ego2global[:3, :3] = ego2global_rot\n        ego2global[:3, -1] = ego2global_tran\n        return sensor2ego, ego2global\n\n    def photo_metric_distortion(self, img, pmd):\n        \"\"\"Call function to perform photometric distortion on images.\n        Args:\n            results (dict): Result dict from loading pipeline.\n        Returns:\n            dict: Result dict with images distorted.\n        \"\"\"\n        if np.random.rand()>pmd.get('rate', 1.0):\n            return img\n\n        img = np.array(img).astype(np.float32)\n        assert img.dtype == np.float32, \\\n            'PhotoMetricDistortion needs the input image of dtype np.float32,' \\\n            ' please set \"to_float32=True\" in \"LoadImageFromFile\" pipeline'\n        # random brightness\n        if np.random.randint(2):\n            delta = np.random.uniform(-pmd['brightness_delta'],\n                                   pmd['brightness_delta'])\n            img += delta\n\n        # mode == 0 --> do random contrast first\n        # mode == 1 --> do random contrast last\n        mode = np.random.randint(2)\n        if mode == 1:\n            if np.random.randint(2):\n                alpha = np.random.uniform(pmd['contrast_lower'],\n                                       pmd['contrast_upper'])\n                img *= alpha\n\n        # convert color from BGR to HSV\n        img = mmcv.bgr2hsv(img)\n\n        # random saturation\n        if np.random.randint(2):\n            img[..., 1] *= np.random.uniform(pmd['saturation_lower'],\n                                          pmd['saturation_upper'])\n\n        # random hue\n        if np.random.randint(2):\n            img[..., 0] += np.random.uniform(-pmd['hue_delta'], pmd['hue_delta'])\n            img[..., 0][img[..., 0] > 360] -= 360\n            img[..., 0][img[..., 0] < 0] += 360\n\n        # convert color from HSV to BGR\n        img = mmcv.hsv2bgr(img)\n\n        # random contrast\n        if mode == 0:\n            if np.random.randint(2):\n                alpha = np.random.uniform(pmd['contrast_lower'],\n                                       pmd['contrast_upper'])\n                img *= alpha\n\n        # randomly swap channels\n        if np.random.randint(2):\n            img = img[..., np.random.permutation(3)]\n        return Image.fromarray(img.astype(np.uint8))\n\n    def get_inputs(self, results, flip=None, scale=None):\n        imgs = []\n        sensor2egos = []\n        ego2globals = []\n        intrins = []\n        post_rots = []\n        post_trans = []\n        cam_names = self.choose_cams()\n        results['cam_names'] = cam_names\n        canvas = []\n        for cam_name in cam_names:\n            cam_data = results['curr']['cams'][cam_name]\n            filename = cam_data['data_path']\n            img = Image.open(filename)\n            post_rot = torch.eye(2)\n            post_tran = torch.zeros(2)\n\n            intrin = torch.Tensor(cam_data['cam_intrinsic'])\n\n            sensor2ego, ego2global = \\\n                self.get_sensor_transforms(results['curr'], cam_name)\n            # image view augmentation (resize, crop, horizontal flip, rotate)\n            img_augs = self.sample_augmentation(\n                H=img.height, W=img.width, flip=flip, scale=scale)\n            resize, resize_dims, crop, flip, rotate = img_augs\n            img, post_rot2, post_tran2 = \\\n                self.img_transform(img, post_rot,\n                                   post_tran,\n                                   resize=resize,\n                                   resize_dims=resize_dims,\n                                   crop=crop,\n                                   flip=flip,\n                                   rotate=rotate)\n\n            # for convenience, make augmentation matrices 3x3\n            post_tran = torch.zeros(3)\n            post_rot = torch.eye(3)\n            post_tran[:2] = post_tran2\n            post_rot[:2, :2] = post_rot2\n\n            if self.is_train and self.data_config.get('pmd', None) is not None:\n                img = self.photo_metric_distortion(img, self.data_config['pmd'])\n\n            canvas.append(np.array(img))\n            imgs.append(self.normalize_img(img))\n\n            if self.sequential:\n                assert 'adjacent' in results\n                for adj_info in results['adjacent']:\n                    filename_adj = adj_info['cams'][cam_name]['data_path']\n                    img_adjacent = Image.open(filename_adj)\n                    if self.opencv_pp:\n                        img_adjacent = \\\n                            self.img_transform_core_opencv(\n                                img_adjacent,\n                                post_rot[:2, :2],\n                                post_tran[:2],\n                                crop)\n                    else:\n                        img_adjacent = self.img_transform_core(\n                            img_adjacent,\n                            resize_dims=resize_dims,\n                            crop=crop,\n                            flip=flip,\n                            rotate=rotate)\n                    imgs.append(self.normalize_img(img_adjacent))\n            intrins.append(intrin)\n            sensor2egos.append(sensor2ego)\n            ego2globals.append(ego2global)\n            post_rots.append(post_rot)\n            post_trans.append(post_tran)\n\n        if self.sequential:\n            for adj_info in results['adjacent']:\n                post_trans.extend(post_trans[:len(cam_names)])\n                post_rots.extend(post_rots[:len(cam_names)])\n                intrins.extend(intrins[:len(cam_names)])\n\n                # align\n                for cam_name in cam_names:\n                    sensor2ego, ego2global = \\\n                        self.get_sensor_transforms(adj_info, cam_name)\n                    sensor2egos.append(sensor2ego)\n                    ego2globals.append(ego2global)\n\n        imgs = torch.stack(imgs)\n\n        sensor2egos = torch.stack(sensor2egos)\n        ego2globals = torch.stack(ego2globals)\n        intrins = torch.stack(intrins)\n        post_rots = torch.stack(post_rots)\n        post_trans = torch.stack(post_trans)\n        results['canvas'] = canvas\n        return (imgs, sensor2egos, ego2globals, intrins, post_rots, post_trans)\n\n    def __call__(self, results):\n        results['img_inputs'] = self.get_inputs(results)\n        return results\n\n\n@PIPELINES.register_module()\nclass LoadAnnotations(object):\n\n    def __call__(self, results):\n        gt_boxes, gt_labels = results['ann_infos']\n        gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n        if len(gt_boxes) == 0:\n            gt_boxes = torch.zeros(0, 9)\n        results['gt_bboxes_3d'] = \\\n            LiDARInstance3DBoxes(gt_boxes, box_dim=gt_boxes.shape[-1],\n                                 origin=(0.5, 0.5, 0.5))\n        results['gt_labels_3d'] = gt_labels\n        return results\n\n\n@PIPELINES.register_module()\nclass BEVAug(object):\n\n    def __init__(self, bda_aug_conf, classes, is_train=True):\n        self.bda_aug_conf = bda_aug_conf\n        self.is_train = is_train\n        self.classes = classes\n\n    def sample_bda_augmentation(self):\n        \"\"\"Generate bda augmentation values based on bda_config.\"\"\"\n        if self.is_train:\n            rotate_bda = np.random.uniform(*self.bda_aug_conf['rot_lim'])\n            scale_bda = np.random.uniform(*self.bda_aug_conf['scale_lim'])\n            flip_dx = np.random.uniform() < self.bda_aug_conf['flip_dx_ratio']\n            flip_dy = np.random.uniform() < self.bda_aug_conf['flip_dy_ratio']\n            translation_std = self.bda_aug_conf.get('tran_lim', [0.0, 0.0, 0.0])\n            tran_bda = np.random.normal(scale=translation_std, size=3).T\n        else:\n            rotate_bda = 0\n            scale_bda = 1.0\n            flip_dx = False\n            flip_dy = False\n            tran_bda = np.zeros((1, 3), dtype=np.float32)\n        return rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda\n\n    def bev_transform(self, gt_boxes, rotate_angle, scale_ratio, flip_dx,\n                      flip_dy, tran_bda):\n        rotate_angle = torch.tensor(rotate_angle / 180 * np.pi)\n        rot_sin = torch.sin(rotate_angle)\n        rot_cos = torch.cos(rotate_angle)\n        rot_mat = torch.Tensor([[rot_cos, -rot_sin, 0], [rot_sin, rot_cos, 0],\n                                [0, 0, 1]])\n        scale_mat = torch.Tensor([[scale_ratio, 0, 0], [0, scale_ratio, 0],\n                                  [0, 0, scale_ratio]])\n        flip_mat = torch.Tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n        if flip_dx:\n            flip_mat = flip_mat @ torch.Tensor([[-1, 0, 0], [0, 1, 0],\n                                                [0, 0, 1]])\n        if flip_dy:\n            flip_mat = flip_mat @ torch.Tensor([[1, 0, 0], [0, -1, 0],\n                                                [0, 0, 1]])\n        rot_mat = flip_mat @ (scale_mat @ rot_mat)\n        if gt_boxes.shape[0] > 0:\n            gt_boxes[:, :3] = (\n                rot_mat @ gt_boxes[:, :3].unsqueeze(-1)).squeeze(-1)\n            gt_boxes[:, 3:6] *= scale_ratio\n            gt_boxes[:, 6] += rotate_angle\n            if flip_dx:\n                gt_boxes[:,\n                         6] = 2 * torch.asin(torch.tensor(1.0)) - gt_boxes[:,\n                                                                           6]\n            if flip_dy:\n                gt_boxes[:, 6] = -gt_boxes[:, 6]\n            gt_boxes[:, 7:] = (\n                rot_mat[:2, :2] @ gt_boxes[:, 7:].unsqueeze(-1)).squeeze(-1)\n            gt_boxes[:, :3] = gt_boxes[:, :3] + tran_bda\n        return gt_boxes, rot_mat\n\n    def __call__(self, results):\n        gt_boxes = results['gt_bboxes_3d'].tensor\n        gt_boxes[:,2] = gt_boxes[:,2] + 0.5*gt_boxes[:,5]\n        rotate_bda, scale_bda, flip_dx, flip_dy, tran_bda = \\\n            self.sample_bda_augmentation()\n        bda_mat = torch.zeros(4, 4)\n        bda_mat[3, 3] = 1\n        gt_boxes, bda_rot = self.bev_transform(gt_boxes, rotate_bda, scale_bda,\n                                               flip_dx, flip_dy, tran_bda)\n        if 'points' in results:\n            points = results['points'].tensor\n            points_aug = (bda_rot @ points[:, :3].unsqueeze(-1)).squeeze(-1)\n            points[:,:3] = points_aug + tran_bda\n            points = results['points'].new_point(points)\n            results['points'] = points\n        bda_mat[:3, :3] = bda_rot\n        bda_mat[:3, 3] = torch.from_numpy(tran_bda)\n        if len(gt_boxes) == 0:\n            gt_boxes = torch.zeros(0, 9)\n        results['gt_bboxes_3d'] = \\\n            LiDARInstance3DBoxes(gt_boxes, box_dim=gt_boxes.shape[-1],\n                                 origin=(0.5, 0.5, 0.5))\n        if 'img_inputs' in results:\n            imgs, rots, trans, intrins = results['img_inputs'][:4]\n            post_rots, post_trans = results['img_inputs'][4:]\n            results['img_inputs'] = (imgs, rots, trans, intrins, post_rots,\n                                     post_trans, bda_mat)\n        if 'voxel_semantics' in results:\n            if flip_dx:\n                results['voxel_semantics'] = results['voxel_semantics'][::-1,...].copy()\n                results['mask_lidar'] = results['mask_lidar'][::-1,...].copy()\n                results['mask_camera'] = results['mask_camera'][::-1,...].copy()\n            if flip_dy:\n                results['voxel_semantics'] = results['voxel_semantics'][:,::-1,...].copy()\n                results['mask_lidar'] = results['mask_lidar'][:,::-1,...].copy()\n                results['mask_camera'] = results['mask_camera'][:,::-1,...].copy()\n        return results\n"
        }
    ]
}