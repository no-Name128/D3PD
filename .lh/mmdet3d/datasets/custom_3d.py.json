{
    "sourceFile": "mmdet3d/datasets/custom_3d.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1719994093034,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1719994093034,
            "name": "Commit-0",
            "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport tempfile\nimport warnings\nfrom os import path as osp\n\nimport mmcv\nimport numpy as np\nfrom torch.utils.data import Dataset\n\nfrom ..core.bbox import get_box_type\nfrom .builder import DATASETS\nfrom .pipelines import Compose\nfrom .utils import extract_result_dict, get_loading_pipeline\n\n\n@DATASETS.register_module()\nclass Custom3DDataset(Dataset):\n    \"\"\"Customized 3D dataset.\n\n    This is the base dataset of SUNRGB-D, ScanNet, nuScenes, and KITTI\n    dataset.\n\n    .. code-block:: none\n\n    [\n        {'sample_idx':\n         'lidar_points': {'lidar_path': velodyne_path,\n                           ....\n                         },\n         'annos': {'box_type_3d':  (str)  'LiDAR/Camera/Depth'\n                   'gt_bboxes_3d':  <np.ndarray> (n, 7)\n                   'gt_names':  [list]\n                   ....\n               }\n         'calib': { .....}\n         'images': { .....}\n        }\n    ]\n\n    Args:\n        data_root (str): Path of dataset root.\n        ann_file (str): Path of annotation file.\n        pipeline (list[dict], optional): Pipeline used for data processing.\n            Defaults to None.\n        classes (tuple[str], optional): Classes used in the dataset.\n            Defaults to None.\n        modality (dict, optional): Modality to specify the sensor data used\n            as input. Defaults to None.\n        box_type_3d (str, optional): Type of 3D box of this dataset.\n            Based on the `box_type_3d`, the dataset will encapsulate the box\n            to its original format then converted them to `box_type_3d`.\n            Defaults to 'LiDAR'. Available options includes\n\n            - 'LiDAR': Box in LiDAR coordinates.\n            - 'Depth': Box in depth coordinates, usually for indoor dataset.\n            - 'Camera': Box in camera coordinates.\n        filter_empty_gt (bool, optional): Whether to filter empty GT.\n            Defaults to True.\n        test_mode (bool, optional): Whether the dataset is in test mode.\n            Defaults to False.\n    \"\"\"\n\n    def __init__(self,\n                 data_root,\n                 ann_file,\n                 pipeline=None,\n                 classes=None,\n                 modality=None,\n                 box_type_3d='LiDAR',\n                 filter_empty_gt=True,\n                 test_mode=False,\n                 file_client_args=dict(backend='disk')):\n        super().__init__()\n        self.data_root = data_root\n        self.ann_file = ann_file\n        self.test_mode = test_mode\n        self.modality = modality\n        self.filter_empty_gt = filter_empty_gt\n        self.box_type_3d, self.box_mode_3d = get_box_type(box_type_3d)\n\n        self.CLASSES = self.get_classes(classes)\n        self.file_client = mmcv.FileClient(**file_client_args)\n        self.cat2id = {name: i for i, name in enumerate(self.CLASSES)}\n\n        # load annotations\n        if hasattr(self.file_client, 'get_local_path'):\n            with self.file_client.get_local_path(self.ann_file) as local_path:\n                self.data_infos = self.load_annotations(open(local_path, 'rb'))\n        else:\n            warnings.warn(\n                'The used MMCV version does not have get_local_path. '\n                f'We treat the {self.ann_file} as local paths and it '\n                'might cause errors if the path is not a local path. '\n                'Please use MMCV>= 1.3.16 if you meet errors.')\n            self.data_infos = self.load_annotations(self.ann_file)\n\n        # process pipeline\n        if pipeline is not None:\n            self.pipeline = Compose(pipeline)\n\n        # set group flag for the samplers\n        if not self.test_mode:\n            self._set_group_flag()\n\n    def load_annotations(self, ann_file):\n        \"\"\"Load annotations from ann_file.\n\n        Args:\n            ann_file (str): Path of the annotation file.\n\n        Returns:\n            list[dict]: List of annotations.\n        \"\"\"\n        # loading data from a file-like object needs file format\n        return mmcv.load(ann_file, file_format='pkl')\n\n    def get_data_info(self, index):\n        \"\"\"Get data info according to the given index.\n\n        Args:\n            index (int): Index of the sample data to get.\n\n        Returns:\n            dict: Data information that will be passed to the data\n                preprocessing pipelines. It includes the following keys:\n\n                - sample_idx (str): Sample index.\n                - pts_filename (str): Filename of point clouds.\n                - file_name (str): Filename of point clouds.\n                - ann_info (dict): Annotation info.\n        \"\"\"\n        info = self.data_infos[index]\n        sample_idx = info['sample_idx']\n        pts_filename = osp.join(self.data_root,\n                                info['lidar_points']['lidar_path'])\n\n        input_dict = dict(\n            pts_filename=pts_filename,\n            sample_idx=sample_idx,\n            file_name=pts_filename)\n\n        if not self.test_mode:\n            annos = self.get_ann_info(index)\n            input_dict['ann_info'] = annos\n            if self.filter_empty_gt and ~(annos['gt_labels_3d'] != -1).any():\n                return None\n        return input_dict\n\n    def get_ann_info(self, index):\n        \"\"\"Get annotation info according to the given index.\n\n        Args:\n            index (int): Index of the annotation data to get.\n\n        Returns:\n            dict: Annotation information consists of the following keys:\n\n                - gt_bboxes_3d (:obj:`LiDARInstance3DBoxes`):\n                    3D ground truth bboxes\n                - gt_labels_3d (np.ndarray): Labels of ground truths.\n                - gt_names (list[str]): Class names of ground truths.\n        \"\"\"\n        info = self.data_infos[index]\n        gt_bboxes_3d = info['annos']['gt_bboxes_3d']\n        gt_names_3d = info['annos']['gt_names']\n        gt_labels_3d = []\n        for cat in gt_names_3d:\n            if cat in self.CLASSES:\n                gt_labels_3d.append(self.CLASSES.index(cat))\n            else:\n                gt_labels_3d.append(-1)\n        gt_labels_3d = np.array(gt_labels_3d)\n\n        # Obtain original box 3d type in info file\n        ori_box_type_3d = info['annos']['box_type_3d']\n        ori_box_type_3d, _ = get_box_type(ori_box_type_3d)\n\n        # turn original box type to target box type\n        gt_bboxes_3d = ori_box_type_3d(\n            gt_bboxes_3d,\n            box_dim=gt_bboxes_3d.shape[-1],\n            origin=(0.5, 0.5, 0.5)).convert_to(self.box_mode_3d)\n\n        anns_results = dict(\n            gt_bboxes_3d=gt_bboxes_3d,\n            gt_labels_3d=gt_labels_3d,\n            gt_names=gt_names_3d)\n        return anns_results\n\n    def pre_pipeline(self, results):\n        \"\"\"Initialization before data preparation.\n\n        Args:\n            results (dict): Dict before data preprocessing.\n\n                - img_fields (list): Image fields.\n                - bbox3d_fields (list): 3D bounding boxes fields.\n                - pts_mask_fields (list): Mask fields of points.\n                - pts_seg_fields (list): Mask fields of point segments.\n                - bbox_fields (list): Fields of bounding boxes.\n                - mask_fields (list): Fields of masks.\n                - seg_fields (list): Segment fields.\n                - box_type_3d (str): 3D box type.\n                - box_mode_3d (str): 3D box mode.\n        \"\"\"\n        results['img_fields'] = []\n        results['bbox3d_fields'] = []\n        results['pts_mask_fields'] = []\n        results['pts_seg_fields'] = []\n        results['bbox_fields'] = []\n        results['mask_fields'] = []\n        results['seg_fields'] = []\n        results['box_type_3d'] = self.box_type_3d\n        results['box_mode_3d'] = self.box_mode_3d\n\n    def prepare_train_data(self, index):\n        \"\"\"Training data preparation.\n\n        Args:\n            index (int): Index for accessing the target data.\n\n        Returns:\n            dict: Training data dict of the corresponding index.\n        \"\"\"\n        input_dict = self.get_data_info(index)\n        if input_dict is None:\n            return None\n        self.pre_pipeline(input_dict)\n        example = self.pipeline(input_dict)\n        if self.filter_empty_gt and \\\n                (example is None or\n                    ~(example['gt_labels_3d']._data != -1).any()):\n            return None\n        return example\n\n    def prepare_test_data(self, index):\n        \"\"\"Prepare data for testing.\n\n        Args:\n            index (int): Index for accessing the target data.\n\n        Returns:\n            dict: Testing data dict of the corresponding index.\n        \"\"\"\n        input_dict = self.get_data_info(index)\n        self.pre_pipeline(input_dict)\n        example = self.pipeline(input_dict)\n        return example\n\n    @classmethod\n    def get_classes(cls, classes=None):\n        \"\"\"Get class names of current dataset.\n\n        Args:\n            classes (Sequence[str] | str): If classes is None, use\n                default CLASSES defined by builtin dataset. If classes is a\n                string, take it as a file name. The file contains the name of\n                classes where each line contains one class name. If classes is\n                a tuple or list, override the CLASSES defined by the dataset.\n\n        Return:\n            list[str]: A list of class names.\n        \"\"\"\n        if classes is None:\n            return cls.CLASSES\n\n        if isinstance(classes, str):\n            # take it as a file path\n            class_names = mmcv.list_from_file(classes)\n        elif isinstance(classes, (tuple, list)):\n            class_names = classes\n        else:\n            raise ValueError(f'Unsupported type {type(classes)} of classes.')\n\n        return class_names\n\n    def format_results(self,\n                       outputs,\n                       pklfile_prefix=None,\n                       submission_prefix=None):\n        \"\"\"Format the results to pkl file.\n\n        Args:\n            outputs (list[dict]): Testing results of the dataset.\n            pklfile_prefix (str): The prefix of pkl files. It includes\n                the file path and the prefix of filename, e.g., \"a/b/prefix\".\n                If not specified, a temp file will be created. Default: None.\n\n        Returns:\n            tuple: (outputs, tmp_dir), outputs is the detection results,\n                tmp_dir is the temporal directory created for saving json\n                files when ``jsonfile_prefix`` is not specified.\n        \"\"\"\n        if pklfile_prefix is None:\n            tmp_dir = tempfile.TemporaryDirectory()\n            pklfile_prefix = osp.join(tmp_dir.name, 'results')\n            out = f'{pklfile_prefix}.pkl'\n        mmcv.dump(outputs, out)\n        return outputs, tmp_dir\n\n    def evaluate(self,\n                 results,\n                 metric=None,\n                 iou_thr=(0.25, 0.5),\n                 logger=None,\n                 show=False,\n                 out_dir=None,\n                 pipeline=None):\n        \"\"\"Evaluate.\n\n        Evaluation in indoor protocol.\n\n        Args:\n            results (list[dict]): List of results.\n            metric (str | list[str], optional): Metrics to be evaluated.\n                Defaults to None.\n            iou_thr (list[float]): AP IoU thresholds. Defaults to (0.25, 0.5).\n            logger (logging.Logger | str, optional): Logger used for printing\n                related information during evaluation. Defaults to None.\n            show (bool, optional): Whether to visualize.\n                Default: False.\n            out_dir (str, optional): Path to save the visualization results.\n                Default: None.\n            pipeline (list[dict], optional): raw data loading for showing.\n                Default: None.\n\n        Returns:\n            dict: Evaluation results.\n        \"\"\"\n        from mmdet3d.core.evaluation import indoor_eval\n        assert isinstance(\n            results, list), f'Expect results to be list, got {type(results)}.'\n        assert len(results) > 0, 'Expect length of results > 0.'\n        assert len(results) == len(self.data_infos)\n        assert isinstance(\n            results[0], dict\n        ), f'Expect elements in results to be dict, got {type(results[0])}.'\n        gt_annos = [info['annos'] for info in self.data_infos]\n        label2cat = {i: cat_id for i, cat_id in enumerate(self.CLASSES)}\n        ret_dict = indoor_eval(\n            gt_annos,\n            results,\n            iou_thr,\n            label2cat,\n            logger=logger,\n            box_type_3d=self.box_type_3d,\n            box_mode_3d=self.box_mode_3d)\n        if show:\n            self.show(results, out_dir, pipeline=pipeline)\n\n        return ret_dict\n\n    def _build_default_pipeline(self):\n        \"\"\"Build the default pipeline for this dataset.\"\"\"\n        raise NotImplementedError('_build_default_pipeline is not implemented '\n                                  f'for dataset {self.__class__.__name__}')\n\n    def _get_pipeline(self, pipeline):\n        \"\"\"Get data loading pipeline in self.show/evaluate function.\n\n        Args:\n            pipeline (list[dict]): Input pipeline. If None is given,\n                get from self.pipeline.\n        \"\"\"\n        if pipeline is None:\n            if not hasattr(self, 'pipeline') or self.pipeline is None:\n                warnings.warn(\n                    'Use default pipeline for data loading, this may cause '\n                    'errors when data is on ceph')\n                return self._build_default_pipeline()\n            loading_pipeline = get_loading_pipeline(self.pipeline.transforms)\n            return Compose(loading_pipeline)\n        return Compose(pipeline)\n\n    def _extract_data(self, index, pipeline, key, load_annos=False):\n        \"\"\"Load data using input pipeline and extract data according to key.\n\n        Args:\n            index (int): Index for accessing the target data.\n            pipeline (:obj:`Compose`): Composed data loading pipeline.\n            key (str | list[str]): One single or a list of data key.\n            load_annos (bool): Whether to load data annotations.\n                If True, need to set self.test_mode as False before loading.\n\n        Returns:\n            np.ndarray | torch.Tensor | list[np.ndarray | torch.Tensor]:\n                A single or a list of loaded data.\n        \"\"\"\n        assert pipeline is not None, 'data loading pipeline is not provided'\n        # when we want to load ground-truth via pipeline (e.g. bbox, seg mask)\n        # we need to set self.test_mode as False so that we have 'annos'\n        if load_annos:\n            original_test_mode = self.test_mode\n            self.test_mode = False\n        input_dict = self.get_data_info(index)\n        self.pre_pipeline(input_dict)\n        example = pipeline(input_dict)\n\n        # extract data items according to keys\n        if isinstance(key, str):\n            data = extract_result_dict(example, key)\n        else:\n            data = [extract_result_dict(example, k) for k in key]\n        if load_annos:\n            self.test_mode = original_test_mode\n\n        return data\n\n    def __len__(self):\n        \"\"\"Return the length of data infos.\n\n        Returns:\n            int: Length of data infos.\n        \"\"\"\n        return len(self.data_infos)\n\n    def _rand_another(self, idx):\n        \"\"\"Randomly get another item with the same flag.\n\n        Returns:\n            int: Another index of item with the same flag.\n        \"\"\"\n        pool = np.where(self.flag == self.flag[idx])[0]\n        return np.random.choice(pool)\n\n    def __getitem__(self, idx):\n        \"\"\"Get item from infos according to the given index.\n\n        Returns:\n            dict: Data dictionary of the corresponding index.\n        \"\"\"\n        if self.test_mode:\n            return self.prepare_test_data(idx)\n        while True:\n            data = self.prepare_train_data(idx)\n            if data is None:\n                idx = self._rand_another(idx)\n                continue\n            return data\n\n    def _set_group_flag(self):\n        \"\"\"Set flag according to image aspect ratio.\n\n        Images with aspect ratio greater than 1 will be set as group 1,\n        otherwise group 0. In 3D datasets, they are all the same, thus are all\n        zeros.\n        \"\"\"\n        self.flag = np.zeros(len(self), dtype=np.uint8)\n"
        }
    ]
}