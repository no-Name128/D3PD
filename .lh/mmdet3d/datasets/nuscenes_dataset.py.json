{
    "sourceFile": "mmdet3d/datasets/nuscenes_dataset.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 4,
            "patches": [
                {
                    "date": 1716014660045,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1716014680135,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -236,8 +236,13 @@\n             pts_filename=info['lidar_path'],\n             sweeps=info['sweeps'],\n             timestamp=info['timestamp'] / 1e6,\n         )\n+        \n+                \n+        if self.modality[\"use_radar\"]:\n+            input_dict[\"radar\"] = info[\"radars\"]\n+        \n         if 'ann_infos' in info:\n             input_dict['ann_infos'] = info['ann_infos']\n         if self.modality['use_camera']:\n             if self.img_info_prototype == 'mmcv':\n"
                },
                {
                    "date": 1716014690154,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -62,85 +62,97 @@\n         ego_cam (str): Specify the ego coordinate relative to a specified\n             camera by its name defined in NuScenes.\n             Defaults to None, which use the mean of all cameras.\n     \"\"\"\n+\n     NameMapping = {\n-        'movable_object.barrier': 'barrier',\n-        'vehicle.bicycle': 'bicycle',\n-        'vehicle.bus.bendy': 'bus',\n-        'vehicle.bus.rigid': 'bus',\n-        'vehicle.car': 'car',\n-        'vehicle.construction': 'construction_vehicle',\n-        'vehicle.motorcycle': 'motorcycle',\n-        'human.pedestrian.adult': 'pedestrian',\n-        'human.pedestrian.child': 'pedestrian',\n-        'human.pedestrian.construction_worker': 'pedestrian',\n-        'human.pedestrian.police_officer': 'pedestrian',\n-        'movable_object.trafficcone': 'traffic_cone',\n-        'vehicle.trailer': 'trailer',\n-        'vehicle.truck': 'truck'\n+        \"movable_object.barrier\": \"barrier\",\n+        \"vehicle.bicycle\": \"bicycle\",\n+        \"vehicle.bus.bendy\": \"bus\",\n+        \"vehicle.bus.rigid\": \"bus\",\n+        \"vehicle.car\": \"car\",\n+        \"vehicle.construction\": \"construction_vehicle\",\n+        \"vehicle.motorcycle\": \"motorcycle\",\n+        \"human.pedestrian.adult\": \"pedestrian\",\n+        \"human.pedestrian.child\": \"pedestrian\",\n+        \"human.pedestrian.construction_worker\": \"pedestrian\",\n+        \"human.pedestrian.police_officer\": \"pedestrian\",\n+        \"movable_object.trafficcone\": \"traffic_cone\",\n+        \"vehicle.trailer\": \"trailer\",\n+        \"vehicle.truck\": \"truck\",\n     }\n     DefaultAttribute = {\n-        'car': 'vehicle.parked',\n-        'pedestrian': 'pedestrian.moving',\n-        'trailer': 'vehicle.parked',\n-        'truck': 'vehicle.parked',\n-        'bus': 'vehicle.moving',\n-        'motorcycle': 'cycle.without_rider',\n-        'construction_vehicle': 'vehicle.parked',\n-        'bicycle': 'cycle.without_rider',\n-        'barrier': '',\n-        'traffic_cone': '',\n+        \"car\": \"vehicle.parked\",\n+        \"pedestrian\": \"pedestrian.moving\",\n+        \"trailer\": \"vehicle.parked\",\n+        \"truck\": \"vehicle.parked\",\n+        \"bus\": \"vehicle.moving\",\n+        \"motorcycle\": \"cycle.without_rider\",\n+        \"construction_vehicle\": \"vehicle.parked\",\n+        \"bicycle\": \"cycle.without_rider\",\n+        \"barrier\": \"\",\n+        \"traffic_cone\": \"\",\n     }\n     AttrMapping = {\n-        'cycle.with_rider': 0,\n-        'cycle.without_rider': 1,\n-        'pedestrian.moving': 2,\n-        'pedestrian.standing': 3,\n-        'pedestrian.sitting_lying_down': 4,\n-        'vehicle.moving': 5,\n-        'vehicle.parked': 6,\n-        'vehicle.stopped': 7,\n+        \"cycle.with_rider\": 0,\n+        \"cycle.without_rider\": 1,\n+        \"pedestrian.moving\": 2,\n+        \"pedestrian.standing\": 3,\n+        \"pedestrian.sitting_lying_down\": 4,\n+        \"vehicle.moving\": 5,\n+        \"vehicle.parked\": 6,\n+        \"vehicle.stopped\": 7,\n     }\n     AttrMapping_rev = [\n-        'cycle.with_rider',\n-        'cycle.without_rider',\n-        'pedestrian.moving',\n-        'pedestrian.standing',\n-        'pedestrian.sitting_lying_down',\n-        'vehicle.moving',\n-        'vehicle.parked',\n-        'vehicle.stopped',\n+        \"cycle.with_rider\",\n+        \"cycle.without_rider\",\n+        \"pedestrian.moving\",\n+        \"pedestrian.standing\",\n+        \"pedestrian.sitting_lying_down\",\n+        \"vehicle.moving\",\n+        \"vehicle.parked\",\n+        \"vehicle.stopped\",\n     ]\n     # https://github.com/nutonomy/nuscenes-devkit/blob/57889ff20678577025326cfc24e57424a829be0a/python-sdk/nuscenes/eval/detection/evaluate.py#L222 # noqa\n     ErrNameMapping = {\n-        'trans_err': 'mATE',\n-        'scale_err': 'mASE',\n-        'orient_err': 'mAOE',\n-        'vel_err': 'mAVE',\n-        'attr_err': 'mAAE'\n+        \"trans_err\": \"mATE\",\n+        \"scale_err\": \"mASE\",\n+        \"orient_err\": \"mAOE\",\n+        \"vel_err\": \"mAVE\",\n+        \"attr_err\": \"mAAE\",\n     }\n-    CLASSES = ('car', 'truck', 'trailer', 'bus', 'construction_vehicle',\n-               'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',\n-               'barrier')\n+    CLASSES = (\n+        \"car\",\n+        \"truck\",\n+        \"trailer\",\n+        \"bus\",\n+        \"construction_vehicle\",\n+        \"bicycle\",\n+        \"motorcycle\",\n+        \"pedestrian\",\n+        \"traffic_cone\",\n+        \"barrier\",\n+    )\n \n-    def __init__(self,\n-                 ann_file,\n-                 pipeline=None,\n-                 data_root=None,\n-                 classes=None,\n-                 load_interval=1,\n-                 with_velocity=True,\n-                 modality=None,\n-                 box_type_3d='LiDAR',\n-                 filter_empty_gt=True,\n-                 test_mode=False,\n-                 eval_version='detection_cvpr_2019',\n-                 use_valid_flag=False,\n-                 img_info_prototype='mmcv',\n-                 multi_adj_frame_id_cfg=None,\n-                 ego_cam='CAM_FRONT',\n-                 stereo=False):\n+    def __init__(\n+        self,\n+        ann_file,\n+        pipeline=None,\n+        data_root=None,\n+        classes=None,\n+        load_interval=1,\n+        with_velocity=True,\n+        modality=None,\n+        box_type_3d=\"LiDAR\",\n+        filter_empty_gt=True,\n+        test_mode=False,\n+        eval_version=\"detection_cvpr_2019\",\n+        use_valid_flag=False,\n+        img_info_prototype=\"mmcv\",\n+        multi_adj_frame_id_cfg=None,\n+        ego_cam=\"CAM_FRONT\",\n+        stereo=False,\n+    ):\n         self.load_interval = load_interval\n         self.use_valid_flag = use_valid_flag\n         super().__init__(\n             data_root=data_root,\n@@ -149,13 +161,15 @@\n             classes=classes,\n             modality=modality,\n             box_type_3d=box_type_3d,\n             filter_empty_gt=filter_empty_gt,\n-            test_mode=test_mode)\n+            test_mode=test_mode,\n+        )\n \n         self.with_velocity = with_velocity\n         self.eval_version = eval_version\n         from nuscenes.eval.detection.config import config_factory\n+\n         self.eval_detection_configs = config_factory(self.eval_version)\n         if self.modality is None:\n             self.modality = dict(\n                 use_camera=False,\n@@ -182,12 +196,12 @@\n                 otherwise, store empty list.\n         \"\"\"\n         info = self.data_infos[idx]\n         if self.use_valid_flag:\n-            mask = info['valid_flag']\n-            gt_names = set(info['gt_names'][mask])\n+            mask = info[\"valid_flag\"]\n+            gt_names = set(info[\"gt_names\"][mask])\n         else:\n-            gt_names = set(info['gt_names'])\n+            gt_names = set(info[\"gt_names\"])\n \n         cat_ids = []\n         for name in gt_names:\n             if name in self.CLASSES:\n@@ -202,13 +216,13 @@\n \n         Returns:\n             list[dict]: List of annotations sorted by timestamps.\n         \"\"\"\n-        data = mmcv.load(ann_file, file_format='pkl')\n-        data_infos = list(sorted(data['infos'], key=lambda e: e['timestamp']))\n-        data_infos = data_infos[::self.load_interval]\n-        self.metadata = data['metadata']\n-        self.version = self.metadata['version']\n+        data = mmcv.load(ann_file, file_format=\"pkl\")\n+        data_infos = list(sorted(data[\"infos\"], key=lambda e: e[\"timestamp\"]))\n+        data_infos = data_infos[:: self.load_interval]\n+        self.metadata = data[\"metadata\"]\n+        self.version = self.metadata[\"version\"]\n         return data_infos\n \n     def get_data_info(self, index):\n         \"\"\"Get data info according to the given index.\n@@ -231,54 +245,51 @@\n         \"\"\"\n         info = self.data_infos[index]\n         # standard protocol modified from SECOND.Pytorch\n         input_dict = dict(\n-            sample_idx=info['token'],\n-            pts_filename=info['lidar_path'],\n-            sweeps=info['sweeps'],\n-            timestamp=info['timestamp'] / 1e6,\n+            sample_idx=info[\"token\"],\n+            pts_filename=info[\"lidar_path\"],\n+            sweeps=info[\"sweeps\"],\n+            timestamp=info[\"timestamp\"] / 1e6,\n         )\n         \n-                \n         if self.modality[\"use_radar\"]:\n             input_dict[\"radar\"] = info[\"radars\"]\n-        \n-        if 'ann_infos' in info:\n-            input_dict['ann_infos'] = info['ann_infos']\n-        if self.modality['use_camera']:\n-            if self.img_info_prototype == 'mmcv':\n+\n+        if \"ann_infos\" in info:\n+            input_dict[\"ann_infos\"] = info[\"ann_infos\"]\n+        if self.modality[\"use_camera\"]:\n+            if self.img_info_prototype == \"mmcv\":\n                 image_paths = []\n                 lidar2img_rts = []\n-                for cam_type, cam_info in info['cams'].items():\n-                    image_paths.append(cam_info['data_path'])\n+                for cam_type, cam_info in info[\"cams\"].items():\n+                    image_paths.append(cam_info[\"data_path\"])\n                     # obtain lidar to image transformation matrix\n-                    lidar2cam_r = np.linalg.inv(\n-                        cam_info['sensor2lidar_rotation'])\n-                    lidar2cam_t = cam_info[\n-                        'sensor2lidar_translation'] @ lidar2cam_r.T\n+                    lidar2cam_r = np.linalg.inv(cam_info[\"sensor2lidar_rotation\"])\n+                    lidar2cam_t = cam_info[\"sensor2lidar_translation\"] @ lidar2cam_r.T\n                     lidar2cam_rt = np.eye(4)\n                     lidar2cam_rt[:3, :3] = lidar2cam_r.T\n                     lidar2cam_rt[3, :3] = -lidar2cam_t\n-                    intrinsic = cam_info['cam_intrinsic']\n+                    intrinsic = cam_info[\"cam_intrinsic\"]\n                     viewpad = np.eye(4)\n-                    viewpad[:intrinsic.shape[0], :intrinsic.\n-                            shape[1]] = intrinsic\n-                    lidar2img_rt = (viewpad @ lidar2cam_rt.T)\n+                    viewpad[: intrinsic.shape[0], : intrinsic.shape[1]] = intrinsic\n+                    lidar2img_rt = viewpad @ lidar2cam_rt.T\n                     lidar2img_rts.append(lidar2img_rt)\n \n                 input_dict.update(\n                     dict(\n                         img_filename=image_paths,\n                         lidar2img=lidar2img_rts,\n-                    ))\n+                    )\n+                )\n \n                 if not self.test_mode:\n                     annos = self.get_ann_info(index)\n-                    input_dict['ann_info'] = annos\n+                    input_dict[\"ann_info\"] = annos\n             else:\n-                assert 'bevdet' in self.img_info_prototype\n+                assert \"bevdet\" in self.img_info_prototype\n                 input_dict.update(dict(curr=info))\n-                if '4d' in self.img_info_prototype:\n+                if \"4d\" in self.img_info_prototype:\n                     info_adj_list = self.get_adj_info(info, index)\n                     input_dict.update(dict(adjacent=info_adj_list))\n         return input_dict\n \n@@ -290,10 +301,9 @@\n             assert self.multi_adj_frame_id_cfg[2] == 1\n             adj_id_list.append(self.multi_adj_frame_id_cfg[1])\n         for select_id in adj_id_list:\n             select_id = max(index - select_id, 0)\n-            if not self.data_infos[select_id]['scene_token'] == info[\n-                    'scene_token']:\n+            if not self.data_infos[select_id][\"scene_token\"] == info[\"scene_token\"]:\n                 info_adj_list.append(info)\n             else:\n                 info_adj_list.append(self.data_infos[select_id])\n         return info_adj_list\n@@ -314,13 +324,13 @@\n         \"\"\"\n         info = self.data_infos[index]\n         # filter out bbox containing no points\n         if self.use_valid_flag:\n-            mask = info['valid_flag']\n+            mask = info[\"valid_flag\"]\n         else:\n-            mask = info['num_lidar_pts'] > 0\n-        gt_bboxes_3d = info['gt_boxes'][mask]\n-        gt_names_3d = info['gt_names'][mask]\n+            mask = info[\"num_lidar_pts\"] > 0\n+        gt_bboxes_3d = info[\"gt_boxes\"][mask]\n+        gt_names_3d = info[\"gt_names\"][mask]\n         gt_labels_3d = []\n         for cat in gt_names_3d:\n             if cat in self.CLASSES:\n                 gt_labels_3d.append(self.CLASSES.index(cat))\n@@ -328,24 +338,22 @@\n                 gt_labels_3d.append(-1)\n         gt_labels_3d = np.array(gt_labels_3d)\n \n         if self.with_velocity:\n-            gt_velocity = info['gt_velocity'][mask]\n+            gt_velocity = info[\"gt_velocity\"][mask]\n             nan_mask = np.isnan(gt_velocity[:, 0])\n             gt_velocity[nan_mask] = [0.0, 0.0]\n             gt_bboxes_3d = np.concatenate([gt_bboxes_3d, gt_velocity], axis=-1)\n \n         # the nuscenes box center is [0.5, 0.5, 0.5], we change it to be\n         # the same as KITTI (0.5, 0.5, 0)\n         gt_bboxes_3d = LiDARInstance3DBoxes(\n-            gt_bboxes_3d,\n-            box_dim=gt_bboxes_3d.shape[-1],\n-            origin=(0.5, 0.5, 0.5)).convert_to(self.box_mode_3d)\n+            gt_bboxes_3d, box_dim=gt_bboxes_3d.shape[-1], origin=(0.5, 0.5, 0.5)\n+        ).convert_to(self.box_mode_3d)\n \n         anns_results = dict(\n-            gt_bboxes_3d=gt_bboxes_3d,\n-            gt_labels_3d=gt_labels_3d,\n-            gt_names=gt_names_3d)\n+            gt_bboxes_3d=gt_bboxes_3d, gt_labels_3d=gt_labels_3d, gt_names=gt_names_3d\n+        )\n         return anns_results\n \n     def _format_bbox(self, results, jsonfile_prefix=None):\n         \"\"\"Convert the results to the standard format.\n@@ -361,19 +369,21 @@\n         \"\"\"\n         nusc_annos = {}\n         mapped_class_names = self.CLASSES\n \n-        print('Start to convert detection format...')\n+        print(\"Start to convert detection format...\")\n         for sample_id, det in enumerate(mmcv.track_iter_progress(results)):\n-            boxes = det['boxes_3d'].tensor.numpy()\n-            scores = det['scores_3d'].numpy()\n-            labels = det['labels_3d'].numpy()\n-            sample_token = self.data_infos[sample_id]['token']\n+            boxes = det[\"boxes_3d\"].tensor.numpy()\n+            scores = det[\"scores_3d\"].numpy()\n+            labels = det[\"labels_3d\"].numpy()\n+            sample_token = self.data_infos[sample_id][\"token\"]\n \n-            trans = self.data_infos[sample_id]['cams'][\n-                self.ego_cam]['ego2global_translation']\n-            rot = self.data_infos[sample_id]['cams'][\n-                self.ego_cam]['ego2global_rotation']\n+            trans = self.data_infos[sample_id][\"cams\"][self.ego_cam][\n+                \"ego2global_translation\"\n+            ]\n+            rot = self.data_infos[sample_id][\"cams\"][self.ego_cam][\n+                \"ego2global_rotation\"\n+            ]\n             rot = pyquaternion.Quaternion(rot)\n             annos = list()\n             for i, box in enumerate(boxes):\n                 name = mapped_class_names[labels[i]]\n@@ -385,27 +395,26 @@\n                 quat = pyquaternion.Quaternion(axis=[0, 0, 1], radians=box_yaw)\n                 nusc_box = NuScenesBox(center, wlh, quat, velocity=box_vel)\n                 nusc_box.rotate(rot)\n                 nusc_box.translate(trans)\n-                if np.sqrt(nusc_box.velocity[0]**2 +\n-                           nusc_box.velocity[1]**2) > 0.2:\n+                if np.sqrt(nusc_box.velocity[0] ** 2 + nusc_box.velocity[1] ** 2) > 0.2:\n                     if name in [\n-                            'car',\n-                            'construction_vehicle',\n-                            'bus',\n-                            'truck',\n-                            'trailer',\n+                        \"car\",\n+                        \"construction_vehicle\",\n+                        \"bus\",\n+                        \"truck\",\n+                        \"trailer\",\n                     ]:\n-                        attr = 'vehicle.moving'\n-                    elif name in ['bicycle', 'motorcycle']:\n-                        attr = 'cycle.with_rider'\n+                        attr = \"vehicle.moving\"\n+                    elif name in [\"bicycle\", \"motorcycle\"]:\n+                        attr = \"cycle.with_rider\"\n                     else:\n                         attr = self.DefaultAttribute[name]\n                 else:\n-                    if name in ['pedestrian']:\n-                        attr = 'pedestrian.standing'\n-                    elif name in ['bus']:\n-                        attr = 'vehicle.stopped'\n+                    if name in [\"pedestrian\"]:\n+                        attr = \"pedestrian.standing\"\n+                    elif name in [\"bus\"]:\n+                        attr = \"vehicle.stopped\"\n                     else:\n                         attr = self.DefaultAttribute[name]\n                 nusc_anno = dict(\n                     sample_token=sample_token,\n@@ -423,23 +432,21 @@\n                 nusc_annos[sample_token].extend(annos)\n             else:\n                 nusc_annos[sample_token] = annos\n         nusc_submissions = {\n-            'meta': self.modality,\n-            'results': nusc_annos,\n+            \"meta\": self.modality,\n+            \"results\": nusc_annos,\n         }\n \n         mmcv.mkdir_or_exist(jsonfile_prefix)\n-        res_path = osp.join(jsonfile_prefix, 'results_nusc.json')\n-        print('Results writes to', res_path)\n+        res_path = osp.join(jsonfile_prefix, \"results_nusc.json\")\n+        print(\"Results writes to\", res_path)\n         mmcv.dump(nusc_submissions, res_path)\n         return res_path\n \n-    def _evaluate_single(self,\n-                         result_path,\n-                         logger=None,\n-                         metric='bbox',\n-                         result_name='pts_bbox'):\n+    def _evaluate_single(\n+        self, result_path, logger=None, metric=\"bbox\", result_name=\"pts_bbox\"\n+    ):\n         \"\"\"Evaluation for a single model in nuScenes protocol.\n \n         Args:\n             result_path (str): Path of the result file.\n@@ -456,41 +463,40 @@\n         from nuscenes import NuScenes\n         from nuscenes.eval.detection.evaluate import NuScenesEval\n \n         output_dir = osp.join(*osp.split(result_path)[:-1])\n-        nusc = NuScenes(\n-            version=self.version, dataroot=self.data_root, verbose=False)\n+        nusc = NuScenes(version=self.version, dataroot=self.data_root, verbose=False)\n         eval_set_map = {\n-            'v1.0-mini': 'mini_val',\n-            'v1.0-trainval': 'val',\n+            \"v1.0-mini\": \"mini_val\",\n+            \"v1.0-trainval\": \"val\",\n         }\n         nusc_eval = NuScenesEval(\n             nusc,\n             config=self.eval_detection_configs,\n             result_path=result_path,\n             eval_set=eval_set_map[self.version],\n             output_dir=output_dir,\n-            verbose=False)\n+            verbose=False,\n+        )\n         nusc_eval.main(render_curves=False)\n \n         # record metrics\n-        metrics = mmcv.load(osp.join(output_dir, 'metrics_summary.json'))\n+        metrics = mmcv.load(osp.join(output_dir, \"metrics_summary.json\"))\n         detail = dict()\n-        metric_prefix = f'{result_name}_NuScenes'\n+        metric_prefix = f\"{result_name}_NuScenes\"\n         for name in self.CLASSES:\n-            for k, v in metrics['label_aps'][name].items():\n-                val = float('{:.4f}'.format(v))\n-                detail['{}/{}_AP_dist_{}'.format(metric_prefix, name, k)] = val\n-            for k, v in metrics['label_tp_errors'][name].items():\n-                val = float('{:.4f}'.format(v))\n-                detail['{}/{}_{}'.format(metric_prefix, name, k)] = val\n-            for k, v in metrics['tp_errors'].items():\n-                val = float('{:.4f}'.format(v))\n-                detail['{}/{}'.format(metric_prefix,\n-                                      self.ErrNameMapping[k])] = val\n+            for k, v in metrics[\"label_aps\"][name].items():\n+                val = float(\"{:.4f}\".format(v))\n+                detail[\"{}/{}_AP_dist_{}\".format(metric_prefix, name, k)] = val\n+            for k, v in metrics[\"label_tp_errors\"][name].items():\n+                val = float(\"{:.4f}\".format(v))\n+                detail[\"{}/{}_{}\".format(metric_prefix, name, k)] = val\n+            for k, v in metrics[\"tp_errors\"].items():\n+                val = float(\"{:.4f}\".format(v))\n+                detail[\"{}/{}\".format(metric_prefix, self.ErrNameMapping[k])] = val\n \n-        detail['{}/NDS'.format(metric_prefix)] = metrics['nd_score']\n-        detail['{}/mAP'.format(metric_prefix)] = metrics['mean_ap']\n+        detail[\"{}/NDS\".format(metric_prefix)] = metrics[\"nd_score\"]\n+        detail[\"{}/mAP\".format(metric_prefix)] = metrics[\"mean_ap\"]\n         return detail\n \n     def format_results(self, results, jsonfile_prefix=None):\n         \"\"\"Format the results to json (standard format for COCO evaluation).\n@@ -506,16 +512,18 @@\n                 dict containing the json filepaths, `tmp_dir` is the temporal\n                 directory created for saving json files when\n                 `jsonfile_prefix` is not specified.\n         \"\"\"\n-        assert isinstance(results, list), 'results must be a list'\n-        assert len(results) == len(self), (\n-            'The length of results is not equal to the dataset len: {} != {}'.\n-            format(len(results), len(self)))\n+        assert isinstance(results, list), \"results must be a list\"\n+        assert len(results) == len(\n+            self\n+        ), \"The length of results is not equal to the dataset len: {} != {}\".format(\n+            len(results), len(self)\n+        )\n \n         if jsonfile_prefix is None:\n             tmp_dir = tempfile.TemporaryDirectory()\n-            jsonfile_prefix = osp.join(tmp_dir.name, 'results')\n+            jsonfile_prefix = osp.join(tmp_dir.name, \"results\")\n         else:\n             tmp_dir = None\n \n         # currently the output prediction results could be in two formats\n@@ -523,30 +531,31 @@\n         # 2. list of dict('pts_bbox' or 'img_bbox':\n         #     dict('boxes_3d': ..., 'scores_3d': ..., 'labels_3d': ...))\n         # this is a workaround to enable evaluation of both formats on nuScenes\n         # refer to https://github.com/open-mmlab/mmdetection3d/issues/449\n-        if not ('pts_bbox' in results[0] or 'img_bbox' in results[0]):\n+        if not (\"pts_bbox\" in results[0] or \"img_bbox\" in results[0]):\n             result_files = self._format_bbox(results, jsonfile_prefix)\n         else:\n             # should take the inner dict out of 'pts_bbox' or 'img_bbox' dict\n             result_files = dict()\n             for name in results[0]:\n-                print(f'\\nFormating bboxes of {name}')\n+                print(f\"\\nFormating bboxes of {name}\")\n                 results_ = [out[name] for out in results]\n                 tmp_file_ = osp.join(jsonfile_prefix, name)\n-                result_files.update(\n-                    {name: self._format_bbox(results_, tmp_file_)})\n+                result_files.update({name: self._format_bbox(results_, tmp_file_)})\n         return result_files, tmp_dir\n \n-    def evaluate(self,\n-                 results,\n-                 metric='bbox',\n-                 logger=None,\n-                 jsonfile_prefix=None,\n-                 result_names=['pts_bbox'],\n-                 show=False,\n-                 out_dir=None,\n-                 pipeline=None):\n+    def evaluate(\n+        self,\n+        results,\n+        metric=\"bbox\",\n+        logger=None,\n+        jsonfile_prefix=None,\n+        result_names=[\"pts_bbox\"],\n+        show=False,\n+        out_dir=None,\n+        pipeline=None,\n+    ):\n         \"\"\"Evaluation in nuScenes protocol.\n \n         Args:\n             results (list[dict]): Testing results of the dataset.\n@@ -571,9 +580,9 @@\n \n         if isinstance(result_files, dict):\n             results_dict = dict()\n             for name in result_names:\n-                print('Evaluating bboxes of {}'.format(name))\n+                print(\"Evaluating bboxes of {}\".format(name))\n                 ret_dict = self._evaluate_single(result_files[name])\n             results_dict.update(ret_dict)\n         elif isinstance(result_files, str):\n             results_dict = self._evaluate_single(result_files)\n@@ -588,22 +597,23 @@\n     def _build_default_pipeline(self):\n         \"\"\"Build the default pipeline for this dataset.\"\"\"\n         pipeline = [\n             dict(\n-                type='LoadPointsFromFile',\n-                coord_type='LIDAR',\n+                type=\"LoadPointsFromFile\",\n+                coord_type=\"LIDAR\",\n                 load_dim=5,\n                 use_dim=5,\n-                file_client_args=dict(backend='disk')),\n+                file_client_args=dict(backend=\"disk\"),\n+            ),\n             dict(\n-                type='LoadPointsFromMultiSweeps',\n+                type=\"LoadPointsFromMultiSweeps\",\n                 sweeps_num=10,\n-                file_client_args=dict(backend='disk')),\n+                file_client_args=dict(backend=\"disk\"),\n+            ),\n             dict(\n-                type='DefaultFormatBundle3D',\n-                class_names=self.CLASSES,\n-                with_label=False),\n-            dict(type='Collect3D', keys=['points'])\n+                type=\"DefaultFormatBundle3D\", class_names=self.CLASSES, with_label=False\n+            ),\n+            dict(type=\"Collect3D\", keys=[\"points\"]),\n         ]\n         return Compose(pipeline)\n \n     def show(self, results, out_dir, show=False, pipeline=None):\n@@ -616,29 +626,33 @@\n                 Default: False.\n             pipeline (list[dict], optional): raw data loading for showing.\n                 Default: None.\n         \"\"\"\n-        assert out_dir is not None, 'Expect out_dir, got none.'\n+        assert out_dir is not None, \"Expect out_dir, got none.\"\n         pipeline = self._get_pipeline(pipeline)\n         for i, result in enumerate(results):\n-            if 'pts_bbox' in result.keys():\n-                result = result['pts_bbox']\n+            if \"pts_bbox\" in result.keys():\n+                result = result[\"pts_bbox\"]\n             data_info = self.data_infos[i]\n-            pts_path = data_info['lidar_path']\n-            file_name = osp.split(pts_path)[-1].split('.')[0]\n-            points = self._extract_data(i, pipeline, 'points').numpy()\n+            pts_path = data_info[\"lidar_path\"]\n+            file_name = osp.split(pts_path)[-1].split(\".\")[0]\n+            points = self._extract_data(i, pipeline, \"points\").numpy()\n             # for now we convert points into depth mode\n-            points = Coord3DMode.convert_point(points, Coord3DMode.LIDAR,\n-                                               Coord3DMode.DEPTH)\n-            inds = result['scores_3d'] > 0.1\n-            gt_bboxes = self.get_ann_info(i)['gt_bboxes_3d'].tensor.numpy()\n-            show_gt_bboxes = Box3DMode.convert(gt_bboxes, Box3DMode.LIDAR,\n-                                               Box3DMode.DEPTH)\n-            pred_bboxes = result['boxes_3d'][inds].tensor.numpy()\n-            show_pred_bboxes = Box3DMode.convert(pred_bboxes, Box3DMode.LIDAR,\n-                                                 Box3DMode.DEPTH)\n-            show_result(points, show_gt_bboxes, show_pred_bboxes, out_dir,\n-                        file_name, show)\n+            points = Coord3DMode.convert_point(\n+                points, Coord3DMode.LIDAR, Coord3DMode.DEPTH\n+            )\n+            inds = result[\"scores_3d\"] > 0.1\n+            gt_bboxes = self.get_ann_info(i)[\"gt_bboxes_3d\"].tensor.numpy()\n+            show_gt_bboxes = Box3DMode.convert(\n+                gt_bboxes, Box3DMode.LIDAR, Box3DMode.DEPTH\n+            )\n+            pred_bboxes = result[\"boxes_3d\"][inds].tensor.numpy()\n+            show_pred_bboxes = Box3DMode.convert(\n+                pred_bboxes, Box3DMode.LIDAR, Box3DMode.DEPTH\n+            )\n+            show_result(\n+                points, show_gt_bboxes, show_pred_bboxes, out_dir, file_name, show\n+            )\n \n \n def output_to_nusc_box(detection, with_velocity=True):\n     \"\"\"Convert the output to the box class in the nuScenes.\n@@ -652,11 +666,11 @@\n \n     Returns:\n         list[:obj:`NuScenesBox`]: List of standard NuScenesBoxes.\n     \"\"\"\n-    box3d = detection['boxes_3d']\n-    scores = detection['scores_3d'].numpy()\n-    labels = detection['labels_3d'].numpy()\n+    box3d = detection[\"boxes_3d\"]\n+    scores = detection[\"scores_3d\"].numpy()\n+    labels = detection[\"labels_3d\"].numpy()\n \n     box_gravity_center = box3d.gravity_center.numpy()\n     box_dims = box3d.dims.numpy()\n     box_yaw = box3d.yaw.numpy()\n@@ -680,18 +694,17 @@\n             nus_box_dims[i],\n             quat,\n             label=labels[i],\n             score=scores[i],\n-            velocity=velocity)\n+            velocity=velocity,\n+        )\n         box_list.append(box)\n     return box_list\n \n \n-def lidar_nusc_box_to_global(info,\n-                             boxes,\n-                             classes,\n-                             eval_configs,\n-                             eval_version='detection_cvpr_2019'):\n+def lidar_nusc_box_to_global(\n+    info, boxes, classes, eval_configs, eval_version=\"detection_cvpr_2019\"\n+):\n     \"\"\"Convert the box from ego to global coordinate.\n \n     Args:\n         info (dict): Info for a specific sample data, including the\n@@ -708,17 +721,17 @@\n     \"\"\"\n     box_list = []\n     for box in boxes:\n         # Move box to ego vehicle coord system\n-        box.rotate(pyquaternion.Quaternion(info['lidar2ego_rotation']))\n-        box.translate(np.array(info['lidar2ego_translation']))\n+        box.rotate(pyquaternion.Quaternion(info[\"lidar2ego_rotation\"]))\n+        box.translate(np.array(info[\"lidar2ego_translation\"]))\n         # filter det in ego.\n         cls_range_map = eval_configs.class_range\n         radius = np.linalg.norm(box.center[:2], 2)\n         det_range = cls_range_map[classes[box.label]]\n         if radius > det_range:\n             continue\n         # Move box to global coord system\n-        box.rotate(pyquaternion.Quaternion(info['ego2global_rotation']))\n-        box.translate(np.array(info['ego2global_translation']))\n+        box.rotate(pyquaternion.Quaternion(info[\"ego2global_rotation\"]))\n+        box.translate(np.array(info[\"ego2global_translation\"]))\n         box_list.append(box)\n     return box_list\n"
                },
                {
                    "date": 1719993837596,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -250,9 +250,9 @@\n             pts_filename=info[\"lidar_path\"],\n             sweeps=info[\"sweeps\"],\n             timestamp=info[\"timestamp\"] / 1e6,\n         )\n-        \n+\n         if self.modality[\"use_radar\"]:\n             input_dict[\"radar\"] = info[\"radars\"]\n \n         if \"ann_infos\" in info:\n@@ -290,8 +290,9 @@\n                 input_dict.update(dict(curr=info))\n                 if \"4d\" in self.img_info_prototype:\n                     info_adj_list = self.get_adj_info(info, index)\n                     input_dict.update(dict(adjacent=info_adj_list))\n+\n         return input_dict\n \n     def get_adj_info(self, info, index):\n         info_adj_list = []\n"
                },
                {
                    "date": 1720688147545,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -261,8 +261,9 @@\n             if self.img_info_prototype == \"mmcv\":\n                 image_paths = []\n                 lidar2img_rts = []\n                 for cam_type, cam_info in info[\"cams\"].items():\n+\n                     image_paths.append(cam_info[\"data_path\"])\n                     # obtain lidar to image transformation matrix\n                     lidar2cam_r = np.linalg.inv(cam_info[\"sensor2lidar_rotation\"])\n                     lidar2cam_t = cam_info[\"sensor2lidar_translation\"] @ lidar2cam_r.T\n@@ -285,8 +286,9 @@\n                 if not self.test_mode:\n                     annos = self.get_ann_info(index)\n                     input_dict[\"ann_info\"] = annos\n             else:\n+\n                 assert \"bevdet\" in self.img_info_prototype\n                 input_dict.update(dict(curr=info))\n                 if \"4d\" in self.img_info_prototype:\n                     info_adj_list = self.get_adj_info(info, index)\n"
                }
            ],
            "date": 1716014660045,
            "name": "Commit-0",
            "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport tempfile\nfrom os import path as osp\n\nimport mmcv\nimport numpy as np\nimport pyquaternion\nfrom nuscenes.utils.data_classes import Box as NuScenesBox\n\nfrom ..core import show_result\nfrom ..core.bbox import Box3DMode, Coord3DMode, LiDARInstance3DBoxes\nfrom .builder import DATASETS\nfrom .custom_3d import Custom3DDataset\nfrom .pipelines import Compose\n\n\n@DATASETS.register_module()\nclass NuScenesDataset(Custom3DDataset):\n    r\"\"\"NuScenes Dataset.\n\n    This class serves as the API for experiments on the NuScenes Dataset.\n\n    Please refer to `NuScenes Dataset <https://www.nuscenes.org/download>`_\n    for data downloading.\n\n    Args:\n        ann_file (str): Path of annotation file.\n        pipeline (list[dict], optional): Pipeline used for data processing.\n            Defaults to None.\n        data_root (str): Path of dataset root.\n        classes (tuple[str], optional): Classes used in the dataset.\n            Defaults to None.\n        load_interval (int, optional): Interval of loading the dataset. It is\n            used to uniformly sample the dataset. Defaults to 1.\n        with_velocity (bool, optional): Whether include velocity prediction\n            into the experiments. Defaults to True.\n        modality (dict, optional): Modality to specify the sensor data used\n            as input. Defaults to None.\n        box_type_3d (str, optional): Type of 3D box of this dataset.\n            Based on the `box_type_3d`, the dataset will encapsulate the box\n            to its original format then converted them to `box_type_3d`.\n            Defaults to 'LiDAR' in this dataset. Available options includes.\n            - 'LiDAR': Box in LiDAR coordinates.\n            - 'Depth': Box in depth coordinates, usually for indoor dataset.\n            - 'Camera': Box in camera coordinates.\n        filter_empty_gt (bool, optional): Whether to filter empty GT.\n            Defaults to True.\n        test_mode (bool, optional): Whether the dataset is in test mode.\n            Defaults to False.\n        eval_version (bool, optional): Configuration version of evaluation.\n            Defaults to  'detection_cvpr_2019'.\n        use_valid_flag (bool, optional): Whether to use `use_valid_flag` key\n            in the info file as mask to filter gt_boxes and gt_names.\n            Defaults to False.\n        img_info_prototype (str, optional): Type of img information.\n            Based on 'img_info_prototype', the dataset will prepare the image\n            data info in the type of 'mmcv' for official image infos,\n            'bevdet' for BEVDet, and 'bevdet4d' for BEVDet4D.\n            Defaults to 'mmcv'.\n        multi_adj_frame_id_cfg (tuple[int]): Define the selected index of\n            reference adjcacent frames.\n        ego_cam (str): Specify the ego coordinate relative to a specified\n            camera by its name defined in NuScenes.\n            Defaults to None, which use the mean of all cameras.\n    \"\"\"\n    NameMapping = {\n        'movable_object.barrier': 'barrier',\n        'vehicle.bicycle': 'bicycle',\n        'vehicle.bus.bendy': 'bus',\n        'vehicle.bus.rigid': 'bus',\n        'vehicle.car': 'car',\n        'vehicle.construction': 'construction_vehicle',\n        'vehicle.motorcycle': 'motorcycle',\n        'human.pedestrian.adult': 'pedestrian',\n        'human.pedestrian.child': 'pedestrian',\n        'human.pedestrian.construction_worker': 'pedestrian',\n        'human.pedestrian.police_officer': 'pedestrian',\n        'movable_object.trafficcone': 'traffic_cone',\n        'vehicle.trailer': 'trailer',\n        'vehicle.truck': 'truck'\n    }\n    DefaultAttribute = {\n        'car': 'vehicle.parked',\n        'pedestrian': 'pedestrian.moving',\n        'trailer': 'vehicle.parked',\n        'truck': 'vehicle.parked',\n        'bus': 'vehicle.moving',\n        'motorcycle': 'cycle.without_rider',\n        'construction_vehicle': 'vehicle.parked',\n        'bicycle': 'cycle.without_rider',\n        'barrier': '',\n        'traffic_cone': '',\n    }\n    AttrMapping = {\n        'cycle.with_rider': 0,\n        'cycle.without_rider': 1,\n        'pedestrian.moving': 2,\n        'pedestrian.standing': 3,\n        'pedestrian.sitting_lying_down': 4,\n        'vehicle.moving': 5,\n        'vehicle.parked': 6,\n        'vehicle.stopped': 7,\n    }\n    AttrMapping_rev = [\n        'cycle.with_rider',\n        'cycle.without_rider',\n        'pedestrian.moving',\n        'pedestrian.standing',\n        'pedestrian.sitting_lying_down',\n        'vehicle.moving',\n        'vehicle.parked',\n        'vehicle.stopped',\n    ]\n    # https://github.com/nutonomy/nuscenes-devkit/blob/57889ff20678577025326cfc24e57424a829be0a/python-sdk/nuscenes/eval/detection/evaluate.py#L222 # noqa\n    ErrNameMapping = {\n        'trans_err': 'mATE',\n        'scale_err': 'mASE',\n        'orient_err': 'mAOE',\n        'vel_err': 'mAVE',\n        'attr_err': 'mAAE'\n    }\n    CLASSES = ('car', 'truck', 'trailer', 'bus', 'construction_vehicle',\n               'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',\n               'barrier')\n\n    def __init__(self,\n                 ann_file,\n                 pipeline=None,\n                 data_root=None,\n                 classes=None,\n                 load_interval=1,\n                 with_velocity=True,\n                 modality=None,\n                 box_type_3d='LiDAR',\n                 filter_empty_gt=True,\n                 test_mode=False,\n                 eval_version='detection_cvpr_2019',\n                 use_valid_flag=False,\n                 img_info_prototype='mmcv',\n                 multi_adj_frame_id_cfg=None,\n                 ego_cam='CAM_FRONT',\n                 stereo=False):\n        self.load_interval = load_interval\n        self.use_valid_flag = use_valid_flag\n        super().__init__(\n            data_root=data_root,\n            ann_file=ann_file,\n            pipeline=pipeline,\n            classes=classes,\n            modality=modality,\n            box_type_3d=box_type_3d,\n            filter_empty_gt=filter_empty_gt,\n            test_mode=test_mode)\n\n        self.with_velocity = with_velocity\n        self.eval_version = eval_version\n        from nuscenes.eval.detection.config import config_factory\n        self.eval_detection_configs = config_factory(self.eval_version)\n        if self.modality is None:\n            self.modality = dict(\n                use_camera=False,\n                use_lidar=True,\n                use_radar=False,\n                use_map=False,\n                use_external=False,\n            )\n\n        self.img_info_prototype = img_info_prototype\n        self.multi_adj_frame_id_cfg = multi_adj_frame_id_cfg\n        self.ego_cam = ego_cam\n        self.stereo = stereo\n\n    def get_cat_ids(self, idx):\n        \"\"\"Get category distribution of single scene.\n\n        Args:\n            idx (int): Index of the data_info.\n\n        Returns:\n            dict[list]: for each category, if the current scene\n                contains such boxes, store a list containing idx,\n                otherwise, store empty list.\n        \"\"\"\n        info = self.data_infos[idx]\n        if self.use_valid_flag:\n            mask = info['valid_flag']\n            gt_names = set(info['gt_names'][mask])\n        else:\n            gt_names = set(info['gt_names'])\n\n        cat_ids = []\n        for name in gt_names:\n            if name in self.CLASSES:\n                cat_ids.append(self.cat2id[name])\n        return cat_ids\n\n    def load_annotations(self, ann_file):\n        \"\"\"Load annotations from ann_file.\n\n        Args:\n            ann_file (str): Path of the annotation file.\n\n        Returns:\n            list[dict]: List of annotations sorted by timestamps.\n        \"\"\"\n        data = mmcv.load(ann_file, file_format='pkl')\n        data_infos = list(sorted(data['infos'], key=lambda e: e['timestamp']))\n        data_infos = data_infos[::self.load_interval]\n        self.metadata = data['metadata']\n        self.version = self.metadata['version']\n        return data_infos\n\n    def get_data_info(self, index):\n        \"\"\"Get data info according to the given index.\n\n        Args:\n            index (int): Index of the sample data to get.\n\n        Returns:\n            dict: Data information that will be passed to the data\n                preprocessing pipelines. It includes the following keys:\n\n                - sample_idx (str): Sample index.\n                - pts_filename (str): Filename of point clouds.\n                - sweeps (list[dict]): Infos of sweeps.\n                - timestamp (float): Sample timestamp.\n                - img_filename (str, optional): Image filename.\n                - lidar2img (list[np.ndarray], optional): Transformations\n                    from lidar to different cameras.\n                - ann_info (dict): Annotation info.\n        \"\"\"\n        info = self.data_infos[index]\n        # standard protocol modified from SECOND.Pytorch\n        input_dict = dict(\n            sample_idx=info['token'],\n            pts_filename=info['lidar_path'],\n            sweeps=info['sweeps'],\n            timestamp=info['timestamp'] / 1e6,\n        )\n        if 'ann_infos' in info:\n            input_dict['ann_infos'] = info['ann_infos']\n        if self.modality['use_camera']:\n            if self.img_info_prototype == 'mmcv':\n                image_paths = []\n                lidar2img_rts = []\n                for cam_type, cam_info in info['cams'].items():\n                    image_paths.append(cam_info['data_path'])\n                    # obtain lidar to image transformation matrix\n                    lidar2cam_r = np.linalg.inv(\n                        cam_info['sensor2lidar_rotation'])\n                    lidar2cam_t = cam_info[\n                        'sensor2lidar_translation'] @ lidar2cam_r.T\n                    lidar2cam_rt = np.eye(4)\n                    lidar2cam_rt[:3, :3] = lidar2cam_r.T\n                    lidar2cam_rt[3, :3] = -lidar2cam_t\n                    intrinsic = cam_info['cam_intrinsic']\n                    viewpad = np.eye(4)\n                    viewpad[:intrinsic.shape[0], :intrinsic.\n                            shape[1]] = intrinsic\n                    lidar2img_rt = (viewpad @ lidar2cam_rt.T)\n                    lidar2img_rts.append(lidar2img_rt)\n\n                input_dict.update(\n                    dict(\n                        img_filename=image_paths,\n                        lidar2img=lidar2img_rts,\n                    ))\n\n                if not self.test_mode:\n                    annos = self.get_ann_info(index)\n                    input_dict['ann_info'] = annos\n            else:\n                assert 'bevdet' in self.img_info_prototype\n                input_dict.update(dict(curr=info))\n                if '4d' in self.img_info_prototype:\n                    info_adj_list = self.get_adj_info(info, index)\n                    input_dict.update(dict(adjacent=info_adj_list))\n        return input_dict\n\n    def get_adj_info(self, info, index):\n        info_adj_list = []\n        adj_id_list = list(range(*self.multi_adj_frame_id_cfg))\n        if self.stereo:\n            assert self.multi_adj_frame_id_cfg[0] == 1\n            assert self.multi_adj_frame_id_cfg[2] == 1\n            adj_id_list.append(self.multi_adj_frame_id_cfg[1])\n        for select_id in adj_id_list:\n            select_id = max(index - select_id, 0)\n            if not self.data_infos[select_id]['scene_token'] == info[\n                    'scene_token']:\n                info_adj_list.append(info)\n            else:\n                info_adj_list.append(self.data_infos[select_id])\n        return info_adj_list\n\n    def get_ann_info(self, index):\n        \"\"\"Get annotation info according to the given index.\n\n        Args:\n            index (int): Index of the annotation data to get.\n\n        Returns:\n            dict: Annotation information consists of the following keys:\n\n                - gt_bboxes_3d (:obj:`LiDARInstance3DBoxes`):\n                    3D ground truth bboxes\n                - gt_labels_3d (np.ndarray): Labels of ground truths.\n                - gt_names (list[str]): Class names of ground truths.\n        \"\"\"\n        info = self.data_infos[index]\n        # filter out bbox containing no points\n        if self.use_valid_flag:\n            mask = info['valid_flag']\n        else:\n            mask = info['num_lidar_pts'] > 0\n        gt_bboxes_3d = info['gt_boxes'][mask]\n        gt_names_3d = info['gt_names'][mask]\n        gt_labels_3d = []\n        for cat in gt_names_3d:\n            if cat in self.CLASSES:\n                gt_labels_3d.append(self.CLASSES.index(cat))\n            else:\n                gt_labels_3d.append(-1)\n        gt_labels_3d = np.array(gt_labels_3d)\n\n        if self.with_velocity:\n            gt_velocity = info['gt_velocity'][mask]\n            nan_mask = np.isnan(gt_velocity[:, 0])\n            gt_velocity[nan_mask] = [0.0, 0.0]\n            gt_bboxes_3d = np.concatenate([gt_bboxes_3d, gt_velocity], axis=-1)\n\n        # the nuscenes box center is [0.5, 0.5, 0.5], we change it to be\n        # the same as KITTI (0.5, 0.5, 0)\n        gt_bboxes_3d = LiDARInstance3DBoxes(\n            gt_bboxes_3d,\n            box_dim=gt_bboxes_3d.shape[-1],\n            origin=(0.5, 0.5, 0.5)).convert_to(self.box_mode_3d)\n\n        anns_results = dict(\n            gt_bboxes_3d=gt_bboxes_3d,\n            gt_labels_3d=gt_labels_3d,\n            gt_names=gt_names_3d)\n        return anns_results\n\n    def _format_bbox(self, results, jsonfile_prefix=None):\n        \"\"\"Convert the results to the standard format.\n\n        Args:\n            results (list[dict]): Testing results of the dataset.\n            jsonfile_prefix (str): The prefix of the output jsonfile.\n                You can specify the output directory/filename by\n                modifying the jsonfile_prefix. Default: None.\n\n        Returns:\n            str: Path of the output json file.\n        \"\"\"\n        nusc_annos = {}\n        mapped_class_names = self.CLASSES\n\n        print('Start to convert detection format...')\n        for sample_id, det in enumerate(mmcv.track_iter_progress(results)):\n            boxes = det['boxes_3d'].tensor.numpy()\n            scores = det['scores_3d'].numpy()\n            labels = det['labels_3d'].numpy()\n            sample_token = self.data_infos[sample_id]['token']\n\n            trans = self.data_infos[sample_id]['cams'][\n                self.ego_cam]['ego2global_translation']\n            rot = self.data_infos[sample_id]['cams'][\n                self.ego_cam]['ego2global_rotation']\n            rot = pyquaternion.Quaternion(rot)\n            annos = list()\n            for i, box in enumerate(boxes):\n                name = mapped_class_names[labels[i]]\n                center = box[:3]\n                wlh = box[[4, 3, 5]]\n                box_yaw = box[6]\n                box_vel = box[7:].tolist()\n                box_vel.append(0)\n                quat = pyquaternion.Quaternion(axis=[0, 0, 1], radians=box_yaw)\n                nusc_box = NuScenesBox(center, wlh, quat, velocity=box_vel)\n                nusc_box.rotate(rot)\n                nusc_box.translate(trans)\n                if np.sqrt(nusc_box.velocity[0]**2 +\n                           nusc_box.velocity[1]**2) > 0.2:\n                    if name in [\n                            'car',\n                            'construction_vehicle',\n                            'bus',\n                            'truck',\n                            'trailer',\n                    ]:\n                        attr = 'vehicle.moving'\n                    elif name in ['bicycle', 'motorcycle']:\n                        attr = 'cycle.with_rider'\n                    else:\n                        attr = self.DefaultAttribute[name]\n                else:\n                    if name in ['pedestrian']:\n                        attr = 'pedestrian.standing'\n                    elif name in ['bus']:\n                        attr = 'vehicle.stopped'\n                    else:\n                        attr = self.DefaultAttribute[name]\n                nusc_anno = dict(\n                    sample_token=sample_token,\n                    translation=nusc_box.center.tolist(),\n                    size=nusc_box.wlh.tolist(),\n                    rotation=nusc_box.orientation.elements.tolist(),\n                    velocity=nusc_box.velocity[:2],\n                    detection_name=name,\n                    detection_score=float(scores[i]),\n                    attribute_name=attr,\n                )\n                annos.append(nusc_anno)\n            # other views results of the same frame should be concatenated\n            if sample_token in nusc_annos:\n                nusc_annos[sample_token].extend(annos)\n            else:\n                nusc_annos[sample_token] = annos\n        nusc_submissions = {\n            'meta': self.modality,\n            'results': nusc_annos,\n        }\n\n        mmcv.mkdir_or_exist(jsonfile_prefix)\n        res_path = osp.join(jsonfile_prefix, 'results_nusc.json')\n        print('Results writes to', res_path)\n        mmcv.dump(nusc_submissions, res_path)\n        return res_path\n\n    def _evaluate_single(self,\n                         result_path,\n                         logger=None,\n                         metric='bbox',\n                         result_name='pts_bbox'):\n        \"\"\"Evaluation for a single model in nuScenes protocol.\n\n        Args:\n            result_path (str): Path of the result file.\n            logger (logging.Logger | str, optional): Logger used for printing\n                related information during evaluation. Default: None.\n            metric (str, optional): Metric name used for evaluation.\n                Default: 'bbox'.\n            result_name (str, optional): Result name in the metric prefix.\n                Default: 'pts_bbox'.\n\n        Returns:\n            dict: Dictionary of evaluation details.\n        \"\"\"\n        from nuscenes import NuScenes\n        from nuscenes.eval.detection.evaluate import NuScenesEval\n\n        output_dir = osp.join(*osp.split(result_path)[:-1])\n        nusc = NuScenes(\n            version=self.version, dataroot=self.data_root, verbose=False)\n        eval_set_map = {\n            'v1.0-mini': 'mini_val',\n            'v1.0-trainval': 'val',\n        }\n        nusc_eval = NuScenesEval(\n            nusc,\n            config=self.eval_detection_configs,\n            result_path=result_path,\n            eval_set=eval_set_map[self.version],\n            output_dir=output_dir,\n            verbose=False)\n        nusc_eval.main(render_curves=False)\n\n        # record metrics\n        metrics = mmcv.load(osp.join(output_dir, 'metrics_summary.json'))\n        detail = dict()\n        metric_prefix = f'{result_name}_NuScenes'\n        for name in self.CLASSES:\n            for k, v in metrics['label_aps'][name].items():\n                val = float('{:.4f}'.format(v))\n                detail['{}/{}_AP_dist_{}'.format(metric_prefix, name, k)] = val\n            for k, v in metrics['label_tp_errors'][name].items():\n                val = float('{:.4f}'.format(v))\n                detail['{}/{}_{}'.format(metric_prefix, name, k)] = val\n            for k, v in metrics['tp_errors'].items():\n                val = float('{:.4f}'.format(v))\n                detail['{}/{}'.format(metric_prefix,\n                                      self.ErrNameMapping[k])] = val\n\n        detail['{}/NDS'.format(metric_prefix)] = metrics['nd_score']\n        detail['{}/mAP'.format(metric_prefix)] = metrics['mean_ap']\n        return detail\n\n    def format_results(self, results, jsonfile_prefix=None):\n        \"\"\"Format the results to json (standard format for COCO evaluation).\n\n        Args:\n            results (list[dict]): Testing results of the dataset.\n            jsonfile_prefix (str): The prefix of json files. It includes\n                the file path and the prefix of filename, e.g., \"a/b/prefix\".\n                If not specified, a temp file will be created. Default: None.\n\n        Returns:\n            tuple: Returns (result_files, tmp_dir), where `result_files` is a\n                dict containing the json filepaths, `tmp_dir` is the temporal\n                directory created for saving json files when\n                `jsonfile_prefix` is not specified.\n        \"\"\"\n        assert isinstance(results, list), 'results must be a list'\n        assert len(results) == len(self), (\n            'The length of results is not equal to the dataset len: {} != {}'.\n            format(len(results), len(self)))\n\n        if jsonfile_prefix is None:\n            tmp_dir = tempfile.TemporaryDirectory()\n            jsonfile_prefix = osp.join(tmp_dir.name, 'results')\n        else:\n            tmp_dir = None\n\n        # currently the output prediction results could be in two formats\n        # 1. list of dict('boxes_3d': ..., 'scores_3d': ..., 'labels_3d': ...)\n        # 2. list of dict('pts_bbox' or 'img_bbox':\n        #     dict('boxes_3d': ..., 'scores_3d': ..., 'labels_3d': ...))\n        # this is a workaround to enable evaluation of both formats on nuScenes\n        # refer to https://github.com/open-mmlab/mmdetection3d/issues/449\n        if not ('pts_bbox' in results[0] or 'img_bbox' in results[0]):\n            result_files = self._format_bbox(results, jsonfile_prefix)\n        else:\n            # should take the inner dict out of 'pts_bbox' or 'img_bbox' dict\n            result_files = dict()\n            for name in results[0]:\n                print(f'\\nFormating bboxes of {name}')\n                results_ = [out[name] for out in results]\n                tmp_file_ = osp.join(jsonfile_prefix, name)\n                result_files.update(\n                    {name: self._format_bbox(results_, tmp_file_)})\n        return result_files, tmp_dir\n\n    def evaluate(self,\n                 results,\n                 metric='bbox',\n                 logger=None,\n                 jsonfile_prefix=None,\n                 result_names=['pts_bbox'],\n                 show=False,\n                 out_dir=None,\n                 pipeline=None):\n        \"\"\"Evaluation in nuScenes protocol.\n\n        Args:\n            results (list[dict]): Testing results of the dataset.\n            metric (str | list[str], optional): Metrics to be evaluated.\n                Default: 'bbox'.\n            logger (logging.Logger | str, optional): Logger used for printing\n                related information during evaluation. Default: None.\n            jsonfile_prefix (str, optional): The prefix of json files including\n                the file path and the prefix of filename, e.g., \"a/b/prefix\".\n                If not specified, a temp file will be created. Default: None.\n            show (bool, optional): Whether to visualize.\n                Default: False.\n            out_dir (str, optional): Path to save the visualization results.\n                Default: None.\n            pipeline (list[dict], optional): raw data loading for showing.\n                Default: None.\n\n        Returns:\n            dict[str, float]: Results of each evaluation metric.\n        \"\"\"\n        result_files, tmp_dir = self.format_results(results, jsonfile_prefix)\n\n        if isinstance(result_files, dict):\n            results_dict = dict()\n            for name in result_names:\n                print('Evaluating bboxes of {}'.format(name))\n                ret_dict = self._evaluate_single(result_files[name])\n            results_dict.update(ret_dict)\n        elif isinstance(result_files, str):\n            results_dict = self._evaluate_single(result_files)\n\n        if tmp_dir is not None:\n            tmp_dir.cleanup()\n\n        if show or out_dir:\n            self.show(results, out_dir, show=show, pipeline=pipeline)\n        return results_dict\n\n    def _build_default_pipeline(self):\n        \"\"\"Build the default pipeline for this dataset.\"\"\"\n        pipeline = [\n            dict(\n                type='LoadPointsFromFile',\n                coord_type='LIDAR',\n                load_dim=5,\n                use_dim=5,\n                file_client_args=dict(backend='disk')),\n            dict(\n                type='LoadPointsFromMultiSweeps',\n                sweeps_num=10,\n                file_client_args=dict(backend='disk')),\n            dict(\n                type='DefaultFormatBundle3D',\n                class_names=self.CLASSES,\n                with_label=False),\n            dict(type='Collect3D', keys=['points'])\n        ]\n        return Compose(pipeline)\n\n    def show(self, results, out_dir, show=False, pipeline=None):\n        \"\"\"Results visualization.\n\n        Args:\n            results (list[dict]): List of bounding boxes results.\n            out_dir (str): Output directory of visualization result.\n            show (bool): Whether to visualize the results online.\n                Default: False.\n            pipeline (list[dict], optional): raw data loading for showing.\n                Default: None.\n        \"\"\"\n        assert out_dir is not None, 'Expect out_dir, got none.'\n        pipeline = self._get_pipeline(pipeline)\n        for i, result in enumerate(results):\n            if 'pts_bbox' in result.keys():\n                result = result['pts_bbox']\n            data_info = self.data_infos[i]\n            pts_path = data_info['lidar_path']\n            file_name = osp.split(pts_path)[-1].split('.')[0]\n            points = self._extract_data(i, pipeline, 'points').numpy()\n            # for now we convert points into depth mode\n            points = Coord3DMode.convert_point(points, Coord3DMode.LIDAR,\n                                               Coord3DMode.DEPTH)\n            inds = result['scores_3d'] > 0.1\n            gt_bboxes = self.get_ann_info(i)['gt_bboxes_3d'].tensor.numpy()\n            show_gt_bboxes = Box3DMode.convert(gt_bboxes, Box3DMode.LIDAR,\n                                               Box3DMode.DEPTH)\n            pred_bboxes = result['boxes_3d'][inds].tensor.numpy()\n            show_pred_bboxes = Box3DMode.convert(pred_bboxes, Box3DMode.LIDAR,\n                                                 Box3DMode.DEPTH)\n            show_result(points, show_gt_bboxes, show_pred_bboxes, out_dir,\n                        file_name, show)\n\n\ndef output_to_nusc_box(detection, with_velocity=True):\n    \"\"\"Convert the output to the box class in the nuScenes.\n\n    Args:\n        detection (dict): Detection results.\n\n            - boxes_3d (:obj:`BaseInstance3DBoxes`): Detection bbox.\n            - scores_3d (torch.Tensor): Detection scores.\n            - labels_3d (torch.Tensor): Predicted box labels.\n\n    Returns:\n        list[:obj:`NuScenesBox`]: List of standard NuScenesBoxes.\n    \"\"\"\n    box3d = detection['boxes_3d']\n    scores = detection['scores_3d'].numpy()\n    labels = detection['labels_3d'].numpy()\n\n    box_gravity_center = box3d.gravity_center.numpy()\n    box_dims = box3d.dims.numpy()\n    box_yaw = box3d.yaw.numpy()\n\n    # our LiDAR coordinate system -> nuScenes box coordinate system\n    nus_box_dims = box_dims[:, [1, 0, 2]]\n\n    box_list = []\n    for i in range(len(box3d)):\n        quat = pyquaternion.Quaternion(axis=[0, 0, 1], radians=box_yaw[i])\n        if with_velocity:\n            velocity = (*box3d.tensor[i, 7:9], 0.0)\n        else:\n            velocity = (0, 0, 0)\n        # velo_val = np.linalg.norm(box3d[i, 7:9])\n        # velo_ori = box3d[i, 6]\n        # velocity = (\n        # velo_val * np.cos(velo_ori), velo_val * np.sin(velo_ori), 0.0)\n        box = NuScenesBox(\n            box_gravity_center[i],\n            nus_box_dims[i],\n            quat,\n            label=labels[i],\n            score=scores[i],\n            velocity=velocity)\n        box_list.append(box)\n    return box_list\n\n\ndef lidar_nusc_box_to_global(info,\n                             boxes,\n                             classes,\n                             eval_configs,\n                             eval_version='detection_cvpr_2019'):\n    \"\"\"Convert the box from ego to global coordinate.\n\n    Args:\n        info (dict): Info for a specific sample data, including the\n            calibration information.\n        boxes (list[:obj:`NuScenesBox`]): List of predicted NuScenesBoxes.\n        classes (list[str]): Mapped classes in the evaluation.\n        eval_configs (object): Evaluation configuration object.\n        eval_version (str, optional): Evaluation version.\n            Default: 'detection_cvpr_2019'\n\n    Returns:\n        list: List of standard NuScenesBoxes in the global\n            coordinate.\n    \"\"\"\n    box_list = []\n    for box in boxes:\n        # Move box to ego vehicle coord system\n        box.rotate(pyquaternion.Quaternion(info['lidar2ego_rotation']))\n        box.translate(np.array(info['lidar2ego_translation']))\n        # filter det in ego.\n        cls_range_map = eval_configs.class_range\n        radius = np.linalg.norm(box.center[:2], 2)\n        det_range = cls_range_map[classes[box.label]]\n        if radius > det_range:\n            continue\n        # Move box to global coord system\n        box.rotate(pyquaternion.Quaternion(info['ego2global_rotation']))\n        box.translate(np.array(info['ego2global_translation']))\n        box_list.append(box)\n    return box_list\n"
        }
    ]
}