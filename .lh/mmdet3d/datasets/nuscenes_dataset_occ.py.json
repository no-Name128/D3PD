{
    "sourceFile": "mmdet3d/datasets/nuscenes_dataset_occ.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1719993813592,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1719993813592,
            "name": "Commit-0",
            "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport os\nimport mmcv\nimport torch\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom .builder import DATASETS\nfrom .nuscenes_dataset import NuScenesDataset\nfrom .occ_metrics import Metric_mIoU, Metric_FScore\n\ncolors_map = np.array(\n    [\n        [0, 0, 0, 255],  # 0 undefined\n        [255, 158, 0, 255],  # 1 car  orange\n        [0, 0, 230, 255],  # 2 pedestrian  Blue\n        [47, 79, 79, 255],  # 3 sign  Darkslategrey\n        [220, 20, 60, 255],  # 4 CYCLIST  Crimson\n        [255, 69, 0, 255],  # 5 traiffic_light  Orangered\n        [255, 140, 0, 255],  # 6 pole  Darkorange\n        [233, 150, 70, 255],  # 7 construction_cone  Darksalmon\n        [255, 61, 99, 255],  # 8 bycycle  Red\n        [112, 128, 144, 255],  # 9 motorcycle  Slategrey\n        [222, 184, 135, 255],  # 10 building Burlywood\n        [0, 175, 0, 255],  # 11 vegetation  Green\n        [165, 42, 42, 255],  # 12 trunk  nuTonomy green\n        [0, 207, 191, 255],  # 13 curb, road, lane_marker, other_ground\n        [75, 0, 75, 255],  # 14 walkable, sidewalk\n        [255, 0, 0, 255],  # 15 unobsrvd\n        [0, 0, 0, 0],  # 16 undefined\n        [0, 0, 0, 0],  # 16 undefined\n    ]\n)\n\n\n@DATASETS.register_module()\nclass NuScenesDatasetOccpancy(NuScenesDataset):\n    def get_data_info(self, index):\n        \"\"\"Get data info according to the given index.\n\n        Args:\n            index (int): Index of the sample data to get.\n\n        Returns:\n            dict: Data information that will be passed to the data\n                preprocessing pipelines. It includes the following keys:\n\n                - sample_idx (str): Sample index.\n                - pts_filename (str): Filename of point clouds.\n                - sweeps (list[dict]): Infos of sweeps.\n                - timestamp (float): Sample timestamp.\n                - img_filename (str, optional): Image filename.\n                - lidar2img (list[np.ndarray], optional): Transformations\n                    from lidar to different cameras.\n                - ann_info (dict): Annotation info.\n        \"\"\"\n        input_dict = super(NuScenesDatasetOccpancy, self).get_data_info(index)\n        # standard protocol modified from SECOND.Pytorch\n        input_dict[\"occ_gt_path\"] = self.data_infos[index][\"occ_path\"]\n        return input_dict\n\n    def evaluate(self, occ_results, runner=None, show_dir=None, **eval_kwargs):\n        self.occ_eval_metrics = Metric_mIoU(\n            num_classes=18, use_lidar_mask=False, use_image_mask=True\n        )\n\n        print(\"\\nStarting Evaluation...\")\n        for index, occ_pred in enumerate(tqdm(occ_results)):\n            info = self.data_infos[index]\n\n            occ_gt = np.load(os.path.join(info[\"occ_path\"], \"labels.npz\"))\n            gt_semantics = occ_gt[\"semantics\"]\n            mask_lidar = occ_gt[\"mask_lidar\"].astype(bool)\n            mask_camera = occ_gt[\"mask_camera\"].astype(bool)\n            # occ_pred = occ_pred\n            self.occ_eval_metrics.add_batch(\n                occ_pred, gt_semantics, mask_lidar, mask_camera\n            )\n\n            if index % 100 == 0 and show_dir is not None:\n                gt_vis = self.vis_occ(gt_semantics)\n                pred_vis = self.vis_occ(occ_pred)\n                mmcv.imwrite(\n                    np.concatenate([gt_vis, pred_vis], axis=1),\n                    os.path.join(show_dir + \"%d.jpg\" % index),\n                )\n\n        return self.occ_eval_metrics.count_miou()\n\n    def vis_occ(self, semantics):\n        # simple visualization of result in BEV\n        semantics_valid = np.logical_not(semantics == 17)\n        d = np.arange(16).reshape(1, 1, 16)\n        d = np.repeat(d, 200, axis=0)\n        d = np.repeat(d, 200, axis=1).astype(np.float32)\n        d = d * semantics_valid\n        selected = np.argmax(d, axis=2)\n\n        selected_torch = torch.from_numpy(selected)\n        semantics_torch = torch.from_numpy(semantics)\n\n        occ_bev_torch = torch.gather(\n            semantics_torch, dim=2, index=selected_torch.unsqueeze(-1)\n        )\n        occ_bev = occ_bev_torch.numpy()\n\n        occ_bev = occ_bev.flatten().astype(np.int32)\n        occ_bev_vis = colors_map[occ_bev].astype(np.uint8)\n        occ_bev_vis = occ_bev_vis.reshape(200, 200, 4)[::-1, ::-1, :3]\n        occ_bev_vis = cv2.resize(occ_bev_vis, (400, 400))\n        return occ_bev_vis\n"
        }
    ]
}