{
    "sourceFile": "tools/analysis_tools/vis.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 11,
            "patches": [
                {
                    "date": 1718584820873,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1724753415409,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,9 +23,9 @@\n     max_lumi = 200\n     colors = np.array(\n         [[max_lumi, 0, max_lumi], [max_lumi, 0, 0], [max_lumi, max_lumi, 0],\n          [0, max_lumi, 0], [0, max_lumi, max_lumi], [0, 0, max_lumi]],\n-        dtype=np.float32)\n+        dtype=np.float32) # (6, 2)\n     if gray == 1:\n         return tuple(colors[-1].tolist())\n     num_rank = len(colors) - 1\n     rank = np.floor(gray * num_rank).astype(np.int)\n"
                },
                {
                    "date": 1724753421295,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,9 +23,9 @@\n     max_lumi = 200\n     colors = np.array(\n         [[max_lumi, 0, max_lumi], [max_lumi, 0, 0], [max_lumi, max_lumi, 0],\n          [0, max_lumi, 0], [0, max_lumi, max_lumi], [0, 0, max_lumi]],\n-        dtype=np.float32) # (6, 2)\n+        dtype=np.float32) # (6, 3)\n     if gray == 1:\n         return tuple(colors[-1].tolist())\n     num_rank = len(colors) - 1\n     rank = np.floor(gray * num_rank).astype(np.int)\n"
                },
                {
                    "date": 1724753438779,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -242,9 +242,9 @@\n                 color = depth2color(p[2])\n                 cv2.circle(\n                     canvas, (int(p[0]), int(p[1])),\n                     radius=0,\n-                    color=color,\n+                    color=(0,0,0)),\n                     thickness=1)\n \n         # draw instances\n         corners_lidar = corners_lidar.reshape(-1, 8, 3)\n"
                },
                {
                    "date": 1724753468477,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -242,9 +242,9 @@\n                 color = depth2color(p[2])\n                 cv2.circle(\n                     canvas, (int(p[0]), int(p[1])),\n                     radius=0,\n-                    color=(0,0,0)),\n+                    color=(0,0,0),\n                     thickness=1)\n \n         # draw instances\n         corners_lidar = corners_lidar.reshape(-1, 8, 3)\n"
                },
                {
                    "date": 1724753529945,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -13,113 +13,113 @@\n \n def check_point_in_img(points, height, width):\n     valid = np.logical_and(points[:, 0] >= 0, points[:, 1] >= 0)\n     valid = np.logical_and(\n-        valid, np.logical_and(points[:, 0] < width, points[:, 1] < height))\n+        valid, np.logical_and(points[:, 0] < width, points[:, 1] < height)\n+    )\n     return valid\n \n \n def depth2color(depth):\n     gray = max(0, min((depth + 2.5) / 3.0, 1.0))\n     max_lumi = 200\n     colors = np.array(\n-        [[max_lumi, 0, max_lumi], [max_lumi, 0, 0], [max_lumi, max_lumi, 0],\n-         [0, max_lumi, 0], [0, max_lumi, max_lumi], [0, 0, max_lumi]],\n-        dtype=np.float32) # (6, 3)\n+        [\n+            [max_lumi, 0, max_lumi],\n+            [max_lumi, 0, 0],\n+            [max_lumi, max_lumi, 0],\n+            [0, max_lumi, 0],\n+            [0, max_lumi, max_lumi],\n+            [0, 0, max_lumi],\n+        ],\n+        dtype=np.float32,\n+    )  # (6, 3)\n     if gray == 1:\n         return tuple(colors[-1].tolist())\n     num_rank = len(colors) - 1\n     rank = np.floor(gray * num_rank).astype(np.int)\n     diff = (gray - rank / num_rank) * num_rank\n-    return tuple(\n-        (colors[rank] + (colors[rank + 1] - colors[rank]) * diff).tolist())\n+    return tuple((colors[rank] + (colors[rank + 1] - colors[rank]) * diff).tolist())\n \n \n def lidar2img(points_lidar, camrera_info):\n-    points_lidar_homogeneous = \\\n-        np.concatenate([points_lidar,\n-                        np.ones((points_lidar.shape[0], 1),\n-                                dtype=points_lidar.dtype)], axis=1)\n+    points_lidar_homogeneous = np.concatenate(\n+        [points_lidar, np.ones((points_lidar.shape[0], 1), dtype=points_lidar.dtype)],\n+        axis=1,\n+    )\n     camera2lidar = np.eye(4, dtype=np.float32)\n-    camera2lidar[:3, :3] = camrera_info['sensor2lidar_rotation']\n-    camera2lidar[:3, 3] = camrera_info['sensor2lidar_translation']\n+    camera2lidar[:3, :3] = camrera_info[\"sensor2lidar_rotation\"]\n+    camera2lidar[:3, 3] = camrera_info[\"sensor2lidar_translation\"]\n     lidar2camera = np.linalg.inv(camera2lidar)\n     points_camera_homogeneous = points_lidar_homogeneous @ lidar2camera.T\n     points_camera = points_camera_homogeneous[:, :3]\n     valid = np.ones((points_camera.shape[0]), dtype=bool)\n     valid = np.logical_and(points_camera[:, -1] > 0.5, valid)\n     points_camera = points_camera / points_camera[:, 2:3]\n-    camera2img = camrera_info['cam_intrinsic']\n+    camera2img = camrera_info[\"cam_intrinsic\"]\n     points_img = points_camera @ camera2img.T\n     points_img = points_img[:, :2]\n     return points_img, valid\n \n \n def get_lidar2global(infos):\n     lidar2ego = np.eye(4, dtype=np.float32)\n-    lidar2ego[:3, :3] = Quaternion(infos['lidar2ego_rotation']).rotation_matrix\n-    lidar2ego[:3, 3] = infos['lidar2ego_translation']\n+    lidar2ego[:3, :3] = Quaternion(infos[\"lidar2ego_rotation\"]).rotation_matrix\n+    lidar2ego[:3, 3] = infos[\"lidar2ego_translation\"]\n     ego2global = np.eye(4, dtype=np.float32)\n-    ego2global[:3, :3] = Quaternion(\n-        infos['ego2global_rotation']).rotation_matrix\n-    ego2global[:3, 3] = infos['ego2global_translation']\n+    ego2global[:3, :3] = Quaternion(infos[\"ego2global_rotation\"]).rotation_matrix\n+    ego2global[:3, 3] = infos[\"ego2global_translation\"]\n     return ego2global @ lidar2ego\n \n \n def parse_args():\n-    parser = argparse.ArgumentParser(description='Visualize the predicted '\n-                                     'result of nuScenes')\n+    parser = argparse.ArgumentParser(\n+        description=\"Visualize the predicted \" \"result of nuScenes\"\n+    )\n+    parser.add_argument(\"res\", help=\"Path to the predicted result in json format\")\n     parser.add_argument(\n-        'res', help='Path to the predicted result in json format')\n+        \"--show-range\", type=int, default=50, help=\"Range of visualization in BEV\"\n+    )\n     parser.add_argument(\n-        '--show-range',\n-        type=int,\n-        default=50,\n-        help='Range of visualization in BEV')\n+        \"--canva-size\", type=int, default=1000, help=\"Size of canva in pixel\"\n+    )\n     parser.add_argument(\n-        '--canva-size', type=int, default=1000, help='Size of canva in pixel')\n+        \"--vis-frames\", type=int, default=500, help=\"Number of frames for visualization\"\n+    )\n     parser.add_argument(\n-        '--vis-frames',\n+        \"--scale-factor\",\n         type=int,\n-        default=500,\n-        help='Number of frames for visualization')\n-    parser.add_argument(\n-        '--scale-factor',\n-        type=int,\n         default=4,\n-        help='Trade-off between image-view and bev in size of '\n-        'the visualized canvas')\n+        help=\"Trade-off between image-view and bev in size of \" \"the visualized canvas\",\n+    )\n     parser.add_argument(\n-        '--vis-thred',\n-        type=float,\n-        default=0.3,\n-        help='Threshold the predicted results')\n-    parser.add_argument('--draw-gt', action='store_true')\n+        \"--vis-thred\", type=float, default=0.3, help=\"Threshold the predicted results\"\n+    )\n+    parser.add_argument(\"--draw-gt\", action=\"store_true\")\n     parser.add_argument(\n-        '--version',\n-        type=str,\n-        default='val',\n-        help='Version of nuScenes dataset')\n+        \"--version\", type=str, default=\"val\", help=\"Version of nuScenes dataset\"\n+    )\n     parser.add_argument(\n-        '--root_path',\n+        \"--root_path\",\n         type=str,\n-        default='./data/nuscenes',\n-        help='Path to nuScenes dataset')\n+        default=\"./data/nuscenes\",\n+        help=\"Path to nuScenes dataset\",\n+    )\n     parser.add_argument(\n-        '--save_path',\n+        \"--save_path\",\n         type=str,\n-        default='./vis',\n-        help='Path to save visualization results')\n+        default=\"./vis\",\n+        help=\"Path to save visualization results\",\n+    )\n     parser.add_argument(\n-        '--format',\n+        \"--format\",\n         type=str,\n-        default='video',\n-        choices=['video', 'image'],\n-        help='The desired format of the visualization result')\n-    parser.add_argument(\n-        '--fps', type=int, default=20, help='Frame rate of video')\n-    parser.add_argument(\n-        '--video-prefix', type=str, default='vis', help='name of video')\n+        default=\"video\",\n+        choices=[\"video\", \"image\"],\n+        help=\"The desired format of the visualization result\",\n+    )\n+    parser.add_argument(\"--fps\", type=int, default=20, help=\"Frame rate of video\")\n+    parser.add_argument(\"--video-prefix\", type=str, default=\"vis\", help=\"name of video\")\n     args = parser.parse_args()\n     return args\n \n \n@@ -128,93 +128,107 @@\n \n def main():\n     args = parse_args()\n     # load predicted results\n-    res = json.load(open(args.res, 'r'))\n+    res = json.load(open(args.res, \"r\"))\n     # load dataset information\n-    info_path = \\\n-        args.root_path + '/bevdetv3-nuscenes_infos_%s.pkl' % args.version\n-    dataset = pickle.load(open(info_path, 'rb'))\n+    info_path = args.root_path + \"/bevdetv3-nuscenes_infos_%s.pkl\" % args.version\n+    dataset = pickle.load(open(info_path, \"rb\"))\n     # prepare save path and medium\n     vis_dir = args.save_path\n     if not os.path.exists(vis_dir):\n         os.makedirs(vis_dir)\n-    print('saving visualized result to %s' % vis_dir)\n+    print(\"saving visualized result to %s\" % vis_dir)\n     scale_factor = args.scale_factor\n     canva_size = args.canva_size\n     show_range = args.show_range\n-    if args.format == 'video':\n-        fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n+    if args.format == \"video\":\n+        fourcc = cv2.VideoWriter_fourcc(*\"MP4V\")\n         vout = cv2.VideoWriter(\n-            os.path.join(vis_dir, '%s.mp4' % args.video_prefix), fourcc,\n-            args.fps, (int(1600 / scale_factor * 3),\n-                       int(900 / scale_factor * 2 + canva_size)))\n+            os.path.join(vis_dir, \"%s.mp4\" % args.video_prefix),\n+            fourcc,\n+            args.fps,\n+            (int(1600 / scale_factor * 3), int(900 / scale_factor * 2 + canva_size)),\n+        )\n \n     draw_boxes_indexes_bev = [(0, 1), (1, 2), (2, 3), (3, 0)]\n-    draw_boxes_indexes_img_view = [(0, 1), (1, 2), (2, 3), (3, 0), (4, 5),\n-                                   (5, 6), (6, 7), (7, 4), (0, 4), (1, 5),\n-                                   (2, 6), (3, 7)]\n+    draw_boxes_indexes_img_view = [\n+        (0, 1),\n+        (1, 2),\n+        (2, 3),\n+        (3, 0),\n+        (4, 5),\n+        (5, 6),\n+        (6, 7),\n+        (7, 4),\n+        (0, 4),\n+        (1, 5),\n+        (2, 6),\n+        (3, 7),\n+    ]\n     views = [\n-        'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT',\n-        'CAM_BACK', 'CAM_BACK_RIGHT'\n+        \"CAM_FRONT_LEFT\",\n+        \"CAM_FRONT\",\n+        \"CAM_FRONT_RIGHT\",\n+        \"CAM_BACK_LEFT\",\n+        \"CAM_BACK\",\n+        \"CAM_BACK_RIGHT\",\n     ]\n-    print('start visualizing results')\n+    print(\"start visualizing results\")\n     for cnt, infos in enumerate(\n-            dataset['infos'][:min(args.vis_frames, len(dataset['infos']))]):\n+        dataset[\"infos\"][: min(args.vis_frames, len(dataset[\"infos\"]))]\n+    ):\n         if cnt % 10 == 0:\n-            print('%d/%d' % (cnt, min(args.vis_frames, len(dataset['infos']))))\n+            print(\"%d/%d\" % (cnt, min(args.vis_frames, len(dataset[\"infos\"]))))\n         # collect instances\n-        pred_res = res['results'][infos['token']]\n+        pred_res = res[\"results\"][infos[\"token\"]]\n         pred_boxes = [\n-            pred_res[rid]['translation'] + pred_res[rid]['size'] + [\n-                Quaternion(pred_res[rid]['rotation']).yaw_pitch_roll[0] +\n-                np.pi / 2\n-            ] for rid in range(len(pred_res))\n+            pred_res[rid][\"translation\"]\n+            + pred_res[rid][\"size\"]\n+            + [Quaternion(pred_res[rid][\"rotation\"]).yaw_pitch_roll[0] + np.pi / 2]\n+            for rid in range(len(pred_res))\n         ]\n         if len(pred_boxes) == 0:\n             corners_lidar = np.zeros((0, 3), dtype=np.float32)\n         else:\n             pred_boxes = np.array(pred_boxes, dtype=np.float32)\n             boxes = LB(pred_boxes, origin=(0.5, 0.5, 0.0))\n             corners_global = boxes.corners.numpy().reshape(-1, 3)\n             corners_global = np.concatenate(\n-                [corners_global,\n-                 np.ones([corners_global.shape[0], 1])],\n-                axis=1)\n+                [corners_global, np.ones([corners_global.shape[0], 1])], axis=1\n+            )\n             l2g = get_lidar2global(infos)\n             corners_lidar = corners_global @ np.linalg.inv(l2g).T\n             corners_lidar = corners_lidar[:, :3]\n-        pred_flag = np.ones((corners_lidar.shape[0] // 8, ), dtype=np.bool)\n-        scores = [\n-            pred_res[rid]['detection_score'] for rid in range(len(pred_res))\n-        ]\n+        pred_flag = np.ones((corners_lidar.shape[0] // 8,), dtype=np.bool)\n+        scores = [pred_res[rid][\"detection_score\"] for rid in range(len(pred_res))]\n         if args.draw_gt:\n-            gt_boxes = infos['gt_boxes']\n+            gt_boxes = infos[\"gt_boxes\"]\n             gt_boxes[:, -1] = gt_boxes[:, -1] + np.pi / 2\n             width = gt_boxes[:, 4].copy()\n             gt_boxes[:, 4] = gt_boxes[:, 3]\n             gt_boxes[:, 3] = width\n-            corners_lidar_gt = \\\n-                LB(infos['gt_boxes'],\n-                   origin=(0.5, 0.5, 0.5)).corners.numpy().reshape(-1, 3)\n-            corners_lidar = np.concatenate([corners_lidar, corners_lidar_gt],\n-                                           axis=0)\n+            corners_lidar_gt = (\n+                LB(infos[\"gt_boxes\"], origin=(0.5, 0.5, 0.5))\n+                .corners.numpy()\n+                .reshape(-1, 3)\n+            )\n+            corners_lidar = np.concatenate([corners_lidar, corners_lidar_gt], axis=0)\n             gt_flag = np.ones((corners_lidar_gt.shape[0] // 8), dtype=np.bool)\n-            pred_flag = np.concatenate(\n-                [pred_flag, np.logical_not(gt_flag)], axis=0)\n-            scores = scores + [0 for _ in range(infos['gt_boxes'].shape[0])]\n+            pred_flag = np.concatenate([pred_flag, np.logical_not(gt_flag)], axis=0)\n+            scores = scores + [0 for _ in range(infos[\"gt_boxes\"].shape[0])]\n         scores = np.array(scores, dtype=np.float32)\n         sort_ids = np.argsort(scores)\n \n         # image view\n         imgs = []\n         for view in views:\n-            img = cv2.imread(infos['cams'][view]['data_path'])\n+            img = cv2.imread(infos[\"cams\"][view][\"data_path\"])\n             # draw instances\n-            corners_img, valid = lidar2img(corners_lidar, infos['cams'][view])\n+            corners_img, valid = lidar2img(corners_lidar, infos[\"cams\"][view])\n             valid = np.logical_and(\n-                valid,\n-                check_point_in_img(corners_img, img.shape[0], img.shape[1]))\n+                valid, check_point_in_img(corners_img, img.shape[0], img.shape[1])\n+            )\n             valid = valid.reshape(-1, 8)\n             corners_img = corners_img.reshape(-1, 8, 2).astype(np.int)\n             for aid in range(valid.shape[0]):\n                 for index in draw_boxes_indexes_img_view:\n@@ -223,41 +237,43 @@\n                             img,\n                             corners_img[aid, index[0]],\n                             corners_img[aid, index[1]],\n                             color=color_map[int(pred_flag[aid])],\n-                            thickness=scale_factor)\n+                            thickness=scale_factor,\n+                        )\n             imgs.append(img)\n \n         # bird-eye-view\n-        canvas = np.zeros((int(canva_size), int(canva_size), 3),\n-                          dtype=np.uint8)\n+        canvas = np.zeros((int(canva_size), int(canva_size), 3), dtype=np.uint8)\n         # draw lidar points\n-        lidar_points = np.fromfile(infos['lidar_path'], dtype=np.float32)\n+        lidar_points = np.fromfile(infos[\"lidar_path\"], dtype=np.float32)\n         lidar_points = lidar_points.reshape(-1, 5)[:, :3]\n         lidar_points[:, 1] = -lidar_points[:, 1]\n-        lidar_points[:, :2] = \\\n+        lidar_points[:, :2] = (\n             (lidar_points[:, :2] + show_range) / show_range / 2.0 * canva_size\n+        )\n         for p in lidar_points:\n-            if check_point_in_img(\n-                    p.reshape(1, 3), canvas.shape[1], canvas.shape[0])[0]:\n+            if check_point_in_img(p.reshape(1, 3), canvas.shape[1], canvas.shape[0])[0]:\n                 color = depth2color(p[2])\n                 cv2.circle(\n-                    canvas, (int(p[0]), int(p[1])),\n+                    canvas,\n+                    (int(p[0]), int(p[1])),\n                     radius=0,\n-                    color=(0,0,0),\n-                    thickness=1)\n+                    color=(0, 0, 0),\n+                    thickness=1,\n+                )\n \n         # draw instances\n         corners_lidar = corners_lidar.reshape(-1, 8, 3)\n         corners_lidar[:, :, 1] = -corners_lidar[:, :, 1]\n         bottom_corners_bev = corners_lidar[:, [0, 3, 7, 4], :2]\n-        bottom_corners_bev = \\\n+        bottom_corners_bev = (\n             (bottom_corners_bev + show_range) / show_range / 2.0 * canva_size\n+        )\n         bottom_corners_bev = np.round(bottom_corners_bev).astype(np.int32)\n         center_bev = corners_lidar[:, [0, 3, 7, 4], :2].mean(axis=1)\n         head_bev = corners_lidar[:, [0, 4], :2].mean(axis=1)\n-        canter_canvas = \\\n-            (center_bev + show_range) / show_range / 2.0 * canva_size\n+        canter_canvas = (center_bev + show_range) / show_range / 2.0 * canva_size\n         center_canvas = canter_canvas.astype(np.int32)\n         head_canvas = (head_bev + show_range) / show_range / 2.0 * canva_size\n         head_canvas = head_canvas.astype(np.int32)\n \n@@ -272,37 +288,45 @@\n                     canvas,\n                     bottom_corners_bev[rid, index[0]],\n                     bottom_corners_bev[rid, index[1]],\n                     [color[0] * score, color[1] * score, color[2] * score],\n-                    thickness=1)\n+                    thickness=1,\n+                )\n             cv2.line(\n                 canvas,\n                 center_canvas[rid],\n                 head_canvas[rid],\n                 [color[0] * score, color[1] * score, color[2] * score],\n                 1,\n-                lineType=8)\n+                lineType=8,\n+            )\n \n         # fuse image-view and bev\n-        img = np.zeros((900 * 2 + canva_size * scale_factor, 1600 * 3, 3),\n-                       dtype=np.uint8)\n+        img = np.zeros(\n+            (900 * 2 + canva_size * scale_factor, 1600 * 3, 3), dtype=np.uint8\n+        )\n         img[:900, :, :] = np.concatenate(imgs[:3], axis=1)\n         img_back = np.concatenate(\n-            [imgs[3][:, ::-1, :], imgs[4][:, ::-1, :], imgs[5][:, ::-1, :]],\n-            axis=1)\n-        img[900 + canva_size * scale_factor:, :, :] = img_back\n-        img = cv2.resize(img, (int(1600 / scale_factor * 3),\n-                               int(900 / scale_factor * 2 + canva_size)))\n+            [imgs[3][:, ::-1, :], imgs[4][:, ::-1, :], imgs[5][:, ::-1, :]], axis=1\n+        )\n+        img[900 + canva_size * scale_factor :, :, :] = img_back\n+        img = cv2.resize(\n+            img,\n+            (int(1600 / scale_factor * 3), int(900 / scale_factor * 2 + canva_size)),\n+        )\n         w_begin = int((1600 * 3 / scale_factor - canva_size) // 2)\n-        img[int(900 / scale_factor):int(900 / scale_factor) + canva_size,\n-            w_begin:w_begin + canva_size, :] = canvas\n+        img[\n+            int(900 / scale_factor) : int(900 / scale_factor) + canva_size,\n+            w_begin : w_begin + canva_size,\n+            :,\n+        ] = canvas\n \n-        if args.format == 'image':\n-            cv2.imwrite(os.path.join(vis_dir, '%s.jpg' % infos['token']), img)\n-        elif args.format == 'video':\n+        if args.format == \"image\":\n+            cv2.imwrite(os.path.join(vis_dir, \"%s.jpg\" % infos[\"token\"]), img)\n+        elif args.format == \"video\":\n             vout.write(img)\n-    if args.format == 'video':\n+    if args.format == \"video\":\n         vout.release()\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     main()\n"
                },
                {
                    "date": 1724753575633,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -257,9 +257,9 @@\n                 cv2.circle(\n                     canvas,\n                     (int(p[0]), int(p[1])),\n                     radius=0,\n-                    color=(0, 0, 0),\n+                    color='white',\n                     thickness=1,\n                 )\n \n         # draw instances\n"
                },
                {
                    "date": 1724753601928,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -257,9 +257,9 @@\n                 cv2.circle(\n                     canvas,\n                     (int(p[0]), int(p[1])),\n                     radius=0,\n-                    color='white',\n+                    color=(0,1,0),\n                     thickness=1,\n                 )\n \n         # draw instances\n"
                },
                {
                    "date": 1724753609667,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -257,9 +257,9 @@\n                 cv2.circle(\n                     canvas,\n                     (int(p[0]), int(p[1])),\n                     radius=0,\n-                    color=(0,1,0),\n+                    color=(0, 1, 0),\n                     thickness=1,\n                 )\n \n         # draw instances\n"
                },
                {
                    "date": 1724753650901,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -257,9 +257,9 @@\n                 cv2.circle(\n                     canvas,\n                     (int(p[0]), int(p[1])),\n                     radius=0,\n-                    color=(0, 1, 0),\n+                    color=(255,255, 255s),\n                     thickness=1,\n                 )\n \n         # draw instances\n"
                },
                {
                    "date": 1724765796420,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -257,9 +257,9 @@\n                 cv2.circle(\n                     canvas,\n                     (int(p[0]), int(p[1])),\n                     radius=0,\n-                    color=(255,255, 255s),\n+                    color=(255, 255, 255),\n                     thickness=1,\n                 )\n \n         # draw instances\n"
                },
                {
                    "date": 1740876924577,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -106,9 +106,10 @@\n     )\n     parser.add_argument(\n         \"--save_path\",\n         type=str,\n-        default=\"./vis\",\n+        # default=\"./vis\",\n+        required=True,\n         help=\"Path to save visualization results\",\n     )\n     parser.add_argument(\n         \"--format\",\n"
                }
            ],
            "date": 1718584820873,
            "name": "Commit-0",
            "content": "# Copyright (c) Phigent Robotics. All rights reserved.\nimport argparse\nimport json\nimport os\nimport pickle\n\nimport cv2\nimport numpy as np\nfrom pyquaternion.quaternion import Quaternion\n\nfrom mmdet3d.core.bbox.structures.lidar_box3d import LiDARInstance3DBoxes as LB\n\n\ndef check_point_in_img(points, height, width):\n    valid = np.logical_and(points[:, 0] >= 0, points[:, 1] >= 0)\n    valid = np.logical_and(\n        valid, np.logical_and(points[:, 0] < width, points[:, 1] < height))\n    return valid\n\n\ndef depth2color(depth):\n    gray = max(0, min((depth + 2.5) / 3.0, 1.0))\n    max_lumi = 200\n    colors = np.array(\n        [[max_lumi, 0, max_lumi], [max_lumi, 0, 0], [max_lumi, max_lumi, 0],\n         [0, max_lumi, 0], [0, max_lumi, max_lumi], [0, 0, max_lumi]],\n        dtype=np.float32)\n    if gray == 1:\n        return tuple(colors[-1].tolist())\n    num_rank = len(colors) - 1\n    rank = np.floor(gray * num_rank).astype(np.int)\n    diff = (gray - rank / num_rank) * num_rank\n    return tuple(\n        (colors[rank] + (colors[rank + 1] - colors[rank]) * diff).tolist())\n\n\ndef lidar2img(points_lidar, camrera_info):\n    points_lidar_homogeneous = \\\n        np.concatenate([points_lidar,\n                        np.ones((points_lidar.shape[0], 1),\n                                dtype=points_lidar.dtype)], axis=1)\n    camera2lidar = np.eye(4, dtype=np.float32)\n    camera2lidar[:3, :3] = camrera_info['sensor2lidar_rotation']\n    camera2lidar[:3, 3] = camrera_info['sensor2lidar_translation']\n    lidar2camera = np.linalg.inv(camera2lidar)\n    points_camera_homogeneous = points_lidar_homogeneous @ lidar2camera.T\n    points_camera = points_camera_homogeneous[:, :3]\n    valid = np.ones((points_camera.shape[0]), dtype=bool)\n    valid = np.logical_and(points_camera[:, -1] > 0.5, valid)\n    points_camera = points_camera / points_camera[:, 2:3]\n    camera2img = camrera_info['cam_intrinsic']\n    points_img = points_camera @ camera2img.T\n    points_img = points_img[:, :2]\n    return points_img, valid\n\n\ndef get_lidar2global(infos):\n    lidar2ego = np.eye(4, dtype=np.float32)\n    lidar2ego[:3, :3] = Quaternion(infos['lidar2ego_rotation']).rotation_matrix\n    lidar2ego[:3, 3] = infos['lidar2ego_translation']\n    ego2global = np.eye(4, dtype=np.float32)\n    ego2global[:3, :3] = Quaternion(\n        infos['ego2global_rotation']).rotation_matrix\n    ego2global[:3, 3] = infos['ego2global_translation']\n    return ego2global @ lidar2ego\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Visualize the predicted '\n                                     'result of nuScenes')\n    parser.add_argument(\n        'res', help='Path to the predicted result in json format')\n    parser.add_argument(\n        '--show-range',\n        type=int,\n        default=50,\n        help='Range of visualization in BEV')\n    parser.add_argument(\n        '--canva-size', type=int, default=1000, help='Size of canva in pixel')\n    parser.add_argument(\n        '--vis-frames',\n        type=int,\n        default=500,\n        help='Number of frames for visualization')\n    parser.add_argument(\n        '--scale-factor',\n        type=int,\n        default=4,\n        help='Trade-off between image-view and bev in size of '\n        'the visualized canvas')\n    parser.add_argument(\n        '--vis-thred',\n        type=float,\n        default=0.3,\n        help='Threshold the predicted results')\n    parser.add_argument('--draw-gt', action='store_true')\n    parser.add_argument(\n        '--version',\n        type=str,\n        default='val',\n        help='Version of nuScenes dataset')\n    parser.add_argument(\n        '--root_path',\n        type=str,\n        default='./data/nuscenes',\n        help='Path to nuScenes dataset')\n    parser.add_argument(\n        '--save_path',\n        type=str,\n        default='./vis',\n        help='Path to save visualization results')\n    parser.add_argument(\n        '--format',\n        type=str,\n        default='video',\n        choices=['video', 'image'],\n        help='The desired format of the visualization result')\n    parser.add_argument(\n        '--fps', type=int, default=20, help='Frame rate of video')\n    parser.add_argument(\n        '--video-prefix', type=str, default='vis', help='name of video')\n    args = parser.parse_args()\n    return args\n\n\ncolor_map = {0: (255, 255, 0), 1: (0, 255, 255)}\n\n\ndef main():\n    args = parse_args()\n    # load predicted results\n    res = json.load(open(args.res, 'r'))\n    # load dataset information\n    info_path = \\\n        args.root_path + '/bevdetv3-nuscenes_infos_%s.pkl' % args.version\n    dataset = pickle.load(open(info_path, 'rb'))\n    # prepare save path and medium\n    vis_dir = args.save_path\n    if not os.path.exists(vis_dir):\n        os.makedirs(vis_dir)\n    print('saving visualized result to %s' % vis_dir)\n    scale_factor = args.scale_factor\n    canva_size = args.canva_size\n    show_range = args.show_range\n    if args.format == 'video':\n        fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n        vout = cv2.VideoWriter(\n            os.path.join(vis_dir, '%s.mp4' % args.video_prefix), fourcc,\n            args.fps, (int(1600 / scale_factor * 3),\n                       int(900 / scale_factor * 2 + canva_size)))\n\n    draw_boxes_indexes_bev = [(0, 1), (1, 2), (2, 3), (3, 0)]\n    draw_boxes_indexes_img_view = [(0, 1), (1, 2), (2, 3), (3, 0), (4, 5),\n                                   (5, 6), (6, 7), (7, 4), (0, 4), (1, 5),\n                                   (2, 6), (3, 7)]\n    views = [\n        'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT',\n        'CAM_BACK', 'CAM_BACK_RIGHT'\n    ]\n    print('start visualizing results')\n    for cnt, infos in enumerate(\n            dataset['infos'][:min(args.vis_frames, len(dataset['infos']))]):\n        if cnt % 10 == 0:\n            print('%d/%d' % (cnt, min(args.vis_frames, len(dataset['infos']))))\n        # collect instances\n        pred_res = res['results'][infos['token']]\n        pred_boxes = [\n            pred_res[rid]['translation'] + pred_res[rid]['size'] + [\n                Quaternion(pred_res[rid]['rotation']).yaw_pitch_roll[0] +\n                np.pi / 2\n            ] for rid in range(len(pred_res))\n        ]\n        if len(pred_boxes) == 0:\n            corners_lidar = np.zeros((0, 3), dtype=np.float32)\n        else:\n            pred_boxes = np.array(pred_boxes, dtype=np.float32)\n            boxes = LB(pred_boxes, origin=(0.5, 0.5, 0.0))\n            corners_global = boxes.corners.numpy().reshape(-1, 3)\n            corners_global = np.concatenate(\n                [corners_global,\n                 np.ones([corners_global.shape[0], 1])],\n                axis=1)\n            l2g = get_lidar2global(infos)\n            corners_lidar = corners_global @ np.linalg.inv(l2g).T\n            corners_lidar = corners_lidar[:, :3]\n        pred_flag = np.ones((corners_lidar.shape[0] // 8, ), dtype=np.bool)\n        scores = [\n            pred_res[rid]['detection_score'] for rid in range(len(pred_res))\n        ]\n        if args.draw_gt:\n            gt_boxes = infos['gt_boxes']\n            gt_boxes[:, -1] = gt_boxes[:, -1] + np.pi / 2\n            width = gt_boxes[:, 4].copy()\n            gt_boxes[:, 4] = gt_boxes[:, 3]\n            gt_boxes[:, 3] = width\n            corners_lidar_gt = \\\n                LB(infos['gt_boxes'],\n                   origin=(0.5, 0.5, 0.5)).corners.numpy().reshape(-1, 3)\n            corners_lidar = np.concatenate([corners_lidar, corners_lidar_gt],\n                                           axis=0)\n            gt_flag = np.ones((corners_lidar_gt.shape[0] // 8), dtype=np.bool)\n            pred_flag = np.concatenate(\n                [pred_flag, np.logical_not(gt_flag)], axis=0)\n            scores = scores + [0 for _ in range(infos['gt_boxes'].shape[0])]\n        scores = np.array(scores, dtype=np.float32)\n        sort_ids = np.argsort(scores)\n\n        # image view\n        imgs = []\n        for view in views:\n            img = cv2.imread(infos['cams'][view]['data_path'])\n            # draw instances\n            corners_img, valid = lidar2img(corners_lidar, infos['cams'][view])\n            valid = np.logical_and(\n                valid,\n                check_point_in_img(corners_img, img.shape[0], img.shape[1]))\n            valid = valid.reshape(-1, 8)\n            corners_img = corners_img.reshape(-1, 8, 2).astype(np.int)\n            for aid in range(valid.shape[0]):\n                for index in draw_boxes_indexes_img_view:\n                    if valid[aid, index[0]] and valid[aid, index[1]]:\n                        cv2.line(\n                            img,\n                            corners_img[aid, index[0]],\n                            corners_img[aid, index[1]],\n                            color=color_map[int(pred_flag[aid])],\n                            thickness=scale_factor)\n            imgs.append(img)\n\n        # bird-eye-view\n        canvas = np.zeros((int(canva_size), int(canva_size), 3),\n                          dtype=np.uint8)\n        # draw lidar points\n        lidar_points = np.fromfile(infos['lidar_path'], dtype=np.float32)\n        lidar_points = lidar_points.reshape(-1, 5)[:, :3]\n        lidar_points[:, 1] = -lidar_points[:, 1]\n        lidar_points[:, :2] = \\\n            (lidar_points[:, :2] + show_range) / show_range / 2.0 * canva_size\n        for p in lidar_points:\n            if check_point_in_img(\n                    p.reshape(1, 3), canvas.shape[1], canvas.shape[0])[0]:\n                color = depth2color(p[2])\n                cv2.circle(\n                    canvas, (int(p[0]), int(p[1])),\n                    radius=0,\n                    color=color,\n                    thickness=1)\n\n        # draw instances\n        corners_lidar = corners_lidar.reshape(-1, 8, 3)\n        corners_lidar[:, :, 1] = -corners_lidar[:, :, 1]\n        bottom_corners_bev = corners_lidar[:, [0, 3, 7, 4], :2]\n        bottom_corners_bev = \\\n            (bottom_corners_bev + show_range) / show_range / 2.0 * canva_size\n        bottom_corners_bev = np.round(bottom_corners_bev).astype(np.int32)\n        center_bev = corners_lidar[:, [0, 3, 7, 4], :2].mean(axis=1)\n        head_bev = corners_lidar[:, [0, 4], :2].mean(axis=1)\n        canter_canvas = \\\n            (center_bev + show_range) / show_range / 2.0 * canva_size\n        center_canvas = canter_canvas.astype(np.int32)\n        head_canvas = (head_bev + show_range) / show_range / 2.0 * canva_size\n        head_canvas = head_canvas.astype(np.int32)\n\n        for rid in sort_ids:\n            score = scores[rid]\n            if score < args.vis_thred and pred_flag[rid]:\n                continue\n            score = min(score * 2.0, 1.0) if pred_flag[rid] else 1.0\n            color = color_map[int(pred_flag[rid])]\n            for index in draw_boxes_indexes_bev:\n                cv2.line(\n                    canvas,\n                    bottom_corners_bev[rid, index[0]],\n                    bottom_corners_bev[rid, index[1]],\n                    [color[0] * score, color[1] * score, color[2] * score],\n                    thickness=1)\n            cv2.line(\n                canvas,\n                center_canvas[rid],\n                head_canvas[rid],\n                [color[0] * score, color[1] * score, color[2] * score],\n                1,\n                lineType=8)\n\n        # fuse image-view and bev\n        img = np.zeros((900 * 2 + canva_size * scale_factor, 1600 * 3, 3),\n                       dtype=np.uint8)\n        img[:900, :, :] = np.concatenate(imgs[:3], axis=1)\n        img_back = np.concatenate(\n            [imgs[3][:, ::-1, :], imgs[4][:, ::-1, :], imgs[5][:, ::-1, :]],\n            axis=1)\n        img[900 + canva_size * scale_factor:, :, :] = img_back\n        img = cv2.resize(img, (int(1600 / scale_factor * 3),\n                               int(900 / scale_factor * 2 + canva_size)))\n        w_begin = int((1600 * 3 / scale_factor - canva_size) // 2)\n        img[int(900 / scale_factor):int(900 / scale_factor) + canva_size,\n            w_begin:w_begin + canva_size, :] = canvas\n\n        if args.format == 'image':\n            cv2.imwrite(os.path.join(vis_dir, '%s.jpg' % infos['token']), img)\n        elif args.format == 'video':\n            vout.write(img)\n    if args.format == 'video':\n        vout.release()\n\n\nif __name__ == '__main__':\n    main()\n"
        }
    ]
}