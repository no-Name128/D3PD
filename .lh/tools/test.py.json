{
    "sourceFile": "tools/test.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 3,
            "patches": [
                {
                    "date": 1734358353628,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1740720920007,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -116,8 +116,13 @@\n         choices=['none', 'pytorch', 'slurm', 'mpi'],\n         default='none',\n         help='job launcher')\n     parser.add_argument('--local_rank', type=int, default=0)\n+    \n+    parser.add_argument(\n+        '--vis-path',\n+        default='none',\n+        help='vis images path',required=True)\n     args = parser.parse_args()\n     if 'LOCAL_RANK' not in os.environ:\n         os.environ['LOCAL_RANK'] = str(args.local_rank)\n \n"
                },
                {
                    "date": 1740720920952,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,19 +7,18 @@\n import torch\n from mmcv import Config, DictAction\n from mmcv.cnn import fuse_conv_bn\n from mmcv.parallel import MMDataParallel, MMDistributedDataParallel\n-from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,\n-                         wrap_fp16_model)\n+from mmcv.runner import get_dist_info, init_dist, load_checkpoint, wrap_fp16_model\n \n import mmdet\n from mmdet3d.apis import single_gpu_test\n from mmdet3d.datasets import build_dataloader, build_dataset\n from mmdet3d.models import build_model\n from mmdet.apis import multi_gpu_test, set_random_seed\n from mmdet.datasets import replace_ImageToTensor\n \n-if mmdet.__version__ > '2.23.0':\n+if mmdet.__version__ > \"2.23.0\":\n     # If mmdet version > 2.23.0, setup_multi_processes would be imported and\n     # used from mmdet instead of mmdet3d.\n     from mmdet.utils import setup_multi_processes\n else:\n@@ -33,124 +32,132 @@\n     from mmdet3d.utils import compat_cfg\n \n \n def parse_args():\n-    parser = argparse.ArgumentParser(\n-        description='MMDet test (and eval) a model')\n-    parser.add_argument('config', help='test config file path')\n-    parser.add_argument('checkpoint', help='checkpoint file')\n-    parser.add_argument('--out', help='output result file in pickle format')\n+    parser = argparse.ArgumentParser(description=\"MMDet test (and eval) a model\")\n+    parser.add_argument(\"config\", help=\"test config file path\")\n+    parser.add_argument(\"checkpoint\", help=\"checkpoint file\")\n+    parser.add_argument(\"--out\", help=\"output result file in pickle format\")\n     parser.add_argument(\n-        '--fuse-conv-bn',\n-        action='store_true',\n-        help='Whether to fuse conv and bn, this will slightly increase'\n-        'the inference speed')\n+        \"--fuse-conv-bn\",\n+        action=\"store_true\",\n+        help=\"Whether to fuse conv and bn, this will slightly increase\"\n+        \"the inference speed\",\n+    )\n     parser.add_argument(\n-        '--gpu-ids',\n+        \"--gpu-ids\",\n         type=int,\n-        nargs='+',\n-        help='(Deprecated, please use --gpu-id) ids of gpus to use '\n-        '(only applicable to non-distributed training)')\n+        nargs=\"+\",\n+        help=\"(Deprecated, please use --gpu-id) ids of gpus to use \"\n+        \"(only applicable to non-distributed training)\",\n+    )\n     parser.add_argument(\n-        '--gpu-id',\n+        \"--gpu-id\",\n         type=int,\n         default=0,\n-        help='id of gpu to use '\n-        '(only applicable to non-distributed testing)')\n+        help=\"id of gpu to use \" \"(only applicable to non-distributed testing)\",\n+    )\n     parser.add_argument(\n-        '--format-only',\n-        action='store_true',\n-        help='Format the output results without perform evaluation. It is'\n-        'useful when you want to format the result to a specific format and '\n-        'submit it to the test server')\n+        \"--format-only\",\n+        action=\"store_true\",\n+        help=\"Format the output results without perform evaluation. It is\"\n+        \"useful when you want to format the result to a specific format and \"\n+        \"submit it to the test server\",\n+    )\n     parser.add_argument(\n-        '--eval',\n+        \"--eval\",\n         type=str,\n-        nargs='+',\n+        nargs=\"+\",\n         help='evaluation metrics, which depends on the dataset, e.g., \"bbox\",'\n-        ' \"segm\", \"proposal\" for COCO, and \"mAP\", \"recall\" for PASCAL VOC')\n-    parser.add_argument('--show', action='store_true', help='show results')\n+        ' \"segm\", \"proposal\" for COCO, and \"mAP\", \"recall\" for PASCAL VOC',\n+    )\n+    parser.add_argument(\"--show\", action=\"store_true\", help=\"show results\")\n+    parser.add_argument(\"--show-dir\", help=\"directory where results will be saved\")\n     parser.add_argument(\n-        '--show-dir', help='directory where results will be saved')\n+        \"--gpu-collect\",\n+        action=\"store_true\",\n+        help=\"whether to use gpu to collect results.\",\n+    )\n     parser.add_argument(\n-        '--gpu-collect',\n-        action='store_true',\n-        help='whether to use gpu to collect results.')\n+        \"--no-aavt\", action=\"store_true\", help=\"Do not align after view transformer.\"\n+    )\n     parser.add_argument(\n-        '--no-aavt',\n-        action='store_true',\n-        help='Do not align after view transformer.')\n+        \"--tmpdir\",\n+        help=\"tmp directory used for collecting results from multiple \"\n+        \"workers, available when gpu-collect is not specified\",\n+    )\n+    parser.add_argument(\"--seed\", type=int, default=0, help=\"random seed\")\n     parser.add_argument(\n-        '--tmpdir',\n-        help='tmp directory used for collecting results from multiple '\n-        'workers, available when gpu-collect is not specified')\n-    parser.add_argument('--seed', type=int, default=0, help='random seed')\n+        \"--deterministic\",\n+        action=\"store_true\",\n+        help=\"whether to set deterministic options for CUDNN backend.\",\n+    )\n     parser.add_argument(\n-        '--deterministic',\n-        action='store_true',\n-        help='whether to set deterministic options for CUDNN backend.')\n-    parser.add_argument(\n-        '--cfg-options',\n-        nargs='+',\n+        \"--cfg-options\",\n+        nargs=\"+\",\n         action=DictAction,\n-        help='override some settings in the used config, the key-value pair '\n-        'in xxx=yyy format will be merged into config file. If the value to '\n+        help=\"override some settings in the used config, the key-value pair \"\n+        \"in xxx=yyy format will be merged into config file. If the value to \"\n         'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n         'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n-        'Note that the quotation marks are necessary and that no white space '\n-        'is allowed.')\n+        \"Note that the quotation marks are necessary and that no white space \"\n+        \"is allowed.\",\n+    )\n     parser.add_argument(\n-        '--options',\n-        nargs='+',\n+        \"--options\",\n+        nargs=\"+\",\n         action=DictAction,\n-        help='custom options for evaluation, the key-value pair in xxx=yyy '\n-        'format will be kwargs for dataset.evaluate() function (deprecate), '\n-        'change to --eval-options instead.')\n+        help=\"custom options for evaluation, the key-value pair in xxx=yyy \"\n+        \"format will be kwargs for dataset.evaluate() function (deprecate), \"\n+        \"change to --eval-options instead.\",\n+    )\n     parser.add_argument(\n-        '--eval-options',\n-        nargs='+',\n+        \"--eval-options\",\n+        nargs=\"+\",\n         action=DictAction,\n-        help='custom options for evaluation, the key-value pair in xxx=yyy '\n-        'format will be kwargs for dataset.evaluate() function')\n+        help=\"custom options for evaluation, the key-value pair in xxx=yyy \"\n+        \"format will be kwargs for dataset.evaluate() function\",\n+    )\n     parser.add_argument(\n-        '--launcher',\n-        choices=['none', 'pytorch', 'slurm', 'mpi'],\n-        default='none',\n-        help='job launcher')\n-    parser.add_argument('--local_rank', type=int, default=0)\n-    \n+        \"--launcher\",\n+        choices=[\"none\", \"pytorch\", \"slurm\", \"mpi\"],\n+        default=\"none\",\n+        help=\"job launcher\",\n+    )\n+    parser.add_argument(\"--local_rank\", type=int, default=0)\n+\n     parser.add_argument(\n-        '--vis-path',\n-        default='none',\n-        help='vis images path',required=True)\n+        \"--vis-path\", default=\"none\", help=\"vis images path\", required=True\n+    )\n     args = parser.parse_args()\n-    if 'LOCAL_RANK' not in os.environ:\n-        os.environ['LOCAL_RANK'] = str(args.local_rank)\n+    if \"LOCAL_RANK\" not in os.environ:\n+        os.environ[\"LOCAL_RANK\"] = str(args.local_rank)\n \n     if args.options and args.eval_options:\n         raise ValueError(\n-            '--options and --eval-options cannot be both specified, '\n-            '--options is deprecated in favor of --eval-options')\n+            \"--options and --eval-options cannot be both specified, \"\n+            \"--options is deprecated in favor of --eval-options\"\n+        )\n     if args.options:\n-        warnings.warn('--options is deprecated in favor of --eval-options')\n+        warnings.warn(\"--options is deprecated in favor of --eval-options\")\n         args.eval_options = args.options\n     return args\n \n \n def main():\n     args = parse_args()\n \n-    assert args.out or args.eval or args.format_only or args.show \\\n-        or args.show_dir, \\\n-        ('Please specify at least one operation (save/eval/format/show the '\n-         'results / save the results) with the argument \"--out\", \"--eval\"'\n-         ', \"--format-only\", \"--show\" or \"--show-dir\"')\n+    assert args.out or args.eval or args.format_only or args.show or args.show_dir, (\n+        \"Please specify at least one operation (save/eval/format/show the \"\n+        'results / save the results) with the argument \"--out\", \"--eval\"'\n+        ', \"--format-only\", \"--show\" or \"--show-dir\"'\n+    )\n \n     if args.eval and args.format_only:\n-        raise ValueError('--eval and --format_only cannot be both specified')\n+        raise ValueError(\"--eval and --format_only cannot be both specified\")\n \n-    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):\n-        raise ValueError('The output file must be a pkl file.')\n+    if args.out is not None and not args.out.endswith((\".pkl\", \".pickle\")):\n+        raise ValueError(\"The output file must be a pkl file.\")\n \n     cfg = Config.fromfile(args.config)\n     if args.cfg_options is not None:\n         cfg.merge_from_dict(args.cfg_options)\n@@ -160,49 +167,51 @@\n     # set multi-process settings\n     setup_multi_processes(cfg)\n \n     # set cudnn_benchmark\n-    if cfg.get('cudnn_benchmark', False):\n+    if cfg.get(\"cudnn_benchmark\", False):\n         torch.backends.cudnn.benchmark = True\n \n     cfg.model.pretrained = None\n \n     if args.gpu_ids is not None:\n         cfg.gpu_ids = args.gpu_ids[0:1]\n-        warnings.warn('`--gpu-ids` is deprecated, please use `--gpu-id`. '\n-                      'Because we only support single GPU mode in '\n-                      'non-distributed testing. Use the first GPU '\n-                      'in `gpu_ids` now.')\n+        warnings.warn(\n+            \"`--gpu-ids` is deprecated, please use `--gpu-id`. \"\n+            \"Because we only support single GPU mode in \"\n+            \"non-distributed testing. Use the first GPU \"\n+            \"in `gpu_ids` now.\"\n+        )\n     else:\n         cfg.gpu_ids = [args.gpu_id]\n \n     # init distributed env first, since logger depends on the dist info.\n-    if args.launcher == 'none':\n+    if args.launcher == \"none\":\n         distributed = False\n     else:\n         distributed = True\n         init_dist(args.launcher, **cfg.dist_params)\n \n     test_dataloader_default_args = dict(\n-        samples_per_gpu=1, workers_per_gpu=2, dist=distributed, shuffle=False)\n+        samples_per_gpu=1, workers_per_gpu=2, dist=distributed, shuffle=False\n+    )\n \n     # in case the test dataset is concatenated\n     if isinstance(cfg.data.test, dict):\n         cfg.data.test.test_mode = True\n-        if cfg.data.test_dataloader.get('samples_per_gpu', 1) > 1:\n+        if cfg.data.test_dataloader.get(\"samples_per_gpu\", 1) > 1:\n             # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n-            cfg.data.test.pipeline = replace_ImageToTensor(\n-                cfg.data.test.pipeline)\n+            cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)\n     elif isinstance(cfg.data.test, list):\n         for ds_cfg in cfg.data.test:\n             ds_cfg.test_mode = True\n-        if cfg.data.test_dataloader.get('samples_per_gpu', 1) > 1:\n+        if cfg.data.test_dataloader.get(\"samples_per_gpu\", 1) > 1:\n             for ds_cfg in cfg.data.test:\n                 ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n \n     test_loader_cfg = {\n         **test_dataloader_default_args,\n-        **cfg.data.get('test_dataloader', {})\n+        **cfg.data.get(\"test_dataloader\", {}),\n     }\n \n     # set random seeds\n     if args.seed is not None:\n@@ -213,30 +222,30 @@\n     data_loader = build_dataloader(dataset, **test_loader_cfg)\n \n     # build the model and load checkpoint\n     if not args.no_aavt:\n-        if '4D' in cfg.model.type:\n-            cfg.model.align_after_view_transfromation=True\n-    if 'num_proposals_test' in cfg and cfg.model.type=='DAL':\n-        cfg.model.pts_bbox_head.num_proposals=cfg.num_proposals_test\n+        if \"4D\" in cfg.model.type:\n+            cfg.model.align_after_view_transfromation = True\n+    if \"num_proposals_test\" in cfg and cfg.model.type == \"DAL\":\n+        cfg.model.pts_bbox_head.num_proposals = cfg.num_proposals_test\n     cfg.model.train_cfg = None\n-    model = build_model(cfg.model, test_cfg=cfg.get('test_cfg'))\n-    fp16_cfg = cfg.get('fp16', None)\n+    model = build_model(cfg.model, test_cfg=cfg.get(\"test_cfg\"))\n+    fp16_cfg = cfg.get(\"fp16\", None)\n     if fp16_cfg is not None:\n         wrap_fp16_model(model)\n-    checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')\n+    checkpoint = load_checkpoint(model, args.checkpoint, map_location=\"cpu\")\n     if args.fuse_conv_bn:\n         model = fuse_conv_bn(model)\n     # old versions did not save class info in checkpoints, this walkaround is\n     # for backward compatibility\n-    if 'CLASSES' in checkpoint.get('meta', {}):\n-        model.CLASSES = checkpoint['meta']['CLASSES']\n+    if \"CLASSES\" in checkpoint.get(\"meta\", {}):\n+        model.CLASSES = checkpoint[\"meta\"][\"CLASSES\"]\n     else:\n         model.CLASSES = dataset.CLASSES\n     # palette for visualization in segmentation tasks\n-    if 'PALETTE' in checkpoint.get('meta', {}):\n-        model.PALETTE = checkpoint['meta']['PALETTE']\n-    elif hasattr(dataset, 'PALETTE'):\n+    if \"PALETTE\" in checkpoint.get(\"meta\", {}):\n+        model.PALETTE = checkpoint[\"meta\"][\"PALETTE\"]\n+    elif hasattr(dataset, \"PALETTE\"):\n         # segmentation dataset has `PALETTE` attribute\n         model.PALETTE = dataset.PALETTE\n \n     if not distributed:\n@@ -245,30 +254,34 @@\n     else:\n         model = MMDistributedDataParallel(\n             model.cuda(),\n             device_ids=[torch.cuda.current_device()],\n-            broadcast_buffers=False)\n-        outputs = multi_gpu_test(model, data_loader, args.tmpdir,\n-                                 args.gpu_collect)\n+            broadcast_buffers=False,\n+        )\n+        outputs = multi_gpu_test(model, data_loader, args.tmpdir, args.gpu_collect)\n \n     rank, _ = get_dist_info()\n     if rank == 0:\n         if args.out:\n-            print(f'\\nwriting results to {args.out}')\n+            print(f\"\\nwriting results to {args.out}\")\n             mmcv.dump(outputs, args.out)\n         kwargs = {} if args.eval_options is None else args.eval_options\n         if args.format_only:\n             dataset.format_results(outputs, **kwargs)\n         if args.eval:\n-            eval_kwargs = cfg.get('evaluation', {}).copy()\n+            eval_kwargs = cfg.get(\"evaluation\", {}).copy()\n             # hard-code way to remove EvalHook args\n             for key in [\n-                    'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best',\n-                    'rule'\n+                \"interval\",\n+                \"tmpdir\",\n+                \"start\",\n+                \"gpu_collect\",\n+                \"save_best\",\n+                \"rule\",\n             ]:\n                 eval_kwargs.pop(key, None)\n             eval_kwargs.update(dict(metric=args.eval, **kwargs))\n             print(dataset.evaluate(outputs, **eval_kwargs))\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     main()\n"
                },
                {
                    "date": 1740720992078,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -124,11 +124,8 @@\n         help=\"job launcher\",\n     )\n     parser.add_argument(\"--local_rank\", type=int, default=0)\n \n-    parser.add_argument(\n-        \"--vis-path\", default=\"none\", help=\"vis images path\", required=True\n-    )\n     args = parser.parse_args()\n     if \"LOCAL_RANK\" not in os.environ:\n         os.environ[\"LOCAL_RANK\"] = str(args.local_rank)\n \n"
                }
            ],
            "date": 1734358353628,
            "name": "Commit-0",
            "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport argparse\nimport os\nimport warnings\n\nimport mmcv\nimport torch\nfrom mmcv import Config, DictAction\nfrom mmcv.cnn import fuse_conv_bn\nfrom mmcv.parallel import MMDataParallel, MMDistributedDataParallel\nfrom mmcv.runner import (get_dist_info, init_dist, load_checkpoint,\n                         wrap_fp16_model)\n\nimport mmdet\nfrom mmdet3d.apis import single_gpu_test\nfrom mmdet3d.datasets import build_dataloader, build_dataset\nfrom mmdet3d.models import build_model\nfrom mmdet.apis import multi_gpu_test, set_random_seed\nfrom mmdet.datasets import replace_ImageToTensor\n\nif mmdet.__version__ > '2.23.0':\n    # If mmdet version > 2.23.0, setup_multi_processes would be imported and\n    # used from mmdet instead of mmdet3d.\n    from mmdet.utils import setup_multi_processes\nelse:\n    from mmdet3d.utils import setup_multi_processes\n\ntry:\n    # If mmdet version > 2.23.0, compat_cfg would be imported and\n    # used from mmdet instead of mmdet3d.\n    from mmdet.utils import compat_cfg\nexcept ImportError:\n    from mmdet3d.utils import compat_cfg\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='MMDet test (and eval) a model')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('checkpoint', help='checkpoint file')\n    parser.add_argument('--out', help='output result file in pickle format')\n    parser.add_argument(\n        '--fuse-conv-bn',\n        action='store_true',\n        help='Whether to fuse conv and bn, this will slightly increase'\n        'the inference speed')\n    parser.add_argument(\n        '--gpu-ids',\n        type=int,\n        nargs='+',\n        help='(Deprecated, please use --gpu-id) ids of gpus to use '\n        '(only applicable to non-distributed training)')\n    parser.add_argument(\n        '--gpu-id',\n        type=int,\n        default=0,\n        help='id of gpu to use '\n        '(only applicable to non-distributed testing)')\n    parser.add_argument(\n        '--format-only',\n        action='store_true',\n        help='Format the output results without perform evaluation. It is'\n        'useful when you want to format the result to a specific format and '\n        'submit it to the test server')\n    parser.add_argument(\n        '--eval',\n        type=str,\n        nargs='+',\n        help='evaluation metrics, which depends on the dataset, e.g., \"bbox\",'\n        ' \"segm\", \"proposal\" for COCO, and \"mAP\", \"recall\" for PASCAL VOC')\n    parser.add_argument('--show', action='store_true', help='show results')\n    parser.add_argument(\n        '--show-dir', help='directory where results will be saved')\n    parser.add_argument(\n        '--gpu-collect',\n        action='store_true',\n        help='whether to use gpu to collect results.')\n    parser.add_argument(\n        '--no-aavt',\n        action='store_true',\n        help='Do not align after view transformer.')\n    parser.add_argument(\n        '--tmpdir',\n        help='tmp directory used for collecting results from multiple '\n        'workers, available when gpu-collect is not specified')\n    parser.add_argument('--seed', type=int, default=0, help='random seed')\n    parser.add_argument(\n        '--deterministic',\n        action='store_true',\n        help='whether to set deterministic options for CUDNN backend.')\n    parser.add_argument(\n        '--cfg-options',\n        nargs='+',\n        action=DictAction,\n        help='override some settings in the used config, the key-value pair '\n        'in xxx=yyy format will be merged into config file. If the value to '\n        'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n        'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n        'Note that the quotation marks are necessary and that no white space '\n        'is allowed.')\n    parser.add_argument(\n        '--options',\n        nargs='+',\n        action=DictAction,\n        help='custom options for evaluation, the key-value pair in xxx=yyy '\n        'format will be kwargs for dataset.evaluate() function (deprecate), '\n        'change to --eval-options instead.')\n    parser.add_argument(\n        '--eval-options',\n        nargs='+',\n        action=DictAction,\n        help='custom options for evaluation, the key-value pair in xxx=yyy '\n        'format will be kwargs for dataset.evaluate() function')\n    parser.add_argument(\n        '--launcher',\n        choices=['none', 'pytorch', 'slurm', 'mpi'],\n        default='none',\n        help='job launcher')\n    parser.add_argument('--local_rank', type=int, default=0)\n    args = parser.parse_args()\n    if 'LOCAL_RANK' not in os.environ:\n        os.environ['LOCAL_RANK'] = str(args.local_rank)\n\n    if args.options and args.eval_options:\n        raise ValueError(\n            '--options and --eval-options cannot be both specified, '\n            '--options is deprecated in favor of --eval-options')\n    if args.options:\n        warnings.warn('--options is deprecated in favor of --eval-options')\n        args.eval_options = args.options\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    assert args.out or args.eval or args.format_only or args.show \\\n        or args.show_dir, \\\n        ('Please specify at least one operation (save/eval/format/show the '\n         'results / save the results) with the argument \"--out\", \"--eval\"'\n         ', \"--format-only\", \"--show\" or \"--show-dir\"')\n\n    if args.eval and args.format_only:\n        raise ValueError('--eval and --format_only cannot be both specified')\n\n    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):\n        raise ValueError('The output file must be a pkl file.')\n\n    cfg = Config.fromfile(args.config)\n    if args.cfg_options is not None:\n        cfg.merge_from_dict(args.cfg_options)\n\n    cfg = compat_cfg(cfg)\n\n    # set multi-process settings\n    setup_multi_processes(cfg)\n\n    # set cudnn_benchmark\n    if cfg.get('cudnn_benchmark', False):\n        torch.backends.cudnn.benchmark = True\n\n    cfg.model.pretrained = None\n\n    if args.gpu_ids is not None:\n        cfg.gpu_ids = args.gpu_ids[0:1]\n        warnings.warn('`--gpu-ids` is deprecated, please use `--gpu-id`. '\n                      'Because we only support single GPU mode in '\n                      'non-distributed testing. Use the first GPU '\n                      'in `gpu_ids` now.')\n    else:\n        cfg.gpu_ids = [args.gpu_id]\n\n    # init distributed env first, since logger depends on the dist info.\n    if args.launcher == 'none':\n        distributed = False\n    else:\n        distributed = True\n        init_dist(args.launcher, **cfg.dist_params)\n\n    test_dataloader_default_args = dict(\n        samples_per_gpu=1, workers_per_gpu=2, dist=distributed, shuffle=False)\n\n    # in case the test dataset is concatenated\n    if isinstance(cfg.data.test, dict):\n        cfg.data.test.test_mode = True\n        if cfg.data.test_dataloader.get('samples_per_gpu', 1) > 1:\n            # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n            cfg.data.test.pipeline = replace_ImageToTensor(\n                cfg.data.test.pipeline)\n    elif isinstance(cfg.data.test, list):\n        for ds_cfg in cfg.data.test:\n            ds_cfg.test_mode = True\n        if cfg.data.test_dataloader.get('samples_per_gpu', 1) > 1:\n            for ds_cfg in cfg.data.test:\n                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n\n    test_loader_cfg = {\n        **test_dataloader_default_args,\n        **cfg.data.get('test_dataloader', {})\n    }\n\n    # set random seeds\n    if args.seed is not None:\n        set_random_seed(args.seed, deterministic=args.deterministic)\n\n    # build the dataloader\n    dataset = build_dataset(cfg.data.test)\n    data_loader = build_dataloader(dataset, **test_loader_cfg)\n\n    # build the model and load checkpoint\n    if not args.no_aavt:\n        if '4D' in cfg.model.type:\n            cfg.model.align_after_view_transfromation=True\n    if 'num_proposals_test' in cfg and cfg.model.type=='DAL':\n        cfg.model.pts_bbox_head.num_proposals=cfg.num_proposals_test\n    cfg.model.train_cfg = None\n    model = build_model(cfg.model, test_cfg=cfg.get('test_cfg'))\n    fp16_cfg = cfg.get('fp16', None)\n    if fp16_cfg is not None:\n        wrap_fp16_model(model)\n    checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')\n    if args.fuse_conv_bn:\n        model = fuse_conv_bn(model)\n    # old versions did not save class info in checkpoints, this walkaround is\n    # for backward compatibility\n    if 'CLASSES' in checkpoint.get('meta', {}):\n        model.CLASSES = checkpoint['meta']['CLASSES']\n    else:\n        model.CLASSES = dataset.CLASSES\n    # palette for visualization in segmentation tasks\n    if 'PALETTE' in checkpoint.get('meta', {}):\n        model.PALETTE = checkpoint['meta']['PALETTE']\n    elif hasattr(dataset, 'PALETTE'):\n        # segmentation dataset has `PALETTE` attribute\n        model.PALETTE = dataset.PALETTE\n\n    if not distributed:\n        model = MMDataParallel(model, device_ids=cfg.gpu_ids)\n        outputs = single_gpu_test(model, data_loader, args.show, args.show_dir)\n    else:\n        model = MMDistributedDataParallel(\n            model.cuda(),\n            device_ids=[torch.cuda.current_device()],\n            broadcast_buffers=False)\n        outputs = multi_gpu_test(model, data_loader, args.tmpdir,\n                                 args.gpu_collect)\n\n    rank, _ = get_dist_info()\n    if rank == 0:\n        if args.out:\n            print(f'\\nwriting results to {args.out}')\n            mmcv.dump(outputs, args.out)\n        kwargs = {} if args.eval_options is None else args.eval_options\n        if args.format_only:\n            dataset.format_results(outputs, **kwargs)\n        if args.eval:\n            eval_kwargs = cfg.get('evaluation', {}).copy()\n            # hard-code way to remove EvalHook args\n            for key in [\n                    'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best',\n                    'rule'\n            ]:\n                eval_kwargs.pop(key, None)\n            eval_kwargs.update(dict(metric=args.eval, **kwargs))\n            print(dataset.evaluate(outputs, **eval_kwargs))\n\n\nif __name__ == '__main__':\n    main()\n"
        }
    ]
}