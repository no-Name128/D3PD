{
    "sourceFile": "tools/data_converter/nuscenes_converter.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 12,
            "patches": [
                {
                    "date": 1716001178524,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1716001278359,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -208,9 +208,55 @@\n             cam_info = obtain_sensor2top(nusc, cam_token, l2e_t, l2e_r_mat,\n                                          e2g_t, e2g_r_mat, cam)\n             cam_info.update(cam_intrinsic=cam_intrinsic)\n             info['cams'].update({cam: cam_info})\n+            \n+        \"\"\" Add radar information to the annotation read file \"\"\"\n+        radar_names = [\n+            \"RADAR_FRONT\",\n+            \"RADAR_FRONT_LEFT\",\n+            \"RADAR_FRONT_RIGHT\",\n+            \"RADAR_BACK_LEFT\",\n+            \"RADAR_BACK_RIGHT\",\n+        ]\n \n+        for radar_name in radar_names:\n+            radar_token = sample[\"data\"][radar_name]\n+            radar_rec = nusc.get(\"sample_data\", radar_token)\n+            sweeps = []\n+\n+            while len(sweeps) < 5:\n+                if not radar_rec[\"prev\"] == \"\":\n+                    radar_path, _, radar_intrin = nusc.get_sample_data(radar_token)\n+\n+                    radar_info = obtain_sensor2top(\n+                        nusc,\n+                        radar_token,\n+                        l2e_t,\n+                        l2e_r_mat,\n+                        e2g_t,\n+                        e2g_r_mat,\n+                        radar_name,\n+                    )\n+                    sweeps.append(radar_info)\n+                    radar_token = radar_rec[\"prev\"]\n+                    radar_rec = nusc.get(\"sample_data\", radar_token)\n+                else:\n+                    radar_path, _, radar_intrin = nusc.get_sample_data(radar_token)\n+\n+                    radar_info = obtain_sensor2top(\n+                        nusc,\n+                        radar_token,\n+                        l2e_t,\n+                        l2e_r_mat,\n+                        e2g_t,\n+                        e2g_r_mat,\n+                        radar_name,\n+                    )\n+                    sweeps.append(radar_info)\n+\n+            info[\"radars\"].update({radar_name: sweeps})\n+\n         # obtain sweeps for a single key-frame\n         sd_rec = nusc.get('sample_data', sample['data']['LIDAR_TOP'])\n         sweeps = []\n         while len(sweeps) < max_sweeps:\n"
                },
                {
                    "date": 1716001288976,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -209,9 +209,9 @@\n                                          e2g_t, e2g_r_mat, cam)\n             cam_info.update(cam_intrinsic=cam_intrinsic)\n             info['cams'].update({cam: cam_info})\n             \n-        \"\"\" Add radar information to the annotation read file \"\"\"\n+        # Add radar information to the annotation read file\n         radar_names = [\n             \"RADAR_FRONT\",\n             \"RADAR_FRONT_LEFT\",\n             \"RADAR_FRONT_RIGHT\",\n"
                },
                {
                    "date": 1716001359038,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -255,8 +255,9 @@\n                     sweeps.append(radar_info)\n \n             info[\"radars\"].update({radar_name: sweeps})\n \n+\n         # obtain sweeps for a single key-frame\n         sd_rec = nusc.get('sample_data', sample['data']['LIDAR_TOP'])\n         sweeps = []\n         while len(sweeps) < max_sweeps:\n"
                },
                {
                    "date": 1716009934330,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -178,8 +178,9 @@\n             'lidar_path': lidar_path,\n             'token': sample['token'],\n             'sweeps': [],\n             'cams': dict(),\n+            \"radars\": dict(),\n             'lidar2ego_translation': cs_record['translation'],\n             'lidar2ego_rotation': cs_record['rotation'],\n             'ego2global_translation': pose_record['translation'],\n             'ego2global_rotation': pose_record['rotation'],\n"
                },
                {
                    "date": 1716009944027,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -13,22 +13,37 @@\n \n from mmdet3d.core.bbox import points_cam2img\n from mmdet3d.datasets import NuScenesDataset\n \n-nus_categories = ('car', 'truck', 'trailer', 'bus', 'construction_vehicle',\n-                  'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',\n-                  'barrier')\n+nus_categories = (\n+    \"car\",\n+    \"truck\",\n+    \"trailer\",\n+    \"bus\",\n+    \"construction_vehicle\",\n+    \"bicycle\",\n+    \"motorcycle\",\n+    \"pedestrian\",\n+    \"traffic_cone\",\n+    \"barrier\",\n+)\n \n-nus_attributes = ('cycle.with_rider', 'cycle.without_rider',\n-                  'pedestrian.moving', 'pedestrian.standing',\n-                  'pedestrian.sitting_lying_down', 'vehicle.moving',\n-                  'vehicle.parked', 'vehicle.stopped', 'None')\n+nus_attributes = (\n+    \"cycle.with_rider\",\n+    \"cycle.without_rider\",\n+    \"pedestrian.moving\",\n+    \"pedestrian.standing\",\n+    \"pedestrian.sitting_lying_down\",\n+    \"vehicle.moving\",\n+    \"vehicle.parked\",\n+    \"vehicle.stopped\",\n+    \"None\",\n+)\n \n \n-def create_nuscenes_infos(root_path,\n-                          info_prefix,\n-                          version='v1.0-trainval',\n-                          max_sweeps=10):\n+def create_nuscenes_infos(\n+    root_path, info_prefix, version=\"v1.0-trainval\", max_sweeps=10\n+):\n     \"\"\"Create info file of nuscene dataset.\n \n     Given the raw data, generate its related info file in pkl format.\n \n@@ -40,65 +55,69 @@\n         max_sweeps (int, optional): Max number of sweeps.\n             Default: 10.\n     \"\"\"\n     from nuscenes.nuscenes import NuScenes\n+\n     nusc = NuScenes(version=version, dataroot=root_path, verbose=True)\n     from nuscenes.utils import splits\n-    available_vers = ['v1.0-trainval', 'v1.0-test', 'v1.0-mini']\n+\n+    available_vers = [\"v1.0-trainval\", \"v1.0-test\", \"v1.0-mini\"]\n     assert version in available_vers\n-    if version == 'v1.0-trainval':\n+    if version == \"v1.0-trainval\":\n         train_scenes = splits.train\n         val_scenes = splits.val\n-    elif version == 'v1.0-test':\n+    elif version == \"v1.0-test\":\n         train_scenes = splits.test\n         val_scenes = []\n-    elif version == 'v1.0-mini':\n+    elif version == \"v1.0-mini\":\n         train_scenes = splits.mini_train\n         val_scenes = splits.mini_val\n     else:\n-        raise ValueError('unknown')\n+        raise ValueError(\"unknown\")\n \n     # filter existing scenes.\n     available_scenes = get_available_scenes(nusc)\n-    available_scene_names = [s['name'] for s in available_scenes]\n-    train_scenes = list(\n-        filter(lambda x: x in available_scene_names, train_scenes))\n+    available_scene_names = [s[\"name\"] for s in available_scenes]\n+    train_scenes = list(filter(lambda x: x in available_scene_names, train_scenes))\n     val_scenes = list(filter(lambda x: x in available_scene_names, val_scenes))\n-    train_scenes = set([\n-        available_scenes[available_scene_names.index(s)]['token']\n-        for s in train_scenes\n-    ])\n-    val_scenes = set([\n-        available_scenes[available_scene_names.index(s)]['token']\n-        for s in val_scenes\n-    ])\n+    train_scenes = set(\n+        [\n+            available_scenes[available_scene_names.index(s)][\"token\"]\n+            for s in train_scenes\n+        ]\n+    )\n+    val_scenes = set(\n+        [available_scenes[available_scene_names.index(s)][\"token\"] for s in val_scenes]\n+    )\n \n-    test = 'test' in version\n+    test = \"test\" in version\n     if test:\n-        print('test scene: {}'.format(len(train_scenes)))\n+        print(\"test scene: {}\".format(len(train_scenes)))\n     else:\n-        print('train scene: {}, val scene: {}'.format(\n-            len(train_scenes), len(val_scenes)))\n+        print(\n+            \"train scene: {}, val scene: {}\".format(len(train_scenes), len(val_scenes))\n+        )\n     train_nusc_infos, val_nusc_infos = _fill_trainval_infos(\n-        nusc, train_scenes, val_scenes, test, max_sweeps=max_sweeps)\n+        nusc, train_scenes, val_scenes, test, max_sweeps=max_sweeps\n+    )\n \n     metadata = dict(version=version)\n     if test:\n-        print('test sample: {}'.format(len(train_nusc_infos)))\n+        print(\"test sample: {}\".format(len(train_nusc_infos)))\n         data = dict(infos=train_nusc_infos, metadata=metadata)\n-        info_path = osp.join(root_path,\n-                             '{}_infos_test.pkl'.format(info_prefix))\n+        info_path = osp.join(root_path, \"{}_infos_test.pkl\".format(info_prefix))\n         mmcv.dump(data, info_path)\n     else:\n-        print('train sample: {}, val sample: {}'.format(\n-            len(train_nusc_infos), len(val_nusc_infos)))\n+        print(\n+            \"train sample: {}, val sample: {}\".format(\n+                len(train_nusc_infos), len(val_nusc_infos)\n+            )\n+        )\n         data = dict(infos=train_nusc_infos, metadata=metadata)\n-        info_path = osp.join(root_path,\n-                             '{}_infos_train.pkl'.format(info_prefix))\n+        info_path = osp.join(root_path, \"{}_infos_train.pkl\".format(info_prefix))\n         mmcv.dump(data, info_path)\n-        data['infos'] = val_nusc_infos\n-        info_val_path = osp.join(root_path,\n-                                 '{}_infos_val.pkl'.format(info_prefix))\n+        data[\"infos\"] = val_nusc_infos\n+        info_val_path = osp.join(root_path, \"{}_infos_val.pkl\".format(info_prefix))\n         mmcv.dump(data, info_val_path)\n \n \n def get_available_scenes(nusc):\n@@ -114,22 +133,22 @@\n         available_scenes (list[dict]): List of basic information for the\n             available scenes.\n     \"\"\"\n     available_scenes = []\n-    print('total scene num: {}'.format(len(nusc.scene)))\n+    print(\"total scene num: {}\".format(len(nusc.scene)))\n     for scene in nusc.scene:\n-        scene_token = scene['token']\n-        scene_rec = nusc.get('scene', scene_token)\n-        sample_rec = nusc.get('sample', scene_rec['first_sample_token'])\n-        sd_rec = nusc.get('sample_data', sample_rec['data']['LIDAR_TOP'])\n+        scene_token = scene[\"token\"]\n+        scene_rec = nusc.get(\"scene\", scene_token)\n+        sample_rec = nusc.get(\"sample\", scene_rec[\"first_sample_token\"])\n+        sd_rec = nusc.get(\"sample_data\", sample_rec[\"data\"][\"LIDAR_TOP\"])\n         has_more_frames = True\n         scene_not_exist = False\n         while has_more_frames:\n-            lidar_path, boxes, _ = nusc.get_sample_data(sd_rec['token'])\n+            lidar_path, boxes, _ = nusc.get_sample_data(sd_rec[\"token\"])\n             lidar_path = str(lidar_path)\n             if os.getcwd() in lidar_path:\n                 # path from lyftdataset is absolute path\n-                lidar_path = lidar_path.split(f'{os.getcwd()}/')[-1]\n+                lidar_path = lidar_path.split(f\"{os.getcwd()}/\")[-1]\n                 # relative path\n             if not mmcv.is_filepath(lidar_path):\n                 scene_not_exist = True\n                 break\n@@ -137,17 +156,13 @@\n                 break\n         if scene_not_exist:\n             continue\n         available_scenes.append(scene)\n-    print('exist scene num: {}'.format(len(available_scenes)))\n+    print(\"exist scene num: {}\".format(len(available_scenes)))\n     return available_scenes\n \n \n-def _fill_trainval_infos(nusc,\n-                         train_scenes,\n-                         val_scenes,\n-                         test=False,\n-                         max_sweeps=10):\n+def _fill_trainval_infos(nusc, train_scenes, val_scenes, test=False, max_sweeps=10):\n     \"\"\"Generate the train/val infos from the raw data.\n \n     Args:\n         nusc (:obj:`NuScenes`): Dataset class in the nuScenes dataset.\n@@ -164,54 +179,54 @@\n     train_nusc_infos = []\n     val_nusc_infos = []\n \n     for sample in mmcv.track_iter_progress(nusc.sample):\n-        lidar_token = sample['data']['LIDAR_TOP']\n-        sd_rec = nusc.get('sample_data', sample['data']['LIDAR_TOP'])\n-        cs_record = nusc.get('calibrated_sensor',\n-                             sd_rec['calibrated_sensor_token'])\n-        pose_record = nusc.get('ego_pose', sd_rec['ego_pose_token'])\n+        lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n+        sd_rec = nusc.get(\"sample_data\", sample[\"data\"][\"LIDAR_TOP\"])\n+        cs_record = nusc.get(\"calibrated_sensor\", sd_rec[\"calibrated_sensor_token\"])\n+        pose_record = nusc.get(\"ego_pose\", sd_rec[\"ego_pose_token\"])\n         lidar_path, boxes, _ = nusc.get_sample_data(lidar_token)\n \n         mmcv.check_file_exist(lidar_path)\n \n         info = {\n-            'lidar_path': lidar_path,\n-            'token': sample['token'],\n-            'sweeps': [],\n-            'cams': dict(),\n+            \"lidar_path\": lidar_path,\n+            \"token\": sample[\"token\"],\n+            \"sweeps\": [],\n+            \"cams\": dict(),\n             \"radars\": dict(),\n-            'lidar2ego_translation': cs_record['translation'],\n-            'lidar2ego_rotation': cs_record['rotation'],\n-            'ego2global_translation': pose_record['translation'],\n-            'ego2global_rotation': pose_record['rotation'],\n-            'timestamp': sample['timestamp'],\n+            \"lidar2ego_translation\": cs_record[\"translation\"],\n+            \"lidar2ego_rotation\": cs_record[\"rotation\"],\n+            \"ego2global_translation\": pose_record[\"translation\"],\n+            \"ego2global_rotation\": pose_record[\"rotation\"],\n+            \"timestamp\": sample[\"timestamp\"],\n         }\n \n-        l2e_r = info['lidar2ego_rotation']\n-        l2e_t = info['lidar2ego_translation']\n-        e2g_r = info['ego2global_rotation']\n-        e2g_t = info['ego2global_translation']\n+        l2e_r = info[\"lidar2ego_rotation\"]\n+        l2e_t = info[\"lidar2ego_translation\"]\n+        e2g_r = info[\"ego2global_rotation\"]\n+        e2g_t = info[\"ego2global_translation\"]\n         l2e_r_mat = Quaternion(l2e_r).rotation_matrix\n         e2g_r_mat = Quaternion(e2g_r).rotation_matrix\n \n         # obtain 6 image's information per frame\n         camera_types = [\n-            'CAM_FRONT',\n-            'CAM_FRONT_RIGHT',\n-            'CAM_FRONT_LEFT',\n-            'CAM_BACK',\n-            'CAM_BACK_LEFT',\n-            'CAM_BACK_RIGHT',\n+            \"CAM_FRONT\",\n+            \"CAM_FRONT_RIGHT\",\n+            \"CAM_FRONT_LEFT\",\n+            \"CAM_BACK\",\n+            \"CAM_BACK_LEFT\",\n+            \"CAM_BACK_RIGHT\",\n         ]\n         for cam in camera_types:\n-            cam_token = sample['data'][cam]\n+            cam_token = sample[\"data\"][cam]\n             cam_path, _, cam_intrinsic = nusc.get_sample_data(cam_token)\n-            cam_info = obtain_sensor2top(nusc, cam_token, l2e_t, l2e_r_mat,\n-                                         e2g_t, e2g_r_mat, cam)\n+            cam_info = obtain_sensor2top(\n+                nusc, cam_token, l2e_t, l2e_r_mat, e2g_t, e2g_r_mat, cam\n+            )\n             cam_info.update(cam_intrinsic=cam_intrinsic)\n-            info['cams'].update({cam: cam_info})\n-            \n+            info[\"cams\"].update({cam: cam_info})\n+\n         # Add radar information to the annotation read file\n         radar_names = [\n             \"RADAR_FRONT\",\n             \"RADAR_FRONT_LEFT\",\n@@ -256,42 +271,45 @@\n                     sweeps.append(radar_info)\n \n             info[\"radars\"].update({radar_name: sweeps})\n \n-\n         # obtain sweeps for a single key-frame\n-        sd_rec = nusc.get('sample_data', sample['data']['LIDAR_TOP'])\n+        sd_rec = nusc.get(\"sample_data\", sample[\"data\"][\"LIDAR_TOP\"])\n         sweeps = []\n         while len(sweeps) < max_sweeps:\n-            if not sd_rec['prev'] == '':\n-                sweep = obtain_sensor2top(nusc, sd_rec['prev'], l2e_t,\n-                                          l2e_r_mat, e2g_t, e2g_r_mat, 'lidar')\n+            if not sd_rec[\"prev\"] == \"\":\n+                sweep = obtain_sensor2top(\n+                    nusc, sd_rec[\"prev\"], l2e_t, l2e_r_mat, e2g_t, e2g_r_mat, \"lidar\"\n+                )\n                 sweeps.append(sweep)\n-                sd_rec = nusc.get('sample_data', sd_rec['prev'])\n+                sd_rec = nusc.get(\"sample_data\", sd_rec[\"prev\"])\n             else:\n                 break\n-        info['sweeps'] = sweeps\n+        info[\"sweeps\"] = sweeps\n         # obtain annotation\n         if not test:\n             annotations = [\n-                nusc.get('sample_annotation', token)\n-                for token in sample['anns']\n+                nusc.get(\"sample_annotation\", token) for token in sample[\"anns\"]\n             ]\n             locs = np.array([b.center for b in boxes]).reshape(-1, 3)\n             dims = np.array([b.wlh for b in boxes]).reshape(-1, 3)\n-            rots = np.array([b.orientation.yaw_pitch_roll[0]\n-                             for b in boxes]).reshape(-1, 1)\n+            rots = np.array([b.orientation.yaw_pitch_roll[0] for b in boxes]).reshape(\n+                -1, 1\n+            )\n             velocity = np.array(\n-                [nusc.box_velocity(token)[:2] for token in sample['anns']])\n+                [nusc.box_velocity(token)[:2] for token in sample[\"anns\"]]\n+            )\n             valid_flag = np.array(\n-                [(anno['num_lidar_pts'] + anno['num_radar_pts']) > 0\n-                 for anno in annotations],\n-                dtype=bool).reshape(-1)\n+                [\n+                    (anno[\"num_lidar_pts\"] + anno[\"num_radar_pts\"]) > 0\n+                    for anno in annotations\n+                ],\n+                dtype=bool,\n+            ).reshape(-1)\n             # convert velo from global to lidar\n             for i in range(len(boxes)):\n                 velo = np.array([*velocity[i], 0.0])\n-                velo = velo @ np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(\n-                    l2e_r_mat).T\n+                velo = velo @ np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n                 velocity[i] = velo[:2]\n \n             names = [b.name for b in boxes]\n             for i in range(len(names)):\n@@ -302,33 +320,28 @@\n             # the format of our lidar coordinate system\n             # which is x_size, y_size, z_size (corresponding to l, w, h)\n             gt_boxes = np.concatenate([locs, dims[:, [1, 0, 2]], rots], axis=1)\n             assert len(gt_boxes) == len(\n-                annotations), f'{len(gt_boxes)}, {len(annotations)}'\n-            info['gt_boxes'] = gt_boxes\n-            info['gt_names'] = names\n-            info['gt_velocity'] = velocity.reshape(-1, 2)\n-            info['num_lidar_pts'] = np.array(\n-                [a['num_lidar_pts'] for a in annotations])\n-            info['num_radar_pts'] = np.array(\n-                [a['num_radar_pts'] for a in annotations])\n-            info['valid_flag'] = valid_flag\n+                annotations\n+            ), f\"{len(gt_boxes)}, {len(annotations)}\"\n+            info[\"gt_boxes\"] = gt_boxes\n+            info[\"gt_names\"] = names\n+            info[\"gt_velocity\"] = velocity.reshape(-1, 2)\n+            info[\"num_lidar_pts\"] = np.array([a[\"num_lidar_pts\"] for a in annotations])\n+            info[\"num_radar_pts\"] = np.array([a[\"num_radar_pts\"] for a in annotations])\n+            info[\"valid_flag\"] = valid_flag\n \n-        if sample['scene_token'] in train_scenes:\n+        if sample[\"scene_token\"] in train_scenes:\n             train_nusc_infos.append(info)\n         else:\n             val_nusc_infos.append(info)\n \n     return train_nusc_infos, val_nusc_infos\n \n \n-def obtain_sensor2top(nusc,\n-                      sensor_token,\n-                      l2e_t,\n-                      l2e_r_mat,\n-                      e2g_t,\n-                      e2g_r_mat,\n-                      sensor_type='lidar'):\n+def obtain_sensor2top(\n+    nusc, sensor_token, l2e_t, l2e_r_mat, e2g_t, e2g_r_mat, sensor_type=\"lidar\"\n+):\n     \"\"\"Obtain the info with RT matric from general sensor to Top LiDAR.\n \n     Args:\n         nusc (class): Dataset class in the nuScenes dataset.\n@@ -344,42 +357,45 @@\n \n     Returns:\n         sweep (dict): Sweep information after transformation.\n     \"\"\"\n-    sd_rec = nusc.get('sample_data', sensor_token)\n-    cs_record = nusc.get('calibrated_sensor',\n-                         sd_rec['calibrated_sensor_token'])\n-    pose_record = nusc.get('ego_pose', sd_rec['ego_pose_token'])\n-    data_path = str(nusc.get_sample_data_path(sd_rec['token']))\n+    sd_rec = nusc.get(\"sample_data\", sensor_token)\n+    cs_record = nusc.get(\"calibrated_sensor\", sd_rec[\"calibrated_sensor_token\"])\n+    pose_record = nusc.get(\"ego_pose\", sd_rec[\"ego_pose_token\"])\n+    data_path = str(nusc.get_sample_data_path(sd_rec[\"token\"]))\n     if os.getcwd() in data_path:  # path from lyftdataset is absolute path\n-        data_path = data_path.split(f'{os.getcwd()}/')[-1]  # relative path\n+        data_path = data_path.split(f\"{os.getcwd()}/\")[-1]  # relative path\n     sweep = {\n-        'data_path': data_path,\n-        'type': sensor_type,\n-        'sample_data_token': sd_rec['token'],\n-        'sensor2ego_translation': cs_record['translation'],\n-        'sensor2ego_rotation': cs_record['rotation'],\n-        'ego2global_translation': pose_record['translation'],\n-        'ego2global_rotation': pose_record['rotation'],\n-        'timestamp': sd_rec['timestamp']\n+        \"data_path\": data_path,\n+        \"type\": sensor_type,\n+        \"sample_data_token\": sd_rec[\"token\"],\n+        \"sensor2ego_translation\": cs_record[\"translation\"],\n+        \"sensor2ego_rotation\": cs_record[\"rotation\"],\n+        \"ego2global_translation\": pose_record[\"translation\"],\n+        \"ego2global_rotation\": pose_record[\"rotation\"],\n+        \"timestamp\": sd_rec[\"timestamp\"],\n     }\n-    l2e_r_s = sweep['sensor2ego_rotation']\n-    l2e_t_s = sweep['sensor2ego_translation']\n-    e2g_r_s = sweep['ego2global_rotation']\n-    e2g_t_s = sweep['ego2global_translation']\n+    l2e_r_s = sweep[\"sensor2ego_rotation\"]\n+    l2e_t_s = sweep[\"sensor2ego_translation\"]\n+    e2g_r_s = sweep[\"ego2global_rotation\"]\n+    e2g_t_s = sweep[\"ego2global_translation\"]\n \n     # obtain the RT from sensor to Top LiDAR\n     # sweep->ego->global->ego'->lidar\n     l2e_r_s_mat = Quaternion(l2e_r_s).rotation_matrix\n     e2g_r_s_mat = Quaternion(e2g_r_s).rotation_matrix\n     R = (l2e_r_s_mat.T @ e2g_r_s_mat.T) @ (\n-        np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T)\n+        np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n+    )\n     T = (l2e_t_s @ e2g_r_s_mat.T + e2g_t_s) @ (\n-        np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T)\n-    T -= e2g_t @ (np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n-                  ) + l2e_t @ np.linalg.inv(l2e_r_mat).T\n-    sweep['sensor2lidar_rotation'] = R.T  # points @ R.T + T\n-    sweep['sensor2lidar_translation'] = T\n+        np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n+    )\n+    T -= (\n+        e2g_t @ (np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T)\n+        + l2e_t @ np.linalg.inv(l2e_r_mat).T\n+    )\n+    sweep[\"sensor2lidar_rotation\"] = R.T  # points @ R.T + T\n+    sweep[\"sensor2lidar_translation\"] = T\n     return sweep\n \n \n def export_2d_annotation(root_path, info_path, version, mono3d=True):\n@@ -393,16 +409,16 @@\n             Default: True.\n     \"\"\"\n     # get bbox annotations for camera\n     camera_types = [\n-        'CAM_FRONT',\n-        'CAM_FRONT_RIGHT',\n-        'CAM_FRONT_LEFT',\n-        'CAM_BACK',\n-        'CAM_BACK_LEFT',\n-        'CAM_BACK_RIGHT',\n+        \"CAM_FRONT\",\n+        \"CAM_FRONT_RIGHT\",\n+        \"CAM_FRONT_LEFT\",\n+        \"CAM_BACK\",\n+        \"CAM_BACK_LEFT\",\n+        \"CAM_BACK_RIGHT\",\n     ]\n-    nusc_infos = mmcv.load(info_path)['infos']\n+    nusc_infos = mmcv.load(info_path)[\"infos\"]\n     nusc = NuScenes(version=version, dataroot=root_path, verbose=True)\n     # info_2d_list = []\n     cat2Ids = [\n         dict(id=nus_categories.index(cat_name), name=cat_name)\n@@ -411,47 +427,46 @@\n     coco_ann_id = 0\n     coco_2d_dict = dict(annotations=[], images=[], categories=cat2Ids)\n     for info in mmcv.track_iter_progress(nusc_infos):\n         for cam in camera_types:\n-            cam_info = info['cams'][cam]\n+            cam_info = info[\"cams\"][cam]\n             coco_infos = get_2d_boxes(\n                 nusc,\n-                cam_info['sample_data_token'],\n-                visibilities=['', '1', '2', '3', '4'],\n-                mono3d=mono3d)\n-            (height, width, _) = mmcv.imread(cam_info['data_path']).shape\n-            coco_2d_dict['images'].append(\n+                cam_info[\"sample_data_token\"],\n+                visibilities=[\"\", \"1\", \"2\", \"3\", \"4\"],\n+                mono3d=mono3d,\n+            )\n+            (height, width, _) = mmcv.imread(cam_info[\"data_path\"]).shape\n+            coco_2d_dict[\"images\"].append(\n                 dict(\n-                    file_name=cam_info['data_path'].split('data/nuscenes/')\n-                    [-1],\n-                    id=cam_info['sample_data_token'],\n-                    token=info['token'],\n-                    cam2ego_rotation=cam_info['sensor2ego_rotation'],\n-                    cam2ego_translation=cam_info['sensor2ego_translation'],\n-                    ego2global_rotation=info['ego2global_rotation'],\n-                    ego2global_translation=info['ego2global_translation'],\n-                    cam_intrinsic=cam_info['cam_intrinsic'],\n+                    file_name=cam_info[\"data_path\"].split(\"data/nuscenes/\")[-1],\n+                    id=cam_info[\"sample_data_token\"],\n+                    token=info[\"token\"],\n+                    cam2ego_rotation=cam_info[\"sensor2ego_rotation\"],\n+                    cam2ego_translation=cam_info[\"sensor2ego_translation\"],\n+                    ego2global_rotation=info[\"ego2global_rotation\"],\n+                    ego2global_translation=info[\"ego2global_translation\"],\n+                    cam_intrinsic=cam_info[\"cam_intrinsic\"],\n                     width=width,\n-                    height=height))\n+                    height=height,\n+                )\n+            )\n             for coco_info in coco_infos:\n                 if coco_info is None:\n                     continue\n                 # add an empty key for coco format\n-                coco_info['segmentation'] = []\n-                coco_info['id'] = coco_ann_id\n-                coco_2d_dict['annotations'].append(coco_info)\n+                coco_info[\"segmentation\"] = []\n+                coco_info[\"id\"] = coco_ann_id\n+                coco_2d_dict[\"annotations\"].append(coco_info)\n                 coco_ann_id += 1\n     if mono3d:\n-        json_prefix = f'{info_path[:-4]}_mono3d'\n+        json_prefix = f\"{info_path[:-4]}_mono3d\"\n     else:\n-        json_prefix = f'{info_path[:-4]}'\n-    mmcv.dump(coco_2d_dict, f'{json_prefix}.coco.json')\n+        json_prefix = f\"{info_path[:-4]}\"\n+    mmcv.dump(coco_2d_dict, f\"{json_prefix}.coco.json\")\n \n \n-def get_2d_boxes(nusc,\n-                 sample_data_token: str,\n-                 visibilities: List[str],\n-                 mono3d=True):\n+def get_2d_boxes(nusc, sample_data_token: str, visibilities: List[str], mono3d=True):\n     \"\"\"Get the 2D annotation records for a given `sample_data_token`.\n \n     Args:\n         sample_data_token (str): Sample data token belonging to a camera\n@@ -464,61 +479,58 @@\n             `sample_data_token`.\n     \"\"\"\n \n     # Get the sample data and the sample corresponding to that sample data.\n-    sd_rec = nusc.get('sample_data', sample_data_token)\n+    sd_rec = nusc.get(\"sample_data\", sample_data_token)\n \n-    assert sd_rec[\n-        'sensor_modality'] == 'camera', 'Error: get_2d_boxes only works' \\\n-        ' for camera sample_data!'\n-    if not sd_rec['is_key_frame']:\n-        raise ValueError(\n-            'The 2D re-projections are available only for keyframes.')\n+    assert sd_rec[\"sensor_modality\"] == \"camera\", (\n+        \"Error: get_2d_boxes only works\" \" for camera sample_data!\"\n+    )\n+    if not sd_rec[\"is_key_frame\"]:\n+        raise ValueError(\"The 2D re-projections are available only for keyframes.\")\n \n-    s_rec = nusc.get('sample', sd_rec['sample_token'])\n+    s_rec = nusc.get(\"sample\", sd_rec[\"sample_token\"])\n \n     # Get the calibrated sensor and ego pose\n     # record to get the transformation matrices.\n-    cs_rec = nusc.get('calibrated_sensor', sd_rec['calibrated_sensor_token'])\n-    pose_rec = nusc.get('ego_pose', sd_rec['ego_pose_token'])\n-    camera_intrinsic = np.array(cs_rec['camera_intrinsic'])\n+    cs_rec = nusc.get(\"calibrated_sensor\", sd_rec[\"calibrated_sensor_token\"])\n+    pose_rec = nusc.get(\"ego_pose\", sd_rec[\"ego_pose_token\"])\n+    camera_intrinsic = np.array(cs_rec[\"camera_intrinsic\"])\n \n     # Get all the annotation with the specified visibilties.\n+    ann_recs = [nusc.get(\"sample_annotation\", token) for token in s_rec[\"anns\"]]\n     ann_recs = [\n-        nusc.get('sample_annotation', token) for token in s_rec['anns']\n+        ann_rec for ann_rec in ann_recs if (ann_rec[\"visibility_token\"] in visibilities)\n     ]\n-    ann_recs = [\n-        ann_rec for ann_rec in ann_recs\n-        if (ann_rec['visibility_token'] in visibilities)\n-    ]\n \n     repro_recs = []\n \n     for ann_rec in ann_recs:\n         # Augment sample_annotation with token information.\n-        ann_rec['sample_annotation_token'] = ann_rec['token']\n-        ann_rec['sample_data_token'] = sample_data_token\n+        ann_rec[\"sample_annotation_token\"] = ann_rec[\"token\"]\n+        ann_rec[\"sample_data_token\"] = sample_data_token\n \n         # Get the box in global coordinates.\n-        box = nusc.get_box(ann_rec['token'])\n+        box = nusc.get_box(ann_rec[\"token\"])\n \n         # Move them to the ego-pose frame.\n-        box.translate(-np.array(pose_rec['translation']))\n-        box.rotate(Quaternion(pose_rec['rotation']).inverse)\n+        box.translate(-np.array(pose_rec[\"translation\"]))\n+        box.rotate(Quaternion(pose_rec[\"rotation\"]).inverse)\n \n         # Move them to the calibrated sensor frame.\n-        box.translate(-np.array(cs_rec['translation']))\n-        box.rotate(Quaternion(cs_rec['rotation']).inverse)\n+        box.translate(-np.array(cs_rec[\"translation\"]))\n+        box.rotate(Quaternion(cs_rec[\"rotation\"]).inverse)\n \n         # Filter out the corners that are not in front of the calibrated\n         # sensor.\n         corners_3d = box.corners()\n         in_front = np.argwhere(corners_3d[2, :] > 0).flatten()\n         corners_3d = corners_3d[:, in_front]\n \n         # Project 3d box to 2d.\n-        corner_coords = view_points(corners_3d, camera_intrinsic,\n-                                    True).T[:, :2].tolist()\n+        corner_coords = (\n+            view_points(corners_3d, camera_intrinsic, True).T[:, :2].tolist()\n+        )\n \n         # Keep only corners that fall within the image.\n         final_coords = post_process_coords(corner_coords)\n \n@@ -529,10 +541,11 @@\n         else:\n             min_x, min_y, max_x, max_y = final_coords\n \n         # Generate dictionary record to be included in the .json file.\n-        repro_rec = generate_record(ann_rec, min_x, min_y, max_x, max_y,\n-                                    sample_data_token, sd_rec['filename'])\n+        repro_rec = generate_record(\n+            ann_rec, min_x, min_y, max_x, max_y, sample_data_token, sd_rec[\"filename\"]\n+        )\n \n         # If mono3d=True, add 3D annotations in camera coordinates\n         if mono3d and (repro_rec is not None):\n             loc = box.center.tolist()\n@@ -545,35 +558,34 @@\n             rot = [-rot]  # convert the rot to our cam coordinate\n \n             global_velo2d = nusc.box_velocity(box.token)[:2]\n             global_velo3d = np.array([*global_velo2d, 0.0])\n-            e2g_r_mat = Quaternion(pose_rec['rotation']).rotation_matrix\n-            c2e_r_mat = Quaternion(cs_rec['rotation']).rotation_matrix\n-            cam_velo3d = global_velo3d @ np.linalg.inv(\n-                e2g_r_mat).T @ np.linalg.inv(c2e_r_mat).T\n+            e2g_r_mat = Quaternion(pose_rec[\"rotation\"]).rotation_matrix\n+            c2e_r_mat = Quaternion(cs_rec[\"rotation\"]).rotation_matrix\n+            cam_velo3d = (\n+                global_velo3d @ np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(c2e_r_mat).T\n+            )\n             velo = cam_velo3d[0::2].tolist()\n \n-            repro_rec['bbox_cam3d'] = loc + dim + rot\n-            repro_rec['velo_cam3d'] = velo\n+            repro_rec[\"bbox_cam3d\"] = loc + dim + rot\n+            repro_rec[\"velo_cam3d\"] = velo\n \n             center3d = np.array(loc).reshape([1, 3])\n-            center2d = points_cam2img(\n-                center3d, camera_intrinsic, with_depth=True)\n-            repro_rec['center2d'] = center2d.squeeze().tolist()\n+            center2d = points_cam2img(center3d, camera_intrinsic, with_depth=True)\n+            repro_rec[\"center2d\"] = center2d.squeeze().tolist()\n             # normalized center2D + depth\n             # if samples with depth < 0 will be removed\n-            if repro_rec['center2d'][2] <= 0:\n+            if repro_rec[\"center2d\"][2] <= 0:\n                 continue\n \n-            ann_token = nusc.get('sample_annotation',\n-                                 box.token)['attribute_tokens']\n+            ann_token = nusc.get(\"sample_annotation\", box.token)[\"attribute_tokens\"]\n             if len(ann_token) == 0:\n-                attr_name = 'None'\n+                attr_name = \"None\"\n             else:\n-                attr_name = nusc.get('attribute', ann_token[0])['name']\n+                attr_name = nusc.get(\"attribute\", ann_token[0])[\"name\"]\n             attr_id = nus_attributes.index(attr_name)\n-            repro_rec['attribute_name'] = attr_name\n-            repro_rec['attribute_id'] = attr_id\n+            repro_rec[\"attribute_name\"] = attr_name\n+            repro_rec[\"attribute_id\"] = attr_id\n \n         repro_recs.append(repro_rec)\n \n     return repro_recs\n@@ -599,9 +611,10 @@\n \n     if polygon_from_2d_box.intersects(img_canvas):\n         img_intersection = polygon_from_2d_box.intersection(img_canvas)\n         intersection_coords = np.array(\n-            [coord for coord in img_intersection.exterior.coords])\n+            [coord for coord in img_intersection.exterior.coords]\n+        )\n \n         min_x = min(intersection_coords[:, 0])\n         min_y = min(intersection_coords[:, 1])\n         max_x = max(intersection_coords[:, 0])\n@@ -611,10 +624,17 @@\n     else:\n         return None\n \n \n-def generate_record(ann_rec: dict, x1: float, y1: float, x2: float, y2: float,\n-                    sample_data_token: str, filename: str) -> OrderedDict:\n+def generate_record(\n+    ann_rec: dict,\n+    x1: float,\n+    y1: float,\n+    x2: float,\n+    y2: float,\n+    sample_data_token: str,\n+    filename: str,\n+) -> OrderedDict:\n     \"\"\"Generate one 2D annotation record given various information on top of\n     the 2D bounding box coordinates.\n \n     Args:\n@@ -637,40 +657,40 @@\n             - bbox (list[float]): left x, top y, dx, dy of 2d box\n             - iscrowd (int): whether the area is crowd\n     \"\"\"\n     repro_rec = OrderedDict()\n-    repro_rec['sample_data_token'] = sample_data_token\n+    repro_rec[\"sample_data_token\"] = sample_data_token\n     coco_rec = dict()\n \n     relevant_keys = [\n-        'attribute_tokens',\n-        'category_name',\n-        'instance_token',\n-        'next',\n-        'num_lidar_pts',\n-        'num_radar_pts',\n-        'prev',\n-        'sample_annotation_token',\n-        'sample_data_token',\n-        'visibility_token',\n+        \"attribute_tokens\",\n+        \"category_name\",\n+        \"instance_token\",\n+        \"next\",\n+        \"num_lidar_pts\",\n+        \"num_radar_pts\",\n+        \"prev\",\n+        \"sample_annotation_token\",\n+        \"sample_data_token\",\n+        \"visibility_token\",\n     ]\n \n     for key, value in ann_rec.items():\n         if key in relevant_keys:\n             repro_rec[key] = value\n \n-    repro_rec['bbox_corners'] = [x1, y1, x2, y2]\n-    repro_rec['filename'] = filename\n+    repro_rec[\"bbox_corners\"] = [x1, y1, x2, y2]\n+    repro_rec[\"filename\"] = filename\n \n-    coco_rec['file_name'] = filename\n-    coco_rec['image_id'] = sample_data_token\n-    coco_rec['area'] = (y2 - y1) * (x2 - x1)\n+    coco_rec[\"file_name\"] = filename\n+    coco_rec[\"image_id\"] = sample_data_token\n+    coco_rec[\"area\"] = (y2 - y1) * (x2 - x1)\n \n-    if repro_rec['category_name'] not in NuScenesDataset.NameMapping:\n+    if repro_rec[\"category_name\"] not in NuScenesDataset.NameMapping:\n         return None\n-    cat_name = NuScenesDataset.NameMapping[repro_rec['category_name']]\n-    coco_rec['category_name'] = cat_name\n-    coco_rec['category_id'] = nus_categories.index(cat_name)\n-    coco_rec['bbox'] = [x1, y1, x2 - x1, y2 - y1]\n-    coco_rec['iscrowd'] = 0\n+    cat_name = NuScenesDataset.NameMapping[repro_rec[\"category_name\"]]\n+    coco_rec[\"category_name\"] = cat_name\n+    coco_rec[\"category_id\"] = nus_categories.index(cat_name)\n+    coco_rec[\"bbox\"] = [x1, y1, x2 - x1, y2 - y1]\n+    coco_rec[\"iscrowd\"] = 0\n \n     return coco_rec\n"
                },
                {
                    "date": 1716189133061,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -318,9 +318,13 @@\n             names = np.array(names)\n             # we need to convert box size to\n             # the format of our lidar coordinate system\n             # which is x_size, y_size, z_size (corresponding to l, w, h)\n-            gt_boxes = np.concatenate([locs, dims[:, [1, 0, 2]], rots], axis=1)\n+            # gt_boxes = np.concatenate([locs, dims[:, [1, 0, 2]], rots], axis=1)\n+            \n+            \n+            # we need to convert rot to SECOND format.\n+            gt_boxes = np.concatenate([locs, dims, -rots - np.pi / 2], axis=1)\n             assert len(gt_boxes) == len(\n                 annotations\n             ), f\"{len(gt_boxes)}, {len(annotations)}\"\n             info[\"gt_boxes\"] = gt_boxes\n"
                },
                {
                    "date": 1716620836261,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -318,13 +318,13 @@\n             names = np.array(names)\n             # we need to convert box size to\n             # the format of our lidar coordinate system\n             # which is x_size, y_size, z_size (corresponding to l, w, h)\n-            # gt_boxes = np.concatenate([locs, dims[:, [1, 0, 2]], rots], axis=1)\n+            gt_boxes = np.concatenate([locs, dims[:, [1, 0, 2]], rots], axis=1)\n             \n             \n             # we need to convert rot to SECOND format.\n-            gt_boxes = np.concatenate([locs, dims, -rots - np.pi / 2], axis=1)\n+            # gt_boxes = np.concatenate([locs, dims, -rots - np.pi / 2], axis=1)\n             assert len(gt_boxes) == len(\n                 annotations\n             ), f\"{len(gt_boxes)}, {len(annotations)}\"\n             info[\"gt_boxes\"] = gt_boxes\n"
                },
                {
                    "date": 1717321361458,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,700 @@\n+# Copyright (c) OpenMMLab. All rights reserved.\n+import os\n+from collections import OrderedDict\n+from os import path as osp\n+from typing import List, Tuple, Union\n+\n+import mmcv\n+import numpy as np\n+from nuscenes.nuscenes import NuScenes\n+from nuscenes.utils.geometry_utils import view_points\n+from pyquaternion import Quaternion\n+from shapely.geometry import MultiPoint, box\n+\n+from mmdet3d.core.bbox import points_cam2img\n+from mmdet3d.datasets import NuScenesDataset\n+\n+nus_categories = (\n+    \"car\",\n+    \"truck\",\n+    \"trailer\",\n+    \"bus\",\n+    \"construction_vehicle\",\n+    \"bicycle\",\n+    \"motorcycle\",\n+    \"pedestrian\",\n+    \"traffic_cone\",\n+    \"barrier\",\n+)\n+\n+nus_attributes = (\n+    \"cycle.with_rider\",\n+    \"cycle.without_rider\",\n+    \"pedestrian.moving\",\n+    \"pedestrian.standing\",\n+    \"pedestrian.sitting_lying_down\",\n+    \"vehicle.moving\",\n+    \"vehicle.parked\",\n+    \"vehicle.stopped\",\n+    \"None\",\n+)\n+\n+\n+def create_nuscenes_infos(\n+    root_path, info_prefix, version=\"v1.0-trainval\", max_sweeps=10\n+):\n+    \"\"\"Create info file of nuscene dataset.\n+\n+    Given the raw data, generate its related info file in pkl format.\n+\n+    Args:\n+        root_path (str): Path of the data root.\n+        info_prefix (str): Prefix of the info file to be generated.\n+        version (str, optional): Version of the data.\n+            Default: 'v1.0-trainval'.\n+        max_sweeps (int, optional): Max number of sweeps.\n+            Default: 10.\n+    \"\"\"\n+    from nuscenes.nuscenes import NuScenes\n+\n+    nusc = NuScenes(version=version, dataroot=root_path, verbose=True)\n+    from nuscenes.utils import splits\n+\n+    available_vers = [\"v1.0-trainval\", \"v1.0-test\", \"v1.0-mini\"]\n+    assert version in available_vers\n+    if version == \"v1.0-trainval\":\n+        train_scenes = splits.train\n+        val_scenes = splits.val\n+    elif version == \"v1.0-test\":\n+        train_scenes = splits.test\n+        val_scenes = []\n+    elif version == \"v1.0-mini\":\n+        train_scenes = splits.mini_train\n+        val_scenes = splits.mini_val\n+    else:\n+        raise ValueError(\"unknown\")\n+\n+    # filter existing scenes.\n+    available_scenes = get_available_scenes(nusc)\n+    available_scene_names = [s[\"name\"] for s in available_scenes]\n+    train_scenes = list(filter(lambda x: x in available_scene_names, train_scenes))\n+    val_scenes = list(filter(lambda x: x in available_scene_names, val_scenes))\n+    train_scenes = set(\n+        [\n+            available_scenes[available_scene_names.index(s)][\"token\"]\n+            for s in train_scenes\n+        ]\n+    )\n+    val_scenes = set(\n+        [available_scenes[available_scene_names.index(s)][\"token\"] for s in val_scenes]\n+    )\n+\n+    test = \"test\" in version\n+    if test:\n+        print(\"test scene: {}\".format(len(train_scenes)))\n+    else:\n+        print(\n+            \"train scene: {}, val scene: {}\".format(len(train_scenes), len(val_scenes))\n+        )\n+    train_nusc_infos, val_nusc_infos = _fill_trainval_infos(\n+        nusc, train_scenes, val_scenes, test, max_sweeps=max_sweeps\n+    )\n+\n+    metadata = dict(version=version)\n+    if test:\n+        print(\"test sample: {}\".format(len(train_nusc_infos)))\n+        data = dict(infos=train_nusc_infos, metadata=metadata)\n+        info_path = osp.join(root_path, \"{}_infos_test.pkl\".format(info_prefix))\n+        mmcv.dump(data, info_path)\n+    else:\n+        print(\n+            \"train sample: {}, val sample: {}\".format(\n+                len(train_nusc_infos), len(val_nusc_infos)\n+            )\n+        )\n+        data = dict(infos=train_nusc_infos, metadata=metadata)\n+        info_path = osp.join(root_path, \"{}_infos_train.pkl\".format(info_prefix))\n+        mmcv.dump(data, info_path)\n+        data[\"infos\"] = val_nusc_infos\n+        info_val_path = osp.join(root_path, \"{}_infos_val.pkl\".format(info_prefix))\n+        mmcv.dump(data, info_val_path)\n+\n+\n+def get_available_scenes(nusc):\n+    \"\"\"Get available scenes from the input nuscenes class.\n+\n+    Given the raw data, get the information of available scenes for\n+    further info generation.\n+\n+    Args:\n+        nusc (class): Dataset class in the nuScenes dataset.\n+\n+    Returns:\n+        available_scenes (list[dict]): List of basic information for the\n+            available scenes.\n+    \"\"\"\n+    available_scenes = []\n+    print(\"total scene num: {}\".format(len(nusc.scene)))\n+    for scene in nusc.scene:\n+        scene_token = scene[\"token\"]\n+        scene_rec = nusc.get(\"scene\", scene_token)\n+        sample_rec = nusc.get(\"sample\", scene_rec[\"first_sample_token\"])\n+        sd_rec = nusc.get(\"sample_data\", sample_rec[\"data\"][\"LIDAR_TOP\"])\n+        has_more_frames = True\n+        scene_not_exist = False\n+        while has_more_frames:\n+            lidar_path, boxes, _ = nusc.get_sample_data(sd_rec[\"token\"])\n+            lidar_path = str(lidar_path)\n+            if os.getcwd() in lidar_path:\n+                # path from lyftdataset is absolute path\n+                lidar_path = lidar_path.split(f\"{os.getcwd()}/\")[-1]\n+                # relative path\n+            if not mmcv.is_filepath(lidar_path):\n+                scene_not_exist = True\n+                break\n+            else:\n+                break\n+        if scene_not_exist:\n+            continue\n+        available_scenes.append(scene)\n+    print(\"exist scene num: {}\".format(len(available_scenes)))\n+    return available_scenes\n+\n+\n+def _fill_trainval_infos(nusc, train_scenes, val_scenes, test=False, max_sweeps=10):\n+    \"\"\"Generate the train/val infos from the raw data.\n+\n+    Args:\n+        nusc (:obj:`NuScenes`): Dataset class in the nuScenes dataset.\n+        train_scenes (list[str]): Basic information of training scenes.\n+        val_scenes (list[str]): Basic information of validation scenes.\n+        test (bool, optional): Whether use the test mode. In test mode, no\n+            annotations can be accessed. Default: False.\n+        max_sweeps (int, optional): Max number of sweeps. Default: 10.\n+\n+    Returns:\n+        tuple[list[dict]]: Information of training set and validation set\n+            that will be saved to the info file.\n+    \"\"\"\n+    train_nusc_infos = []\n+    val_nusc_infos = []\n+\n+    for sample in mmcv.track_iter_progress(nusc.sample):\n+        lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n+        sd_rec = nusc.get(\"sample_data\", sample[\"data\"][\"LIDAR_TOP\"])\n+        cs_record = nusc.get(\"calibrated_sensor\", sd_rec[\"calibrated_sensor_token\"])\n+        pose_record = nusc.get(\"ego_pose\", sd_rec[\"ego_pose_token\"])\n+        lidar_path, boxes, _ = nusc.get_sample_data(lidar_token)\n+\n+        mmcv.check_file_exist(lidar_path)\n+\n+        info = {\n+            \"lidar_path\": lidar_path,\n+            \"token\": sample[\"token\"],\n+            \"sweeps\": [],\n+            \"cams\": dict(),\n+            \"radars\": dict(),\n+            \"lidar2ego_translation\": cs_record[\"translation\"],\n+            \"lidar2ego_rotation\": cs_record[\"rotation\"],\n+            \"ego2global_translation\": pose_record[\"translation\"],\n+            \"ego2global_rotation\": pose_record[\"rotation\"],\n+            \"timestamp\": sample[\"timestamp\"],\n+        }\n+\n+        l2e_r = info[\"lidar2ego_rotation\"]\n+        l2e_t = info[\"lidar2ego_translation\"]\n+        e2g_r = info[\"ego2global_rotation\"]\n+        e2g_t = info[\"ego2global_translation\"]\n+        l2e_r_mat = Quaternion(l2e_r).rotation_matrix\n+        e2g_r_mat = Quaternion(e2g_r).rotation_matrix\n+\n+        # obtain 6 image's information per frame\n+        camera_types = [\n+            \"CAM_FRONT\",\n+            \"CAM_FRONT_RIGHT\",\n+            \"CAM_FRONT_LEFT\",\n+            \"CAM_BACK\",\n+            \"CAM_BACK_LEFT\",\n+            \"CAM_BACK_RIGHT\",\n+        ]\n+        for cam in camera_types:\n+            cam_token = sample[\"data\"][cam]\n+            cam_path, _, cam_intrinsic = nusc.get_sample_data(cam_token)\n+            cam_info = obtain_sensor2top(\n+                nusc, cam_token, l2e_t, l2e_r_mat, e2g_t, e2g_r_mat, cam\n+            )\n+            cam_info.update(cam_intrinsic=cam_intrinsic)\n+            info[\"cams\"].update({cam: cam_info})\n+\n+        # Add radar information to the annotation read file\n+        radar_names = [\n+            \"RADAR_FRONT\",\n+            \"RADAR_FRONT_LEFT\",\n+            \"RADAR_FRONT_RIGHT\",\n+            \"RADAR_BACK_LEFT\",\n+            \"RADAR_BACK_RIGHT\",\n+        ]\n+\n+        for radar_name in radar_names:\n+            radar_token = sample[\"data\"][radar_name]\n+            radar_rec = nusc.get(\"sample_data\", radar_token)\n+            sweeps = []\n+\n+            while len(sweeps) < 5:\n+                if not radar_rec[\"prev\"] == \"\":\n+                    radar_path, _, radar_intrin = nusc.get_sample_data(radar_token)\n+\n+                    radar_info = obtain_sensor2top(\n+                        nusc,\n+                        radar_token,\n+                        l2e_t,\n+                        l2e_r_mat,\n+                        e2g_t,\n+                        e2g_r_mat,\n+                        radar_name,\n+                    )\n+                    sweeps.append(radar_info)\n+                    radar_token = radar_rec[\"prev\"]\n+                    radar_rec = nusc.get(\"sample_data\", radar_token)\n+                else:\n+                    radar_path, _, radar_intrin = nusc.get_sample_data(radar_token)\n+\n+                    radar_info = obtain_sensor2top(\n+                        nusc,\n+                        radar_token,\n+                        l2e_t,\n+                        l2e_r_mat,\n+                        e2g_t,\n+                        e2g_r_mat,\n+                        radar_name,\n+                    )\n+                    sweeps.append(radar_info)\n+\n+            info[\"radars\"].update({radar_name: sweeps})\n+\n+        # obtain sweeps for a single key-frame\n+        sd_rec = nusc.get(\"sample_data\", sample[\"data\"][\"LIDAR_TOP\"])\n+        sweeps = []\n+        while len(sweeps) < max_sweeps:\n+            if not sd_rec[\"prev\"] == \"\":\n+                sweep = obtain_sensor2top(\n+                    nusc, sd_rec[\"prev\"], l2e_t, l2e_r_mat, e2g_t, e2g_r_mat, \"lidar\"\n+                )\n+                sweeps.append(sweep)\n+                sd_rec = nusc.get(\"sample_data\", sd_rec[\"prev\"])\n+            else:\n+                break\n+        info[\"sweeps\"] = sweeps\n+        # obtain annotation\n+        if not test:\n+            annotations = [\n+                nusc.get(\"sample_annotation\", token) for token in sample[\"anns\"]\n+            ]\n+            locs = np.array([b.center for b in boxes]).reshape(-1, 3)\n+            dims = np.array([b.wlh for b in boxes]).reshape(-1, 3)\n+            rots = np.array([b.orientation.yaw_pitch_roll[0] for b in boxes]).reshape(\n+                -1, 1\n+            )\n+            velocity = np.array(\n+                [nusc.box_velocity(token)[:2] for token in sample[\"anns\"]]\n+            )\n+            valid_flag = np.array(\n+                [\n+                    (anno[\"num_lidar_pts\"] + anno[\"num_radar_pts\"]) > 0\n+                    for anno in annotations\n+                ],\n+                dtype=bool,\n+            ).reshape(-1)\n+            # convert velo from global to lidar\n+            for i in range(len(boxes)):\n+                velo = np.array([*velocity[i], 0.0])\n+                velo = velo @ np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n+                velocity[i] = velo[:2]\n+\n+            names = [b.name for b in boxes]\n+            for i in range(len(names)):\n+                if names[i] in NuScenesDataset.NameMapping:\n+                    names[i] = NuScenesDataset.NameMapping[names[i]]\n+            names = np.array(names)\n+            # we need to convert box size to\n+            # the format of our lidar coordinate system\n+            # which is x_size, y_size, z_size (corresponding to l, w, h)\n+            gt_boxes = np.concatenate([locs, dims[:, [1, 0, 2]], rots], axis=1)\n+            \n+            \n+            # we need to convert rot to SECOND format.\n+            # gt_boxes = np.concatenate([locs, dims, -rots - np.pi / 2], axis=1)\n+            assert len(gt_boxes) == len(\n+                annotations\n+            ), f\"{len(gt_boxes)}, {len(annotations)}\"\n+            info[\"gt_boxes\"] = gt_boxes\n+            info[\"gt_names\"] = names\n+            info[\"gt_velocity\"] = velocity.reshape(-1, 2)\n+            info[\"num_lidar_pts\"] = np.array([a[\"num_lidar_pts\"] for a in annotations])\n+            info[\"num_radar_pts\"] = np.array([a[\"num_radar_pts\"] for a in annotations])\n+            info[\"valid_flag\"] = valid_flag\n+\n+        if sample[\"scene_token\"] in train_scenes:\n+            train_nusc_infos.append(info)\n+        else:\n+            val_nusc_infos.append(info)\n+\n+    return train_nusc_infos, val_nusc_infos\n+\n+\n+def obtain_sensor2top(\n+    nusc, sensor_token, l2e_t, l2e_r_mat, e2g_t, e2g_r_mat, sensor_type=\"lidar\"\n+):\n+    \"\"\"Obtain the info with RT matric from general sensor to Top LiDAR.\n+\n+    Args:\n+        nusc (class): Dataset class in the nuScenes dataset.\n+        sensor_token (str): Sample data token corresponding to the\n+            specific sensor type.\n+        l2e_t (np.ndarray): Translation from lidar to ego in shape (1, 3).\n+        l2e_r_mat (np.ndarray): Rotation matrix from lidar to ego\n+            in shape (3, 3).\n+        e2g_t (np.ndarray): Translation from ego to global in shape (1, 3).\n+        e2g_r_mat (np.ndarray): Rotation matrix from ego to global\n+            in shape (3, 3).\n+        sensor_type (str, optional): Sensor to calibrate. Default: 'lidar'.\n+\n+    Returns:\n+        sweep (dict): Sweep information after transformation.\n+    \"\"\"\n+    sd_rec = nusc.get(\"sample_data\", sensor_token)\n+    cs_record = nusc.get(\"calibrated_sensor\", sd_rec[\"calibrated_sensor_token\"])\n+    pose_record = nusc.get(\"ego_pose\", sd_rec[\"ego_pose_token\"])\n+    data_path = str(nusc.get_sample_data_path(sd_rec[\"token\"]))\n+    if os.getcwd() in data_path:  # path from lyftdataset is absolute path\n+        data_path = data_path.split(f\"{os.getcwd()}/\")[-1]  # relative path\n+    sweep = {\n+        \"data_path\": data_path,\n+        \"type\": sensor_type,\n+        \"sample_data_token\": sd_rec[\"token\"],\n+        \"sensor2ego_translation\": cs_record[\"translation\"],\n+        \"sensor2ego_rotation\": cs_record[\"rotation\"],\n+        \"ego2global_translation\": pose_record[\"translation\"],\n+        \"ego2global_rotation\": pose_record[\"rotation\"],\n+        \"timestamp\": sd_rec[\"timestamp\"],\n+    }\n+    l2e_r_s = sweep[\"sensor2ego_rotation\"]\n+    l2e_t_s = sweep[\"sensor2ego_translation\"]\n+    e2g_r_s = sweep[\"ego2global_rotation\"]\n+    e2g_t_s = sweep[\"ego2global_translation\"]\n+\n+    # obtain the RT from sensor to Top LiDAR\n+    # sweep->ego->global->ego'->lidar\n+    l2e_r_s_mat = Quaternion(l2e_r_s).rotation_matrix\n+    e2g_r_s_mat = Quaternion(e2g_r_s).rotation_matrix\n+    R = (l2e_r_s_mat.T @ e2g_r_s_mat.T) @ (\n+        np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n+    )\n+    T = (l2e_t_s @ e2g_r_s_mat.T + e2g_t_s) @ (\n+        np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n+    )\n+    T -= (\n+        e2g_t @ (np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T)\n+        + l2e_t @ np.linalg.inv(l2e_r_mat).T\n+    )\n+    sweep[\"sensor2lidar_rotation\"] = R.T  # points @ R.T + T\n+    sweep[\"sensor2lidar_translation\"] = T\n+    return sweep\n+\n+\n+def export_2d_annotation(root_path, info_path, version, mono3d=True):\n+    \"\"\"Export 2d annotation from the info file and raw data.\n+\n+    Args:\n+        root_path (str): Root path of the raw data.\n+        info_path (str): Path of the info file.\n+        version (str): Dataset version.\n+        mono3d (bool, optional): Whether to export mono3d annotation.\n+            Default: True.\n+    \"\"\"\n+    # get bbox annotations for camera\n+    camera_types = [\n+        \"CAM_FRONT\",\n+        \"CAM_FRONT_RIGHT\",\n+        \"CAM_FRONT_LEFT\",\n+        \"CAM_BACK\",\n+        \"CAM_BACK_LEFT\",\n+        \"CAM_BACK_RIGHT\",\n+    ]\n+    nusc_infos = mmcv.load(info_path)[\"infos\"]\n+    nusc = NuScenes(version=version, dataroot=root_path, verbose=True)\n+    # info_2d_list = []\n+    cat2Ids = [\n+        dict(id=nus_categories.index(cat_name), name=cat_name)\n+        for cat_name in nus_categories\n+    ]\n+    coco_ann_id = 0\n+    coco_2d_dict = dict(annotations=[], images=[], categories=cat2Ids)\n+    for info in mmcv.track_iter_progress(nusc_infos):\n+        for cam in camera_types:\n+            cam_info = info[\"cams\"][cam]\n+            coco_infos = get_2d_boxes(\n+                nusc,\n+                cam_info[\"sample_data_token\"],\n+                visibilities=[\"\", \"1\", \"2\", \"3\", \"4\"],\n+                mono3d=mono3d,\n+            )\n+            (height, width, _) = mmcv.imread(cam_info[\"data_path\"]).shape\n+            coco_2d_dict[\"images\"].append(\n+                dict(\n+                    file_name=cam_info[\"data_path\"].split(\"data/nuscenes/\")[-1],\n+                    id=cam_info[\"sample_data_token\"],\n+                    token=info[\"token\"],\n+                    cam2ego_rotation=cam_info[\"sensor2ego_rotation\"],\n+                    cam2ego_translation=cam_info[\"sensor2ego_translation\"],\n+                    ego2global_rotation=info[\"ego2global_rotation\"],\n+                    ego2global_translation=info[\"ego2global_translation\"],\n+                    cam_intrinsic=cam_info[\"cam_intrinsic\"],\n+                    width=width,\n+                    height=height,\n+                )\n+            )\n+            for coco_info in coco_infos:\n+                if coco_info is None:\n+                    continue\n+                # add an empty key for coco format\n+                coco_info[\"segmentation\"] = []\n+                coco_info[\"id\"] = coco_ann_id\n+                coco_2d_dict[\"annotations\"].append(coco_info)\n+                coco_ann_id += 1\n+    if mono3d:\n+        json_prefix = f\"{info_path[:-4]}_mono3d\"\n+    else:\n+        json_prefix = f\"{info_path[:-4]}\"\n+    mmcv.dump(coco_2d_dict, f\"{json_prefix}.coco.json\")\n+\n+\n+def get_2d_boxes(nusc, sample_data_token: str, visibilities: List[str], mono3d=True):\n+    \"\"\"Get the 2D annotation records for a given `sample_data_token`.\n+\n+    Args:\n+        sample_data_token (str): Sample data token belonging to a camera\n+            keyframe.\n+        visibilities (list[str]): Visibility filter.\n+        mono3d (bool): Whether to get boxes with mono3d annotation.\n+\n+    Return:\n+        list[dict]: List of 2D annotation record that belongs to the input\n+            `sample_data_token`.\n+    \"\"\"\n+\n+    # Get the sample data and the sample corresponding to that sample data.\n+    sd_rec = nusc.get(\"sample_data\", sample_data_token)\n+\n+    assert sd_rec[\"sensor_modality\"] == \"camera\", (\n+        \"Error: get_2d_boxes only works\" \" for camera sample_data!\"\n+    )\n+    if not sd_rec[\"is_key_frame\"]:\n+        raise ValueError(\"The 2D re-projections are available only for keyframes.\")\n+\n+    s_rec = nusc.get(\"sample\", sd_rec[\"sample_token\"])\n+\n+    # Get the calibrated sensor and ego pose\n+    # record to get the transformation matrices.\n+    cs_rec = nusc.get(\"calibrated_sensor\", sd_rec[\"calibrated_sensor_token\"])\n+    pose_rec = nusc.get(\"ego_pose\", sd_rec[\"ego_pose_token\"])\n+    camera_intrinsic = np.array(cs_rec[\"camera_intrinsic\"])\n+\n+    # Get all the annotation with the specified visibilties.\n+    ann_recs = [nusc.get(\"sample_annotation\", token) for token in s_rec[\"anns\"]]\n+    ann_recs = [\n+        ann_rec for ann_rec in ann_recs if (ann_rec[\"visibility_token\"] in visibilities)\n+    ]\n+\n+    repro_recs = []\n+\n+    for ann_rec in ann_recs:\n+        # Augment sample_annotation with token information.\n+        ann_rec[\"sample_annotation_token\"] = ann_rec[\"token\"]\n+        ann_rec[\"sample_data_token\"] = sample_data_token\n+\n+        # Get the box in global coordinates.\n+        box = nusc.get_box(ann_rec[\"token\"])\n+\n+        # Move them to the ego-pose frame.\n+        box.translate(-np.array(pose_rec[\"translation\"]))\n+        box.rotate(Quaternion(pose_rec[\"rotation\"]).inverse)\n+\n+        # Move them to the calibrated sensor frame.\n+        box.translate(-np.array(cs_rec[\"translation\"]))\n+        box.rotate(Quaternion(cs_rec[\"rotation\"]).inverse)\n+\n+        # Filter out the corners that are not in front of the calibrated\n+        # sensor.\n+        corners_3d = box.corners()\n+        in_front = np.argwhere(corners_3d[2, :] > 0).flatten()\n+        corners_3d = corners_3d[:, in_front]\n+\n+        # Project 3d box to 2d.\n+        corner_coords = (\n+            view_points(corners_3d, camera_intrinsic, True).T[:, :2].tolist()\n+        )\n+\n+        # Keep only corners that fall within the image.\n+        final_coords = post_process_coords(corner_coords)\n+\n+        # Skip if the convex hull of the re-projected corners\n+        # does not intersect the image canvas.\n+        if final_coords is None:\n+            continue\n+        else:\n+            min_x, min_y, max_x, max_y = final_coords\n+\n+        # Generate dictionary record to be included in the .json file.\n+        repro_rec = generate_record(\n+            ann_rec, min_x, min_y, max_x, max_y, sample_data_token, sd_rec[\"filename\"]\n+        )\n+\n+        # If mono3d=True, add 3D annotations in camera coordinates\n+        if mono3d and (repro_rec is not None):\n+            loc = box.center.tolist()\n+\n+            dim = box.wlh\n+            dim[[0, 1, 2]] = dim[[1, 2, 0]]  # convert wlh to our lhw\n+            dim = dim.tolist()\n+\n+            rot = box.orientation.yaw_pitch_roll[0]\n+            rot = [-rot]  # convert the rot to our cam coordinate\n+\n+            global_velo2d = nusc.box_velocity(box.token)[:2]\n+            global_velo3d = np.array([*global_velo2d, 0.0])\n+            e2g_r_mat = Quaternion(pose_rec[\"rotation\"]).rotation_matrix\n+            c2e_r_mat = Quaternion(cs_rec[\"rotation\"]).rotation_matrix\n+            cam_velo3d = (\n+                global_velo3d @ np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(c2e_r_mat).T\n+            )\n+            velo = cam_velo3d[0::2].tolist()\n+\n+            repro_rec[\"bbox_cam3d\"] = loc + dim + rot\n+            repro_rec[\"velo_cam3d\"] = velo\n+\n+            center3d = np.array(loc).reshape([1, 3])\n+            center2d = points_cam2img(center3d, camera_intrinsic, with_depth=True)\n+            repro_rec[\"center2d\"] = center2d.squeeze().tolist()\n+            # normalized center2D + depth\n+            # if samples with depth < 0 will be removed\n+            if repro_rec[\"center2d\"][2] <= 0:\n+                continue\n+\n+            ann_token = nusc.get(\"sample_annotation\", box.token)[\"attribute_tokens\"]\n+            if len(ann_token) == 0:\n+                attr_name = \"None\"\n+            else:\n+                attr_name = nusc.get(\"attribute\", ann_token[0])[\"name\"]\n+            attr_id = nus_attributes.index(attr_name)\n+            repro_rec[\"attribute_name\"] = attr_name\n+            repro_rec[\"attribute_id\"] = attr_id\n+\n+        repro_recs.append(repro_rec)\n+\n+    return repro_recs\n+\n+\n+def post_process_coords(\n+    corner_coords: List, imsize: Tuple[int, int] = (1600, 900)\n+) -> Union[Tuple[float, float, float, float], None]:\n+    \"\"\"Get the intersection of the convex hull of the reprojected bbox corners\n+    and the image canvas, return None if no intersection.\n+\n+    Args:\n+        corner_coords (list[int]): Corner coordinates of reprojected\n+            bounding box.\n+        imsize (tuple[int]): Size of the image canvas.\n+\n+    Return:\n+        tuple [float]: Intersection of the convex hull of the 2D box\n+            corners and the image canvas.\n+    \"\"\"\n+    polygon_from_2d_box = MultiPoint(corner_coords).convex_hull\n+    img_canvas = box(0, 0, imsize[0], imsize[1])\n+\n+    if polygon_from_2d_box.intersects(img_canvas):\n+        img_intersection = polygon_from_2d_box.intersection(img_canvas)\n+        intersection_coords = np.array(\n+            [coord for coord in img_intersection.exterior.coords]\n+        )\n+\n+        min_x = min(intersection_coords[:, 0])\n+        min_y = min(intersection_coords[:, 1])\n+        max_x = max(intersection_coords[:, 0])\n+        max_y = max(intersection_coords[:, 1])\n+\n+        return min_x, min_y, max_x, max_y\n+    else:\n+        return None\n+\n+\n+def generate_record(\n+    ann_rec: dict,\n+    x1: float,\n+    y1: float,\n+    x2: float,\n+    y2: float,\n+    sample_data_token: str,\n+    filename: str,\n+) -> OrderedDict:\n+    \"\"\"Generate one 2D annotation record given various information on top of\n+    the 2D bounding box coordinates.\n+\n+    Args:\n+        ann_rec (dict): Original 3d annotation record.\n+        x1 (float): Minimum value of the x coordinate.\n+        y1 (float): Minimum value of the y coordinate.\n+        x2 (float): Maximum value of the x coordinate.\n+        y2 (float): Maximum value of the y coordinate.\n+        sample_data_token (str): Sample data token.\n+        filename (str):The corresponding image file where the annotation\n+            is present.\n+\n+    Returns:\n+        dict: A sample 2D annotation record.\n+            - file_name (str): file name\n+            - image_id (str): sample data token\n+            - area (float): 2d box area\n+            - category_name (str): category name\n+            - category_id (int): category id\n+            - bbox (list[float]): left x, top y, dx, dy of 2d box\n+            - iscrowd (int): whether the area is crowd\n+    \"\"\"\n+    repro_rec = OrderedDict()\n+    repro_rec[\"sample_data_token\"] = sample_data_token\n+    coco_rec = dict()\n+\n+    relevant_keys = [\n+        \"attribute_tokens\",\n+        \"category_name\",\n+        \"instance_token\",\n+        \"next\",\n+        \"num_lidar_pts\",\n+        \"num_radar_pts\",\n+        \"prev\",\n+        \"sample_annotation_token\",\n+        \"sample_data_token\",\n+        \"visibility_token\",\n+    ]\n+\n+    for key, value in ann_rec.items():\n+        if key in relevant_keys:\n+            repro_rec[key] = value\n+\n+    repro_rec[\"bbox_corners\"] = [x1, y1, x2, y2]\n+    repro_rec[\"filename\"] = filename\n+\n+    coco_rec[\"file_name\"] = filename\n+    coco_rec[\"image_id\"] = sample_data_token\n+    coco_rec[\"area\"] = (y2 - y1) * (x2 - x1)\n+\n+    if repro_rec[\"category_name\"] not in NuScenesDataset.NameMapping:\n+        return None\n+    cat_name = NuScenesDataset.NameMapping[repro_rec[\"category_name\"]]\n+    coco_rec[\"category_name\"] = cat_name\n+    coco_rec[\"category_id\"] = nus_categories.index(cat_name)\n+    coco_rec[\"bbox\"] = [x1, y1, x2 - x1, y2 - y1]\n+    coco_rec[\"iscrowd\"] = 0\n+\n+    return coco_rec\n"
                },
                {
                    "date": 1717321362996,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -697,704 +697,4 @@\n     coco_rec[\"bbox\"] = [x1, y1, x2 - x1, y2 - y1]\n     coco_rec[\"iscrowd\"] = 0\n \n     return coco_rec\n-# Copyright (c) OpenMMLab. All rights reserved.\n-import os\n-from collections import OrderedDict\n-from os import path as osp\n-from typing import List, Tuple, Union\n-\n-import mmcv\n-import numpy as np\n-from nuscenes.nuscenes import NuScenes\n-from nuscenes.utils.geometry_utils import view_points\n-from pyquaternion import Quaternion\n-from shapely.geometry import MultiPoint, box\n-\n-from mmdet3d.core.bbox import points_cam2img\n-from mmdet3d.datasets import NuScenesDataset\n-\n-nus_categories = (\n-    \"car\",\n-    \"truck\",\n-    \"trailer\",\n-    \"bus\",\n-    \"construction_vehicle\",\n-    \"bicycle\",\n-    \"motorcycle\",\n-    \"pedestrian\",\n-    \"traffic_cone\",\n-    \"barrier\",\n-)\n-\n-nus_attributes = (\n-    \"cycle.with_rider\",\n-    \"cycle.without_rider\",\n-    \"pedestrian.moving\",\n-    \"pedestrian.standing\",\n-    \"pedestrian.sitting_lying_down\",\n-    \"vehicle.moving\",\n-    \"vehicle.parked\",\n-    \"vehicle.stopped\",\n-    \"None\",\n-)\n-\n-\n-def create_nuscenes_infos(\n-    root_path, info_prefix, version=\"v1.0-trainval\", max_sweeps=10\n-):\n-    \"\"\"Create info file of nuscene dataset.\n-\n-    Given the raw data, generate its related info file in pkl format.\n-\n-    Args:\n-        root_path (str): Path of the data root.\n-        info_prefix (str): Prefix of the info file to be generated.\n-        version (str, optional): Version of the data.\n-            Default: 'v1.0-trainval'.\n-        max_sweeps (int, optional): Max number of sweeps.\n-            Default: 10.\n-    \"\"\"\n-    from nuscenes.nuscenes import NuScenes\n-\n-    nusc = NuScenes(version=version, dataroot=root_path, verbose=True)\n-    from nuscenes.utils import splits\n-\n-    available_vers = [\"v1.0-trainval\", \"v1.0-test\", \"v1.0-mini\"]\n-    assert version in available_vers\n-    if version == \"v1.0-trainval\":\n-        train_scenes = splits.train\n-        val_scenes = splits.val\n-    elif version == \"v1.0-test\":\n-        train_scenes = splits.test\n-        val_scenes = []\n-    elif version == \"v1.0-mini\":\n-        train_scenes = splits.mini_train\n-        val_scenes = splits.mini_val\n-    else:\n-        raise ValueError(\"unknown\")\n-\n-    # filter existing scenes.\n-    available_scenes = get_available_scenes(nusc)\n-    available_scene_names = [s[\"name\"] for s in available_scenes]\n-    train_scenes = list(filter(lambda x: x in available_scene_names, train_scenes))\n-    val_scenes = list(filter(lambda x: x in available_scene_names, val_scenes))\n-    train_scenes = set(\n-        [\n-            available_scenes[available_scene_names.index(s)][\"token\"]\n-            for s in train_scenes\n-        ]\n-    )\n-    val_scenes = set(\n-        [available_scenes[available_scene_names.index(s)][\"token\"] for s in val_scenes]\n-    )\n-\n-    test = \"test\" in version\n-    if test:\n-        print(\"test scene: {}\".format(len(train_scenes)))\n-    else:\n-        print(\n-            \"train scene: {}, val scene: {}\".format(len(train_scenes), len(val_scenes))\n-        )\n-    train_nusc_infos, val_nusc_infos = _fill_trainval_infos(\n-        nusc, train_scenes, val_scenes, test, max_sweeps=max_sweeps\n-    )\n-\n-    metadata = dict(version=version)\n-    if test:\n-        print(\"test sample: {}\".format(len(train_nusc_infos)))\n-        data = dict(infos=train_nusc_infos, metadata=metadata)\n-        info_path = osp.join(root_path, \"{}_infos_test.pkl\".format(info_prefix))\n-        mmcv.dump(data, info_path)\n-    else:\n-        print(\n-            \"train sample: {}, val sample: {}\".format(\n-                len(train_nusc_infos), len(val_nusc_infos)\n-            )\n-        )\n-        data = dict(infos=train_nusc_infos, metadata=metadata)\n-        info_path = osp.join(root_path, \"{}_infos_train.pkl\".format(info_prefix))\n-        mmcv.dump(data, info_path)\n-        data[\"infos\"] = val_nusc_infos\n-        info_val_path = osp.join(root_path, \"{}_infos_val.pkl\".format(info_prefix))\n-        mmcv.dump(data, info_val_path)\n-\n-\n-def get_available_scenes(nusc):\n-    \"\"\"Get available scenes from the input nuscenes class.\n-\n-    Given the raw data, get the information of available scenes for\n-    further info generation.\n-\n-    Args:\n-        nusc (class): Dataset class in the nuScenes dataset.\n-\n-    Returns:\n-        available_scenes (list[dict]): List of basic information for the\n-            available scenes.\n-    \"\"\"\n-    available_scenes = []\n-    print(\"total scene num: {}\".format(len(nusc.scene)))\n-    for scene in nusc.scene:\n-        scene_token = scene[\"token\"]\n-        scene_rec = nusc.get(\"scene\", scene_token)\n-        sample_rec = nusc.get(\"sample\", scene_rec[\"first_sample_token\"])\n-        sd_rec = nusc.get(\"sample_data\", sample_rec[\"data\"][\"LIDAR_TOP\"])\n-        has_more_frames = True\n-        scene_not_exist = False\n-        while has_more_frames:\n-            lidar_path, boxes, _ = nusc.get_sample_data(sd_rec[\"token\"])\n-            lidar_path = str(lidar_path)\n-            if os.getcwd() in lidar_path:\n-                # path from lyftdataset is absolute path\n-                lidar_path = lidar_path.split(f\"{os.getcwd()}/\")[-1]\n-                # relative path\n-            if not mmcv.is_filepath(lidar_path):\n-                scene_not_exist = True\n-                break\n-            else:\n-                break\n-        if scene_not_exist:\n-            continue\n-        available_scenes.append(scene)\n-    print(\"exist scene num: {}\".format(len(available_scenes)))\n-    return available_scenes\n-\n-\n-def _fill_trainval_infos(nusc, train_scenes, val_scenes, test=False, max_sweeps=10):\n-    \"\"\"Generate the train/val infos from the raw data.\n-\n-    Args:\n-        nusc (:obj:`NuScenes`): Dataset class in the nuScenes dataset.\n-        train_scenes (list[str]): Basic information of training scenes.\n-        val_scenes (list[str]): Basic information of validation scenes.\n-        test (bool, optional): Whether use the test mode. In test mode, no\n-            annotations can be accessed. Default: False.\n-        max_sweeps (int, optional): Max number of sweeps. Default: 10.\n-\n-    Returns:\n-        tuple[list[dict]]: Information of training set and validation set\n-            that will be saved to the info file.\n-    \"\"\"\n-    train_nusc_infos = []\n-    val_nusc_infos = []\n-\n-    for sample in mmcv.track_iter_progress(nusc.sample):\n-        lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n-        sd_rec = nusc.get(\"sample_data\", sample[\"data\"][\"LIDAR_TOP\"])\n-        cs_record = nusc.get(\"calibrated_sensor\", sd_rec[\"calibrated_sensor_token\"])\n-        pose_record = nusc.get(\"ego_pose\", sd_rec[\"ego_pose_token\"])\n-        lidar_path, boxes, _ = nusc.get_sample_data(lidar_token)\n-\n-        mmcv.check_file_exist(lidar_path)\n-\n-        info = {\n-            \"lidar_path\": lidar_path,\n-            \"token\": sample[\"token\"],\n-            \"sweeps\": [],\n-            \"cams\": dict(),\n-            \"radars\": dict(),\n-            \"lidar2ego_translation\": cs_record[\"translation\"],\n-            \"lidar2ego_rotation\": cs_record[\"rotation\"],\n-            \"ego2global_translation\": pose_record[\"translation\"],\n-            \"ego2global_rotation\": pose_record[\"rotation\"],\n-            \"timestamp\": sample[\"timestamp\"],\n-        }\n-\n-        l2e_r = info[\"lidar2ego_rotation\"]\n-        l2e_t = info[\"lidar2ego_translation\"]\n-        e2g_r = info[\"ego2global_rotation\"]\n-        e2g_t = info[\"ego2global_translation\"]\n-        l2e_r_mat = Quaternion(l2e_r).rotation_matrix\n-        e2g_r_mat = Quaternion(e2g_r).rotation_matrix\n-\n-        # obtain 6 image's information per frame\n-        camera_types = [\n-            \"CAM_FRONT\",\n-            \"CAM_FRONT_RIGHT\",\n-            \"CAM_FRONT_LEFT\",\n-            \"CAM_BACK\",\n-            \"CAM_BACK_LEFT\",\n-            \"CAM_BACK_RIGHT\",\n-        ]\n-        for cam in camera_types:\n-            cam_token = sample[\"data\"][cam]\n-            cam_path, _, cam_intrinsic = nusc.get_sample_data(cam_token)\n-            cam_info = obtain_sensor2top(\n-                nusc, cam_token, l2e_t, l2e_r_mat, e2g_t, e2g_r_mat, cam\n-            )\n-            cam_info.update(cam_intrinsic=cam_intrinsic)\n-            info[\"cams\"].update({cam: cam_info})\n-\n-        # Add radar information to the annotation read file\n-        radar_names = [\n-            \"RADAR_FRONT\",\n-            \"RADAR_FRONT_LEFT\",\n-            \"RADAR_FRONT_RIGHT\",\n-            \"RADAR_BACK_LEFT\",\n-            \"RADAR_BACK_RIGHT\",\n-        ]\n-\n-        for radar_name in radar_names:\n-            radar_token = sample[\"data\"][radar_name]\n-            radar_rec = nusc.get(\"sample_data\", radar_token)\n-            sweeps = []\n-\n-            while len(sweeps) < 5:\n-                if not radar_rec[\"prev\"] == \"\":\n-                    radar_path, _, radar_intrin = nusc.get_sample_data(radar_token)\n-\n-                    radar_info = obtain_sensor2top(\n-                        nusc,\n-                        radar_token,\n-                        l2e_t,\n-                        l2e_r_mat,\n-                        e2g_t,\n-                        e2g_r_mat,\n-                        radar_name,\n-                    )\n-                    sweeps.append(radar_info)\n-                    radar_token = radar_rec[\"prev\"]\n-                    radar_rec = nusc.get(\"sample_data\", radar_token)\n-                else:\n-                    radar_path, _, radar_intrin = nusc.get_sample_data(radar_token)\n-\n-                    radar_info = obtain_sensor2top(\n-                        nusc,\n-                        radar_token,\n-                        l2e_t,\n-                        l2e_r_mat,\n-                        e2g_t,\n-                        e2g_r_mat,\n-                        radar_name,\n-                    )\n-                    sweeps.append(radar_info)\n-\n-            info[\"radars\"].update({radar_name: sweeps})\n-\n-        # obtain sweeps for a single key-frame\n-        sd_rec = nusc.get(\"sample_data\", sample[\"data\"][\"LIDAR_TOP\"])\n-        sweeps = []\n-        while len(sweeps) < max_sweeps:\n-            if not sd_rec[\"prev\"] == \"\":\n-                sweep = obtain_sensor2top(\n-                    nusc, sd_rec[\"prev\"], l2e_t, l2e_r_mat, e2g_t, e2g_r_mat, \"lidar\"\n-                )\n-                sweeps.append(sweep)\n-                sd_rec = nusc.get(\"sample_data\", sd_rec[\"prev\"])\n-            else:\n-                break\n-        info[\"sweeps\"] = sweeps\n-        # obtain annotation\n-        if not test:\n-            annotations = [\n-                nusc.get(\"sample_annotation\", token) for token in sample[\"anns\"]\n-            ]\n-            locs = np.array([b.center for b in boxes]).reshape(-1, 3)\n-            dims = np.array([b.wlh for b in boxes]).reshape(-1, 3)\n-            rots = np.array([b.orientation.yaw_pitch_roll[0] for b in boxes]).reshape(\n-                -1, 1\n-            )\n-            velocity = np.array(\n-                [nusc.box_velocity(token)[:2] for token in sample[\"anns\"]]\n-            )\n-            valid_flag = np.array(\n-                [\n-                    (anno[\"num_lidar_pts\"] + anno[\"num_radar_pts\"]) > 0\n-                    for anno in annotations\n-                ],\n-                dtype=bool,\n-            ).reshape(-1)\n-            # convert velo from global to lidar\n-            for i in range(len(boxes)):\n-                velo = np.array([*velocity[i], 0.0])\n-                velo = velo @ np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n-                velocity[i] = velo[:2]\n-\n-            names = [b.name for b in boxes]\n-            for i in range(len(names)):\n-                if names[i] in NuScenesDataset.NameMapping:\n-                    names[i] = NuScenesDataset.NameMapping[names[i]]\n-            names = np.array(names)\n-            # we need to convert box size to\n-            # the format of our lidar coordinate system\n-            # which is x_size, y_size, z_size (corresponding to l, w, h)\n-            gt_boxes = np.concatenate([locs, dims[:, [1, 0, 2]], rots], axis=1)\n-            \n-            \n-            # we need to convert rot to SECOND format.\n-            # gt_boxes = np.concatenate([locs, dims, -rots - np.pi / 2], axis=1)\n-            assert len(gt_boxes) == len(\n-                annotations\n-            ), f\"{len(gt_boxes)}, {len(annotations)}\"\n-            info[\"gt_boxes\"] = gt_boxes\n-            info[\"gt_names\"] = names\n-            info[\"gt_velocity\"] = velocity.reshape(-1, 2)\n-            info[\"num_lidar_pts\"] = np.array([a[\"num_lidar_pts\"] for a in annotations])\n-            info[\"num_radar_pts\"] = np.array([a[\"num_radar_pts\"] for a in annotations])\n-            info[\"valid_flag\"] = valid_flag\n-\n-        if sample[\"scene_token\"] in train_scenes:\n-            train_nusc_infos.append(info)\n-        else:\n-            val_nusc_infos.append(info)\n-\n-    return train_nusc_infos, val_nusc_infos\n-\n-\n-def obtain_sensor2top(\n-    nusc, sensor_token, l2e_t, l2e_r_mat, e2g_t, e2g_r_mat, sensor_type=\"lidar\"\n-):\n-    \"\"\"Obtain the info with RT matric from general sensor to Top LiDAR.\n-\n-    Args:\n-        nusc (class): Dataset class in the nuScenes dataset.\n-        sensor_token (str): Sample data token corresponding to the\n-            specific sensor type.\n-        l2e_t (np.ndarray): Translation from lidar to ego in shape (1, 3).\n-        l2e_r_mat (np.ndarray): Rotation matrix from lidar to ego\n-            in shape (3, 3).\n-        e2g_t (np.ndarray): Translation from ego to global in shape (1, 3).\n-        e2g_r_mat (np.ndarray): Rotation matrix from ego to global\n-            in shape (3, 3).\n-        sensor_type (str, optional): Sensor to calibrate. Default: 'lidar'.\n-\n-    Returns:\n-        sweep (dict): Sweep information after transformation.\n-    \"\"\"\n-    sd_rec = nusc.get(\"sample_data\", sensor_token)\n-    cs_record = nusc.get(\"calibrated_sensor\", sd_rec[\"calibrated_sensor_token\"])\n-    pose_record = nusc.get(\"ego_pose\", sd_rec[\"ego_pose_token\"])\n-    data_path = str(nusc.get_sample_data_path(sd_rec[\"token\"]))\n-    if os.getcwd() in data_path:  # path from lyftdataset is absolute path\n-        data_path = data_path.split(f\"{os.getcwd()}/\")[-1]  # relative path\n-    sweep = {\n-        \"data_path\": data_path,\n-        \"type\": sensor_type,\n-        \"sample_data_token\": sd_rec[\"token\"],\n-        \"sensor2ego_translation\": cs_record[\"translation\"],\n-        \"sensor2ego_rotation\": cs_record[\"rotation\"],\n-        \"ego2global_translation\": pose_record[\"translation\"],\n-        \"ego2global_rotation\": pose_record[\"rotation\"],\n-        \"timestamp\": sd_rec[\"timestamp\"],\n-    }\n-    l2e_r_s = sweep[\"sensor2ego_rotation\"]\n-    l2e_t_s = sweep[\"sensor2ego_translation\"]\n-    e2g_r_s = sweep[\"ego2global_rotation\"]\n-    e2g_t_s = sweep[\"ego2global_translation\"]\n-\n-    # obtain the RT from sensor to Top LiDAR\n-    # sweep->ego->global->ego'->lidar\n-    l2e_r_s_mat = Quaternion(l2e_r_s).rotation_matrix\n-    e2g_r_s_mat = Quaternion(e2g_r_s).rotation_matrix\n-    R = (l2e_r_s_mat.T @ e2g_r_s_mat.T) @ (\n-        np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n-    )\n-    T = (l2e_t_s @ e2g_r_s_mat.T + e2g_t_s) @ (\n-        np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n-    )\n-    T -= (\n-        e2g_t @ (np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T)\n-        + l2e_t @ np.linalg.inv(l2e_r_mat).T\n-    )\n-    sweep[\"sensor2lidar_rotation\"] = R.T  # points @ R.T + T\n-    sweep[\"sensor2lidar_translation\"] = T\n-    return sweep\n-\n-\n-def export_2d_annotation(root_path, info_path, version, mono3d=True):\n-    \"\"\"Export 2d annotation from the info file and raw data.\n-\n-    Args:\n-        root_path (str): Root path of the raw data.\n-        info_path (str): Path of the info file.\n-        version (str): Dataset version.\n-        mono3d (bool, optional): Whether to export mono3d annotation.\n-            Default: True.\n-    \"\"\"\n-    # get bbox annotations for camera\n-    camera_types = [\n-        \"CAM_FRONT\",\n-        \"CAM_FRONT_RIGHT\",\n-        \"CAM_FRONT_LEFT\",\n-        \"CAM_BACK\",\n-        \"CAM_BACK_LEFT\",\n-        \"CAM_BACK_RIGHT\",\n-    ]\n-    nusc_infos = mmcv.load(info_path)[\"infos\"]\n-    nusc = NuScenes(version=version, dataroot=root_path, verbose=True)\n-    # info_2d_list = []\n-    cat2Ids = [\n-        dict(id=nus_categories.index(cat_name), name=cat_name)\n-        for cat_name in nus_categories\n-    ]\n-    coco_ann_id = 0\n-    coco_2d_dict = dict(annotations=[], images=[], categories=cat2Ids)\n-    for info in mmcv.track_iter_progress(nusc_infos):\n-        for cam in camera_types:\n-            cam_info = info[\"cams\"][cam]\n-            coco_infos = get_2d_boxes(\n-                nusc,\n-                cam_info[\"sample_data_token\"],\n-                visibilities=[\"\", \"1\", \"2\", \"3\", \"4\"],\n-                mono3d=mono3d,\n-            )\n-            (height, width, _) = mmcv.imread(cam_info[\"data_path\"]).shape\n-            coco_2d_dict[\"images\"].append(\n-                dict(\n-                    file_name=cam_info[\"data_path\"].split(\"data/nuscenes/\")[-1],\n-                    id=cam_info[\"sample_data_token\"],\n-                    token=info[\"token\"],\n-                    cam2ego_rotation=cam_info[\"sensor2ego_rotation\"],\n-                    cam2ego_translation=cam_info[\"sensor2ego_translation\"],\n-                    ego2global_rotation=info[\"ego2global_rotation\"],\n-                    ego2global_translation=info[\"ego2global_translation\"],\n-                    cam_intrinsic=cam_info[\"cam_intrinsic\"],\n-                    width=width,\n-                    height=height,\n-                )\n-            )\n-            for coco_info in coco_infos:\n-                if coco_info is None:\n-                    continue\n-                # add an empty key for coco format\n-                coco_info[\"segmentation\"] = []\n-                coco_info[\"id\"] = coco_ann_id\n-                coco_2d_dict[\"annotations\"].append(coco_info)\n-                coco_ann_id += 1\n-    if mono3d:\n-        json_prefix = f\"{info_path[:-4]}_mono3d\"\n-    else:\n-        json_prefix = f\"{info_path[:-4]}\"\n-    mmcv.dump(coco_2d_dict, f\"{json_prefix}.coco.json\")\n-\n-\n-def get_2d_boxes(nusc, sample_data_token: str, visibilities: List[str], mono3d=True):\n-    \"\"\"Get the 2D annotation records for a given `sample_data_token`.\n-\n-    Args:\n-        sample_data_token (str): Sample data token belonging to a camera\n-            keyframe.\n-        visibilities (list[str]): Visibility filter.\n-        mono3d (bool): Whether to get boxes with mono3d annotation.\n-\n-    Return:\n-        list[dict]: List of 2D annotation record that belongs to the input\n-            `sample_data_token`.\n-    \"\"\"\n-\n-    # Get the sample data and the sample corresponding to that sample data.\n-    sd_rec = nusc.get(\"sample_data\", sample_data_token)\n-\n-    assert sd_rec[\"sensor_modality\"] == \"camera\", (\n-        \"Error: get_2d_boxes only works\" \" for camera sample_data!\"\n-    )\n-    if not sd_rec[\"is_key_frame\"]:\n-        raise ValueError(\"The 2D re-projections are available only for keyframes.\")\n-\n-    s_rec = nusc.get(\"sample\", sd_rec[\"sample_token\"])\n-\n-    # Get the calibrated sensor and ego pose\n-    # record to get the transformation matrices.\n-    cs_rec = nusc.get(\"calibrated_sensor\", sd_rec[\"calibrated_sensor_token\"])\n-    pose_rec = nusc.get(\"ego_pose\", sd_rec[\"ego_pose_token\"])\n-    camera_intrinsic = np.array(cs_rec[\"camera_intrinsic\"])\n-\n-    # Get all the annotation with the specified visibilties.\n-    ann_recs = [nusc.get(\"sample_annotation\", token) for token in s_rec[\"anns\"]]\n-    ann_recs = [\n-        ann_rec for ann_rec in ann_recs if (ann_rec[\"visibility_token\"] in visibilities)\n-    ]\n-\n-    repro_recs = []\n-\n-    for ann_rec in ann_recs:\n-        # Augment sample_annotation with token information.\n-        ann_rec[\"sample_annotation_token\"] = ann_rec[\"token\"]\n-        ann_rec[\"sample_data_token\"] = sample_data_token\n-\n-        # Get the box in global coordinates.\n-        box = nusc.get_box(ann_rec[\"token\"])\n-\n-        # Move them to the ego-pose frame.\n-        box.translate(-np.array(pose_rec[\"translation\"]))\n-        box.rotate(Quaternion(pose_rec[\"rotation\"]).inverse)\n-\n-        # Move them to the calibrated sensor frame.\n-        box.translate(-np.array(cs_rec[\"translation\"]))\n-        box.rotate(Quaternion(cs_rec[\"rotation\"]).inverse)\n-\n-        # Filter out the corners that are not in front of the calibrated\n-        # sensor.\n-        corners_3d = box.corners()\n-        in_front = np.argwhere(corners_3d[2, :] > 0).flatten()\n-        corners_3d = corners_3d[:, in_front]\n-\n-        # Project 3d box to 2d.\n-        corner_coords = (\n-            view_points(corners_3d, camera_intrinsic, True).T[:, :2].tolist()\n-        )\n-\n-        # Keep only corners that fall within the image.\n-        final_coords = post_process_coords(corner_coords)\n-\n-        # Skip if the convex hull of the re-projected corners\n-        # does not intersect the image canvas.\n-        if final_coords is None:\n-            continue\n-        else:\n-            min_x, min_y, max_x, max_y = final_coords\n-\n-        # Generate dictionary record to be included in the .json file.\n-        repro_rec = generate_record(\n-            ann_rec, min_x, min_y, max_x, max_y, sample_data_token, sd_rec[\"filename\"]\n-        )\n-\n-        # If mono3d=True, add 3D annotations in camera coordinates\n-        if mono3d and (repro_rec is not None):\n-            loc = box.center.tolist()\n-\n-            dim = box.wlh\n-            dim[[0, 1, 2]] = dim[[1, 2, 0]]  # convert wlh to our lhw\n-            dim = dim.tolist()\n-\n-            rot = box.orientation.yaw_pitch_roll[0]\n-            rot = [-rot]  # convert the rot to our cam coordinate\n-\n-            global_velo2d = nusc.box_velocity(box.token)[:2]\n-            global_velo3d = np.array([*global_velo2d, 0.0])\n-            e2g_r_mat = Quaternion(pose_rec[\"rotation\"]).rotation_matrix\n-            c2e_r_mat = Quaternion(cs_rec[\"rotation\"]).rotation_matrix\n-            cam_velo3d = (\n-                global_velo3d @ np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(c2e_r_mat).T\n-            )\n-            velo = cam_velo3d[0::2].tolist()\n-\n-            repro_rec[\"bbox_cam3d\"] = loc + dim + rot\n-            repro_rec[\"velo_cam3d\"] = velo\n-\n-            center3d = np.array(loc).reshape([1, 3])\n-            center2d = points_cam2img(center3d, camera_intrinsic, with_depth=True)\n-            repro_rec[\"center2d\"] = center2d.squeeze().tolist()\n-            # normalized center2D + depth\n-            # if samples with depth < 0 will be removed\n-            if repro_rec[\"center2d\"][2] <= 0:\n-                continue\n-\n-            ann_token = nusc.get(\"sample_annotation\", box.token)[\"attribute_tokens\"]\n-            if len(ann_token) == 0:\n-                attr_name = \"None\"\n-            else:\n-                attr_name = nusc.get(\"attribute\", ann_token[0])[\"name\"]\n-            attr_id = nus_attributes.index(attr_name)\n-            repro_rec[\"attribute_name\"] = attr_name\n-            repro_rec[\"attribute_id\"] = attr_id\n-\n-        repro_recs.append(repro_rec)\n-\n-    return repro_recs\n-\n-\n-def post_process_coords(\n-    corner_coords: List, imsize: Tuple[int, int] = (1600, 900)\n-) -> Union[Tuple[float, float, float, float], None]:\n-    \"\"\"Get the intersection of the convex hull of the reprojected bbox corners\n-    and the image canvas, return None if no intersection.\n-\n-    Args:\n-        corner_coords (list[int]): Corner coordinates of reprojected\n-            bounding box.\n-        imsize (tuple[int]): Size of the image canvas.\n-\n-    Return:\n-        tuple [float]: Intersection of the convex hull of the 2D box\n-            corners and the image canvas.\n-    \"\"\"\n-    polygon_from_2d_box = MultiPoint(corner_coords).convex_hull\n-    img_canvas = box(0, 0, imsize[0], imsize[1])\n-\n-    if polygon_from_2d_box.intersects(img_canvas):\n-        img_intersection = polygon_from_2d_box.intersection(img_canvas)\n-        intersection_coords = np.array(\n-            [coord for coord in img_intersection.exterior.coords]\n-        )\n-\n-        min_x = min(intersection_coords[:, 0])\n-        min_y = min(intersection_coords[:, 1])\n-        max_x = max(intersection_coords[:, 0])\n-        max_y = max(intersection_coords[:, 1])\n-\n-        return min_x, min_y, max_x, max_y\n-    else:\n-        return None\n-\n-\n-def generate_record(\n-    ann_rec: dict,\n-    x1: float,\n-    y1: float,\n-    x2: float,\n-    y2: float,\n-    sample_data_token: str,\n-    filename: str,\n-) -> OrderedDict:\n-    \"\"\"Generate one 2D annotation record given various information on top of\n-    the 2D bounding box coordinates.\n-\n-    Args:\n-        ann_rec (dict): Original 3d annotation record.\n-        x1 (float): Minimum value of the x coordinate.\n-        y1 (float): Minimum value of the y coordinate.\n-        x2 (float): Maximum value of the x coordinate.\n-        y2 (float): Maximum value of the y coordinate.\n-        sample_data_token (str): Sample data token.\n-        filename (str):The corresponding image file where the annotation\n-            is present.\n-\n-    Returns:\n-        dict: A sample 2D annotation record.\n-            - file_name (str): file name\n-            - image_id (str): sample data token\n-            - area (float): 2d box area\n-            - category_name (str): category name\n-            - category_id (int): category id\n-            - bbox (list[float]): left x, top y, dx, dy of 2d box\n-            - iscrowd (int): whether the area is crowd\n-    \"\"\"\n-    repro_rec = OrderedDict()\n-    repro_rec[\"sample_data_token\"] = sample_data_token\n-    coco_rec = dict()\n-\n-    relevant_keys = [\n-        \"attribute_tokens\",\n-        \"category_name\",\n-        \"instance_token\",\n-        \"next\",\n-        \"num_lidar_pts\",\n-        \"num_radar_pts\",\n-        \"prev\",\n-        \"sample_annotation_token\",\n-        \"sample_data_token\",\n-        \"visibility_token\",\n-    ]\n-\n-    for key, value in ann_rec.items():\n-        if key in relevant_keys:\n-            repro_rec[key] = value\n-\n-    repro_rec[\"bbox_corners\"] = [x1, y1, x2, y2]\n-    repro_rec[\"filename\"] = filename\n-\n-    coco_rec[\"file_name\"] = filename\n-    coco_rec[\"image_id\"] = sample_data_token\n-    coco_rec[\"area\"] = (y2 - y1) * (x2 - x1)\n-\n-    if repro_rec[\"category_name\"] not in NuScenesDataset.NameMapping:\n-        return None\n-    cat_name = NuScenesDataset.NameMapping[repro_rec[\"category_name\"]]\n-    coco_rec[\"category_name\"] = cat_name\n-    coco_rec[\"category_id\"] = nus_categories.index(cat_name)\n-    coco_rec[\"bbox\"] = [x1, y1, x2 - x1, y2 - y1]\n-    coco_rec[\"iscrowd\"] = 0\n-\n-    return coco_rec\n"
                },
                {
                    "date": 1718502195042,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,700 @@\n+# Copyright (c) OpenMMLab. All rights reserved.\n+import os\n+from collections import OrderedDict\n+from os import path as osp\n+from typing import List, Tuple, Union\n+\n+import mmcv\n+import numpy as np\n+from nuscenes.nuscenes import NuScenes\n+from nuscenes.utils.geometry_utils import view_points\n+from pyquaternion import Quaternion\n+from shapely.geometry import MultiPoint, box\n+\n+from mmdet3d.core.bbox import points_cam2img\n+from mmdet3d.datasets import NuScenesDataset\n+\n+nus_categories = (\n+    \"car\",\n+    \"truck\",\n+    \"trailer\",\n+    \"bus\",\n+    \"construction_vehicle\",\n+    \"bicycle\",\n+    \"motorcycle\",\n+    \"pedestrian\",\n+    \"traffic_cone\",\n+    \"barrier\",\n+)\n+\n+nus_attributes = (\n+    \"cycle.with_rider\",\n+    \"cycle.without_rider\",\n+    \"pedestrian.moving\",\n+    \"pedestrian.standing\",\n+    \"pedestrian.sitting_lying_down\",\n+    \"vehicle.moving\",\n+    \"vehicle.parked\",\n+    \"vehicle.stopped\",\n+    \"None\",\n+)\n+\n+\n+def create_nuscenes_infos(\n+    root_path, info_prefix, version=\"v1.0-trainval\", max_sweeps=10\n+):\n+    \"\"\"Create info file of nuscene dataset.\n+\n+    Given the raw data, generate its related info file in pkl format.\n+\n+    Args:\n+        root_path (str): Path of the data root.\n+        info_prefix (str): Prefix of the info file to be generated.\n+        version (str, optional): Version of the data.\n+            Default: 'v1.0-trainval'.\n+        max_sweeps (int, optional): Max number of sweeps.\n+            Default: 10.\n+    \"\"\"\n+    from nuscenes.nuscenes import NuScenes\n+\n+    nusc = NuScenes(version=version, dataroot=root_path, verbose=True)\n+    from nuscenes.utils import splits\n+\n+    available_vers = [\"v1.0-trainval\", \"v1.0-test\", \"v1.0-mini\"]\n+    assert version in available_vers\n+    if version == \"v1.0-trainval\":\n+        train_scenes = splits.train\n+        val_scenes = splits.val\n+    elif version == \"v1.0-test\":\n+        train_scenes = splits.test\n+        val_scenes = []\n+    elif version == \"v1.0-mini\":\n+        train_scenes = splits.mini_train\n+        val_scenes = splits.mini_val\n+    else:\n+        raise ValueError(\"unknown\")\n+\n+    # filter existing scenes.\n+    available_scenes = get_available_scenes(nusc)\n+    available_scene_names = [s[\"name\"] for s in available_scenes]\n+    train_scenes = list(filter(lambda x: x in available_scene_names, train_scenes))\n+    val_scenes = list(filter(lambda x: x in available_scene_names, val_scenes))\n+    train_scenes = set(\n+        [\n+            available_scenes[available_scene_names.index(s)][\"token\"]\n+            for s in train_scenes\n+        ]\n+    )\n+    val_scenes = set(\n+        [available_scenes[available_scene_names.index(s)][\"token\"] for s in val_scenes]\n+    )\n+\n+    test = \"test\" in version\n+    if test:\n+        print(\"test scene: {}\".format(len(train_scenes)))\n+    else:\n+        print(\n+            \"train scene: {}, val scene: {}\".format(len(train_scenes), len(val_scenes))\n+        )\n+    train_nusc_infos, val_nusc_infos = _fill_trainval_infos(\n+        nusc, train_scenes, val_scenes, test, max_sweeps=max_sweeps\n+    )\n+\n+    metadata = dict(version=version)\n+    if test:\n+        print(\"test sample: {}\".format(len(train_nusc_infos)))\n+        data = dict(infos=train_nusc_infos, metadata=metadata)\n+        info_path = osp.join(root_path, \"{}_infos_test.pkl\".format(info_prefix))\n+        mmcv.dump(data, info_path)\n+    else:\n+        print(\n+            \"train sample: {}, val sample: {}\".format(\n+                len(train_nusc_infos), len(val_nusc_infos)\n+            )\n+        )\n+        data = dict(infos=train_nusc_infos, metadata=metadata)\n+        info_path = osp.join(root_path, \"{}_infos_train.pkl\".format(info_prefix))\n+        mmcv.dump(data, info_path)\n+        data[\"infos\"] = val_nusc_infos\n+        info_val_path = osp.join(root_path, \"{}_infos_val.pkl\".format(info_prefix))\n+        mmcv.dump(data, info_val_path)\n+\n+\n+def get_available_scenes(nusc):\n+    \"\"\"Get available scenes from the input nuscenes class.\n+\n+    Given the raw data, get the information of available scenes for\n+    further info generation.\n+\n+    Args:\n+        nusc (class): Dataset class in the nuScenes dataset.\n+\n+    Returns:\n+        available_scenes (list[dict]): List of basic information for the\n+            available scenes.\n+    \"\"\"\n+    available_scenes = []\n+    print(\"total scene num: {}\".format(len(nusc.scene)))\n+    for scene in nusc.scene:\n+        scene_token = scene[\"token\"]\n+        scene_rec = nusc.get(\"scene\", scene_token)\n+        sample_rec = nusc.get(\"sample\", scene_rec[\"first_sample_token\"])\n+        sd_rec = nusc.get(\"sample_data\", sample_rec[\"data\"][\"LIDAR_TOP\"])\n+        has_more_frames = True\n+        scene_not_exist = False\n+        while has_more_frames:\n+            lidar_path, boxes, _ = nusc.get_sample_data(sd_rec[\"token\"])\n+            lidar_path = str(lidar_path)\n+            if os.getcwd() in lidar_path:\n+                # path from lyftdataset is absolute path\n+                lidar_path = lidar_path.split(f\"{os.getcwd()}/\")[-1]\n+                # relative path\n+            if not mmcv.is_filepath(lidar_path):\n+                scene_not_exist = True\n+                break\n+            else:\n+                break\n+        if scene_not_exist:\n+            continue\n+        available_scenes.append(scene)\n+    print(\"exist scene num: {}\".format(len(available_scenes)))\n+    return available_scenes\n+\n+\n+def _fill_trainval_infos(nusc, train_scenes, val_scenes, test=False, max_sweeps=10):\n+    \"\"\"Generate the train/val infos from the raw data.\n+\n+    Args:\n+        nusc (:obj:`NuScenes`): Dataset class in the nuScenes dataset.\n+        train_scenes (list[str]): Basic information of training scenes.\n+        val_scenes (list[str]): Basic information of validation scenes.\n+        test (bool, optional): Whether use the test mode. In test mode, no\n+            annotations can be accessed. Default: False.\n+        max_sweeps (int, optional): Max number of sweeps. Default: 10.\n+\n+    Returns:\n+        tuple[list[dict]]: Information of training set and validation set\n+            that will be saved to the info file.\n+    \"\"\"\n+    train_nusc_infos = []\n+    val_nusc_infos = []\n+\n+    for sample in mmcv.track_iter_progress(nusc.sample):\n+        lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n+        sd_rec = nusc.get(\"sample_data\", sample[\"data\"][\"LIDAR_TOP\"])\n+        cs_record = nusc.get(\"calibrated_sensor\", sd_rec[\"calibrated_sensor_token\"])\n+        pose_record = nusc.get(\"ego_pose\", sd_rec[\"ego_pose_token\"])\n+        lidar_path, boxes, _ = nusc.get_sample_data(lidar_token)\n+\n+        mmcv.check_file_exist(lidar_path)\n+\n+        info = {\n+            \"lidar_path\": lidar_path,\n+            \"token\": sample[\"token\"],\n+            \"sweeps\": [],\n+            \"cams\": dict(),\n+            \"radars\": dict(),\n+            \"lidar2ego_translation\": cs_record[\"translation\"],\n+            \"lidar2ego_rotation\": cs_record[\"rotation\"],\n+            \"ego2global_translation\": pose_record[\"translation\"],\n+            \"ego2global_rotation\": pose_record[\"rotation\"],\n+            \"timestamp\": sample[\"timestamp\"],\n+        }\n+\n+        l2e_r = info[\"lidar2ego_rotation\"]\n+        l2e_t = info[\"lidar2ego_translation\"]\n+        e2g_r = info[\"ego2global_rotation\"]\n+        e2g_t = info[\"ego2global_translation\"]\n+        l2e_r_mat = Quaternion(l2e_r).rotation_matrix\n+        e2g_r_mat = Quaternion(e2g_r).rotation_matrix\n+\n+        # obtain 6 image's information per frame\n+        camera_types = [\n+            \"CAM_FRONT\",\n+            \"CAM_FRONT_RIGHT\",\n+            \"CAM_FRONT_LEFT\",\n+            \"CAM_BACK\",\n+            \"CAM_BACK_LEFT\",\n+            \"CAM_BACK_RIGHT\",\n+        ]\n+        for cam in camera_types:\n+            cam_token = sample[\"data\"][cam]\n+            cam_path, _, cam_intrinsic = nusc.get_sample_data(cam_token)\n+            cam_info = obtain_sensor2top(\n+                nusc, cam_token, l2e_t, l2e_r_mat, e2g_t, e2g_r_mat, cam\n+            )\n+            cam_info.update(cam_intrinsic=cam_intrinsic)\n+            info[\"cams\"].update({cam: cam_info})\n+\n+        # Add radar information to the annotation read file\n+        radar_names = [\n+            \"RADAR_FRONT\",\n+            \"RADAR_FRONT_LEFT\",\n+            \"RADAR_FRONT_RIGHT\",\n+            \"RADAR_BACK_LEFT\",\n+            \"RADAR_BACK_RIGHT\",\n+        ]\n+\n+        for radar_name in radar_names:\n+            radar_token = sample[\"data\"][radar_name]\n+            radar_rec = nusc.get(\"sample_data\", radar_token)\n+            sweeps = []\n+\n+            while len(sweeps) < 5:\n+                if not radar_rec[\"prev\"] == \"\":\n+                    radar_path, _, radar_intrin = nusc.get_sample_data(radar_token)\n+\n+                    radar_info = obtain_sensor2top(\n+                        nusc,\n+                        radar_token,\n+                        l2e_t,\n+                        l2e_r_mat,\n+                        e2g_t,\n+                        e2g_r_mat,\n+                        radar_name,\n+                    )\n+                    sweeps.append(radar_info)\n+                    radar_token = radar_rec[\"prev\"]\n+                    radar_rec = nusc.get(\"sample_data\", radar_token)\n+                else:\n+                    radar_path, _, radar_intrin = nusc.get_sample_data(radar_token)\n+\n+                    radar_info = obtain_sensor2top(\n+                        nusc,\n+                        radar_token,\n+                        l2e_t,\n+                        l2e_r_mat,\n+                        e2g_t,\n+                        e2g_r_mat,\n+                        radar_name,\n+                    )\n+                    sweeps.append(radar_info)\n+\n+            info[\"radars\"].update({radar_name: sweeps})\n+\n+        # obtain sweeps for a single key-frame\n+        sd_rec = nusc.get(\"sample_data\", sample[\"data\"][\"LIDAR_TOP\"])\n+        sweeps = []\n+        while len(sweeps) < max_sweeps:\n+            if not sd_rec[\"prev\"] == \"\":\n+                sweep = obtain_sensor2top(\n+                    nusc, sd_rec[\"prev\"], l2e_t, l2e_r_mat, e2g_t, e2g_r_mat, \"lidar\"\n+                )\n+                sweeps.append(sweep)\n+                sd_rec = nusc.get(\"sample_data\", sd_rec[\"prev\"])\n+            else:\n+                break\n+        info[\"sweeps\"] = sweeps\n+        # obtain annotation\n+        if not test:\n+            annotations = [\n+                nusc.get(\"sample_annotation\", token) for token in sample[\"anns\"]\n+            ]\n+            locs = np.array([b.center for b in boxes]).reshape(-1, 3)\n+            dims = np.array([b.wlh for b in boxes]).reshape(-1, 3)\n+            rots = np.array([b.orientation.yaw_pitch_roll[0] for b in boxes]).reshape(\n+                -1, 1\n+            )\n+            velocity = np.array(\n+                [nusc.box_velocity(token)[:2] for token in sample[\"anns\"]]\n+            )\n+            valid_flag = np.array(\n+                [\n+                    (anno[\"num_lidar_pts\"] + anno[\"num_radar_pts\"]) > 0\n+                    for anno in annotations\n+                ],\n+                dtype=bool,\n+            ).reshape(-1)\n+            # convert velo from global to lidar\n+            for i in range(len(boxes)):\n+                velo = np.array([*velocity[i], 0.0])\n+                velo = velo @ np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n+                velocity[i] = velo[:2]\n+\n+            names = [b.name for b in boxes]\n+            for i in range(len(names)):\n+                if names[i] in NuScenesDataset.NameMapping:\n+                    names[i] = NuScenesDataset.NameMapping[names[i]]\n+            names = np.array(names)\n+            # we need to convert box size to\n+            # the format of our lidar coordinate system\n+            # which is x_size, y_size, z_size (corresponding to l, w, h)\n+            gt_boxes = np.concatenate([locs, dims[:, [1, 0, 2]], rots], axis=1)\n+            \n+            \n+            # we need to convert rot to SECOND format.\n+            # gt_boxes = np.concatenate([locs, dims, -rots - np.pi / 2], axis=1)\n+            assert len(gt_boxes) == len(\n+                annotations\n+            ), f\"{len(gt_boxes)}, {len(annotations)}\"\n+            info[\"gt_boxes\"] = gt_boxes\n+            info[\"gt_names\"] = names\n+            info[\"gt_velocity\"] = velocity.reshape(-1, 2)\n+            info[\"num_lidar_pts\"] = np.array([a[\"num_lidar_pts\"] for a in annotations])\n+            info[\"num_radar_pts\"] = np.array([a[\"num_radar_pts\"] for a in annotations])\n+            info[\"valid_flag\"] = valid_flag\n+\n+        if sample[\"scene_token\"] in train_scenes:\n+            train_nusc_infos.append(info)\n+        else:\n+            val_nusc_infos.append(info)\n+\n+    return train_nusc_infos, val_nusc_infos\n+\n+\n+def obtain_sensor2top(\n+    nusc, sensor_token, l2e_t, l2e_r_mat, e2g_t, e2g_r_mat, sensor_type=\"lidar\"\n+):\n+    \"\"\"Obtain the info with RT matric from general sensor to Top LiDAR.\n+\n+    Args:\n+        nusc (class): Dataset class in the nuScenes dataset.\n+        sensor_token (str): Sample data token corresponding to the\n+            specific sensor type.\n+        l2e_t (np.ndarray): Translation from lidar to ego in shape (1, 3).\n+        l2e_r_mat (np.ndarray): Rotation matrix from lidar to ego\n+            in shape (3, 3).\n+        e2g_t (np.ndarray): Translation from ego to global in shape (1, 3).\n+        e2g_r_mat (np.ndarray): Rotation matrix from ego to global\n+            in shape (3, 3).\n+        sensor_type (str, optional): Sensor to calibrate. Default: 'lidar'.\n+\n+    Returns:\n+        sweep (dict): Sweep information after transformation.\n+    \"\"\"\n+    sd_rec = nusc.get(\"sample_data\", sensor_token)\n+    cs_record = nusc.get(\"calibrated_sensor\", sd_rec[\"calibrated_sensor_token\"])\n+    pose_record = nusc.get(\"ego_pose\", sd_rec[\"ego_pose_token\"])\n+    data_path = str(nusc.get_sample_data_path(sd_rec[\"token\"]))\n+    if os.getcwd() in data_path:  # path from lyftdataset is absolute path\n+        data_path = data_path.split(f\"{os.getcwd()}/\")[-1]  # relative path\n+    sweep = {\n+        \"data_path\": data_path,\n+        \"type\": sensor_type,\n+        \"sample_data_token\": sd_rec[\"token\"],\n+        \"sensor2ego_translation\": cs_record[\"translation\"],\n+        \"sensor2ego_rotation\": cs_record[\"rotation\"],\n+        \"ego2global_translation\": pose_record[\"translation\"],\n+        \"ego2global_rotation\": pose_record[\"rotation\"],\n+        \"timestamp\": sd_rec[\"timestamp\"],\n+    }\n+    l2e_r_s = sweep[\"sensor2ego_rotation\"]\n+    l2e_t_s = sweep[\"sensor2ego_translation\"]\n+    e2g_r_s = sweep[\"ego2global_rotation\"]\n+    e2g_t_s = sweep[\"ego2global_translation\"]\n+\n+    # obtain the RT from sensor to Top LiDAR\n+    # sweep->ego->global->ego'->lidar\n+    l2e_r_s_mat = Quaternion(l2e_r_s).rotation_matrix\n+    e2g_r_s_mat = Quaternion(e2g_r_s).rotation_matrix\n+    R = (l2e_r_s_mat.T @ e2g_r_s_mat.T) @ (\n+        np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n+    )\n+    T = (l2e_t_s @ e2g_r_s_mat.T + e2g_t_s) @ (\n+        np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n+    )\n+    T -= (\n+        e2g_t @ (np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T)\n+        + l2e_t @ np.linalg.inv(l2e_r_mat).T\n+    )\n+    sweep[\"sensor2lidar_rotation\"] = R.T  # points @ R.T + T\n+    sweep[\"sensor2lidar_translation\"] = T\n+    return sweep\n+\n+\n+def export_2d_annotation(root_path, info_path, version, mono3d=True):\n+    \"\"\"Export 2d annotation from the info file and raw data.\n+\n+    Args:\n+        root_path (str): Root path of the raw data.\n+        info_path (str): Path of the info file.\n+        version (str): Dataset version.\n+        mono3d (bool, optional): Whether to export mono3d annotation.\n+            Default: True.\n+    \"\"\"\n+    # get bbox annotations for camera\n+    camera_types = [\n+        \"CAM_FRONT\",\n+        \"CAM_FRONT_RIGHT\",\n+        \"CAM_FRONT_LEFT\",\n+        \"CAM_BACK\",\n+        \"CAM_BACK_LEFT\",\n+        \"CAM_BACK_RIGHT\",\n+    ]\n+    nusc_infos = mmcv.load(info_path)[\"infos\"]\n+    nusc = NuScenes(version=version, dataroot=root_path, verbose=True)\n+    # info_2d_list = []\n+    cat2Ids = [\n+        dict(id=nus_categories.index(cat_name), name=cat_name)\n+        for cat_name in nus_categories\n+    ]\n+    coco_ann_id = 0\n+    coco_2d_dict = dict(annotations=[], images=[], categories=cat2Ids)\n+    for info in mmcv.track_iter_progress(nusc_infos):\n+        for cam in camera_types:\n+            cam_info = info[\"cams\"][cam]\n+            coco_infos = get_2d_boxes(\n+                nusc,\n+                cam_info[\"sample_data_token\"],\n+                visibilities=[\"\", \"1\", \"2\", \"3\", \"4\"],\n+                mono3d=mono3d,\n+            )\n+            (height, width, _) = mmcv.imread(cam_info[\"data_path\"]).shape\n+            coco_2d_dict[\"images\"].append(\n+                dict(\n+                    file_name=cam_info[\"data_path\"].split(\"data/nuscenes/\")[-1],\n+                    id=cam_info[\"sample_data_token\"],\n+                    token=info[\"token\"],\n+                    cam2ego_rotation=cam_info[\"sensor2ego_rotation\"],\n+                    cam2ego_translation=cam_info[\"sensor2ego_translation\"],\n+                    ego2global_rotation=info[\"ego2global_rotation\"],\n+                    ego2global_translation=info[\"ego2global_translation\"],\n+                    cam_intrinsic=cam_info[\"cam_intrinsic\"],\n+                    width=width,\n+                    height=height,\n+                )\n+            )\n+            for coco_info in coco_infos:\n+                if coco_info is None:\n+                    continue\n+                # add an empty key for coco format\n+                coco_info[\"segmentation\"] = []\n+                coco_info[\"id\"] = coco_ann_id\n+                coco_2d_dict[\"annotations\"].append(coco_info)\n+                coco_ann_id += 1\n+    if mono3d:\n+        json_prefix = f\"{info_path[:-4]}_mono3d\"\n+    else:\n+        json_prefix = f\"{info_path[:-4]}\"\n+    mmcv.dump(coco_2d_dict, f\"{json_prefix}.coco.json\")\n+\n+\n+def get_2d_boxes(nusc, sample_data_token: str, visibilities: List[str], mono3d=True):\n+    \"\"\"Get the 2D annotation records for a given `sample_data_token`.\n+\n+    Args:\n+        sample_data_token (str): Sample data token belonging to a camera\n+            keyframe.\n+        visibilities (list[str]): Visibility filter.\n+        mono3d (bool): Whether to get boxes with mono3d annotation.\n+\n+    Return:\n+        list[dict]: List of 2D annotation record that belongs to the input\n+            `sample_data_token`.\n+    \"\"\"\n+\n+    # Get the sample data and the sample corresponding to that sample data.\n+    sd_rec = nusc.get(\"sample_data\", sample_data_token)\n+\n+    assert sd_rec[\"sensor_modality\"] == \"camera\", (\n+        \"Error: get_2d_boxes only works\" \" for camera sample_data!\"\n+    )\n+    if not sd_rec[\"is_key_frame\"]:\n+        raise ValueError(\"The 2D re-projections are available only for keyframes.\")\n+\n+    s_rec = nusc.get(\"sample\", sd_rec[\"sample_token\"])\n+\n+    # Get the calibrated sensor and ego pose\n+    # record to get the transformation matrices.\n+    cs_rec = nusc.get(\"calibrated_sensor\", sd_rec[\"calibrated_sensor_token\"])\n+    pose_rec = nusc.get(\"ego_pose\", sd_rec[\"ego_pose_token\"])\n+    camera_intrinsic = np.array(cs_rec[\"camera_intrinsic\"])\n+\n+    # Get all the annotation with the specified visibilties.\n+    ann_recs = [nusc.get(\"sample_annotation\", token) for token in s_rec[\"anns\"]]\n+    ann_recs = [\n+        ann_rec for ann_rec in ann_recs if (ann_rec[\"visibility_token\"] in visibilities)\n+    ]\n+\n+    repro_recs = []\n+\n+    for ann_rec in ann_recs:\n+        # Augment sample_annotation with token information.\n+        ann_rec[\"sample_annotation_token\"] = ann_rec[\"token\"]\n+        ann_rec[\"sample_data_token\"] = sample_data_token\n+\n+        # Get the box in global coordinates.\n+        box = nusc.get_box(ann_rec[\"token\"])\n+\n+        # Move them to the ego-pose frame.\n+        box.translate(-np.array(pose_rec[\"translation\"]))\n+        box.rotate(Quaternion(pose_rec[\"rotation\"]).inverse)\n+\n+        # Move them to the calibrated sensor frame.\n+        box.translate(-np.array(cs_rec[\"translation\"]))\n+        box.rotate(Quaternion(cs_rec[\"rotation\"]).inverse)\n+\n+        # Filter out the corners that are not in front of the calibrated\n+        # sensor.\n+        corners_3d = box.corners()\n+        in_front = np.argwhere(corners_3d[2, :] > 0).flatten()\n+        corners_3d = corners_3d[:, in_front]\n+\n+        # Project 3d box to 2d.\n+        corner_coords = (\n+            view_points(corners_3d, camera_intrinsic, True).T[:, :2].tolist()\n+        )\n+\n+        # Keep only corners that fall within the image.\n+        final_coords = post_process_coords(corner_coords)\n+\n+        # Skip if the convex hull of the re-projected corners\n+        # does not intersect the image canvas.\n+        if final_coords is None:\n+            continue\n+        else:\n+            min_x, min_y, max_x, max_y = final_coords\n+\n+        # Generate dictionary record to be included in the .json file.\n+        repro_rec = generate_record(\n+            ann_rec, min_x, min_y, max_x, max_y, sample_data_token, sd_rec[\"filename\"]\n+        )\n+\n+        # If mono3d=True, add 3D annotations in camera coordinates\n+        if mono3d and (repro_rec is not None):\n+            loc = box.center.tolist()\n+\n+            dim = box.wlh\n+            dim[[0, 1, 2]] = dim[[1, 2, 0]]  # convert wlh to our lhw\n+            dim = dim.tolist()\n+\n+            rot = box.orientation.yaw_pitch_roll[0]\n+            rot = [-rot]  # convert the rot to our cam coordinate\n+\n+            global_velo2d = nusc.box_velocity(box.token)[:2]\n+            global_velo3d = np.array([*global_velo2d, 0.0])\n+            e2g_r_mat = Quaternion(pose_rec[\"rotation\"]).rotation_matrix\n+            c2e_r_mat = Quaternion(cs_rec[\"rotation\"]).rotation_matrix\n+            cam_velo3d = (\n+                global_velo3d @ np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(c2e_r_mat).T\n+            )\n+            velo = cam_velo3d[0::2].tolist()\n+\n+            repro_rec[\"bbox_cam3d\"] = loc + dim + rot\n+            repro_rec[\"velo_cam3d\"] = velo\n+\n+            center3d = np.array(loc).reshape([1, 3])\n+            center2d = points_cam2img(center3d, camera_intrinsic, with_depth=True)\n+            repro_rec[\"center2d\"] = center2d.squeeze().tolist()\n+            # normalized center2D + depth\n+            # if samples with depth < 0 will be removed\n+            if repro_rec[\"center2d\"][2] <= 0:\n+                continue\n+\n+            ann_token = nusc.get(\"sample_annotation\", box.token)[\"attribute_tokens\"]\n+            if len(ann_token) == 0:\n+                attr_name = \"None\"\n+            else:\n+                attr_name = nusc.get(\"attribute\", ann_token[0])[\"name\"]\n+            attr_id = nus_attributes.index(attr_name)\n+            repro_rec[\"attribute_name\"] = attr_name\n+            repro_rec[\"attribute_id\"] = attr_id\n+\n+        repro_recs.append(repro_rec)\n+\n+    return repro_recs\n+\n+\n+def post_process_coords(\n+    corner_coords: List, imsize: Tuple[int, int] = (1600, 900)\n+) -> Union[Tuple[float, float, float, float], None]:\n+    \"\"\"Get the intersection of the convex hull of the reprojected bbox corners\n+    and the image canvas, return None if no intersection.\n+\n+    Args:\n+        corner_coords (list[int]): Corner coordinates of reprojected\n+            bounding box.\n+        imsize (tuple[int]): Size of the image canvas.\n+\n+    Return:\n+        tuple [float]: Intersection of the convex hull of the 2D box\n+            corners and the image canvas.\n+    \"\"\"\n+    polygon_from_2d_box = MultiPoint(corner_coords).convex_hull\n+    img_canvas = box(0, 0, imsize[0], imsize[1])\n+\n+    if polygon_from_2d_box.intersects(img_canvas):\n+        img_intersection = polygon_from_2d_box.intersection(img_canvas)\n+        intersection_coords = np.array(\n+            [coord for coord in img_intersection.exterior.coords]\n+        )\n+\n+        min_x = min(intersection_coords[:, 0])\n+        min_y = min(intersection_coords[:, 1])\n+        max_x = max(intersection_coords[:, 0])\n+        max_y = max(intersection_coords[:, 1])\n+\n+        return min_x, min_y, max_x, max_y\n+    else:\n+        return None\n+\n+\n+def generate_record(\n+    ann_rec: dict,\n+    x1: float,\n+    y1: float,\n+    x2: float,\n+    y2: float,\n+    sample_data_token: str,\n+    filename: str,\n+) -> OrderedDict:\n+    \"\"\"Generate one 2D annotation record given various information on top of\n+    the 2D bounding box coordinates.\n+\n+    Args:\n+        ann_rec (dict): Original 3d annotation record.\n+        x1 (float): Minimum value of the x coordinate.\n+        y1 (float): Minimum value of the y coordinate.\n+        x2 (float): Maximum value of the x coordinate.\n+        y2 (float): Maximum value of the y coordinate.\n+        sample_data_token (str): Sample data token.\n+        filename (str):The corresponding image file where the annotation\n+            is present.\n+\n+    Returns:\n+        dict: A sample 2D annotation record.\n+            - file_name (str): file name\n+            - image_id (str): sample data token\n+            - area (float): 2d box area\n+            - category_name (str): category name\n+            - category_id (int): category id\n+            - bbox (list[float]): left x, top y, dx, dy of 2d box\n+            - iscrowd (int): whether the area is crowd\n+    \"\"\"\n+    repro_rec = OrderedDict()\n+    repro_rec[\"sample_data_token\"] = sample_data_token\n+    coco_rec = dict()\n+\n+    relevant_keys = [\n+        \"attribute_tokens\",\n+        \"category_name\",\n+        \"instance_token\",\n+        \"next\",\n+        \"num_lidar_pts\",\n+        \"num_radar_pts\",\n+        \"prev\",\n+        \"sample_annotation_token\",\n+        \"sample_data_token\",\n+        \"visibility_token\",\n+    ]\n+\n+    for key, value in ann_rec.items():\n+        if key in relevant_keys:\n+            repro_rec[key] = value\n+\n+    repro_rec[\"bbox_corners\"] = [x1, y1, x2, y2]\n+    repro_rec[\"filename\"] = filename\n+\n+    coco_rec[\"file_name\"] = filename\n+    coco_rec[\"image_id\"] = sample_data_token\n+    coco_rec[\"area\"] = (y2 - y1) * (x2 - x1)\n+\n+    if repro_rec[\"category_name\"] not in NuScenesDataset.NameMapping:\n+        return None\n+    cat_name = NuScenesDataset.NameMapping[repro_rec[\"category_name\"]]\n+    coco_rec[\"category_name\"] = cat_name\n+    coco_rec[\"category_id\"] = nus_categories.index(cat_name)\n+    coco_rec[\"bbox\"] = [x1, y1, x2 - x1, y2 - y1]\n+    coco_rec[\"iscrowd\"] = 0\n+\n+    return coco_rec\n"
                },
                {
                    "date": 1718502196390,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -697,704 +697,4 @@\n     coco_rec[\"bbox\"] = [x1, y1, x2 - x1, y2 - y1]\n     coco_rec[\"iscrowd\"] = 0\n \n     return coco_rec\n-# Copyright (c) OpenMMLab. All rights reserved.\n-import os\n-from collections import OrderedDict\n-from os import path as osp\n-from typing import List, Tuple, Union\n-\n-import mmcv\n-import numpy as np\n-from nuscenes.nuscenes import NuScenes\n-from nuscenes.utils.geometry_utils import view_points\n-from pyquaternion import Quaternion\n-from shapely.geometry import MultiPoint, box\n-\n-from mmdet3d.core.bbox import points_cam2img\n-from mmdet3d.datasets import NuScenesDataset\n-\n-nus_categories = (\n-    \"car\",\n-    \"truck\",\n-    \"trailer\",\n-    \"bus\",\n-    \"construction_vehicle\",\n-    \"bicycle\",\n-    \"motorcycle\",\n-    \"pedestrian\",\n-    \"traffic_cone\",\n-    \"barrier\",\n-)\n-\n-nus_attributes = (\n-    \"cycle.with_rider\",\n-    \"cycle.without_rider\",\n-    \"pedestrian.moving\",\n-    \"pedestrian.standing\",\n-    \"pedestrian.sitting_lying_down\",\n-    \"vehicle.moving\",\n-    \"vehicle.parked\",\n-    \"vehicle.stopped\",\n-    \"None\",\n-)\n-\n-\n-def create_nuscenes_infos(\n-    root_path, info_prefix, version=\"v1.0-trainval\", max_sweeps=10\n-):\n-    \"\"\"Create info file of nuscene dataset.\n-\n-    Given the raw data, generate its related info file in pkl format.\n-\n-    Args:\n-        root_path (str): Path of the data root.\n-        info_prefix (str): Prefix of the info file to be generated.\n-        version (str, optional): Version of the data.\n-            Default: 'v1.0-trainval'.\n-        max_sweeps (int, optional): Max number of sweeps.\n-            Default: 10.\n-    \"\"\"\n-    from nuscenes.nuscenes import NuScenes\n-\n-    nusc = NuScenes(version=version, dataroot=root_path, verbose=True)\n-    from nuscenes.utils import splits\n-\n-    available_vers = [\"v1.0-trainval\", \"v1.0-test\", \"v1.0-mini\"]\n-    assert version in available_vers\n-    if version == \"v1.0-trainval\":\n-        train_scenes = splits.train\n-        val_scenes = splits.val\n-    elif version == \"v1.0-test\":\n-        train_scenes = splits.test\n-        val_scenes = []\n-    elif version == \"v1.0-mini\":\n-        train_scenes = splits.mini_train\n-        val_scenes = splits.mini_val\n-    else:\n-        raise ValueError(\"unknown\")\n-\n-    # filter existing scenes.\n-    available_scenes = get_available_scenes(nusc)\n-    available_scene_names = [s[\"name\"] for s in available_scenes]\n-    train_scenes = list(filter(lambda x: x in available_scene_names, train_scenes))\n-    val_scenes = list(filter(lambda x: x in available_scene_names, val_scenes))\n-    train_scenes = set(\n-        [\n-            available_scenes[available_scene_names.index(s)][\"token\"]\n-            for s in train_scenes\n-        ]\n-    )\n-    val_scenes = set(\n-        [available_scenes[available_scene_names.index(s)][\"token\"] for s in val_scenes]\n-    )\n-\n-    test = \"test\" in version\n-    if test:\n-        print(\"test scene: {}\".format(len(train_scenes)))\n-    else:\n-        print(\n-            \"train scene: {}, val scene: {}\".format(len(train_scenes), len(val_scenes))\n-        )\n-    train_nusc_infos, val_nusc_infos = _fill_trainval_infos(\n-        nusc, train_scenes, val_scenes, test, max_sweeps=max_sweeps\n-    )\n-\n-    metadata = dict(version=version)\n-    if test:\n-        print(\"test sample: {}\".format(len(train_nusc_infos)))\n-        data = dict(infos=train_nusc_infos, metadata=metadata)\n-        info_path = osp.join(root_path, \"{}_infos_test.pkl\".format(info_prefix))\n-        mmcv.dump(data, info_path)\n-    else:\n-        print(\n-            \"train sample: {}, val sample: {}\".format(\n-                len(train_nusc_infos), len(val_nusc_infos)\n-            )\n-        )\n-        data = dict(infos=train_nusc_infos, metadata=metadata)\n-        info_path = osp.join(root_path, \"{}_infos_train.pkl\".format(info_prefix))\n-        mmcv.dump(data, info_path)\n-        data[\"infos\"] = val_nusc_infos\n-        info_val_path = osp.join(root_path, \"{}_infos_val.pkl\".format(info_prefix))\n-        mmcv.dump(data, info_val_path)\n-\n-\n-def get_available_scenes(nusc):\n-    \"\"\"Get available scenes from the input nuscenes class.\n-\n-    Given the raw data, get the information of available scenes for\n-    further info generation.\n-\n-    Args:\n-        nusc (class): Dataset class in the nuScenes dataset.\n-\n-    Returns:\n-        available_scenes (list[dict]): List of basic information for the\n-            available scenes.\n-    \"\"\"\n-    available_scenes = []\n-    print(\"total scene num: {}\".format(len(nusc.scene)))\n-    for scene in nusc.scene:\n-        scene_token = scene[\"token\"]\n-        scene_rec = nusc.get(\"scene\", scene_token)\n-        sample_rec = nusc.get(\"sample\", scene_rec[\"first_sample_token\"])\n-        sd_rec = nusc.get(\"sample_data\", sample_rec[\"data\"][\"LIDAR_TOP\"])\n-        has_more_frames = True\n-        scene_not_exist = False\n-        while has_more_frames:\n-            lidar_path, boxes, _ = nusc.get_sample_data(sd_rec[\"token\"])\n-            lidar_path = str(lidar_path)\n-            if os.getcwd() in lidar_path:\n-                # path from lyftdataset is absolute path\n-                lidar_path = lidar_path.split(f\"{os.getcwd()}/\")[-1]\n-                # relative path\n-            if not mmcv.is_filepath(lidar_path):\n-                scene_not_exist = True\n-                break\n-            else:\n-                break\n-        if scene_not_exist:\n-            continue\n-        available_scenes.append(scene)\n-    print(\"exist scene num: {}\".format(len(available_scenes)))\n-    return available_scenes\n-\n-\n-def _fill_trainval_infos(nusc, train_scenes, val_scenes, test=False, max_sweeps=10):\n-    \"\"\"Generate the train/val infos from the raw data.\n-\n-    Args:\n-        nusc (:obj:`NuScenes`): Dataset class in the nuScenes dataset.\n-        train_scenes (list[str]): Basic information of training scenes.\n-        val_scenes (list[str]): Basic information of validation scenes.\n-        test (bool, optional): Whether use the test mode. In test mode, no\n-            annotations can be accessed. Default: False.\n-        max_sweeps (int, optional): Max number of sweeps. Default: 10.\n-\n-    Returns:\n-        tuple[list[dict]]: Information of training set and validation set\n-            that will be saved to the info file.\n-    \"\"\"\n-    train_nusc_infos = []\n-    val_nusc_infos = []\n-\n-    for sample in mmcv.track_iter_progress(nusc.sample):\n-        lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n-        sd_rec = nusc.get(\"sample_data\", sample[\"data\"][\"LIDAR_TOP\"])\n-        cs_record = nusc.get(\"calibrated_sensor\", sd_rec[\"calibrated_sensor_token\"])\n-        pose_record = nusc.get(\"ego_pose\", sd_rec[\"ego_pose_token\"])\n-        lidar_path, boxes, _ = nusc.get_sample_data(lidar_token)\n-\n-        mmcv.check_file_exist(lidar_path)\n-\n-        info = {\n-            \"lidar_path\": lidar_path,\n-            \"token\": sample[\"token\"],\n-            \"sweeps\": [],\n-            \"cams\": dict(),\n-            \"radars\": dict(),\n-            \"lidar2ego_translation\": cs_record[\"translation\"],\n-            \"lidar2ego_rotation\": cs_record[\"rotation\"],\n-            \"ego2global_translation\": pose_record[\"translation\"],\n-            \"ego2global_rotation\": pose_record[\"rotation\"],\n-            \"timestamp\": sample[\"timestamp\"],\n-        }\n-\n-        l2e_r = info[\"lidar2ego_rotation\"]\n-        l2e_t = info[\"lidar2ego_translation\"]\n-        e2g_r = info[\"ego2global_rotation\"]\n-        e2g_t = info[\"ego2global_translation\"]\n-        l2e_r_mat = Quaternion(l2e_r).rotation_matrix\n-        e2g_r_mat = Quaternion(e2g_r).rotation_matrix\n-\n-        # obtain 6 image's information per frame\n-        camera_types = [\n-            \"CAM_FRONT\",\n-            \"CAM_FRONT_RIGHT\",\n-            \"CAM_FRONT_LEFT\",\n-            \"CAM_BACK\",\n-            \"CAM_BACK_LEFT\",\n-            \"CAM_BACK_RIGHT\",\n-        ]\n-        for cam in camera_types:\n-            cam_token = sample[\"data\"][cam]\n-            cam_path, _, cam_intrinsic = nusc.get_sample_data(cam_token)\n-            cam_info = obtain_sensor2top(\n-                nusc, cam_token, l2e_t, l2e_r_mat, e2g_t, e2g_r_mat, cam\n-            )\n-            cam_info.update(cam_intrinsic=cam_intrinsic)\n-            info[\"cams\"].update({cam: cam_info})\n-\n-        # Add radar information to the annotation read file\n-        radar_names = [\n-            \"RADAR_FRONT\",\n-            \"RADAR_FRONT_LEFT\",\n-            \"RADAR_FRONT_RIGHT\",\n-            \"RADAR_BACK_LEFT\",\n-            \"RADAR_BACK_RIGHT\",\n-        ]\n-\n-        for radar_name in radar_names:\n-            radar_token = sample[\"data\"][radar_name]\n-            radar_rec = nusc.get(\"sample_data\", radar_token)\n-            sweeps = []\n-\n-            while len(sweeps) < 5:\n-                if not radar_rec[\"prev\"] == \"\":\n-                    radar_path, _, radar_intrin = nusc.get_sample_data(radar_token)\n-\n-                    radar_info = obtain_sensor2top(\n-                        nusc,\n-                        radar_token,\n-                        l2e_t,\n-                        l2e_r_mat,\n-                        e2g_t,\n-                        e2g_r_mat,\n-                        radar_name,\n-                    )\n-                    sweeps.append(radar_info)\n-                    radar_token = radar_rec[\"prev\"]\n-                    radar_rec = nusc.get(\"sample_data\", radar_token)\n-                else:\n-                    radar_path, _, radar_intrin = nusc.get_sample_data(radar_token)\n-\n-                    radar_info = obtain_sensor2top(\n-                        nusc,\n-                        radar_token,\n-                        l2e_t,\n-                        l2e_r_mat,\n-                        e2g_t,\n-                        e2g_r_mat,\n-                        radar_name,\n-                    )\n-                    sweeps.append(radar_info)\n-\n-            info[\"radars\"].update({radar_name: sweeps})\n-\n-        # obtain sweeps for a single key-frame\n-        sd_rec = nusc.get(\"sample_data\", sample[\"data\"][\"LIDAR_TOP\"])\n-        sweeps = []\n-        while len(sweeps) < max_sweeps:\n-            if not sd_rec[\"prev\"] == \"\":\n-                sweep = obtain_sensor2top(\n-                    nusc, sd_rec[\"prev\"], l2e_t, l2e_r_mat, e2g_t, e2g_r_mat, \"lidar\"\n-                )\n-                sweeps.append(sweep)\n-                sd_rec = nusc.get(\"sample_data\", sd_rec[\"prev\"])\n-            else:\n-                break\n-        info[\"sweeps\"] = sweeps\n-        # obtain annotation\n-        if not test:\n-            annotations = [\n-                nusc.get(\"sample_annotation\", token) for token in sample[\"anns\"]\n-            ]\n-            locs = np.array([b.center for b in boxes]).reshape(-1, 3)\n-            dims = np.array([b.wlh for b in boxes]).reshape(-1, 3)\n-            rots = np.array([b.orientation.yaw_pitch_roll[0] for b in boxes]).reshape(\n-                -1, 1\n-            )\n-            velocity = np.array(\n-                [nusc.box_velocity(token)[:2] for token in sample[\"anns\"]]\n-            )\n-            valid_flag = np.array(\n-                [\n-                    (anno[\"num_lidar_pts\"] + anno[\"num_radar_pts\"]) > 0\n-                    for anno in annotations\n-                ],\n-                dtype=bool,\n-            ).reshape(-1)\n-            # convert velo from global to lidar\n-            for i in range(len(boxes)):\n-                velo = np.array([*velocity[i], 0.0])\n-                velo = velo @ np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n-                velocity[i] = velo[:2]\n-\n-            names = [b.name for b in boxes]\n-            for i in range(len(names)):\n-                if names[i] in NuScenesDataset.NameMapping:\n-                    names[i] = NuScenesDataset.NameMapping[names[i]]\n-            names = np.array(names)\n-            # we need to convert box size to\n-            # the format of our lidar coordinate system\n-            # which is x_size, y_size, z_size (corresponding to l, w, h)\n-            gt_boxes = np.concatenate([locs, dims[:, [1, 0, 2]], rots], axis=1)\n-            \n-            \n-            # we need to convert rot to SECOND format.\n-            # gt_boxes = np.concatenate([locs, dims, -rots - np.pi / 2], axis=1)\n-            assert len(gt_boxes) == len(\n-                annotations\n-            ), f\"{len(gt_boxes)}, {len(annotations)}\"\n-            info[\"gt_boxes\"] = gt_boxes\n-            info[\"gt_names\"] = names\n-            info[\"gt_velocity\"] = velocity.reshape(-1, 2)\n-            info[\"num_lidar_pts\"] = np.array([a[\"num_lidar_pts\"] for a in annotations])\n-            info[\"num_radar_pts\"] = np.array([a[\"num_radar_pts\"] for a in annotations])\n-            info[\"valid_flag\"] = valid_flag\n-\n-        if sample[\"scene_token\"] in train_scenes:\n-            train_nusc_infos.append(info)\n-        else:\n-            val_nusc_infos.append(info)\n-\n-    return train_nusc_infos, val_nusc_infos\n-\n-\n-def obtain_sensor2top(\n-    nusc, sensor_token, l2e_t, l2e_r_mat, e2g_t, e2g_r_mat, sensor_type=\"lidar\"\n-):\n-    \"\"\"Obtain the info with RT matric from general sensor to Top LiDAR.\n-\n-    Args:\n-        nusc (class): Dataset class in the nuScenes dataset.\n-        sensor_token (str): Sample data token corresponding to the\n-            specific sensor type.\n-        l2e_t (np.ndarray): Translation from lidar to ego in shape (1, 3).\n-        l2e_r_mat (np.ndarray): Rotation matrix from lidar to ego\n-            in shape (3, 3).\n-        e2g_t (np.ndarray): Translation from ego to global in shape (1, 3).\n-        e2g_r_mat (np.ndarray): Rotation matrix from ego to global\n-            in shape (3, 3).\n-        sensor_type (str, optional): Sensor to calibrate. Default: 'lidar'.\n-\n-    Returns:\n-        sweep (dict): Sweep information after transformation.\n-    \"\"\"\n-    sd_rec = nusc.get(\"sample_data\", sensor_token)\n-    cs_record = nusc.get(\"calibrated_sensor\", sd_rec[\"calibrated_sensor_token\"])\n-    pose_record = nusc.get(\"ego_pose\", sd_rec[\"ego_pose_token\"])\n-    data_path = str(nusc.get_sample_data_path(sd_rec[\"token\"]))\n-    if os.getcwd() in data_path:  # path from lyftdataset is absolute path\n-        data_path = data_path.split(f\"{os.getcwd()}/\")[-1]  # relative path\n-    sweep = {\n-        \"data_path\": data_path,\n-        \"type\": sensor_type,\n-        \"sample_data_token\": sd_rec[\"token\"],\n-        \"sensor2ego_translation\": cs_record[\"translation\"],\n-        \"sensor2ego_rotation\": cs_record[\"rotation\"],\n-        \"ego2global_translation\": pose_record[\"translation\"],\n-        \"ego2global_rotation\": pose_record[\"rotation\"],\n-        \"timestamp\": sd_rec[\"timestamp\"],\n-    }\n-    l2e_r_s = sweep[\"sensor2ego_rotation\"]\n-    l2e_t_s = sweep[\"sensor2ego_translation\"]\n-    e2g_r_s = sweep[\"ego2global_rotation\"]\n-    e2g_t_s = sweep[\"ego2global_translation\"]\n-\n-    # obtain the RT from sensor to Top LiDAR\n-    # sweep->ego->global->ego'->lidar\n-    l2e_r_s_mat = Quaternion(l2e_r_s).rotation_matrix\n-    e2g_r_s_mat = Quaternion(e2g_r_s).rotation_matrix\n-    R = (l2e_r_s_mat.T @ e2g_r_s_mat.T) @ (\n-        np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n-    )\n-    T = (l2e_t_s @ e2g_r_s_mat.T + e2g_t_s) @ (\n-        np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n-    )\n-    T -= (\n-        e2g_t @ (np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T)\n-        + l2e_t @ np.linalg.inv(l2e_r_mat).T\n-    )\n-    sweep[\"sensor2lidar_rotation\"] = R.T  # points @ R.T + T\n-    sweep[\"sensor2lidar_translation\"] = T\n-    return sweep\n-\n-\n-def export_2d_annotation(root_path, info_path, version, mono3d=True):\n-    \"\"\"Export 2d annotation from the info file and raw data.\n-\n-    Args:\n-        root_path (str): Root path of the raw data.\n-        info_path (str): Path of the info file.\n-        version (str): Dataset version.\n-        mono3d (bool, optional): Whether to export mono3d annotation.\n-            Default: True.\n-    \"\"\"\n-    # get bbox annotations for camera\n-    camera_types = [\n-        \"CAM_FRONT\",\n-        \"CAM_FRONT_RIGHT\",\n-        \"CAM_FRONT_LEFT\",\n-        \"CAM_BACK\",\n-        \"CAM_BACK_LEFT\",\n-        \"CAM_BACK_RIGHT\",\n-    ]\n-    nusc_infos = mmcv.load(info_path)[\"infos\"]\n-    nusc = NuScenes(version=version, dataroot=root_path, verbose=True)\n-    # info_2d_list = []\n-    cat2Ids = [\n-        dict(id=nus_categories.index(cat_name), name=cat_name)\n-        for cat_name in nus_categories\n-    ]\n-    coco_ann_id = 0\n-    coco_2d_dict = dict(annotations=[], images=[], categories=cat2Ids)\n-    for info in mmcv.track_iter_progress(nusc_infos):\n-        for cam in camera_types:\n-            cam_info = info[\"cams\"][cam]\n-            coco_infos = get_2d_boxes(\n-                nusc,\n-                cam_info[\"sample_data_token\"],\n-                visibilities=[\"\", \"1\", \"2\", \"3\", \"4\"],\n-                mono3d=mono3d,\n-            )\n-            (height, width, _) = mmcv.imread(cam_info[\"data_path\"]).shape\n-            coco_2d_dict[\"images\"].append(\n-                dict(\n-                    file_name=cam_info[\"data_path\"].split(\"data/nuscenes/\")[-1],\n-                    id=cam_info[\"sample_data_token\"],\n-                    token=info[\"token\"],\n-                    cam2ego_rotation=cam_info[\"sensor2ego_rotation\"],\n-                    cam2ego_translation=cam_info[\"sensor2ego_translation\"],\n-                    ego2global_rotation=info[\"ego2global_rotation\"],\n-                    ego2global_translation=info[\"ego2global_translation\"],\n-                    cam_intrinsic=cam_info[\"cam_intrinsic\"],\n-                    width=width,\n-                    height=height,\n-                )\n-            )\n-            for coco_info in coco_infos:\n-                if coco_info is None:\n-                    continue\n-                # add an empty key for coco format\n-                coco_info[\"segmentation\"] = []\n-                coco_info[\"id\"] = coco_ann_id\n-                coco_2d_dict[\"annotations\"].append(coco_info)\n-                coco_ann_id += 1\n-    if mono3d:\n-        json_prefix = f\"{info_path[:-4]}_mono3d\"\n-    else:\n-        json_prefix = f\"{info_path[:-4]}\"\n-    mmcv.dump(coco_2d_dict, f\"{json_prefix}.coco.json\")\n-\n-\n-def get_2d_boxes(nusc, sample_data_token: str, visibilities: List[str], mono3d=True):\n-    \"\"\"Get the 2D annotation records for a given `sample_data_token`.\n-\n-    Args:\n-        sample_data_token (str): Sample data token belonging to a camera\n-            keyframe.\n-        visibilities (list[str]): Visibility filter.\n-        mono3d (bool): Whether to get boxes with mono3d annotation.\n-\n-    Return:\n-        list[dict]: List of 2D annotation record that belongs to the input\n-            `sample_data_token`.\n-    \"\"\"\n-\n-    # Get the sample data and the sample corresponding to that sample data.\n-    sd_rec = nusc.get(\"sample_data\", sample_data_token)\n-\n-    assert sd_rec[\"sensor_modality\"] == \"camera\", (\n-        \"Error: get_2d_boxes only works\" \" for camera sample_data!\"\n-    )\n-    if not sd_rec[\"is_key_frame\"]:\n-        raise ValueError(\"The 2D re-projections are available only for keyframes.\")\n-\n-    s_rec = nusc.get(\"sample\", sd_rec[\"sample_token\"])\n-\n-    # Get the calibrated sensor and ego pose\n-    # record to get the transformation matrices.\n-    cs_rec = nusc.get(\"calibrated_sensor\", sd_rec[\"calibrated_sensor_token\"])\n-    pose_rec = nusc.get(\"ego_pose\", sd_rec[\"ego_pose_token\"])\n-    camera_intrinsic = np.array(cs_rec[\"camera_intrinsic\"])\n-\n-    # Get all the annotation with the specified visibilties.\n-    ann_recs = [nusc.get(\"sample_annotation\", token) for token in s_rec[\"anns\"]]\n-    ann_recs = [\n-        ann_rec for ann_rec in ann_recs if (ann_rec[\"visibility_token\"] in visibilities)\n-    ]\n-\n-    repro_recs = []\n-\n-    for ann_rec in ann_recs:\n-        # Augment sample_annotation with token information.\n-        ann_rec[\"sample_annotation_token\"] = ann_rec[\"token\"]\n-        ann_rec[\"sample_data_token\"] = sample_data_token\n-\n-        # Get the box in global coordinates.\n-        box = nusc.get_box(ann_rec[\"token\"])\n-\n-        # Move them to the ego-pose frame.\n-        box.translate(-np.array(pose_rec[\"translation\"]))\n-        box.rotate(Quaternion(pose_rec[\"rotation\"]).inverse)\n-\n-        # Move them to the calibrated sensor frame.\n-        box.translate(-np.array(cs_rec[\"translation\"]))\n-        box.rotate(Quaternion(cs_rec[\"rotation\"]).inverse)\n-\n-        # Filter out the corners that are not in front of the calibrated\n-        # sensor.\n-        corners_3d = box.corners()\n-        in_front = np.argwhere(corners_3d[2, :] > 0).flatten()\n-        corners_3d = corners_3d[:, in_front]\n-\n-        # Project 3d box to 2d.\n-        corner_coords = (\n-            view_points(corners_3d, camera_intrinsic, True).T[:, :2].tolist()\n-        )\n-\n-        # Keep only corners that fall within the image.\n-        final_coords = post_process_coords(corner_coords)\n-\n-        # Skip if the convex hull of the re-projected corners\n-        # does not intersect the image canvas.\n-        if final_coords is None:\n-            continue\n-        else:\n-            min_x, min_y, max_x, max_y = final_coords\n-\n-        # Generate dictionary record to be included in the .json file.\n-        repro_rec = generate_record(\n-            ann_rec, min_x, min_y, max_x, max_y, sample_data_token, sd_rec[\"filename\"]\n-        )\n-\n-        # If mono3d=True, add 3D annotations in camera coordinates\n-        if mono3d and (repro_rec is not None):\n-            loc = box.center.tolist()\n-\n-            dim = box.wlh\n-            dim[[0, 1, 2]] = dim[[1, 2, 0]]  # convert wlh to our lhw\n-            dim = dim.tolist()\n-\n-            rot = box.orientation.yaw_pitch_roll[0]\n-            rot = [-rot]  # convert the rot to our cam coordinate\n-\n-            global_velo2d = nusc.box_velocity(box.token)[:2]\n-            global_velo3d = np.array([*global_velo2d, 0.0])\n-            e2g_r_mat = Quaternion(pose_rec[\"rotation\"]).rotation_matrix\n-            c2e_r_mat = Quaternion(cs_rec[\"rotation\"]).rotation_matrix\n-            cam_velo3d = (\n-                global_velo3d @ np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(c2e_r_mat).T\n-            )\n-            velo = cam_velo3d[0::2].tolist()\n-\n-            repro_rec[\"bbox_cam3d\"] = loc + dim + rot\n-            repro_rec[\"velo_cam3d\"] = velo\n-\n-            center3d = np.array(loc).reshape([1, 3])\n-            center2d = points_cam2img(center3d, camera_intrinsic, with_depth=True)\n-            repro_rec[\"center2d\"] = center2d.squeeze().tolist()\n-            # normalized center2D + depth\n-            # if samples with depth < 0 will be removed\n-            if repro_rec[\"center2d\"][2] <= 0:\n-                continue\n-\n-            ann_token = nusc.get(\"sample_annotation\", box.token)[\"attribute_tokens\"]\n-            if len(ann_token) == 0:\n-                attr_name = \"None\"\n-            else:\n-                attr_name = nusc.get(\"attribute\", ann_token[0])[\"name\"]\n-            attr_id = nus_attributes.index(attr_name)\n-            repro_rec[\"attribute_name\"] = attr_name\n-            repro_rec[\"attribute_id\"] = attr_id\n-\n-        repro_recs.append(repro_rec)\n-\n-    return repro_recs\n-\n-\n-def post_process_coords(\n-    corner_coords: List, imsize: Tuple[int, int] = (1600, 900)\n-) -> Union[Tuple[float, float, float, float], None]:\n-    \"\"\"Get the intersection of the convex hull of the reprojected bbox corners\n-    and the image canvas, return None if no intersection.\n-\n-    Args:\n-        corner_coords (list[int]): Corner coordinates of reprojected\n-            bounding box.\n-        imsize (tuple[int]): Size of the image canvas.\n-\n-    Return:\n-        tuple [float]: Intersection of the convex hull of the 2D box\n-            corners and the image canvas.\n-    \"\"\"\n-    polygon_from_2d_box = MultiPoint(corner_coords).convex_hull\n-    img_canvas = box(0, 0, imsize[0], imsize[1])\n-\n-    if polygon_from_2d_box.intersects(img_canvas):\n-        img_intersection = polygon_from_2d_box.intersection(img_canvas)\n-        intersection_coords = np.array(\n-            [coord for coord in img_intersection.exterior.coords]\n-        )\n-\n-        min_x = min(intersection_coords[:, 0])\n-        min_y = min(intersection_coords[:, 1])\n-        max_x = max(intersection_coords[:, 0])\n-        max_y = max(intersection_coords[:, 1])\n-\n-        return min_x, min_y, max_x, max_y\n-    else:\n-        return None\n-\n-\n-def generate_record(\n-    ann_rec: dict,\n-    x1: float,\n-    y1: float,\n-    x2: float,\n-    y2: float,\n-    sample_data_token: str,\n-    filename: str,\n-) -> OrderedDict:\n-    \"\"\"Generate one 2D annotation record given various information on top of\n-    the 2D bounding box coordinates.\n-\n-    Args:\n-        ann_rec (dict): Original 3d annotation record.\n-        x1 (float): Minimum value of the x coordinate.\n-        y1 (float): Minimum value of the y coordinate.\n-        x2 (float): Maximum value of the x coordinate.\n-        y2 (float): Maximum value of the y coordinate.\n-        sample_data_token (str): Sample data token.\n-        filename (str):The corresponding image file where the annotation\n-            is present.\n-\n-    Returns:\n-        dict: A sample 2D annotation record.\n-            - file_name (str): file name\n-            - image_id (str): sample data token\n-            - area (float): 2d box area\n-            - category_name (str): category name\n-            - category_id (int): category id\n-            - bbox (list[float]): left x, top y, dx, dy of 2d box\n-            - iscrowd (int): whether the area is crowd\n-    \"\"\"\n-    repro_rec = OrderedDict()\n-    repro_rec[\"sample_data_token\"] = sample_data_token\n-    coco_rec = dict()\n-\n-    relevant_keys = [\n-        \"attribute_tokens\",\n-        \"category_name\",\n-        \"instance_token\",\n-        \"next\",\n-        \"num_lidar_pts\",\n-        \"num_radar_pts\",\n-        \"prev\",\n-        \"sample_annotation_token\",\n-        \"sample_data_token\",\n-        \"visibility_token\",\n-    ]\n-\n-    for key, value in ann_rec.items():\n-        if key in relevant_keys:\n-            repro_rec[key] = value\n-\n-    repro_rec[\"bbox_corners\"] = [x1, y1, x2, y2]\n-    repro_rec[\"filename\"] = filename\n-\n-    coco_rec[\"file_name\"] = filename\n-    coco_rec[\"image_id\"] = sample_data_token\n-    coco_rec[\"area\"] = (y2 - y1) * (x2 - x1)\n-\n-    if repro_rec[\"category_name\"] not in NuScenesDataset.NameMapping:\n-        return None\n-    cat_name = NuScenesDataset.NameMapping[repro_rec[\"category_name\"]]\n-    coco_rec[\"category_name\"] = cat_name\n-    coco_rec[\"category_id\"] = nus_categories.index(cat_name)\n-    coco_rec[\"bbox\"] = [x1, y1, x2 - x1, y2 - y1]\n-    coco_rec[\"iscrowd\"] = 0\n-\n-    return coco_rec\n"
                },
                {
                    "date": 1734358697852,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -319,10 +319,9 @@\n             # we need to convert box size to\n             # the format of our lidar coordinate system\n             # which is x_size, y_size, z_size (corresponding to l, w, h)\n             gt_boxes = np.concatenate([locs, dims[:, [1, 0, 2]], rots], axis=1)\n-            \n-            \n+\n             # we need to convert rot to SECOND format.\n             # gt_boxes = np.concatenate([locs, dims, -rots - np.pi / 2], axis=1)\n             assert len(gt_boxes) == len(\n                 annotations\n"
                }
            ],
            "date": 1716001178524,
            "name": "Commit-0",
            "content": "# Copyright (c) OpenMMLab. All rights reserved.\nimport os\nfrom collections import OrderedDict\nfrom os import path as osp\nfrom typing import List, Tuple, Union\n\nimport mmcv\nimport numpy as np\nfrom nuscenes.nuscenes import NuScenes\nfrom nuscenes.utils.geometry_utils import view_points\nfrom pyquaternion import Quaternion\nfrom shapely.geometry import MultiPoint, box\n\nfrom mmdet3d.core.bbox import points_cam2img\nfrom mmdet3d.datasets import NuScenesDataset\n\nnus_categories = ('car', 'truck', 'trailer', 'bus', 'construction_vehicle',\n                  'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',\n                  'barrier')\n\nnus_attributes = ('cycle.with_rider', 'cycle.without_rider',\n                  'pedestrian.moving', 'pedestrian.standing',\n                  'pedestrian.sitting_lying_down', 'vehicle.moving',\n                  'vehicle.parked', 'vehicle.stopped', 'None')\n\n\ndef create_nuscenes_infos(root_path,\n                          info_prefix,\n                          version='v1.0-trainval',\n                          max_sweeps=10):\n    \"\"\"Create info file of nuscene dataset.\n\n    Given the raw data, generate its related info file in pkl format.\n\n    Args:\n        root_path (str): Path of the data root.\n        info_prefix (str): Prefix of the info file to be generated.\n        version (str, optional): Version of the data.\n            Default: 'v1.0-trainval'.\n        max_sweeps (int, optional): Max number of sweeps.\n            Default: 10.\n    \"\"\"\n    from nuscenes.nuscenes import NuScenes\n    nusc = NuScenes(version=version, dataroot=root_path, verbose=True)\n    from nuscenes.utils import splits\n    available_vers = ['v1.0-trainval', 'v1.0-test', 'v1.0-mini']\n    assert version in available_vers\n    if version == 'v1.0-trainval':\n        train_scenes = splits.train\n        val_scenes = splits.val\n    elif version == 'v1.0-test':\n        train_scenes = splits.test\n        val_scenes = []\n    elif version == 'v1.0-mini':\n        train_scenes = splits.mini_train\n        val_scenes = splits.mini_val\n    else:\n        raise ValueError('unknown')\n\n    # filter existing scenes.\n    available_scenes = get_available_scenes(nusc)\n    available_scene_names = [s['name'] for s in available_scenes]\n    train_scenes = list(\n        filter(lambda x: x in available_scene_names, train_scenes))\n    val_scenes = list(filter(lambda x: x in available_scene_names, val_scenes))\n    train_scenes = set([\n        available_scenes[available_scene_names.index(s)]['token']\n        for s in train_scenes\n    ])\n    val_scenes = set([\n        available_scenes[available_scene_names.index(s)]['token']\n        for s in val_scenes\n    ])\n\n    test = 'test' in version\n    if test:\n        print('test scene: {}'.format(len(train_scenes)))\n    else:\n        print('train scene: {}, val scene: {}'.format(\n            len(train_scenes), len(val_scenes)))\n    train_nusc_infos, val_nusc_infos = _fill_trainval_infos(\n        nusc, train_scenes, val_scenes, test, max_sweeps=max_sweeps)\n\n    metadata = dict(version=version)\n    if test:\n        print('test sample: {}'.format(len(train_nusc_infos)))\n        data = dict(infos=train_nusc_infos, metadata=metadata)\n        info_path = osp.join(root_path,\n                             '{}_infos_test.pkl'.format(info_prefix))\n        mmcv.dump(data, info_path)\n    else:\n        print('train sample: {}, val sample: {}'.format(\n            len(train_nusc_infos), len(val_nusc_infos)))\n        data = dict(infos=train_nusc_infos, metadata=metadata)\n        info_path = osp.join(root_path,\n                             '{}_infos_train.pkl'.format(info_prefix))\n        mmcv.dump(data, info_path)\n        data['infos'] = val_nusc_infos\n        info_val_path = osp.join(root_path,\n                                 '{}_infos_val.pkl'.format(info_prefix))\n        mmcv.dump(data, info_val_path)\n\n\ndef get_available_scenes(nusc):\n    \"\"\"Get available scenes from the input nuscenes class.\n\n    Given the raw data, get the information of available scenes for\n    further info generation.\n\n    Args:\n        nusc (class): Dataset class in the nuScenes dataset.\n\n    Returns:\n        available_scenes (list[dict]): List of basic information for the\n            available scenes.\n    \"\"\"\n    available_scenes = []\n    print('total scene num: {}'.format(len(nusc.scene)))\n    for scene in nusc.scene:\n        scene_token = scene['token']\n        scene_rec = nusc.get('scene', scene_token)\n        sample_rec = nusc.get('sample', scene_rec['first_sample_token'])\n        sd_rec = nusc.get('sample_data', sample_rec['data']['LIDAR_TOP'])\n        has_more_frames = True\n        scene_not_exist = False\n        while has_more_frames:\n            lidar_path, boxes, _ = nusc.get_sample_data(sd_rec['token'])\n            lidar_path = str(lidar_path)\n            if os.getcwd() in lidar_path:\n                # path from lyftdataset is absolute path\n                lidar_path = lidar_path.split(f'{os.getcwd()}/')[-1]\n                # relative path\n            if not mmcv.is_filepath(lidar_path):\n                scene_not_exist = True\n                break\n            else:\n                break\n        if scene_not_exist:\n            continue\n        available_scenes.append(scene)\n    print('exist scene num: {}'.format(len(available_scenes)))\n    return available_scenes\n\n\ndef _fill_trainval_infos(nusc,\n                         train_scenes,\n                         val_scenes,\n                         test=False,\n                         max_sweeps=10):\n    \"\"\"Generate the train/val infos from the raw data.\n\n    Args:\n        nusc (:obj:`NuScenes`): Dataset class in the nuScenes dataset.\n        train_scenes (list[str]): Basic information of training scenes.\n        val_scenes (list[str]): Basic information of validation scenes.\n        test (bool, optional): Whether use the test mode. In test mode, no\n            annotations can be accessed. Default: False.\n        max_sweeps (int, optional): Max number of sweeps. Default: 10.\n\n    Returns:\n        tuple[list[dict]]: Information of training set and validation set\n            that will be saved to the info file.\n    \"\"\"\n    train_nusc_infos = []\n    val_nusc_infos = []\n\n    for sample in mmcv.track_iter_progress(nusc.sample):\n        lidar_token = sample['data']['LIDAR_TOP']\n        sd_rec = nusc.get('sample_data', sample['data']['LIDAR_TOP'])\n        cs_record = nusc.get('calibrated_sensor',\n                             sd_rec['calibrated_sensor_token'])\n        pose_record = nusc.get('ego_pose', sd_rec['ego_pose_token'])\n        lidar_path, boxes, _ = nusc.get_sample_data(lidar_token)\n\n        mmcv.check_file_exist(lidar_path)\n\n        info = {\n            'lidar_path': lidar_path,\n            'token': sample['token'],\n            'sweeps': [],\n            'cams': dict(),\n            'lidar2ego_translation': cs_record['translation'],\n            'lidar2ego_rotation': cs_record['rotation'],\n            'ego2global_translation': pose_record['translation'],\n            'ego2global_rotation': pose_record['rotation'],\n            'timestamp': sample['timestamp'],\n        }\n\n        l2e_r = info['lidar2ego_rotation']\n        l2e_t = info['lidar2ego_translation']\n        e2g_r = info['ego2global_rotation']\n        e2g_t = info['ego2global_translation']\n        l2e_r_mat = Quaternion(l2e_r).rotation_matrix\n        e2g_r_mat = Quaternion(e2g_r).rotation_matrix\n\n        # obtain 6 image's information per frame\n        camera_types = [\n            'CAM_FRONT',\n            'CAM_FRONT_RIGHT',\n            'CAM_FRONT_LEFT',\n            'CAM_BACK',\n            'CAM_BACK_LEFT',\n            'CAM_BACK_RIGHT',\n        ]\n        for cam in camera_types:\n            cam_token = sample['data'][cam]\n            cam_path, _, cam_intrinsic = nusc.get_sample_data(cam_token)\n            cam_info = obtain_sensor2top(nusc, cam_token, l2e_t, l2e_r_mat,\n                                         e2g_t, e2g_r_mat, cam)\n            cam_info.update(cam_intrinsic=cam_intrinsic)\n            info['cams'].update({cam: cam_info})\n\n        # obtain sweeps for a single key-frame\n        sd_rec = nusc.get('sample_data', sample['data']['LIDAR_TOP'])\n        sweeps = []\n        while len(sweeps) < max_sweeps:\n            if not sd_rec['prev'] == '':\n                sweep = obtain_sensor2top(nusc, sd_rec['prev'], l2e_t,\n                                          l2e_r_mat, e2g_t, e2g_r_mat, 'lidar')\n                sweeps.append(sweep)\n                sd_rec = nusc.get('sample_data', sd_rec['prev'])\n            else:\n                break\n        info['sweeps'] = sweeps\n        # obtain annotation\n        if not test:\n            annotations = [\n                nusc.get('sample_annotation', token)\n                for token in sample['anns']\n            ]\n            locs = np.array([b.center for b in boxes]).reshape(-1, 3)\n            dims = np.array([b.wlh for b in boxes]).reshape(-1, 3)\n            rots = np.array([b.orientation.yaw_pitch_roll[0]\n                             for b in boxes]).reshape(-1, 1)\n            velocity = np.array(\n                [nusc.box_velocity(token)[:2] for token in sample['anns']])\n            valid_flag = np.array(\n                [(anno['num_lidar_pts'] + anno['num_radar_pts']) > 0\n                 for anno in annotations],\n                dtype=bool).reshape(-1)\n            # convert velo from global to lidar\n            for i in range(len(boxes)):\n                velo = np.array([*velocity[i], 0.0])\n                velo = velo @ np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(\n                    l2e_r_mat).T\n                velocity[i] = velo[:2]\n\n            names = [b.name for b in boxes]\n            for i in range(len(names)):\n                if names[i] in NuScenesDataset.NameMapping:\n                    names[i] = NuScenesDataset.NameMapping[names[i]]\n            names = np.array(names)\n            # we need to convert box size to\n            # the format of our lidar coordinate system\n            # which is x_size, y_size, z_size (corresponding to l, w, h)\n            gt_boxes = np.concatenate([locs, dims[:, [1, 0, 2]], rots], axis=1)\n            assert len(gt_boxes) == len(\n                annotations), f'{len(gt_boxes)}, {len(annotations)}'\n            info['gt_boxes'] = gt_boxes\n            info['gt_names'] = names\n            info['gt_velocity'] = velocity.reshape(-1, 2)\n            info['num_lidar_pts'] = np.array(\n                [a['num_lidar_pts'] for a in annotations])\n            info['num_radar_pts'] = np.array(\n                [a['num_radar_pts'] for a in annotations])\n            info['valid_flag'] = valid_flag\n\n        if sample['scene_token'] in train_scenes:\n            train_nusc_infos.append(info)\n        else:\n            val_nusc_infos.append(info)\n\n    return train_nusc_infos, val_nusc_infos\n\n\ndef obtain_sensor2top(nusc,\n                      sensor_token,\n                      l2e_t,\n                      l2e_r_mat,\n                      e2g_t,\n                      e2g_r_mat,\n                      sensor_type='lidar'):\n    \"\"\"Obtain the info with RT matric from general sensor to Top LiDAR.\n\n    Args:\n        nusc (class): Dataset class in the nuScenes dataset.\n        sensor_token (str): Sample data token corresponding to the\n            specific sensor type.\n        l2e_t (np.ndarray): Translation from lidar to ego in shape (1, 3).\n        l2e_r_mat (np.ndarray): Rotation matrix from lidar to ego\n            in shape (3, 3).\n        e2g_t (np.ndarray): Translation from ego to global in shape (1, 3).\n        e2g_r_mat (np.ndarray): Rotation matrix from ego to global\n            in shape (3, 3).\n        sensor_type (str, optional): Sensor to calibrate. Default: 'lidar'.\n\n    Returns:\n        sweep (dict): Sweep information after transformation.\n    \"\"\"\n    sd_rec = nusc.get('sample_data', sensor_token)\n    cs_record = nusc.get('calibrated_sensor',\n                         sd_rec['calibrated_sensor_token'])\n    pose_record = nusc.get('ego_pose', sd_rec['ego_pose_token'])\n    data_path = str(nusc.get_sample_data_path(sd_rec['token']))\n    if os.getcwd() in data_path:  # path from lyftdataset is absolute path\n        data_path = data_path.split(f'{os.getcwd()}/')[-1]  # relative path\n    sweep = {\n        'data_path': data_path,\n        'type': sensor_type,\n        'sample_data_token': sd_rec['token'],\n        'sensor2ego_translation': cs_record['translation'],\n        'sensor2ego_rotation': cs_record['rotation'],\n        'ego2global_translation': pose_record['translation'],\n        'ego2global_rotation': pose_record['rotation'],\n        'timestamp': sd_rec['timestamp']\n    }\n    l2e_r_s = sweep['sensor2ego_rotation']\n    l2e_t_s = sweep['sensor2ego_translation']\n    e2g_r_s = sweep['ego2global_rotation']\n    e2g_t_s = sweep['ego2global_translation']\n\n    # obtain the RT from sensor to Top LiDAR\n    # sweep->ego->global->ego'->lidar\n    l2e_r_s_mat = Quaternion(l2e_r_s).rotation_matrix\n    e2g_r_s_mat = Quaternion(e2g_r_s).rotation_matrix\n    R = (l2e_r_s_mat.T @ e2g_r_s_mat.T) @ (\n        np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T)\n    T = (l2e_t_s @ e2g_r_s_mat.T + e2g_t_s) @ (\n        np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T)\n    T -= e2g_t @ (np.linalg.inv(e2g_r_mat).T @ np.linalg.inv(l2e_r_mat).T\n                  ) + l2e_t @ np.linalg.inv(l2e_r_mat).T\n    sweep['sensor2lidar_rotation'] = R.T  # points @ R.T + T\n    sweep['sensor2lidar_translation'] = T\n    return sweep\n\n\ndef export_2d_annotation(root_path, info_path, version, mono3d=True):\n    \"\"\"Export 2d annotation from the info file and raw data.\n\n    Args:\n        root_path (str): Root path of the raw data.\n        info_path (str): Path of the info file.\n        version (str): Dataset version.\n        mono3d (bool, optional): Whether to export mono3d annotation.\n            Default: True.\n    \"\"\"\n    # get bbox annotations for camera\n    camera_types = [\n        'CAM_FRONT',\n        'CAM_FRONT_RIGHT',\n        'CAM_FRONT_LEFT',\n        'CAM_BACK',\n        'CAM_BACK_LEFT',\n        'CAM_BACK_RIGHT',\n    ]\n    nusc_infos = mmcv.load(info_path)['infos']\n    nusc = NuScenes(version=version, dataroot=root_path, verbose=True)\n    # info_2d_list = []\n    cat2Ids = [\n        dict(id=nus_categories.index(cat_name), name=cat_name)\n        for cat_name in nus_categories\n    ]\n    coco_ann_id = 0\n    coco_2d_dict = dict(annotations=[], images=[], categories=cat2Ids)\n    for info in mmcv.track_iter_progress(nusc_infos):\n        for cam in camera_types:\n            cam_info = info['cams'][cam]\n            coco_infos = get_2d_boxes(\n                nusc,\n                cam_info['sample_data_token'],\n                visibilities=['', '1', '2', '3', '4'],\n                mono3d=mono3d)\n            (height, width, _) = mmcv.imread(cam_info['data_path']).shape\n            coco_2d_dict['images'].append(\n                dict(\n                    file_name=cam_info['data_path'].split('data/nuscenes/')\n                    [-1],\n                    id=cam_info['sample_data_token'],\n                    token=info['token'],\n                    cam2ego_rotation=cam_info['sensor2ego_rotation'],\n                    cam2ego_translation=cam_info['sensor2ego_translation'],\n                    ego2global_rotation=info['ego2global_rotation'],\n                    ego2global_translation=info['ego2global_translation'],\n                    cam_intrinsic=cam_info['cam_intrinsic'],\n                    width=width,\n                    height=height))\n            for coco_info in coco_infos:\n                if coco_info is None:\n                    continue\n                # add an empty key for coco format\n                coco_info['segmentation'] = []\n                coco_info['id'] = coco_ann_id\n                coco_2d_dict['annotations'].append(coco_info)\n                coco_ann_id += 1\n    if mono3d:\n        json_prefix = f'{info_path[:-4]}_mono3d'\n    else:\n        json_prefix = f'{info_path[:-4]}'\n    mmcv.dump(coco_2d_dict, f'{json_prefix}.coco.json')\n\n\ndef get_2d_boxes(nusc,\n                 sample_data_token: str,\n                 visibilities: List[str],\n                 mono3d=True):\n    \"\"\"Get the 2D annotation records for a given `sample_data_token`.\n\n    Args:\n        sample_data_token (str): Sample data token belonging to a camera\n            keyframe.\n        visibilities (list[str]): Visibility filter.\n        mono3d (bool): Whether to get boxes with mono3d annotation.\n\n    Return:\n        list[dict]: List of 2D annotation record that belongs to the input\n            `sample_data_token`.\n    \"\"\"\n\n    # Get the sample data and the sample corresponding to that sample data.\n    sd_rec = nusc.get('sample_data', sample_data_token)\n\n    assert sd_rec[\n        'sensor_modality'] == 'camera', 'Error: get_2d_boxes only works' \\\n        ' for camera sample_data!'\n    if not sd_rec['is_key_frame']:\n        raise ValueError(\n            'The 2D re-projections are available only for keyframes.')\n\n    s_rec = nusc.get('sample', sd_rec['sample_token'])\n\n    # Get the calibrated sensor and ego pose\n    # record to get the transformation matrices.\n    cs_rec = nusc.get('calibrated_sensor', sd_rec['calibrated_sensor_token'])\n    pose_rec = nusc.get('ego_pose', sd_rec['ego_pose_token'])\n    camera_intrinsic = np.array(cs_rec['camera_intrinsic'])\n\n    # Get all the annotation with the specified visibilties.\n    ann_recs = [\n        nusc.get('sample_annotation', token) for token in s_rec['anns']\n    ]\n    ann_recs = [\n        ann_rec for ann_rec in ann_recs\n        if (ann_rec['visibility_token'] in visibilities)\n    ]\n\n    repro_recs = []\n\n    for ann_rec in ann_recs:\n        # Augment sample_annotation with token information.\n        ann_rec['sample_annotation_token'] = ann_rec['token']\n        ann_rec['sample_data_token'] = sample_data_token\n\n        # Get the box in global coordinates.\n        box = nusc.get_box(ann_rec['token'])\n\n        # Move them to the ego-pose frame.\n        box.translate(-np.array(pose_rec['translation']))\n        box.rotate(Quaternion(pose_rec['rotation']).inverse)\n\n        # Move them to the calibrated sensor frame.\n        box.translate(-np.array(cs_rec['translation']))\n        box.rotate(Quaternion(cs_rec['rotation']).inverse)\n\n        # Filter out the corners that are not in front of the calibrated\n        # sensor.\n        corners_3d = box.corners()\n        in_front = np.argwhere(corners_3d[2, :] > 0).flatten()\n        corners_3d = corners_3d[:, in_front]\n\n        # Project 3d box to 2d.\n        corner_coords = view_points(corners_3d, camera_intrinsic,\n                                    True).T[:, :2].tolist()\n\n        # Keep only corners that fall within the image.\n        final_coords = post_process_coords(corner_coords)\n\n        # Skip if the convex hull of the re-projected corners\n        # does not intersect the image canvas.\n        if final_coords is None:\n            continue\n        else:\n            min_x, min_y, max_x, max_y = final_coords\n\n        # Generate dictionary record to be included in the .json file.\n        repro_rec = generate_record(ann_rec, min_x, min_y, max_x, max_y,\n                                    sample_data_token, sd_rec['filename'])\n\n        # If mono3d=True, add 3D annotations in camera coordinates\n        if mono3d and (repro_rec is not None):\n            loc = box.center.tolist()\n\n            dim = box.wlh\n            dim[[0, 1, 2]] = dim[[1, 2, 0]]  # convert wlh to our lhw\n            dim = dim.tolist()\n\n            rot = box.orientation.yaw_pitch_roll[0]\n            rot = [-rot]  # convert the rot to our cam coordinate\n\n            global_velo2d = nusc.box_velocity(box.token)[:2]\n            global_velo3d = np.array([*global_velo2d, 0.0])\n            e2g_r_mat = Quaternion(pose_rec['rotation']).rotation_matrix\n            c2e_r_mat = Quaternion(cs_rec['rotation']).rotation_matrix\n            cam_velo3d = global_velo3d @ np.linalg.inv(\n                e2g_r_mat).T @ np.linalg.inv(c2e_r_mat).T\n            velo = cam_velo3d[0::2].tolist()\n\n            repro_rec['bbox_cam3d'] = loc + dim + rot\n            repro_rec['velo_cam3d'] = velo\n\n            center3d = np.array(loc).reshape([1, 3])\n            center2d = points_cam2img(\n                center3d, camera_intrinsic, with_depth=True)\n            repro_rec['center2d'] = center2d.squeeze().tolist()\n            # normalized center2D + depth\n            # if samples with depth < 0 will be removed\n            if repro_rec['center2d'][2] <= 0:\n                continue\n\n            ann_token = nusc.get('sample_annotation',\n                                 box.token)['attribute_tokens']\n            if len(ann_token) == 0:\n                attr_name = 'None'\n            else:\n                attr_name = nusc.get('attribute', ann_token[0])['name']\n            attr_id = nus_attributes.index(attr_name)\n            repro_rec['attribute_name'] = attr_name\n            repro_rec['attribute_id'] = attr_id\n\n        repro_recs.append(repro_rec)\n\n    return repro_recs\n\n\ndef post_process_coords(\n    corner_coords: List, imsize: Tuple[int, int] = (1600, 900)\n) -> Union[Tuple[float, float, float, float], None]:\n    \"\"\"Get the intersection of the convex hull of the reprojected bbox corners\n    and the image canvas, return None if no intersection.\n\n    Args:\n        corner_coords (list[int]): Corner coordinates of reprojected\n            bounding box.\n        imsize (tuple[int]): Size of the image canvas.\n\n    Return:\n        tuple [float]: Intersection of the convex hull of the 2D box\n            corners and the image canvas.\n    \"\"\"\n    polygon_from_2d_box = MultiPoint(corner_coords).convex_hull\n    img_canvas = box(0, 0, imsize[0], imsize[1])\n\n    if polygon_from_2d_box.intersects(img_canvas):\n        img_intersection = polygon_from_2d_box.intersection(img_canvas)\n        intersection_coords = np.array(\n            [coord for coord in img_intersection.exterior.coords])\n\n        min_x = min(intersection_coords[:, 0])\n        min_y = min(intersection_coords[:, 1])\n        max_x = max(intersection_coords[:, 0])\n        max_y = max(intersection_coords[:, 1])\n\n        return min_x, min_y, max_x, max_y\n    else:\n        return None\n\n\ndef generate_record(ann_rec: dict, x1: float, y1: float, x2: float, y2: float,\n                    sample_data_token: str, filename: str) -> OrderedDict:\n    \"\"\"Generate one 2D annotation record given various information on top of\n    the 2D bounding box coordinates.\n\n    Args:\n        ann_rec (dict): Original 3d annotation record.\n        x1 (float): Minimum value of the x coordinate.\n        y1 (float): Minimum value of the y coordinate.\n        x2 (float): Maximum value of the x coordinate.\n        y2 (float): Maximum value of the y coordinate.\n        sample_data_token (str): Sample data token.\n        filename (str):The corresponding image file where the annotation\n            is present.\n\n    Returns:\n        dict: A sample 2D annotation record.\n            - file_name (str): file name\n            - image_id (str): sample data token\n            - area (float): 2d box area\n            - category_name (str): category name\n            - category_id (int): category id\n            - bbox (list[float]): left x, top y, dx, dy of 2d box\n            - iscrowd (int): whether the area is crowd\n    \"\"\"\n    repro_rec = OrderedDict()\n    repro_rec['sample_data_token'] = sample_data_token\n    coco_rec = dict()\n\n    relevant_keys = [\n        'attribute_tokens',\n        'category_name',\n        'instance_token',\n        'next',\n        'num_lidar_pts',\n        'num_radar_pts',\n        'prev',\n        'sample_annotation_token',\n        'sample_data_token',\n        'visibility_token',\n    ]\n\n    for key, value in ann_rec.items():\n        if key in relevant_keys:\n            repro_rec[key] = value\n\n    repro_rec['bbox_corners'] = [x1, y1, x2, y2]\n    repro_rec['filename'] = filename\n\n    coco_rec['file_name'] = filename\n    coco_rec['image_id'] = sample_data_token\n    coco_rec['area'] = (y2 - y1) * (x2 - x1)\n\n    if repro_rec['category_name'] not in NuScenesDataset.NameMapping:\n        return None\n    cat_name = NuScenesDataset.NameMapping[repro_rec['category_name']]\n    coco_rec['category_name'] = cat_name\n    coco_rec['category_id'] = nus_categories.index(cat_name)\n    coco_rec['bbox'] = [x1, y1, x2 - x1, y2 - y1]\n    coco_rec['iscrowd'] = 0\n\n    return coco_rec\n"
        }
    ]
}