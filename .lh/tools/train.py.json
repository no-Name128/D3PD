{
    "sourceFile": "tools/train.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1716023356148,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1716023356148,
            "name": "Commit-0",
            "content": "# Copyright (c) OpenMMLab. All rights reserved.\nfrom __future__ import division\nimport argparse\nimport copy\nimport os\nimport time\nimport warnings\nfrom os import path as osp\n\nimport mmcv\nimport torch\nimport torch.distributed as dist\nfrom mmcv import Config, DictAction\nfrom mmcv.runner import get_dist_info, init_dist\n\nfrom mmdet import __version__ as mmdet_version\nfrom mmdet3d import __version__ as mmdet3d_version\nfrom mmdet3d.apis import init_random_seed, train_model\nfrom mmdet3d.datasets import build_dataset\nfrom mmdet3d.models import build_model\nfrom mmdet3d.utils import collect_env, get_root_logger\nfrom mmdet.apis import set_random_seed\nfrom mmseg import __version__ as mmseg_version\n\ntry:\n    # If mmdet version > 2.20.0, setup_multi_processes would be imported and\n    # used from mmdet instead of mmdet3d.\n    from mmdet.utils import setup_multi_processes\nexcept ImportError:\n    from mmdet3d.utils import setup_multi_processes\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train a detector')\n    parser.add_argument('config', help='train config file path')\n    parser.add_argument('--work-dir', help='the dir to save logs and models')\n    parser.add_argument(\n        '--resume-from', help='the checkpoint file to resume from')\n    parser.add_argument(\n        '--auto-resume',\n        action='store_true',\n        help='resume from the latest checkpoint automatically')\n    parser.add_argument(\n        '--validate',\n        action='store_true',\n        help='whether not to evaluate the checkpoint during training')\n    group_gpus = parser.add_mutually_exclusive_group()\n    group_gpus.add_argument(\n        '--gpus',\n        type=int,\n        help='(Deprecated, please use --gpu-id) number of gpus to use '\n        '(only applicable to non-distributed training)')\n    group_gpus.add_argument(\n        '--gpu-ids',\n        type=int,\n        nargs='+',\n        help='(Deprecated, please use --gpu-id) ids of gpus to use '\n        '(only applicable to non-distributed training)')\n    group_gpus.add_argument(\n        '--gpu-id',\n        type=int,\n        default=0,\n        help='number of gpus to use '\n        '(only applicable to non-distributed training)')\n    parser.add_argument('--seed', type=int, default=0, help='random seed')\n    parser.add_argument(\n        '--diff-seed',\n        action='store_true',\n        help='Whether or not set different seeds for different ranks')\n    parser.add_argument(\n        '--deterministic',\n        action='store_true',\n        help='whether to set deterministic options for CUDNN backend.')\n    parser.add_argument(\n        '--options',\n        nargs='+',\n        action=DictAction,\n        help='override some settings in the used config, the key-value pair '\n        'in xxx=yyy format will be merged into config file (deprecate), '\n        'change to --cfg-options instead.')\n    parser.add_argument(\n        '--cfg-options',\n        nargs='+',\n        action=DictAction,\n        help='override some settings in the used config, the key-value pair '\n        'in xxx=yyy format will be merged into config file. If the value to '\n        'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n        'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n        'Note that the quotation marks are necessary and that no white space '\n        'is allowed.')\n    parser.add_argument(\n        '--launcher',\n        choices=['none', 'pytorch', 'slurm', 'mpi'],\n        default='none',\n        help='job launcher')\n    parser.add_argument('--local_rank', type=int, default=0)\n    parser.add_argument(\n        '--autoscale-lr',\n        action='store_true',\n        help='automatically scale lr with the number of gpus')\n    args = parser.parse_args()\n    if 'LOCAL_RANK' not in os.environ:\n        os.environ['LOCAL_RANK'] = str(args.local_rank)\n\n    if args.options and args.cfg_options:\n        raise ValueError(\n            '--options and --cfg-options cannot be both specified, '\n            '--options is deprecated in favor of --cfg-options')\n    if args.options:\n        warnings.warn('--options is deprecated in favor of --cfg-options')\n        args.cfg_options = args.options\n\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    cfg = Config.fromfile(args.config)\n    if args.cfg_options is not None:\n        cfg.merge_from_dict(args.cfg_options)\n\n    # set multi-process settings\n    setup_multi_processes(cfg)\n\n    # set cudnn_benchmark\n    if cfg.get('cudnn_benchmark', False):\n        torch.backends.cudnn.benchmark = True\n\n    # work_dir is determined in this priority: CLI > segment in file > filename\n    if args.work_dir is not None:\n        # update configs according to CLI args if args.work_dir is not None\n        cfg.work_dir = args.work_dir\n    elif cfg.get('work_dir', None) is None:\n        # use config filename as default work_dir if cfg.work_dir is None\n        cfg.work_dir = osp.join('./work_dirs',\n                                osp.splitext(osp.basename(args.config))[0])\n    if args.resume_from is not None:\n        cfg.resume_from = args.resume_from\n\n    if args.auto_resume:\n        cfg.auto_resume = args.auto_resume\n        warnings.warn('`--auto-resume` is only supported when mmdet'\n                      'version >= 2.20.0 for 3D detection model or'\n                      'mmsegmentation verision >= 0.21.0 for 3D'\n                      'segmentation model')\n\n    if args.gpus is not None:\n        cfg.gpu_ids = range(1)\n        warnings.warn('`--gpus` is deprecated because we only support '\n                      'single GPU mode in non-distributed training. '\n                      'Use `gpus=1` now.')\n    if args.gpu_ids is not None:\n        cfg.gpu_ids = args.gpu_ids[0:1]\n        warnings.warn('`--gpu-ids` is deprecated, please use `--gpu-id`. '\n                      'Because we only support single GPU mode in '\n                      'non-distributed training. Use the first GPU '\n                      'in `gpu_ids` now.')\n    if args.gpus is None and args.gpu_ids is None:\n        cfg.gpu_ids = [args.gpu_id]\n\n    if args.autoscale_lr:\n        # apply the linear scaling rule (https://arxiv.org/abs/1706.02677)\n        cfg.optimizer['lr'] = cfg.optimizer['lr'] * len(cfg.gpu_ids) / 8\n\n    # init distributed env first, since logger depends on the dist info.\n    if args.launcher == 'none':\n        distributed = False\n    else:\n        distributed = True\n        init_dist(args.launcher, **cfg.dist_params)\n        # re-set gpu_ids with distributed training mode\n        _, world_size = get_dist_info()\n        cfg.gpu_ids = range(world_size)\n\n    # create work_dir\n    mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n    # dump config\n    cfg.dump(osp.join(cfg.work_dir, osp.basename(args.config)))\n    # init the logger before other steps\n    timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n    log_file = osp.join(cfg.work_dir, f'{timestamp}.log')\n    # specify logger name, if we still use 'mmdet', the output info will be\n    # filtered and won't be saved in the log_file\n    # TODO: ugly workaround to judge whether we are training det or seg model\n    if cfg.model.type in ['EncoderDecoder3D']:\n        logger_name = 'mmseg'\n    else:\n        logger_name = 'mmdet'\n    logger = get_root_logger(\n        log_file=log_file, log_level=cfg.log_level, name=logger_name)\n\n    # init the meta dict to record some important information such as\n    # environment info and seed, which will be logged\n    meta = dict()\n    # log env info\n    env_info_dict = collect_env()\n    env_info = '\\n'.join([(f'{k}: {v}') for k, v in env_info_dict.items()])\n    dash_line = '-' * 60 + '\\n'\n    logger.info('Environment info:\\n' + dash_line + env_info + '\\n' +\n                dash_line)\n    meta['env_info'] = env_info\n    meta['config'] = cfg.pretty_text\n\n    # log some basic info\n    logger.info(f'Distributed training: {distributed}')\n    logger.info(f'Config:\\n{cfg.pretty_text}')\n\n    # set random seeds\n    seed = init_random_seed(args.seed)\n    seed = seed + dist.get_rank() if args.diff_seed else seed\n    logger.info(f'Set random seed to {seed}, '\n                f'deterministic: {args.deterministic}')\n    set_random_seed(seed, deterministic=args.deterministic)\n    cfg.seed = seed\n    meta['seed'] = seed\n    meta['exp_name'] = osp.basename(args.config)\n\n    model = build_model(\n        cfg.model,\n        train_cfg=cfg.get('train_cfg'),\n        test_cfg=cfg.get('test_cfg'))\n    model.init_weights()\n\n    logger.info(f'Model:\\n{model}')\n    datasets = [build_dataset(cfg.data.train)]\n    if len(cfg.workflow) == 2:\n        val_dataset = copy.deepcopy(cfg.data.val)\n        # in case we use a dataset wrapper\n        if 'dataset' in cfg.data.train:\n            val_dataset.pipeline = cfg.data.train.dataset.pipeline\n        else:\n            val_dataset.pipeline = cfg.data.train.pipeline\n        # set test_mode=False here in deep copied config\n        # which do not affect AP/AR calculation later\n        # refer to https://mmdetection3d.readthedocs.io/en/latest/tutorials/customize_runtime.html#customize-workflow  # noqa\n        val_dataset.test_mode = False\n        datasets.append(build_dataset(val_dataset))\n    if cfg.checkpoint_config is not None:\n        # save mmdet version, config file content and class names in\n        # checkpoints as meta data\n        cfg.checkpoint_config.meta = dict(\n            mmdet_version=mmdet_version,\n            mmseg_version=mmseg_version,\n            mmdet3d_version=mmdet3d_version,\n            config=cfg.pretty_text,\n            CLASSES=datasets[0].CLASSES,\n            PALETTE=datasets[0].PALETTE  # for segmentors\n            if hasattr(datasets[0], 'PALETTE') else None)\n\n    # remove objectsample augmentation in the 2nd stage\n    if cfg.get('two_stage', False):\n        assert len(cfg.workflow) == 1\n        assert cfg.data.train.dataset.pipeline[5].type =='ObjectSample'\n        assert len(cfg.data.train.dataset.pipeline)==15\n        cfg.data.train.dataset.pipeline = \\\n            [cfg.data.train.dataset.pipeline[kid] for kid in\n             [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13]]\n        cfg.data.train.dataset.pipeline.append(\n            dict(type='Collect3D',\n                 keys=['points', 'gt_bboxes_3d', 'gt_depth',\n                'gt_labels_3d', 'img_inputs']))\n        datasets.append(build_dataset(cfg.data.train))\n    # add an attribute for visualization convenience\n    model.CLASSES = datasets[0].CLASSES\n    train_model(\n        model,\n        datasets,\n        cfg,\n        distributed=distributed,\n        validate=args.validate,\n        timestamp=timestamp,\n        meta=meta)\n\n\nif __name__ == '__main__':\n    main()\n"
        }
    ]
}